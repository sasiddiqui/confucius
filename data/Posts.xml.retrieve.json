{"documents": [{"body": {"answer": "<p>\"Backprop\" is the same as \"backpropagation\": it's just a shorter way to say it. It is sometimes abbreviated as \"BP\".</p>\n", "question": "<p>What does \"backprop\" mean? I've Googled it, but it's showing backpropagation.</p>\n\n<p>Is the \"backprop\" term basically the same as \"backpropagation\" or does it have a different meaning?</p>\n"}, "id": "3"}, {"body": {"answer": "<p>You can use <a href=\"https://github.com/topikachu/python-ev3\" rel=\"nofollow\">python-ev3</a> which can be used to program Lego Mindstorms EV3 using Python on ev3dev.</p>\n\n<p>See: <a href=\"http://www.ev3dev.org/docs/tutorials/setting-up-python-pycharm/\" rel=\"nofollow\">Setting Up a Python Development Environment with PyCharm</a></p>\n", "question": "<p>I have a LEGO Mindstorms EV3 and I'm wondering if there's any way I could start coding the bot in Python rather than the default drag-and-drop system. Is a Mindstorm considered AI?</p>\n\n<p>Is this possible?</p>\n\n<hr>\n\n<p>My goal is to write a basic walking program in Python. The bot is the EV3RSTORM. I searched and found <a href=\"http://bitsandbricks.no/2014/01/19/getting-started-with-python-on-ev3/\" rel=\"nofollow\">this</a>, but don't understand it. </p>\n"}, "id": "8"}, {"body": {"answer": "<p>Noise in the data, to a reasonable amount, may help the network to generalize better. Sometime, it has the opposite effect. It partly depends on the kind of noise (\"true\" vs. artificial).</p>\n\n<p>The <a href=\"ftp://ftp.sas.com/pub/neural/FAQ3.html#A_noise\">AI FAQ on ANN</a> gives a good overview. Except:</p>\n\n<blockquote>\n  <p>Noise in the actual data is never a good thing, since it limits the accuracy of generalization that can be achieved no matter how extensive the training set is. On the other hand, injecting artificial noise (jitter) into the inputs during training is one of several ways to improve generalization for smooth functions when you have a small training set.</p>\n</blockquote>\n\n<p>In some field, such as computer vision, it's common to increase the size of the training set by copying some samples and adding some noises or other transformation.</p>\n", "question": "<p>Does increasing the noise in data help to improve the learning ability of a network? Does it make any difference or does it depend on the problem being solved? How is it affect the generalization process overall?</p>\n"}, "id": "9"}, {"body": {"answer": "<p>We typically think of machine learning models as modeling two different parts of the training data--the underlying generalizable truth (the signal), and the randomness specific to that dataset (the noise).</p>\n\n<p>Fitting both of those parts increases training set accuracy, but fitting the signal also increases test set accuracy (and real-world performance) while fitting the noise decreases both. So we use things like regularization and dropout and similar techniques in order to make it harder to fit the noise, and so more likely to fit the signal.</p>\n\n<p>Just increasing the amount of noise in the training data is one such approach, but seems unlikely to be as useful. Compare random jitter to adversarial boosting, for example; the first will slowly and indirectly improve robustness whereas the latter will dramatically and directly improve it.</p>\n", "question": "<p>Does increasing the noise in data help to improve the learning ability of a network? Does it make any difference or does it depend on the problem being solved? How is it affect the generalization process overall?</p>\n"}, "id": "11"}, {"body": {"answer": "<p>There is no direct way to find the optimal number of them: people empirically try and see (e.g., using cross-validation). The most common search techniques are random, manual, and grid searches. </p>\n\n<p>There exist more advanced techniques such as Gaussian processes, e.g. <em><a href=\"http://arxiv.org/abs/1609.08703\" rel=\"nofollow\">Optimizing Neural Network Hyperparameters with Gaussian Processes for Dialog Act Classification</a>, IEEE SLT 2016</em>.</p>\n", "question": "<p>When you're writing your algorithm, how do you know how many neurons you need per single layer? Are there any methods for finding the optimal number of them, or is it a rule of thumb?</p>\n"}, "id": "12"}, {"body": {"answer": "<blockquote>\n  <p>Is a Mindstorm considered AI?</p>\n</blockquote>\n\n<p>This depends on what type of software you write in it... The algorithms you write could be seen as AI. </p>\n\n<p>You can absolutely use Python to progam it (or java or other languages). Check <a href=\"http://bitsandbricks.no/2014/01/19/getting-started-with-python-on-ev3/\" rel=\"nofollow\">this link</a> for a tutorial. </p>\n", "question": "<p>I have a LEGO Mindstorms EV3 and I'm wondering if there's any way I could start coding the bot in Python rather than the default drag-and-drop system. Is a Mindstorm considered AI?</p>\n\n<p>Is this possible?</p>\n\n<hr>\n\n<p>My goal is to write a basic walking program in Python. The bot is the EV3RSTORM. I searched and found <a href=\"http://bitsandbricks.no/2014/01/19/getting-started-with-python-on-ev3/\" rel=\"nofollow\">this</a>, but don't understand it. </p>\n"}, "id": "14"}, {"body": {"answer": "<blockquote>\n  <p>To put it simply in layman terms, what are the possible threats from AI? </p>\n</blockquote>\n\n<p>Currently, there are no threat. </p>\n\n<p>The threat comes if humans create a so-called ultraintelligent machine, a machine that can surpass all intellectual activities by any human. This would be the last invention man would need to do, since this machine is better in inventing machines than humans are (since that is an intellectual activity).  However, this could cause the machine to invent machines that can destruct humans, and we can't stop them because they are so much smarter than we are.</p>\n\n<p>This is all hypothetical, no one has even a clue of what an ultraintelligent machine looks like. </p>\n\n<blockquote>\n  <p>If we know that AI is so dangerous why are we still promoting it? Why is it not banned?</p>\n</blockquote>\n\n<p>As I said before, the existence of a ultraintelligent machine is hypothetical. Artificial Intelligence has lots of useful applications (more than this answer can contain), and if we develop it, we get even more useful applications. We just have to be careful that the machines won't overtake us. </p>\n", "question": "<p>This quote by Stephen Hawking has been in headlines for quite some time:</p>\n\n<blockquote>\n  <p>Artificial Intelligence could wipe out humanity when it gets too clever as humans will be like ants.</p>\n</blockquote>\n\n<p>Why does he say this? To put it simply in layman terms: what are the possible threats from AI? If we know that AI is so dangerous why are we still promoting it? Why is it not banned?</p>\n\n<p>What are the adverse consequences of the so called <a href=\"https://en.wikipedia.org/wiki/Technological_singularity\" rel=\"nofollow\">Technological Singularity</a>? </p>\n"}, "id": "18"}, {"body": {"answer": "<p>Because he did not yet know how far away current AI is... Working in an media AI lab, I get this question a lot. But really... we are still a long way from this. The robots still do everything we detailledly describe them to do. Instead of seeing the robot as intelligent, I would look to the human programmer for where the creativity really happens.</p>\n", "question": "<p>This quote by Stephen Hawking has been in headlines for quite some time:</p>\n\n<blockquote>\n  <p>Artificial Intelligence could wipe out humanity when it gets too clever as humans will be like ants.</p>\n</blockquote>\n\n<p>Why does he say this? To put it simply in layman terms: what are the possible threats from AI? If we know that AI is so dangerous why are we still promoting it? Why is it not banned?</p>\n\n<p>What are the adverse consequences of the so called <a href=\"https://en.wikipedia.org/wiki/Technological_singularity\" rel=\"nofollow\">Technological Singularity</a>? </p>\n"}, "id": "19"}, {"body": {"answer": "<p>It rather depends on how one defines several of the terms used. For example:</p>\n\n<ul>\n<li>Whether the term `expected' is interpreted in a formal (i.e.\nstatistical) sense.  </li>\n<li>Whether it's assumed that humans have any kind of utilitarian\n`performance measure'.</li>\n</ul>\n\n<p>The motivation for this description of `agent' arose from a desire to have a quantitative model - it's not clear that such a model is a good fit for human cognition.</p>\n\n<p>However, there are alternative definitions of agents, for example the <a href=\"https://en.wikipedia.org/wiki/Belief%E2%80%93desire%E2%80%93intention_software_model\" rel=\"nofollow\">BDI model</a> which are rather more open-ended and hence more obviously applicable to humans.</p>\n", "question": "<p>The intelligent agent definition of intelligence states that an agent is intelligent if it acts so to maximize the expected value of a performance measure based on past experience and knowledge. (paraphrased from <a href=\"http://en.wikipedia.org/wiki/Philosophy_of_artificial_intelligence#Intelligent_agent_definition\" rel=\"nofollow\">Wikipedia</a>)</p>\n\n<p>Does this mean that humans are not intelligent? I think we all make mistakes that imply that we are not maximizing the expected value of a performance measure.</p>\n"}, "id": "20"}, {"body": {"answer": "<p>It's not just Hawking, you hear variations on this refrain from a lot of people.  And given that they're mostly very smart, well educated, well informed people (Elon Musk is another, for example), it probably shouldn't be dismissed out of hand.</p>\n\n<p>Anyway, the basic idea seems to be this: If we create \"real\" artificial intelligence, at some point, it will be able to improve itself, which improves it's ability to improve itself, which means it can improve it's ability to improve itself even more, and so on... a runaway cascade leading to \"superhuman intelligence\".  That is to say, leading to something that more intelligent than we area.</p>\n\n<p>So what happens if there is an entity on this planet which is literally more intelligent than us (humans)? Would it be a threat to us?  Well, it certainly seems reasonable to speculate that it <em>could</em> be so.   OTOH, we have no particular reason, right now, to think that it <em>will</em> be so. </p>\n\n<p>So it seems that Hawking, Musk, etc. are just coming down on the more cautious / fearful side of things.  Since we don't <em>know</em> if a superhuman AI will be dangerous or not, and given that it could be unstoppable if it were to become malicious (remember, it's smarter than we are!), it's a reasonable thing to take under consideration.</p>\n\n<p>Eliezer Yudkowsky has also written quite a bit on this subject, including come up with the famous \"AI Box\" experiment.  I think anybody interested in this topic should read some of his material.</p>\n\n<p><a href=\"http://www.yudkowsky.net/singularity/aibox/\" rel=\"nofollow\">http://www.yudkowsky.net/singularity/aibox/</a></p>\n", "question": "<p>This quote by Stephen Hawking has been in headlines for quite some time:</p>\n\n<blockquote>\n  <p>Artificial Intelligence could wipe out humanity when it gets too clever as humans will be like ants.</p>\n</blockquote>\n\n<p>Why does he say this? To put it simply in layman terms: what are the possible threats from AI? If we know that AI is so dangerous why are we still promoting it? Why is it not banned?</p>\n\n<p>What are the adverse consequences of the so called <a href=\"https://en.wikipedia.org/wiki/Technological_singularity\" rel=\"nofollow\">Technological Singularity</a>? </p>\n"}, "id": "22"}, {"body": {"answer": "<p>As Andrew Ng <a href=\"http://www.theregister.co.uk/2015/03/19/andrew_ng_baidu_ai/\" rel=\"nofollow\">said</a>, worrying about such threat from AI is like worrying about of overpopulation on Mars. It is science fiction. </p>\n\n<p><a href=\"http://i.stack.imgur.com/m6jnl.png\" rel=\"nofollow\"><img src=\"http://i.stack.imgur.com/m6jnl.png\" alt=\"enter image description here\"></a></p>\n\n<p>That being said, given the rise of (much weaker) robots and other (semi-)autonomous agents, the fields of the law and ethics are increasingly incorporating them, e.g. see <a href=\"https://en.wikipedia.org/wiki/Roboethics\" rel=\"nofollow\">Roboethics</a>.</p>\n", "question": "<p>This quote by Stephen Hawking has been in headlines for quite some time:</p>\n\n<blockquote>\n  <p>Artificial Intelligence could wipe out humanity when it gets too clever as humans will be like ants.</p>\n</blockquote>\n\n<p>Why does he say this? To put it simply in layman terms: what are the possible threats from AI? If we know that AI is so dangerous why are we still promoting it? Why is it not banned?</p>\n\n<p>What are the adverse consequences of the so called <a href=\"https://en.wikipedia.org/wiki/Technological_singularity\" rel=\"nofollow\">Technological Singularity</a>? </p>\n"}, "id": "23"}, {"body": {"answer": "<p>He says this because it can happen. If something becomes smarter than us, why would it continue to serve us? The worst case scenario is that it takes over all manufacturing processes and consumes all matter to convert it into material capable of computation, extending outward infinitely until all matter is consumed.</p>\n\n<p>We know that AI is dangerous but it doesn't matter because most people don't believe in it. It goes against every comfort religion has to offer. Man is the end-all-be-all of the universe and if that fact is disputed, people will feel out of place and purposeless.</p>\n\n<p>The fact is most people just don't acknowledge it's possible, or that it will happen in our lifetimes, even though many reputable AI experts put the occurrence of the singularity within two decades. If people truly acknowledged that AI that was smarter than them was possible, wouldn't they be living differently? Wouldn't they be looking to do things that they enjoy, knowing that whatever it is they do that they dread will be automated? Wouldn't everyone be calling for a universal basic income?</p>\n\n<p>The other reason we don't ban it is because its promise is so great. One researcher could be augmented by 1,000 digital research assistants. All manual labor could be automated. For the first time, technology offers us real freedom to do whatever we please.</p>\n\n<p>But even in this best case scenario where it doesn't overtake us, humans still have to adapt and alter their economic system to one where labor isn't necessary. Otherwise, those who aren't technically-trained will starve and revolt.</p>\n", "question": "<p>This quote by Stephen Hawking has been in headlines for quite some time:</p>\n\n<blockquote>\n  <p>Artificial Intelligence could wipe out humanity when it gets too clever as humans will be like ants.</p>\n</blockquote>\n\n<p>Why does he say this? To put it simply in layman terms: what are the possible threats from AI? If we know that AI is so dangerous why are we still promoting it? Why is it not banned?</p>\n\n<p>What are the adverse consequences of the so called <a href=\"https://en.wikipedia.org/wiki/Technological_singularity\" rel=\"nofollow\">Technological Singularity</a>? </p>\n"}, "id": "24"}, {"body": {"answer": "<p>There are a number of long resources to answer this sort of question: consider Stuart Armstrong's book <a href=\"http://rads.stackoverflow.com/amzn/click/B00IB4N4KU\" rel=\"nofollow\">Smarter Than Us</a>, Nick Bostrom's book <a href=\"http://rads.stackoverflow.com/amzn/click/B00LOOCGB2\" rel=\"nofollow\">Superintelligence</a>, which grew out of this <a href=\"http://www.nickbostrom.com/views/superintelligence.pdf\" rel=\"nofollow\">edge.org answer</a>, <a href=\"http://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html\" rel=\"nofollow\">Tim Urban's explanation</a>, or <a href=\"https://aisafety.wordpress.com/\" rel=\"nofollow\">Michael Cohen's explanation</a>.</p>\n\n<p>But here's my (somewhat shorter) answer: intelligence is all about decision-making, and we don't have any reason to believe that humans are anywhere near close to being the best possible at decision-making. Once we are able to build an AI AI researcher (that is, a computer that knows how to make computers better at thinking), the economic and military relevance of humans will rapidly disappear as any decision that could be made by a human could be made better by a computer. (Why have human generals instead of robot generals, human engineers instead of robot engineers, and so on.)</p>\n\n<p>This isn't necessarily a catastrophe. If the Vulcans showed up tomorrow and brought better decision-making to Earth, we could avoid a lot of misery. The hard part is making sure that what we get are Vulcans who want us around and happy, instead of something that doesn't share our values.</p>\n", "question": "<p>This quote by Stephen Hawking has been in headlines for quite some time:</p>\n\n<blockquote>\n  <p>Artificial Intelligence could wipe out humanity when it gets too clever as humans will be like ants.</p>\n</blockquote>\n\n<p>Why does he say this? To put it simply in layman terms: what are the possible threats from AI? If we know that AI is so dangerous why are we still promoting it? Why is it not banned?</p>\n\n<p>What are the adverse consequences of the so called <a href=\"https://en.wikipedia.org/wiki/Technological_singularity\" rel=\"nofollow\">Technological Singularity</a>? </p>\n"}, "id": "25"}, {"body": {"answer": "<p>The problem of the Turing Test is that it tests the machines ability to resemble humans. Not necessarily every form of AI has to resemble humans. This makes the Turing Test less reliable. However, it is still useful since it is an actual test. It is also noteworthy that there is a prize for passing or coming closest to passing the Turing Test, the <a href=\"https://en.wikipedia.org/wiki/Loebner_Prize\" rel=\"nofollow\">Loebner Prize</a>.</p>\n\n<p>The intelligent agent definition of intelligence states that an agent is intelligent if it acts so to maximize the expected value of a performance measure based on past experience and knowledge. (paraphrased from <a href=\"http://en.wikipedia.org/wiki/Philosophy_of_artificial_intelligence#Intelligent_agent_definition\" rel=\"nofollow\">Wikipedia</a>). This definition is used more often and does not depend on the ability to resemble humans. However, it is harder to test this. </p>\n", "question": "<p>The <a href=\"https://en.wikipedia.org/wiki/Turing_test\">Turing Test</a> was the first test of artificial intelligence and is now a bit outdated. The <a href=\"https://en.wikipedia.org/wiki/Turing_test#Total_Turing_test\">Total Turing Test</a> aims to be a more modern test which requires a much more sophisticated system. What techniques can we use to identify an artificial intelligence (weak AI) and an <a href=\"https://en.wikipedia.org/wiki/Artificial_general_intelligence\">artificial general intelligence</a> (strong AI)?</p>\n"}, "id": "27"}, {"body": {"answer": "<p>It's analogous to analogue versus digital, or the many shades of gray in between black and white: when evaluating the truthiness of a result, in binary boolean it's either true or false (0 or 1), but when utilizing fuzzy logic, it's an estimated probability between 0 and 1 (such as 0.75 being mostly probably true). It's useful for making calculated decisions when all information needed isn't necessarily available.</p>\n\n<p><a href=\"https://en.wikipedia.org/wiki/Fuzzy_logic\" rel=\"nofollow\">Wikipedia has a fantastic page for this</a>.</p>\n", "question": "<p>I'm new to A.I. and I'd like to know in simple words, what is the fuzzy logic concept? How does it help, and when is it used?</p>\n"}, "id": "31"}, {"body": {"answer": "<p><em>As complexity rises, precise statements lose meaning and meaningful statements lose precision.</em> (Albert Einstein).</p>\n\n<p>Fuzzy logic deals with reasoning that is approximate rather than fixed and exact. This may make the reasoning more meaningful for a human:</p>\n\n<p><a href=\"http://i.stack.imgur.com/xdHPJ.png\"><img src=\"http://i.stack.imgur.com/xdHPJ.png\" alt=\"enter image description here\"></a></p>\n\n<hr>\n\n<p>Fuzzy logic is an extension of Boolean logic by Lotfi Zadeh in 1965 based on the\nmathematical theory of fuzzy sets, which is a generalization of the classical set theory.\nBy introducing the notion of <em>degree in the verification</em> of a condition, thus enabling a\ncondition to be in a state other than true or false, fuzzy logic provides a very valuable\nflexibility for reasoning, which makes it possible to take into account inaccuracies and\nuncertainties.</p>\n\n<p>One advantage of fuzzy logic in order to formalize human reasoning is that the rules\nare set in natural language. For example, here are some rules of conduct that a driver\nfollows, assuming that he does not want to lose his driver\u2019s licence:</p>\n\n<p><a href=\"http://i.stack.imgur.com/TM2UE.png\"><img src=\"http://i.stack.imgur.com/TM2UE.png\" alt=\"enter image description here\"></a></p>\n\n<p>Intuitively, it thus seems that the input variables like in this example are approximately\nappreciated by the brain, such as the degree of verification of a condition in fuzzy\nlogic.</p>\n\n<hr>\n\n<p>I've written a short <a href=\"https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=kz2aIc8AAAAJ&amp;citation_for_view=kz2aIc8AAAAJ:eQOLeE2rZwMC\">introduction to fuzzy logic</a> that goes into a bit more details but should be very accessible.</p>\n", "question": "<p>I'm new to A.I. and I'd like to know in simple words, what is the fuzzy logic concept? How does it help, and when is it used?</p>\n"}, "id": "32"}, {"body": {"answer": "<p>The concept of \"the singularity\" is when machines outsmart the humans. Although Stephen Hawking opinion is that this situation is inevitable, but I think it'll be very difficult to reach that point, because every A.I. algorithm needs to be programmed by humans, therefore it would be always more limited than its creator.</p>\n\n<p>We would probably know that point, when humanity will lost control over Artificial Intelligence where super-smart AI would be in competition with humans and maybe creating more sophisticated intelligent beings, but currently it's more like science fiction (aka <a href=\"https://en.wikipedia.org/wiki/Skynet_(Terminator)\" rel=\"nofollow\">Terminator's Skynet</a>).</p>\n\n<p>The risk could involve killing people (like self-flying war <em>drones</em> making their own decision), destroying countries or even the whole planet (like A.I. connected to the nuclear weapons (aka <a href=\"https://en.wikipedia.org/wiki/WarGames\" rel=\"nofollow\">WarGames</a> movie), but it doesn't prove the point that the machines would be smarter than humans.</p>\n", "question": "<p>I've heard the idea of the technological singularity, what is it and how does it relate to Artificial Intelligence?  Is this the theoretical point where Artificial Intelligence machines have progressed to the point where they grow and learn on their own beyond what humans can do and their growth takes off?  How would we know when we reach this point?</p>\n"}, "id": "33"}, {"body": {"answer": "<p>This is kind of an opinion question, and it's probably more a question of philosophy than anything.  But in terms of how things are commonly defined, I'll say \"yes, genetic algorithms are part of AI\".  That is, if you pick up a comprehensive book on artificial intelligence, there will probably be a chapter on genetic algorithms\" (or more broadly, \"evolutionary algorithms\"). </p>\n\n<p>One area that has been extensively studied in the past, is the idea of using genetic algorithms to train neural networks.  I don't know if people are still actively researching this topic or not, but it at least illustrates that GA's are part of the overall rubric of AI in one regard.</p>\n", "question": "<p>Since human intelligence presumably is a function of a natural genetic algorithm in nature, is using a genetic algorithm in a computer an example of artificial intelligence?  If not, how do they differ?  Or perhaps some are and some are not expressing artificial intelligence depending upon the scale of the algorithm and what it evolves into?</p>\n"}, "id": "38"}, {"body": {"answer": "<p>The rhetorical point of the Turing Test is that it places the 'test' for 'humanity' in <em>observable outcomes</em>, instead of in <em>internal components</em>. If you would behave the same in interacting with an AI as you would with a person, how could <em>you</em> know the difference between them?</p>\n\n<p>But that doesn't mean it's reliable, because intelligence has many different components and there are many sorts of intellectual tasks. The Turing Test, in some respects, is about the reaction of people to behavior, which is not at all reliable--remember that many people thought <a href=\"https://en.wikipedia.org/wiki/ELIZA\">ELIZA</a>, a very simple chatbot, was an excellent listener and got deeply emotionally involved very quickly. It calls to mind the <a href=\"https://www.youtube.com/watch?v=dBqhIVyfsRg\">Ikea commercial about throwing out a lamp</a>, where the emotional attachment comes <em>from the human viewer</em> (and the music), rather than from the lamp.</p>\n\n<p>Turing tests for specific economic activities are much more practically interesting--if one can write an AI that replaces an Uber driver, for example, what that will imply is much clearer than if someone can create a conversational chatbot.</p>\n", "question": "<p>The <a href=\"https://en.wikipedia.org/wiki/Turing_test\">Turing Test</a> was the first test of artificial intelligence and is now a bit outdated. The <a href=\"https://en.wikipedia.org/wiki/Turing_test#Total_Turing_test\">Total Turing Test</a> aims to be a more modern test which requires a much more sophisticated system. What techniques can we use to identify an artificial intelligence (weak AI) and an <a href=\"https://en.wikipedia.org/wiki/Artificial_general_intelligence\">artificial general intelligence</a> (strong AI)?</p>\n"}, "id": "39"}, {"body": {"answer": "<p>Fuzzy logic is based on regular boolean logic. Boolean logic means you are working with truth values of either true or false (or 1 or 0 if you prefer). Fuzzy logic is the same apart from you can have truth values which are in-between true and false, that is to say you are working with any number between 0 (inclusive) and 1 (inclusive). The fact that you can have a 'partially true and partially false' truth value is where the word \"fuzzy\" comes from. Natural languages often use fuzzy logic like \"that balloon is red\" meaning that balloon could be any colour which is similar enough to red, or \"the shower is warm\". Here is a rough diagram for how \"the temperature of the shower is warm\" could be represented in terms of fuzzy logic (the y axis being the truth value and the x axis being the temperature):</p>\n\n<p><a href=\"http://i.stack.imgur.com/G7szY.png\" rel=\"nofollow\"><img src=\"http://i.stack.imgur.com/G7szY.png\" alt=\"y-axis=truth value of statement about temperature, x-axis=temperature\"></a></p>\n\n<p>Fuzzy logic can be applied to boolean operations such as <strong>and</strong>, <strong>or</strong>, and <strong>not</strong>. These would work like:</p>\n\n<pre><code>A and B = A x B = min(A,B)\nA or B  = 1-(1-A)x(1-B) = max(A,B)\nnot A   = 1-A\n(where A and B are real values from 0 (inclusive) to 1 (inclusive) and\nmin and max return the smallest and the largest value of the two arguments given)\n</code></pre>\n\n<p>You can then use the three \"basic fuzzy logic operations\" to build all other \"fuzzy logic operations\", just like you can use the three \"basic boolean operations\" to build all other \"boolean logic operations\".</p>\n\n<p>Sources:\n<a href=\"https://en.wikipedia.org/wiki/Fuzzy_logic\" rel=\"nofollow\">Fuzzy logic wikipedia</a>, \n<a href=\"https://en.wikipedia.org/wiki/Boolean_algebra\" rel=\"nofollow\">Boolean algebra wikipedia</a>,\n<a href=\"https://www.youtube.com/watch?v=r804UF8Ia4c\" rel=\"nofollow\">Explanation of fuzzy logic on Youtube</a></p>\n\n<p>Note: if anyone could suggest some more reliable sources in the comments, I will happily add them to the list (I understand that the current aren't too reliable).</p>\n", "question": "<p>I'm new to A.I. and I'd like to know in simple words, what is the fuzzy logic concept? How does it help, and when is it used?</p>\n"}, "id": "43"}, {"body": {"answer": "<p>Dropout means that every individual data point is only used to fit a random subset of the neurons. This is done to make the neural network more like an ensemble model.</p>\n\n<p>That is, just as a random forest is averaging together the results of many individual decision trees, you can see a network network trained using dropout as averaging together the results of many individual neural networks (with 'results' understood to mean activations at every layer, rather than just the output layer).</p>\n", "question": "<p>What purpose does the \"dropout\" method serve and how does it improve the overall performance of the neural network?</p>\n"}, "id": "44"}, {"body": {"answer": "<p>The <a href=\"https://en.wikipedia.org/wiki/Technological_singularity\" rel=\"nofollow\">technological singularity</a> is a theoretical point in time at which a self-improving <a href=\"https://en.wikipedia.org/wiki/Artificial_general_intelligence\" rel=\"nofollow\">artificial general intelligence</a> becomes able to understand and manipulate concepts outside of the human brain's range, that is, the moment when it can understand things humans, by biological design, can't.</p>\n\n<p>The fuzziness about the singularity comes from the fact that, from the singularity onwards, history is effectively unpredictable. Humankind would be unable to predict any future events, or explain any present events, as science itself becomes incapable of describing machine-triggered events. Essentially, machines would think of us the same way we think of ants. Thus, we can make no predictions past the singularity. Furthermore, as a logical consequence, we'd be unable to define the point at which the singularity may occur at all, or even recognize it when it happens.</p>\n\n<p>However, in order for the singularity to take place, AGI needs to be developed, and <a href=\"https://en.wikipedia.org/wiki/Artificial_general_intelligence#Feasibility\" rel=\"nofollow\">whether that is possible is quite a hot debate</a> right now. Moreover, an algorithm that creates superhuman intelligence out of bits and bytes would have to be designed. By definition, a human programmer wouldn't be able to do such a thing, as his/her brain would need to be able to comprehend concepts beyond its range. There is also the argument that an intelligence explosion (the mechanism by which a technological singularity would theoretically be formed) would be impossible due to the difficulty of the design challenge of making itself more intelligent, getting larger proportionally to its intelligence, and that the difficulty of the design itself may overtake the intelligence required to solve said challenge (last point credit to <a href=\"http://ai.stackexchange.com/users/47/god-of-llamas\">god of llamas</a> in the comments).</p>\n\n<p>Also, there are related theories involving machines taking over humankind and all of that sci-fi narrative. However, that's unlikely to happen, if <a href=\"https://en.wikipedia.org/wiki/Three_Laws_of_Robotics\" rel=\"nofollow\">Asimov's laws</a> are followed appropriately. Even if Asimov's laws were not enough, a series of constraints would still be necessary in order to avoid the misuse of AGI by misintentioned individuals, and Asimov's laws are the nearest we have to that.</p>\n", "question": "<p>I've heard the idea of the technological singularity, what is it and how does it relate to Artificial Intelligence?  Is this the theoretical point where Artificial Intelligence machines have progressed to the point where they grow and learn on their own beyond what humans can do and their growth takes off?  How would we know when we reach this point?</p>\n"}, "id": "45"}, {"body": {"answer": "<p>The notion of genetics used in Genetic Algorithms (GAs) is a <em>very</em> stripped down version relative to genetics in nature, essentially consisting of a population of 'genes' (representing solutions to some predefined problem) subject to `survival of the fittest' during iterated application of recombination and mutation.</p>\n\n<p>Nowadays, the term 'Computational Intelligence' (CI) tends to be used to describe computational techniques intended to produce `the appearance of intelligence by <em>any</em> computational means', rather than specifically attempting to mimic the mechanisms that are believed to give rise to human (or animal) intelligence.</p>\n\n<p>That said, the distinction between CI and AI is not so hard and fast, and arguably arose during the `AI Winter' when the term AI was out of fashion.</p>\n", "question": "<p>Since human intelligence presumably is a function of a natural genetic algorithm in nature, is using a genetic algorithm in a computer an example of artificial intelligence?  If not, how do they differ?  Or perhaps some are and some are not expressing artificial intelligence depending upon the scale of the algorithm and what it evolves into?</p>\n"}, "id": "47"}, {"body": {"answer": "<p>Hidden layers by themselves aren't useful. If you had hidden layers that were linear, the end result would still be a linear function of the inputs, and so you could collapse an arbitrary number of linear layers down to a single layer.</p>\n\n<p>This is why we use nonlinear <a href=\"https://en.wikipedia.org/wiki/Activation_function\" rel=\"nofollow\">activation functions</a>, like RELU. This allows us to add a level of nonlinear complexity with each hidden layer, and with arbitrarily many hidden layers we can construct arbitrarily complicated nonlinear functions.</p>\n\n<p>Because we can (at least in theory) capture any degree of complexity, we think of neural networks as \"universal learners,\" in that a large enough network could mimic any function.</p>\n", "question": "<p>Why anybody would want to use the \"hidden layers\"? How they enhance the learning ability of the network in comparison to the network which doesn't have them (linear models)?</p>\n"}, "id": "48"}, {"body": {"answer": "<p>Machine learning is a subset of artificial intelligence. Roughly speaking, it corresponds to its learning side. There is no \"official\" definitions, boundaries are a bit fuzzy.</p>\n", "question": "<p>These two terms seem to be related, especially in their application in computer science and software engineering.  Is one a subset of another?  Is one a tool used to build a system for the other?  What are their differences and why are they significant?</p>\n"}, "id": "49"}, {"body": {"answer": "<p>\"Hidden\" layers really aren't all that special... a hidden layer is really no more than any layer that isn't input or output.  So even a very simple 3 layer NN has 1 hidden layer.  So I think the question isn't really \"how do hidden layers help?\" as much as \"why are deeper networks better?\".  </p>\n\n<p>And the answer to that latter question is an area of active research.  Even top experts like Geoffrey Hinton and Andrew Ng will freely admit that we don't really understand why deep neural networks work.  That is, we don't understand them in complete detail anyway.</p>\n\n<p>That said, the theory, as I understand it goes something like this...  successive layers of the network learn successively more sophisticated features, which build on the features from preceding layers.  So, for example, an NN used for facial recognition might work like this: the first layer detects edges and nothing else.  The next layer up recognizes geometric shapes (boxes, circles, etc).  The next layer up recognizes primitive features of a face, like eyes, noses, jaw, etc.   The next layer up then recognizes composites based on combinations of \"eye\" features, \"nose\" features, and so on.  </p>\n\n<p>So, in theory, deeper networks (more hidden layers) are better in that they develop a more granular / detailed representation of \"thing\" being recognized.  </p>\n", "question": "<p>Why anybody would want to use the \"hidden layers\"? How they enhance the learning ability of the network in comparison to the network which doesn't have them (linear models)?</p>\n"}, "id": "51"}, {"body": {"answer": "<p>Machine learning has been defined by many people in different ways. One definition says, that machine learning (ML) is the field of study that gives computers the <em>ability to learn</em> without being explicitly programmed.</p>\n\n<p>Given the above definition, we might say that machine learning is geared towards problems, for which we have (lots of) data (experience), from which a program can learn and can get better at a task.</p>\n\n<p>Artificial intelligence has many more aspects, where machines do not get better at tasks by learning from data, but may exhibit <em>intelligence</em> through rules (e.g. expert systems like <a href=\"https://en.wikipedia.org/wiki/Mycin\">Mycin</a>), <a href=\"http://rads.stackoverflow.com/amzn/click/0201403757\">logic</a> or algorithms, e.g. <a href=\"https://en.wikipedia.org/wiki/Pathfinding\">finding paths</a>.</p>\n\n<p>The TOC of <a href=\"http://aima.cs.berkeley.edu/\"><em>Artificial Intelligence: A Modern Approach</em></a> shows more research fields of AI, like <em>Constraint Satisfaction Problems</em>, <em>Probabilistic Reasoning</em> or <em>Philosophical Foundations</em>.</p>\n", "question": "<p>These two terms seem to be related, especially in their application in computer science and software engineering.  Is one a subset of another?  Is one a tool used to build a system for the other?  What are their differences and why are they significant?</p>\n"}, "id": "53"}, {"body": {"answer": "<blockquote>\n  <p>Machine learning is a science that involves development of\n  self-learning algorithms. These algorithms are more generic in nature\n  that it can be applied to various domain related problems.</p>\n  \n  <p>Artificial Intelligence is a science to develop a system or software\n  to mimic human to respond and behave in a circumference. As field with\n  extremely broad scope, AI has defined its goal into multiple chunks.\n  Later each chuck has become a separate field of study to solve its\n  problem.</p>\n</blockquote>\n\n<p><em><a href=\"http://shakthydoss.com/what-is-the-difference-between-artificial-intelligence-machine-learning-statistics-and-data-mining/\" rel=\"nofollow\">Sakthi Dasan Sekar</a></em></p>\n", "question": "<p>These two terms seem to be related, especially in their application in computer science and software engineering.  Is one a subset of another?  Is one a tool used to build a system for the other?  What are their differences and why are they significant?</p>\n"}, "id": "55"}, {"body": {"answer": "<p>Many terms have 'mostly' the same meanings, and so the differences are just in emphasis, perspective, or historical descent. People disagree as to which label refers to the superset or the subset; there are people who will call AI a branch of ML and people who will call ML a branch of AI.</p>\n\n<p>I typically hear Machine Learning used as a form of 'applied statistics' where we specify a learning problem in enough detail that we can just feed training data into it and get a useful model out the other side.</p>\n\n<p>I typically hear Artificial Intelligence as a catch-all term to refer to any sort of intelligence embedded in the environment or in code. This is a very expansive definition, and others use narrower ones (such as focusing on artificial <em>general</em> intelligence, which is not domain-specific). (Taken to an extreme, my version includes thermostats.)</p>\n\n<p>This is also a good time to point out other StackExchange sites, <a href=\"http://stats.stackexchange.com/\">Cross Validated</a> and <a href=\"http://datascience.stackexchange.com/\">Data Science</a>, which have quite a bit of overlap with this sit.</p>\n", "question": "<p>These two terms seem to be related, especially in their application in computer science and software engineering.  Is one a subset of another?  Is one a tool used to build a system for the other?  What are their differences and why are they significant?</p>\n"}, "id": "56"}, {"body": {"answer": "<p>The \"singularity,\" viewed narrowly, refers to a point at which economic growth is so fast that we can't make useful predictions about what the future past that point will look like.</p>\n\n<p>It's often used interchangeably with \"intelligence explosion,\" which is when we get so-called Strong AI, which is AI that is intelligent enough to understand and improve itself. It seems reasonable to expect that the intelligence explosion would immediately lead to an economic singularity, but the reverse is not necessarily true.</p>\n", "question": "<p>I've heard the idea of the technological singularity, what is it and how does it relate to Artificial Intelligence?  Is this the theoretical point where Artificial Intelligence machines have progressed to the point where they grow and learn on their own beyond what humans can do and their growth takes off?  How would we know when we reach this point?</p>\n"}, "id": "57"}, {"body": {"answer": "<blockquote>\n  <p>The creator of Artificial Intelligence studies begins with the work of\n  pioneer computer scientist Alan Turing (1912-1954) who in the\n  1930's evolved a concept of a \"Turing Machine.\"</p>\n</blockquote>\n\n<p><em><a href=\"http://www.enotes.com/homework-help/when-did-artifical-intelligence-research-start-158283\" rel=\"nofollow\">eNotes</a></em>  </p>\n\n<p>In layman's terms, it began in the 1930s, and they were most likely called \"Machines\" not AI.</p>\n", "question": "<p>When did research into Artificial Intelligence first begin?  Was it called Artificial Intelligence then or was there another name?</p>\n"}, "id": "59"}, {"body": {"answer": "<p>Quantum computers can help further develop A.I. algorithms and solve the problems to the extent of our creativity and ability to define the problem. For example breaking cryptography can take seconds, where it can takes thousands of years for standard computers. The same with artificial intelligence, it can predict all the combinations for the given problem defined by algorithm. This is due to superposition of multiple states of quantum bits.</p>\n\n<p>Currently, quantum computers are still in the early stages of development and can perform complex calculation. There are already technologies like <a href=\"https://en.wikipedia.org/wiki/D-Wave_Systems\" rel=\"nofollow\">D-Wave</a> systems which are used by Google and NASA for complex data analysis, using Multi-Qubit type quantum computers for <a href=\"https://en.wikipedia.org/wiki/Navier%E2%80%93Stokes_equations\" rel=\"nofollow\">solving NSE fluid dynamics problems</a> of interest or global surveillance for military purposes, and many more which we're not aware.</p>\n\n<p>Currently there are only a few quantum computers available to the public, like <a href=\"http://www.research.ibm.com/quantum/\" rel=\"nofollow\">IBM Quantum Experience</a> (the world\u2019s first quantum computing platform delivered via the IBM Cloud), but it's programming on quantum logic gates levels, so we're many years behind creating artificial intelligence available to public. There are some <a href=\"https://en.wikipedia.org/wiki/Quantum_programming\" rel=\"nofollow\">quantum computing languages</a> such as QCL, Q or Quipper, but I'm not aware any libraries which can provide artificial intelligence frameworks. It doesn't mean it's not there, and I'm sure huge companies and governments organisations are using it for their agenda to outcome the competition (like financial market analysis, etc.).</p>\n", "question": "<p>What aspects of quantum computers, if any, can help to further develop Artificial Intelligence?</p>\n"}, "id": "61"}, {"body": {"answer": "<p>Human intelligence is <strong>not</strong> an example of natural genetic algorithms.</p>\n\n<p>Genetic algorithms have collections of solutions that are collided with each other to make new solutions, eventually returning the best solution. Human intelligence is a network of neurons doing information processing, and almost all of it doesn't behave the same way.</p>\n\n<p>But that something doesn't behave in the same way that human intelligence does doesn't mean that it's not an AI algorithm; I would include 'genetic algorithms' as a numerical optimization technique, and since optimization and intelligence are deeply linked any numerical optimization technique could be seen as an AI technique.</p>\n", "question": "<p>Since human intelligence presumably is a function of a natural genetic algorithm in nature, is using a genetic algorithm in a computer an example of artificial intelligence?  If not, how do they differ?  Or perhaps some are and some are not expressing artificial intelligence depending upon the scale of the algorithm and what it evolves into?</p>\n"}, "id": "62"}, {"body": {"answer": "<p>Short answer: No.</p>\n\n<p>Longer answer: It depends on what IQ exactly is, and when the question is asked compared to ongoing development. The topic you're referring to is actually more commonly described as AGI, or Artificial General Intelligence, as opposed to AI, which could be any narrow problem solving capability represented in software/hardware.</p>\n\n<p><a href=\"https://en.wikipedia.org/wiki/Intelligence_quotient\">Intelligence quotient</a> is a rough estimate of how well humans are able to generally answer questions they have not previously encountered, but as a predictor it is somewhat flawed, and has many criticisms and detractors.</p>\n\n<p>Currently (2016), no known programs have the ability to generalize, or apply learning from one domain to solving problems in an arbitrarily different domain through an abstract understanding. (However there are programs which can effectively analyze, or break down some information domains into simpler representations.) This seems likely to change as time goes on and both hardware and software techniques are developed toward this goal. Experts widely disagree as to the likely timing and approach of these developments, as well as to the most probable outcomes.</p>\n\n<p>It's also worth noting that there seems to be a large deficit of understanding as to what exactly consciousness is, and disagreement over whether there is ever likely to be anything in the field of artificial intelligence that compares to it.</p>\n", "question": "<p>Can an AI program have an IQ?</p>\n\n<p>In other words, can the IQ of an AI program be measured?</p>\n\n<p>Like how humans can do an IQ test.</p>\n"}, "id": "65"}, {"body": {"answer": "<p><strong><a href=\"https://en.wikipedia.org/wiki/John_McCarthy_(computer_scientist)\" rel=\"nofollow\">John McCarthy</a></strong> (1927 - 2011) was an American computer scientist. A pioneer in the foundations of artificial intelligence research, <strong>he coined the term \"artificial intelligence\"</strong>. He was one of the creators of the (original) Lisp programming language, which was quite involved in early AI research in the 1960's and 1970's.</p>\n\n<p>He coined the term in 1955, and organized the first Artificial Intelligence conference in 1956, while working as a math teacher at Dartmouth. He founded the AI labs at MIT and Stanford.</p>\n\n<p>He's responsible for developing several other important concepts in today's mainstream computer science. Namely, he developed <a href=\"https://en.wikipedia.org/wiki/Garbage_collection_(computer_science)\" rel=\"nofollow\">garbage collection</a> (used by a Lisp interpreter) and designed the first <a href=\"https://en.wikipedia.org/wiki/Time-sharing\" rel=\"nofollow\">time-sharing systems</a>.</p>\n\n<p>On a side note, McCarthy predicted that creating a truly intelligent machine would require \"1.8 Einsteins and one-tenth the resources of the Manhattan Project.\"</p>\n", "question": "<p>Who first coined the term Artificial Intelligence, is there a published research paper which is the first to use that term?</p>\n"}, "id": "66"}, {"body": {"answer": "<p>It doesn't make much sense to have a single threshold with \"unintelligent\" below it and \"intelligent\" above it.</p>\n\n<p>I think it makes more sense to have a gradation of intelligence by cognitive task. Inverting a matrix is a 'cognitive task,' and one where working memory pays off immensely; computers have been much better at that cognitive task than humans for a long time.</p>\n\n<p>What the AlphaGo victory represents has several components. One is that we have algorithms that are competitive with the best board-game playing humans at doing tactical and strategic thinking in the well-described world of Go. Another is that the deeper structure of the human visual system seems to have been duplicated, and so we have algorithms that can recognize patterns as well as humans--with <em>very</em> limited resolution. (AlphaGo is seeing one pixel per stone, whereas we have very, very high-resolution eyes and the visual cortex to match.)</p>\n\n<p>Different people have different intuitions, but it seems to me that visual intelligence is a huge component of human intelligence in general. If we know most of the secrets of human visual intelligence, that means there might be many tasks that computers could now perform as well as humans (if provided the correct training data).</p>\n", "question": "<p>I read that in the spring of 2016 a computer <a href=\"https://en.wikipedia.org/wiki/Computer_Go\" rel=\"nofollow\">Go program</a> was finally able to beat a professional human for the first time.  Now that this milestone has been reached, does that represent a significant advance in artificial intelligence techniques or was it just a matter of even more processing power being applied to the problem?  What are some of the methods used to program the successful Go playing program, and are those methods considered to be artificial intelligence?</p>\n"}, "id": "69"}, {"body": {"answer": "<p>The earliest research into thinking machines was inspired by a confluence of ideas that became prevalent in the late 30s, 40s and early 50s (e.g. <a href=\"https://en.wikipedia.org/wiki/Logic\">formal logic</a>, automata, <a href=\"https://en.wikipedia.org/wiki/Robot#Remote-controlled_systems\">robots</a>). Although the <a href=\"https://en.wikipedia.org/wiki/Turing_test\">Turing test</a> was proposed in 1950s by <a href=\"https://en.wikipedia.org/wiki/Alan_Turing\">Alan Turing</a>, the work culminated back in the 1940s in the invention of the programmable digital computers, an abstract essence of mathematical reasoning. These ideas were inspired by a handful of scientists from a variety of fields who began seriously considering the possibility of building an electronic brain. The field of artificial intelligence research was founded as an academic discipline in 1956.</p>\n\n<p>However the concept of artificial beings is not new and it's as old as Greek myths of Hephaestus and Pygmalion which incorporated the idea of intelligent robots (such as <em>Talos</em>) and artificial beings (such as <em>Galatea</em> and <em>Pandora</em>).</p>\n\n<p>See the following articles at Wikipedia for further details:</p>\n\n<ul>\n<li><a href=\"https://en.wikipedia.org/wiki/Artificial_intelligence#History\">Artificial intelligence (AI)</a> </li>\n<li><a href=\"https://en.wikipedia.org/wiki/History_of_artificial_intelligence\">History of artificial intelligence</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Timeline_of_artificial_intelligence\">Timeline of artificial intelligence</a></li>\n</ul>\n", "question": "<p>When did research into Artificial Intelligence first begin?  Was it called Artificial Intelligence then or was there another name?</p>\n"}, "id": "71"}, {"body": {"answer": "<p>Instead of talking about just SAT solvers, let me talk about optimization in general. Many economic problems can be cast as optimization problems: for example, FedEx may have a list of packages and the destinations for those packages, and must decide which packages to put on which trucks, and what order to deliver those packages in.</p>\n\n<p>If you write out a mathematical description of this problem, there are a truly stunning number of possible solutions, and a well-defined way to evaluate which of two solutions is better. A solver is an algorithm that will evaluate a solution, come up with another solution, and then evaluate that one, and so on.</p>\n\n<p>In small cases and simple problems, the solver can also terminate with a proof that it is actually the best solution possible. But typically instead the solver just reports \"this is the best solution that I've seen,\" and that's used. An improvement in the solver means you can reliably get lower-cost solutions than you were seeing before.</p>\n\n<p>For the SAT problem specifically, the Wikipedia page on <a href=\"https://en.wikipedia.org/wiki/Boolean_satisfiability_problem\" rel=\"nofollow\">SAT</a> gives some examples:</p>\n\n<blockquote>\n  <p>Since the SAT problem is NP-complete, only algorithms with exponential worst-case complexity are known for it. In spite of this, efficient and scalable algorithms for SAT were developed over the last decade[when?] and have contributed to dramatic advances in our ability to automatically solve problem instances involving tens of thousands of variables and millions of constraints (i.e. clauses).[1] Examples of such problems in electronic design automation (EDA) include formal equivalence checking, model checking, formal verification of pipelined microprocessors,[12] automatic test pattern generation, routing of FPGAs,[14] planning, and scheduling problems, and so on. A SAT-solving engine is now considered to be an essential component in the EDA toolbox.</p>\n</blockquote>\n", "question": "<p>Why somebody would use SAT solvers (<a href=\"https://en.wikipedia.org/wiki/Boolean_satisfiability_problem\" rel=\"nofollow\">Boolean satisfiability problem</a>) to solve their real world problems?</p>\n\n<p>Are there any examples of the real uses of this model?</p>\n"}, "id": "72"}, {"body": {"answer": "<p>A Markov model includes the probability of transitioning to each state considering the current state. \"Each state\" may be just one point - whether it rained on specific day, for instance - or it might look like multiple things - like a pair of words. You've probably seen automatically generated weird text that <em>almost</em> makes sense, like <a href=\"https://blog.codinghorror.com/markov-and-you/\">Garkov</a> (the output of a Markov model based on the Garfield comic strips). That Coding Horror article also mentions the applications of Markov techniques to Google's PageRank.</p>\n\n<p>Markov models are really only powerful when they have a lot of input to work with. If a machine looked through a lot of English text, it would get a pretty good idea of what words generally come after other words. Or after looking through someone's location history, it could figure out where that person is likely to go next from a certain place. Constantly updating the \"input corpus\" as more data is received would let the machine tune the probabilities of all the state transitions.</p>\n\n<p>Genetic algorithms are fairly different things. They create functions by shuffling around parts of functions and seeing how good each function is at a certain task. A child algorithm will depend on its parents, but Markov models are interested mostly in predicting what thing will come next in a sequence, not creating a new chunk of code. You might be able to use a Markov model to spit out a candidate function, though, depending on how simple the \"alphabet\" is. You could even then give more weight to the transitions in successful algorithms.</p>\n", "question": "<p>I believe a Markov chain is a sequence of events where each subsequent event depends probabilistically on the current event.  What are examples of the application of a Markov chain and can it be used to create artificial intelligence?  Would a genetic algorithm be an example of a Markov chain since each generation depends upon the state of the prior generation?</p>\n"}, "id": "73"}, {"body": {"answer": "<p>There are at least two questions in your question: </p>\n\n<blockquote>\n  <p>What are some of the methods used to program the successful go playing program?</p>\n</blockquote>\n\n<p>and</p>\n\n<blockquote>\n  <p>Are those methods considered to be artificial intelligence?</p>\n</blockquote>\n\n<p>The first question is deep and technical, the second broad and philosophical.</p>\n\n<p>The methods have been described in: <a href=\"https://gogameguru.com/i/2016/03/deepmind-mastering-go.pdf\">Mastering the Game of Go with Deep Neural Networks and Tree Search</a>.</p>\n\n<p>The problem of Go or perfect information games in general is that:</p>\n\n<blockquote>\n  <p>exhaustive search is infeasible.</p>\n</blockquote>\n\n<p>So the methods will concentrate on shrinking the search space in an efficient way.</p>\n\n<p>Methods and structures described in the paper include:</p>\n\n<ul>\n<li>learning from expert human players in a supervised fashion</li>\n<li>learning by playing against itself (reinforcement learning)</li>\n<li>Monte-Carlo tree search (MCTS) combined with policy and value networks</li>\n</ul>\n\n<p>The second question has no definite answer, as you will have at least two angles on AI: <a href=\"https://en.wikipedia.org/wiki/Chinese_room#Strong_AI\">strong</a> and <a href=\"https://en.wikipedia.org/wiki/Weak_AI\">weak</a>.</p>\n\n<blockquote>\n  <p>All real-world systems labeled \"artificial intelligence\" of any sort are <strong>weak AI at most</strong>.</p>\n</blockquote>\n\n<p>So yes, it is artificial intelligence, but it is non-sentient.</p>\n", "question": "<p>I read that in the spring of 2016 a computer <a href=\"https://en.wikipedia.org/wiki/Computer_Go\" rel=\"nofollow\">Go program</a> was finally able to beat a professional human for the first time.  Now that this milestone has been reached, does that represent a significant advance in artificial intelligence techniques or was it just a matter of even more processing power being applied to the problem?  What are some of the methods used to program the successful Go playing program, and are those methods considered to be artificial intelligence?</p>\n"}, "id": "76"}, {"body": {"answer": "<p>One person working in this space is Dr. Woody Barfield.  He just wrote a book titled \"<a href=\"http://www.springer.com/us/book/9783319250489\" rel=\"nofollow\">Cyberhumans: Our Future With Machines</a>\" that focuses largely on the legal/policy issues around AI (and related topics).  In addition to the book, he is continuing with other research in this area.</p>\n", "question": "<p>As AI gains capabilities, and becomes more prevalent in society, our legal system will encounter questions it has not encountered before.  For example, if a self-driving car is involved in an accident while being controlled by the AI, who is at fault?  The \"driver\" (who's really just a passenger), the programmer(s) who made the AI, or the AI itself?</p>\n\n<p>So, what's on the cutting edge in terms of these kinds of issues at the intersection of law and artificial intelligence?</p>\n"}, "id": "79"}, {"body": {"answer": "<p>Yes, as Franck has rightly put, \"backprop\" means backpropogation, which is frequently used in the domain of neural networks for error optimization.</p>\n\n<p>For a detailed explanation, I would point out <a href=\"http://neuralnetworksanddeeplearning.com/chap2.html\" rel=\"nofollow\">this tutorial</a> on the concept of backpropogation by a very good book of Michael Nielsen. </p>\n", "question": "<p>What does \"backprop\" mean? I've Googled it, but it's showing backpropagation.</p>\n\n<p>Is the \"backprop\" term basically the same as \"backpropagation\" or does it have a different meaning?</p>\n"}, "id": "83"}, {"body": {"answer": "<p>The \"Turing Test\" is generally taken to mean an updated version of the Imitation Game Alan Turing proposed in his 1951 paper of the same name. An early version had a human (male or female) and a computer, and a judge had to decide which is which, and what gender they were if human. If they were correct less than 50% then the computer was considered \"intelligent.\"</p>\n\n<p>The current generally accepted version requires only one contestant, and a judge to decide whether it is human or machine. So yes, sometimes this will be a placebo, effectively, if we consider a human to be a placebo.</p>\n\n<p>Your first and fourth questions are related - and there are no strict guidelines. If the computer can fool a greater number of judges then it will of course be considered a better AI.</p>\n\n<p>The University of Toronto has a validity section in <a href=\"http://www.psych.utoronto.ca/users/reingold/courses/ai/turing.html\">this paper on Turing</a>, which includes a link to <a href=\"http://ciips.ee.uwa.edu.au/Papers/Technical_Reports/1997/05/Index.html\">Jason Hutchens' commentary</a> on why the Turing test may not be relevant (humans may also fail it) and the <a href=\"http://www.loebner.net/Prizef/loebner-prize.html\">Loebner Prize</a>, a formal instantiation of a Turing Test .</p>\n", "question": "<p>What are the specific requirements of the Turing Test?</p>\n\n<ul>\n<li>What requirements if any must the evaluator fulfill in order to be qualified to give the test?</li>\n<li>Must there always be two participants to the conversation (one human and one computer) or can there be more</li>\n<li>Are placebo tests (where there is not actually a computer involbed) allowed or encouraged?</li>\n<li>Can there be multiple evaluators? If so does the decision need to be unanimous among all evaluators in order for the machine to have passed the test?</li>\n</ul>\n"}, "id": "85"}, {"body": {"answer": "<p>There are several examples. For example, one instance of using Statistical AI from my workplace is:</p>\n\n<ol>\n<li>Analyzing the behaviour of the customer and their food-ordering trends, and then trying to upsell by reccommending them the dishes which they might like to order. This can be done through the apriori and FP-growth algorithms. We then, automated the algorithm, and then the algorithm improves itself through a <code>Ordered/Not-Ordered</code> metric.</li>\n<li>Self-driving cars. They use reinforcement and supervised learning algorithms for learning the route and the gradient/texture of the surface.</li>\n</ol>\n", "question": "<p>I believe that statistical AI uses inductive thought processes.  For example, deducing a trend from a pattern.  What are some examples of successfully applying statistical AI to real world problems.</p>\n"}, "id": "87"}, {"body": {"answer": "<p>If a computer is just brute-forcing the solution, it's not learning anything or using any kind of intelligence at all, and therefore it shouldn't be called \"artificial intelligence.\" It has to make decisions based on what's happened before in similar instances. For something to be intelligent, it needs a way to keep track of what it's learned. A chess program might have a really awesome measurement algorithm to use on every possible board state, but if it's always trying each state and never storing what it learns about different approaches, it's not intelligent.</p>\n", "question": "<p>Some programs do exhaustive searches for a solution while others do heuristic searches.  For example, in chess, the search for the best next move tends to be more exhaustive in nature whereas in go, the search for the best next move tends to be more heuristic in nature due to the much larger search space.</p>\n\n<p>Is the technique of brute force exhaustive searching for a good answer considered to be AI or is it generally required that heuristic algorithms be used before being deemed AI?  If so, is the chess playing computer beating a human professional seen as a meaningful milestone?</p>\n"}, "id": "89"}, {"body": {"answer": "<p>Short answer: The difference is mostly in the number of layers. </p>\n\n<p>For a long time, it was believed that \"1-2 hidden layers are enough for most tasks\" and it was impractical to use more than that, because training neural networks can be very computationally demanding.</p>\n\n<p>Nowadays, computers are capable of much more, so people have started to use networks with more layers and found that they work very well for some tasks.</p>\n\n<p>The word \"deep\" is there simply to distinguish these networks from the traditional, \"more shallow\" ones.</p>\n", "question": "<p>How is a neural network having the \"deep\" adjective actually distinguished from other similar networks?</p>\n"}, "id": "93"}, {"body": {"answer": "<p>Deep Learning is just (feed forward)neural networks with many layers.</p>\n\n<p>However, deep belief networks, Deep Boltzman networks, etc are not considered(debatable) as Deep Learning, as their topology is different (they ave undirected networks in their topology).</p>\n\n<p><a href=\"http://stats.stackexchange.com/a/59854/84191\">Helpful Reference</a></p>\n", "question": "<p>How is a neural network having the \"deep\" adjective actually distinguished from other similar networks?</p>\n"}, "id": "95"}, {"body": {"answer": "<p>I believe it would be more correct to say that (some) search engines <em>use</em> AI.  Broadly saying \"search engines are AI\" is not really correct.  At core, most search engines are nothing more than an inverted text index using something like tf\u2013idf scoring.  That's a very mechanical / simple thing that nobody would really call AI. </p>\n\n<p>But more sophisticated search engines may <em>use</em> AI or AI techniques to do things like semantic analysis - so they can actually \"answer questions\" instead of just looking up words in an index.  </p>\n", "question": "<p>Are search engines considered AI because of the way they analyze what you search for and remember it? Or how they send you ads of what you've searched for recently? Is this considered AI or just smart?</p>\n"}, "id": "97"}, {"body": {"answer": "<p><strong>Deep Network is nothing but a neural network which has multiple layers.</strong> <code>Multiple</code> can be subjective.</p>\n\n<p>However, any network which has >=6 or 7 layers are considered deep. So, the above would form a very basic definition of a deep network. </p>\n", "question": "<p>I'm pretty sure this a noob-y question, but what is Deep Network? As of now it is the most popular tag on AI. Is there a reason for this? </p>\n\n<hr>\n\n<p>Please note, I am not asking how to distinguish a deep network from a neural network, I am simply asking for the definition of deep network.</p>\n"}, "id": "98"}, {"body": {"answer": "<p>Deep networks have two main differences with 'normal' networks.</p>\n\n<p>The first is that computational power and training datasets have grown immensely, meaning that it's practical to run larger networks and statistically valid (that is, we have enough training examples that we won't just run into over-fitting problems with larger networks).</p>\n\n<p>The second is that back propagation is limited the more layers you have; each layer represents a gradient of the error, and so by the time one is about six layers deep there isn't much error left to modify the neuron weights. But one might reasonably expect earlier neurons to be more important than later neurons, since they represent 'concepts' that are closer to the raw inputs.</p>\n\n<p>New training techniques sidestep this problem, typically by doing unsupervised learning on the raw inputs, creating higher-level 'concepts' that are then useful as inputs for supervised learning.</p>\n\n<p>(For example, consider the problem of determining whether or not an image contains a cat from the pixels. The early layers of the network should be doing things like detecting edges, which one could expect to be shared among all images and mostly independent of what one is trying to do with the output layers, thus also hard to train through 'cat-not cat' signals many layers up.</p>\n", "question": "<p>I'm pretty sure this a noob-y question, but what is Deep Network? As of now it is the most popular tag on AI. Is there a reason for this? </p>\n\n<hr>\n\n<p>Please note, I am not asking how to distinguish a deep network from a neural network, I am simply asking for the definition of deep network.</p>\n"}, "id": "100"}, {"body": {"answer": "<p>If one thinks of intelligence as a continuous measure of optimization power (that is, how much better are outcomes for any unit of cognitive effort expended), then exhaustive search has non-zero intelligence (in that it does actually give better outcomes as more effort is expended) but <em>very, very low</em> intelligence (as the outcomes are better mostly by luck, and the amount of effort expended can be impossibly large).</p>\n", "question": "<p>Some programs do exhaustive searches for a solution while others do heuristic searches.  For example, in chess, the search for the best next move tends to be more exhaustive in nature whereas in go, the search for the best next move tends to be more heuristic in nature due to the much larger search space.</p>\n\n<p>Is the technique of brute force exhaustive searching for a good answer considered to be AI or is it generally required that heuristic algorithms be used before being deemed AI?  If so, is the chess playing computer beating a human professional seen as a meaningful milestone?</p>\n"}, "id": "101"}, {"body": {"answer": "<p>There are really two questions here, that I can see.  One is \"what were the specific requirements of the original Turing test, as stated by Turing himself.\"   The other is \"what should the specific requirements of a modern Turing test be?\"  Things have advanced a lot since Turing's day, and I think it's reasonable for us to consider extending / modifying his test to reflect our current understanding.</p>\n\n<p>The answer to the first question is easy enough to look up, so I think the interesting one is the second one.  What <em>should</em> a test to determine intelligence look like? With that in mind, I think the answer to all four questions posed by the OP is \"it depends\".   I don't think there's universal consensus on how to structure a perfect Turing test, so a given experimenter is really free to set things up however he/she wants.  </p>\n\n<p>This is all, of course, based on the assumption that the Turing test, or a Turing-test-like test is actually of value.  That's not necessarily a given.  Consider that, to some extent, what we're talking about is designing an AI with an exceptional ability for deceit!  That is, assuming the questioner is allowed to simply ask \"are you human\", then we have to assume that the AI is supposed to lie if it wants to pass the test.  So one might rightly ask, is designing a system to be really good at telling lies, a valuable approach to AI?</p>\n", "question": "<p>What are the specific requirements of the Turing Test?</p>\n\n<ul>\n<li>What requirements if any must the evaluator fulfill in order to be qualified to give the test?</li>\n<li>Must there always be two participants to the conversation (one human and one computer) or can there be more</li>\n<li>Are placebo tests (where there is not actually a computer involbed) allowed or encouraged?</li>\n<li>Can there be multiple evaluators? If so does the decision need to be unanimous among all evaluators in order for the machine to have passed the test?</li>\n</ul>\n"}, "id": "102"}, {"body": {"answer": "<p>The term <em>classical AI</em> refers to the concept of intelligence that was broadly accepted after the <a href=\"https://en.wikipedia.org/wiki/Dartmouth_Conferences\" rel=\"nofollow\">Dartmouth Conference</a> and basically refers to a kind of intelligence that is strongly symbolic and oriented to logic and language processing. One basic point is the duality <strong>body</strong> vs. <strong>mind</strong>. It's in this period that the mind start to be compared with computer software.</p>\n\n<h2>Two classical historic examples of this conception of intelligence</h2>\n\n<ul>\n<li><p><a href=\"https://upload.wikimedia.org/wikipedia/commons/b/be/Deep_Blue.jpg\" rel=\"nofollow\">Deep Blue</a>, whose aim in life was to be the master of chess, ruling over the (not-so) intelligent mankind</p></li>\n<li><p><a href=\"http://www.manifestation.com/neurotoys/eliza.php3\" rel=\"nofollow\">Eliza</a>, a computer-based therapist that turned out to <a href=\"https://en.wikipedia.org/wiki/Joseph_Weizenbaum\" rel=\"nofollow\">trigger a critic</a> to the classical AI</p></li>\n</ul>\n\n<h2>Two technical examples of classical AI</h2>\n\n<ul>\n<li><p>Expert systems, which are computer programs that strongly rely on the type of constrains and conclusions that you refer to, *in order to accomplish feats of apparent intelligence](<a href=\"https://www.britannica.com/technology/expert-system\" rel=\"nofollow\">https://www.britannica.com/technology/expert-system</a>)</p></li>\n<li><p>Fuzzy logic, <a href=\"https://de.mathworks.com/help/fuzzy/what-is-fuzzy-logic.html?requestedDomain=www.mathworks.com\" rel=\"nofollow\"><em>which is an extension of multivalued logic</em></a>, but with continuous values instead of discrete ones</p></li>\n</ul>\n\n<p>Note that in all cases the <em>hardware</em> (once compared with the <em>body</em>) does not play any role: Intelligence is abstract and independent from the material world.</p>\n", "question": "<p>I believe that Classical AI uses deductive thought processes. For example, given as a set of constraints, deduce a conclusion.  What are some examples of successfully applying Classical AI to real world problems.</p>\n"}, "id": "105"}, {"body": {"answer": "<blockquote>\n  <p>Now that this milestone has been reached, does that represent a significant advance in artificial intelligence techniques or was it just a matter of ever more processing power being applied to the problem? </p>\n</blockquote>\n\n<p>Neither, really. It is a milestone and a significant advance in computers beating humans in games, but the techniques used are only relevant to that game, not for other purposes in AI. </p>\n\n<p>The solution lies in humans analysing the game and implementing algorithms for finding a good move. This is the main reason that a computer can beat the humans, together with the fact that it can calculate much faster and that it doesn't make really bad moves by not seeing something.</p>\n\n<p>Processing power helps, but the game-tree complexity for go is very large, estimated to be larger than 10<sup>200</sup>, whereas the game-tree complexity for chess is only 10<sup>120</sup> (known as the Shannon number), so chess is less hard. This means that for neither chess nor go a database can be created with all possible positions. </p>\n\n<p>The fact that Deep Blue beat Kasparov in a six-game match in 1997 was quite a development, since this was one of the first \"hard\" games where a computer beat a top human. But it still isn't really Artificial Intelligence, more analysing the game. Implementing an opening and endgame book was a large part for chess, the middle game was done using analysis, I don't know the details. </p>\n", "question": "<p>I read that in the spring of 2016 a computer <a href=\"https://en.wikipedia.org/wiki/Computer_Go\" rel=\"nofollow\">Go program</a> was finally able to beat a professional human for the first time.  Now that this milestone has been reached, does that represent a significant advance in artificial intelligence techniques or was it just a matter of even more processing power being applied to the problem?  What are some of the methods used to program the successful Go playing program, and are those methods considered to be artificial intelligence?</p>\n"}, "id": "106"}, {"body": {"answer": "<p>There're many online services that uses statistical neural networks for recommendations. For example we have <a href=\"http://imhonet.ru\" rel=\"nofollow\">well known service</a> here in Russia that could give it's users recommendations for movies and shows to watch and books to read. It's recommendation core is based on many things known about user: what movies/books he or she loves and what not, analyses his or her friends likes and do on. While you have only few items rayed it will give you very strange recommendations but then it becomes more correct and really could give you some true gems.</p>\n", "question": "<p>I believe that statistical AI uses inductive thought processes.  For example, deducing a trend from a pattern.  What are some examples of successfully applying statistical AI to real world problems.</p>\n"}, "id": "107"}, {"body": {"answer": "<p>Quantum computers are super awesome at matrix multiplication, <a href=\"http://twistedoakstudios.com/blog/Post8887_what-quantum-computers-do-faster-with-caveats\">with some limitations</a>. Quantum superposition allows each bit to be in <em>a lot</em> more states than just zero or one, and quantum gates can fiddle those bits in many different ways. Because of that, a quantum computer can process a lot of information at once for certain applications.</p>\n\n<p>One of those applications is the <a href=\"http://algorithmicassertions.com/quantum/2014/03/07/Building-your-own-Quantum-Fourier-Transform.html\">Fourier transform</a>, which is useful in a lot of problems, like <a href=\"http://dsp.stackexchange.com/q/69\">signal analysis</a> and array processing. There's also <a href=\"http://twistedoakstudios.com/blog/Post2644_grovers-quantum-search-algorithm\">Grover's quantum search algorithm</a>, which finds the single value for which a given function returns something different. If an AI problem can be expressed in a mathematical form <a href=\"http://algorithmicassertions.com/quantum/2014/04/27/The-Not-Quantum-Laplace-Transform.html\">amenable to quantum computing</a>, it can receive great speedups. Sufficient speedups could transform an AI idea from \"theoretically interesting but insanely slow\" to \"quite practical once we get a good handle on quantum computing.\"</p>\n", "question": "<p>What aspects of quantum computers, if any, can help to further develop Artificial Intelligence?</p>\n"}, "id": "114"}, {"body": {"answer": "<p>In my opinion python and java have taken over from LISP. Many people use them, there is a large amount of libraries available. And more importantly, they are easy to integrate in web technologies. </p>\n", "question": "<p>I know that language of <strong><code>Lisp</code></strong> was used early on when working on artificial intelligence problems.  Is it still being used today for significant work?  If not, is there a new language that has taken its place as the most common one being used for work in AI today?</p>\n"}, "id": "115"}, {"body": {"answer": "<p>The advantage of a declarative language like Prolog is that it can be used to express facts and inference rules separately from control flow.  </p>\n\n<p>This allows the developer to focus on the data and inference rules (the knowledge model), and allows the developer to extend the knowledge model more easily.</p>\n\n<p>I should add that in practice, this dichotomy between facts/rules on the one hand, and control flow on the other, is not strict. A knowledge engineer who writes a code base in Prolog does sometimes have to consider control flow. The \"!\" operator is used so that the developer can influence the evaluation of the rules.</p>\n", "question": "<p>What specific advantages of declarative languages make them more applicable to AI than imperative languages?  What can declarative languages do easily that other languages styles find difficult for this kind of problem?</p>\n"}, "id": "116"}, {"body": {"answer": "<p>A good answer to this question depends on what you want to use the labels for.</p>\n\n<p>When I think about \"optimization,\" I think about a solution space and a cost function; that is, there are many possible answers that could be returned and we can know what the cost is of any particular answer.</p>\n\n<p>In this view, the answer is \"yes\"--pattern recognition is a case where each pattern is a possible answer, and the optimization method is trying to find the one where the cost is lowest (that is, where the answer matches what you want it to match).</p>\n\n<p>But most interesting optimization problems are characterized by exponential solution spaces and clean cost functions, and so can be thought of more as 'search' problems, whereas most pattern recognition problems are characterized by simple solution spaces and complicated cost functions, and it might feel unnatural to put the two of them together.</p>\n\n<p>(In general, I do think that optimization and intelligence are deeply linked enough that optimization power is a good measure of intelligence, and certainly a better measure of the <em>practical</em> use of intelligence than pattern recognition.)</p>\n", "question": "<p>In <a href=\"https://youtu.be/oSdPmxRCWws?t=30\">this video</a> an expert says, \"One way of thinking about what intelligence is [specifically with regard to artificial intelligence], is as an optimization process.\"</p>\n\n<p>Can intelligence always be thought of as an optimization process, and can artificial intelligence always be modeled as an optimization problem? What about pattern recognition? Or is he mischaracterizing?</p>\n"}, "id": "117"}, {"body": {"answer": "<p>I don't think it makes sense to decide activation functions based on desired properties of the output; you can easily insert a calibration step that maps the 'neural network score' to whatever units you actually want to use (dollars, probability, etc.).</p>\n\n<p>So I think preference between different activation functions mostly boils down to the <a href=\"https://en.wikipedia.org/wiki/Activation_function#Comparison_of_activation_functions\" rel=\"nofollow\">different properties</a> of those activation functions (like whether or not they're continuously differentiable). Because there's just a linear transformation between the two, I think that means there isn't a meaningful difference between them.</p>\n", "question": "<p>Two common activation functions used in deep learning are the hyperbolic tangent function and the sigmoid activation function. I understand that the hyperbolic tangent is just a rescaling and translation of the sigmoid function (i,e tanh(z) = 2*sigma(z) - 1). </p>\n\n<p>Is there a significant difference between these two activation functions, and in particular, <strong>when is one preferable to the other</strong>?</p>\n\n<p>I realize that in some cases (like when estimating probabilities) outputs in the range of [0,1] are more convenient than outputs that range from [-1,1]. I want to know if there are differences <strong>other than convenience</strong> which distinguish the two activation functions.</p>\n"}, "id": "119"}, {"body": {"answer": "<p>My impression is that fuzzy logic has mostly declined in relevance and <a href=\"https://en.wikipedia.org/wiki/Probabilistic_logic\" rel=\"nofollow\">probabilistic logic</a> has taken over its niche. (See the <a href=\"https://en.wikipedia.org/wiki/Fuzzy_logic#Comparison_to_probability\" rel=\"nofollow\">comparison on Wikipedia</a>.) The two are somewhat deeply related, and so it's mostly a change in perspective and language.</p>\n\n<p>That is, fuzzy logic mostly applies to <em>labels</em> which have <em>uncertain ranges</em>. An object that's cool but not too cool <em>could</em> be described as either cold or warm, and fuzzy logic handles this by assigning some fractional truth value to the 'cold' and 'warm' labels and no truth to the 'hot' label.</p>\n\n<p>Probabilistic logic focuses more on the probability of some fact given some observations, and is deeply focused on the uncertainty of observations. When we look at an email, we track our belief that the email is \"spam\" and shouldn't be shown to the user with some number, and adjust that number as we see evidence for and against it being spam.</p>\n", "question": "<p><a href=\"http://ai.stackexchange.com/questions/10/what-is-fuzzy-logic\">Fuzzy logic</a> is the logic where every statement can have any real truth value between 0 and 1.</p>\n\n<p>How can fuzzy logic be used in creating AI? Is it useful for certain decision problems involving multiple inputs? Can you give an example of an AI that uses it?</p>\n"}, "id": "121"}, {"body": {"answer": "<p>A classical example of fuzzy logic in an AI is the expert system Mycin.</p>\n\n<p>Fuzzy logic can be used to deal with probabilities and uncertainties. </p>\n\n<p>If one looks at, for example, predicate logic, then every statement is either true or false. In reality, we don't have this mathematical certainty. </p>\n\n<p>For example, let's say a physician (or expert system) sees a symptom that can be attributed to a few different diseases (say A, B and C). The physician will now attribute a higher likelihood to the possibility of the patient having any of these three diseases. There is no definite true or false statement, but there is a change of weights. This can be reflected in fuzzy logic, but not so easily in symbolic logic.</p>\n", "question": "<p><a href=\"http://ai.stackexchange.com/questions/10/what-is-fuzzy-logic\">Fuzzy logic</a> is the logic where every statement can have any real truth value between 0 and 1.</p>\n\n<p>How can fuzzy logic be used in creating AI? Is it useful for certain decision problems involving multiple inputs? Can you give an example of an AI that uses it?</p>\n"}, "id": "122"}, {"body": {"answer": "<p>After he lays out his argument, he deals with some counterarguments. The following looks like the weakest one to me:</p>\n\n<blockquote>\n  <p>We can use the same analogy also against those who, finding a formula their first machine cannot produce as being true, concede that that machine is indeed inadequate, but thereupon seek to construct a second, more adequate, machine, in which the formula can be produced as being true. This they can indeed do: but then the second machine will have a G\u00f6delian formula all of its own, constructed by applying G\u00f6del's procedure to the formal system which represents its (the second machine's) own, enlarged, scheme of operations. And this formula the second machine will not be able to produce as being true, while a mind will be able to see that it is true. And if now a third machine is constructed, able to do what the second machine was unable to do, exactly the same will happen: there will be yet a third formula, the G\u00f6delian formula for the formal system corresponding to the third machine's scheme of operations, which the third machine is unable to produce as being true, while a mind will still be able to see that it is true. And so it will go on.</p>\n</blockquote>\n\n<p>In short, by making the system more complex, it can see the inadequacy of a less complex system, but a yet more complex system can see its inadequacy. But from whence comes the claim that a mind <em>could</em> see the inadequacy in the <em>n</em>th machine? If, say, the G\u00f6delian formula had as many components to it as a human brain had neurons, it seems suspect to claim that the human <em>could</em> evaluate that formula and identify that it is in fact a G\u00f6delian formula, rather than a similar but not quite identical sentence.  </p>\n", "question": "<p>In <a href=\"http://users.ox.ac.uk/~jrlucas/Godel/mmg.html\" rel=\"nofollow\">Minds, Machines and G\u00f6del</a> (1959), J. R. Lucas shows that any human mathematician can not be represented by an algorithmic automaton (a Turing Machine, but any computer is equivalent to it by the Church-Turing thesis), using G\u00f6del's incompleteness theorem. </p>\n\n<p>As I understand it, he states that since the computer is an algorithm and hence a formal system, G\u00f6del's incompleteness theorem applies. But a human mathematician also has to work in a formal axiom system to prove a theorem, so wouldn't it apply there as well? </p>\n"}, "id": "124"}, {"body": {"answer": "<p>Yes, it applies. If a statement cannot be derived in a finite number of steps, then it doesn't matter if the person trying to prove it is a human or a computer.  </p>\n\n<p>The mathematician has one advantage over a standard theorem proving algorithm: the mathematician can \"step out of the system\" (as Douglas Hofstadter called in <em>G&ouml;del, Escher, Bach</em>), and start thinking <em>about</em> the system. From this point of view, the mathematician may find that the derivation is impossible.</p>\n\n<p>However, an AI for proving theorems could be programmed to recognize patterns in the derivation, just like our hypothetical mathematician, and start reasoning <em>about</em> the formal system to derive properties of the formal system itself.\nBoth the AI and the mathematician would still be bound by the laws of mathematics, and not be able to prove a theorem if it was mathematically improvable.</p>\n", "question": "<p>In <a href=\"http://users.ox.ac.uk/~jrlucas/Godel/mmg.html\" rel=\"nofollow\">Minds, Machines and G\u00f6del</a> (1959), J. R. Lucas shows that any human mathematician can not be represented by an algorithmic automaton (a Turing Machine, but any computer is equivalent to it by the Church-Turing thesis), using G\u00f6del's incompleteness theorem. </p>\n\n<p>As I understand it, he states that since the computer is an algorithm and hence a formal system, G\u00f6del's incompleteness theorem applies. But a human mathematician also has to work in a formal axiom system to prove a theorem, so wouldn't it apply there as well? </p>\n"}, "id": "125"}, {"body": {"answer": "<p>Depends on who you ask! John Searle, who proposed this argument, would say \"yes\", but others would say it is irrelevant. The Turing Test does not stipulate that a machine must actually \"understand\" what it is doing, as long as it seems that way to a human. You could argue that our \"thinking\" is only a more sophisticated form of clever algorithmics.</p>\n", "question": "<p>Back in college, I had a Complexity Theory teacher who stated that artificial intelligence was a contradiction in terms. If it could be calculated mechanically, he argued, it wasn't intelligence.</p>\n\n<p>This seems to be a variant of the Chinese Room argument. This argument is a metaphor, where  a person is put in a room full of Chinese books. This person doesn't understand a word of Chinese, but is slipped messages in Chinese under the door. The person has to use the books, which contain transformation rules, to answer these messages. The person can apply the transformation rules, but does not understand what (s)he is communicating.</p>\n\n<p>Does the chinese room argument hold? Can we argue that artificial intelligence is merely clever algorithmics?</p>\n"}, "id": "126"}, {"body": {"answer": "<p>There are two broad types of responses to philosophical queries like this.</p>\n\n<p>The first is to make analogies and refer to intuition; one could, for example, actually calculate the necessary size for such a Chinese room, and suggest that it exists outside the realm of intuition and thus any analogies using it are suspect.</p>\n\n<p>The second is to try to define the terms more precisely. If by \"intelligence\" we mean not \"the magic thing that humans do\" but \"information processing,\" then we can say \"yes, obviously the Chinese Room involves successful information processing.\"</p>\n\n<p>I tend to prefer the second because it forces conversations towards <em>observable outcomes</em>, and puts the difficulty of defining a term like \"intelligence\" on the person who wants to make claims about it. If \"understanding\" is allowed to have an amorphous definition, then <em>any</em> system could be said to have or not have understanding. But if \"understand\" is itself understood in terms of observable behavior, then it becomes increasingly difficult to construct an example of a system that \"is not intelligent\" and yet shares all the observable consequences of intelligence.</p>\n", "question": "<p>Back in college, I had a Complexity Theory teacher who stated that artificial intelligence was a contradiction in terms. If it could be calculated mechanically, he argued, it wasn't intelligence.</p>\n\n<p>This seems to be a variant of the Chinese Room argument. This argument is a metaphor, where  a person is put in a room full of Chinese books. This person doesn't understand a word of Chinese, but is slipped messages in Chinese under the door. The person has to use the books, which contain transformation rules, to answer these messages. The person can apply the transformation rules, but does not understand what (s)he is communicating.</p>\n\n<p>Does the chinese room argument hold? Can we argue that artificial intelligence is merely clever algorithmics?</p>\n"}, "id": "127"}, {"body": {"answer": "<p>It depends on the definition of (artificial) intelligence. </p>\n\n<p>The position that Searle originally tried to refute with the Chinese room experiment was the so-called position of strong AI: An appropriately programmed computer would have a mind in the exact same sense as humans have minds. </p>\n\n<p>Alan Turing tried to give an definition of artificial intelligence with the Turing Test, stating that a machine is intelligent if it can pass the test. The Turing Test is introduced <a href=\"https://en.wikipedia.org/wiki/Turing_test\">here</a>. I won't explain it in detail because it is not really relevant to the answer. If you define (artificial) intelligence as Turing did, then the Chinese room experiment is not valid.</p>\n\n<p>So the point of the Chinese room experiment is to show that an appropriately programmed computer is not the same as a human mind, and therefore that Turing's Test is not a good one. </p>\n", "question": "<p>Back in college, I had a Complexity Theory teacher who stated that artificial intelligence was a contradiction in terms. If it could be calculated mechanically, he argued, it wasn't intelligence.</p>\n\n<p>This seems to be a variant of the Chinese Room argument. This argument is a metaphor, where  a person is put in a room full of Chinese books. This person doesn't understand a word of Chinese, but is slipped messages in Chinese under the door. The person has to use the books, which contain transformation rules, to answer these messages. The person can apply the transformation rules, but does not understand what (s)he is communicating.</p>\n\n<p>Does the chinese room argument hold? Can we argue that artificial intelligence is merely clever algorithmics?</p>\n"}, "id": "128"}, {"body": {"answer": "<p>Genetic algorithms are an <em>analogy</em> to biology, not a copy of it. The core piece of the analogy is that the \"phenotype,\" or the observable portion of a solution, is constructed from the \"genotype,\" or the internal portion of a solution. </p>\n\n<p>For example, a number (the phenotype) can be stored as a binary series of 0s and 1s (the genotype), and by changing individual bits we make potentially dramatic changes in the resulting number, and by combining two genotypes we can get a broad range of 'related' numbers.</p>\n\n<p>Epigenetics are a wrinkle in the genotype -> phenotype mapping, making it a non-deterministic function, and so incorporating them would degrade the performance of a genetic algorithm by adding unnecessary noise.</p>\n", "question": "<p>What designs for genetic algorithms are there, if they are classified differently and/or have different names, that leverage models for epigenetics in evolution? What are the pros/cons of the designs? Are there vast insufficiencies or wide-open questions about their usefulness? </p>\n"}, "id": "129"}, {"body": {"answer": "<p>The following thread has many answers regarding why LISP used to be thought of as the AI language: <a href=\"http://stackoverflow.com/questions/130475/why-is-lisp-used-for-ai\">Why is Lisp used for AI</a> and the following is an answer by Peter Norvig, who wrote a popular textbook on the subject and is currently Director of Research at Google: <a href=\"https://www.quora.com/Is-it-true-that-Lisp-is-highly-used-programming-language-in-AI\" rel=\"nofollow\">Is it true that Lisp is highly used programming language in AI?</a></p>\n\n<p>I am not overly familiar with the history, but I think LISP was oversold to industry as \"the AI language\". It is a good language for humans to think in and pioneered many important ideas which have since been incorporated into many modern languages (see <a href=\"https://en.wikipedia.org/wiki/Lisp_(programming_language)\" rel=\"nofollow\">the Wikipedia page</a>), but it is no way the \"best\". It was likely also popular because it is very expressive: you can write short programs to represent complex ideas, a property it shares with other functional languages in use such as Scala. This also means that it is easy to write a program that is very hard to debug in LISP. Modern functional languages have been trying to do better in this regard through typing etc.</p>\n\n<p>The paradigm for AI that currently receives most attention is Machine Learning, i.e. learning hypothesis from data, as opposed to previous approaches like Expert Systems where experts wrote rules for the AI to follow. Python is currently the most widely used language for prototyping machine learning algorithms and has many libraries and an active community. Another important detail about modern AI is the volume of data it uses. Big Data analysis is done using cluster computing systems like Hadoop (with code written in Java) and Spark (with code written in Python or Scala). Often, the core time-intensive subroutines are written in C, but this is often done in the form of third-party libraries.</p>\n\n<p>Finally it must be said that the AI Winter of the 80s was not because we did not have the right language, but because we did not have the right algorithms and did not have enough computational power. This has changed as GPUs have gotten better. If you're trying to learn AI, spend your time studying algorithms and not languages.</p>\n", "question": "<p>I know that language of <strong><code>Lisp</code></strong> was used early on when working on artificial intelligence problems.  Is it still being used today for significant work?  If not, is there a new language that has taken its place as the most common one being used for work in AI today?</p>\n"}, "id": "131"}, {"body": {"answer": "<p>You described a single layer feed forward network. They can have multiple layers. The significance of the weights is that they make a linear transformation from the output of the previous layer and hand it to the node they are going to. To say it more simplistically, they specify how important (and in what way: negative or positive) is the activation of node they are coming from to activating the node they are going to.</p>\n\n<p>In your example, since there is only one layer (a row of input nodes and a row of output nodes) it is easy to explain what each node represents. However in multilayer feed forward networks they can become abstract representations which makes it difficult to explain them and therefore explain what the weights that come to them or go out of them represent.</p>\n\n<p>Another way of thinking about it is that they describe hyperplanes in the space of the output of the previous node layer. If each output from the previous layer represents a point in space, a hyperplane decides which part of the space should give a positive value to the plane's corresponding node in the next layer and which part should give a negative input to it. It actually cuts that space into two halves. If you consider the input space of a multi-layer feed forward network, the weights of the first layer parametrize hyperplanes, however in the next layers they can represent non-linear surfaces in the input space.</p>\n", "question": "<p>In a feedforward neural network the inputs are fed directly to the outputs via a series of <strong>weights</strong>.</p>\n\n<p>What purpose do the weights serve and how are they significant in this neural network?</p>\n"}, "id": "132"}, {"body": {"answer": "<p>The answer to a lot of those questions depends on how the device is programmed. A computer capable of driving around and recognizing where the road goes is likely to have the ability to visually distinguish a human from an animal, whether that be based on outline, image, or size. With sufficiently sharp image recognition, it might be able to count the number and kind of people in another vehicle. It could even use existing data on the likelihood of injury to people in different kinds of vehicles.</p>\n\n<p>Ultimately, people disagree on the ethical choices involved. Perhaps there could be \"ethics settings\" for the user/owner to configure, like \"consider life count only\" vs. \"younger lives are more valuable.\" I personally would think it's not terribly controversial that a machine should damage itself before harming a human, but people disagree on how important pet lives are. If explicit kill-this-first settings make people uneasy, the answers could be determined from a questionnaire given to the user.</p>\n", "question": "<p>Obviously driverless cars aren't perfect, so imagine that the Google car (as an example) got into difficult situation.</p>\n\n<p>Here are a few examples of unfortunate situations caused by set of events:</p>\n\n<ul>\n<li>the car is heading toward a crowd of 10 people crossing the road, so it cannot stop in time, but it can avoid killing 10 people by hitting the wall (killing the passengers),</li>\n<li>avoiding killing the rider of the motorcycle considering that the probability of survival is greater for the passenger of the car,</li>\n<li>killing animal on the street in favour of human being,</li>\n<li>changing lanes to crash into another car to avoid killing a dog,</li>\n</ul>\n\n<p>And here are few dilemmas:</p>\n\n<ul>\n<li>Does the algorithm recognize the difference between a human being and an animal?</li>\n<li>Does the size of the human being or animal matter?</li>\n<li>Does it count how many passengers it has vs. people in the front?</li>\n<li>Does it \"know\" when babies/children are on board?</li>\n<li>Does it take into the account the age (e.g. killing the older first)?</li>\n</ul>\n\n<p>How would an algorithm decide what should it do from the technical perspective? Is it being aware of above (counting the probability of kills), or not (killing people just to avoid its own destruction)?</p>\n\n<p>Related articles:</p>\n\n<ul>\n<li><a href=\"https://www.technologyreview.com/s/542626/why-self-driving-cars-must-be-programmed-to-kill/\">Why Self-Driving Cars Must Be Programmed to Kill</a></li>\n<li><a href=\"https://www.technologyreview.com/s/539731/how-to-help-self-driving-cars-make-ethical-decisions/\">How to Help Self-Driving Cars Make Ethical Decisions</a></li>\n</ul>\n"}, "id": "134"}, {"body": {"answer": "<p>The graph that represents a Deep Boltzmann Machine can be any weighted undirected graph. \nHowever, the graph that represents a Deep Belief Network must be a connection of graphs that represent Restricted Boltzmann Machines. Those graphs are biparite, so there are two groups of vertices in those graphs so that every edge connects two vertices from different groups. Those groups are usually the visible and hidden components of the machine. </p>\n\n<p>Learning is hard and impractical and hard in a general Deep Boltzmann Machine, but easier and practical in a  Restricted Boltzmann Machine and hence in a  Deep Belief Network, which is a connection of some of these machines. </p>\n", "question": "<p>What are the main differences between <a href=\"https://en.wikipedia.org/wiki/Boltzmann_machine\" rel=\"nofollow\">Deep Boltzmann Machines</a> (DBM) recurrent neural network and <a href=\"https://en.wikipedia.org/wiki/Deep_belief_network\" rel=\"nofollow\">Deep Belief Network</a> (which is based on RBMs)?</p>\n"}, "id": "135"}, {"body": {"answer": "<p>First of all, for a detailed view of the argument, check out the <a href=\"http://plato.stanford.edu/entries/chinese-room/\" rel=\"nofollow\">SEP entry on the Chinese Room</a>.</p>\n\n<p>I consider the CRA as an indicator of you definition of intelligence. If the argument holds, yes, the person in the room understands Chinese. However, let's sum up the three replies discussed in the SEP entry:</p>\n\n<ol>\n<li><p>The <em>man</em> himself doesn't understand Chinese (he wouldn't be able to understand it when outside the room), but the <em>system</em> man+room understands it. Accepting that reply suggests that there can exist an intelligent system which parts aren't themselves intelligent (which can be argued of the human body itself).</p></li>\n<li><p>The system doesn't understand Chinese, as it cannot interact with the world in the same way a robot or a human could (i.e. it cannot learn, is limited in the set of questions it can answer)</p></li>\n<li><p>The system doesn't understand Chinese (depending on your definition of <em>understanding</em>), and you couldn't say a human performing the same feats as the Chinese room understands Chinese either.</p></li>\n</ol>\n\n<p>So whether the argument, or a variant of it holds, depends on your definitions of <em>intelligent</em>, <em>understanding</em>, on how you define the system, etc. The point being that the thought experiment is a nice way to differentiate between the definitions (and many, many debates have been held about them), in order to avoid talking past each other endlessly.</p>\n", "question": "<p>Back in college, I had a Complexity Theory teacher who stated that artificial intelligence was a contradiction in terms. If it could be calculated mechanically, he argued, it wasn't intelligence.</p>\n\n<p>This seems to be a variant of the Chinese Room argument. This argument is a metaphor, where  a person is put in a room full of Chinese books. This person doesn't understand a word of Chinese, but is slipped messages in Chinese under the door. The person has to use the books, which contain transformation rules, to answer these messages. The person can apply the transformation rules, but does not understand what (s)he is communicating.</p>\n\n<p>Does the chinese room argument hold? Can we argue that artificial intelligence is merely clever algorithmics?</p>\n"}, "id": "137"}, {"body": {"answer": "<p>Generalization error is the error obtained by applying a model to data it has not seem before. So, if you want to measure generalization error, you need to remove a subset from your data and don't train your model on it. After training, you verify your model accuracy (or other performance measure) on the subset you have removed, since your model hasn't seem it before. Hence, this subset is called a \"<a href=\"https://en.wikipedia.org/wiki/Test_set\" rel=\"nofollow\">test set</a>\". </p>\n\n<p>Additionally, another subset can also be used for parameter selection, which we call a \"<a href=\"https://en.wikipedia.org/wiki/Test_set#Validation_set\" rel=\"nofollow\">validation set</a>\". We can't use the training set for parameter tuning, since it does not measure generalization error, but we can't use the test set too, since our parameter tuning would overfit test data. That's why we need a third subset.</p>\n\n<p>Finally, in order to obtain more predictive performance measures, we can use many different train/test partitions and average the results. This is called \"<a href=\"https://en.wikipedia.org/wiki/Cross-validation_(statistics)\" rel=\"nofollow\">cross-validation</a>\".</p>\n", "question": "<p>How would you estimate the generalisation error? What are the methods of achieving this?</p>\n"}, "id": "138"}, {"body": {"answer": "<p>There's no objective reason to state that declarative languages are better suited for AI development. However, there's indeed a bias towards them in practice. Although most functional languages are impure (that is, they allow side effects), and such can't count as \"declarative\", a few languages are purely functional (that is, they don't allow side effects), most prominently <a href=\"https://en.wikipedia.org/wiki/Declarative_programming#Functional_programming\" rel=\"nofollow\">Haskell</a>. Purity is key here. In Haskell, <a href=\"http://stackoverflow.com/a/4066401/5249858\">even I/O is pure</a>.</p>\n\n<p>The key difference between imperative languages and (purely) functional languages is in the way they describe the program. An imperative program describes <em>how</em> to do stuff, that is, algorithms. It specifies the specific instructions that the machine must carry on in order to perform the computation. OTOH, purely functional languages describe <em>what</em> is to be computed, that is, the relationship between the input and the output. In mathematics, \"function\" is just a fancy name for a relationship between an input and an output.</p>\n\n<p>Again, in the mathematical sense, the only variability is that of the function's arguments. That is, the function's output depends solely on its input (arguments). This is known as <a href=\"https://en.wikipedia.org/wiki/Referential_transparency\" rel=\"nofollow\">referential transparency</a>. Referential transparency states that:</p>\n\n<p><a href=\"http://i.stack.imgur.com/ssAGM.png\" rel=\"nofollow\"><img src=\"http://i.stack.imgur.com/ssAGM.png\" alt=\"enter image description here\"></a></p>\n\n<p>Where <code>\u03d5</code> is the set of all functions, and <code>\u03b4(f)</code> is <code>f</code>'s domain. For the typical imperative language's definition of \"function\", the above doesn't hold. For instance, C's <code>getchar()</code> does not always return the same value.</p>\n\n<p>Let's say we want to calculate the set of the ten least prime numbers whose least significant digit is 3. First, in mathematical notation:</p>\n\n<p><a href=\"http://i.stack.imgur.com/F94i9.png\" rel=\"nofollow\"><img src=\"http://i.stack.imgur.com/F94i9.png\" alt=\"enter image description here\"></a></p>\n\n<p>Where <code>G(n, s)</code> is the set of the lesser <code>n</code>th elements from <code>s</code>. In mathematics, you don't worry about how is the set <code>S</code> supposed to be computed, but rather about <code>S</code>'s definition itself.</p>\n\n<p>Now, in Python (in imperative style):</p>\n\n<pre><code>def is_prime(n):\n  for x in range(2, n):\n    if n % x == 0:\n      return False\n\n  return True\n\ndef foo():\n  s = set()\n  n = 2\n  while len(s) &lt; 10:\n    if is_prime(n) and n % 10 == 3:\n       s.append(n)\n\n  return set(s)\n</code></pre>\n\n<p>In Python, we care about (and are responsible for) the algorithm being used to compute the set. We specify, pretty much in recipe-style, how to build the set from scratch. If there's an algorithm that may be better suited for checking whether a number is prime or odd, but we don't use it, it's our fault, not Python's.</p>\n\n<p>Finally, Haskell steps in:</p>\n\n<pre><code>import qualified Data.Set as Set\n\nisPrime :: Integer -&gt; Bool\nisPrime n = ( == 1 ) . length . filter ( == 0 ) . map ( n `mod` ) $ [ 2 .. n ]\n\ns = Set.fromList . take 10 . filter ( ( == 3 ) . ( `mod` 10 ) ) . filter isPrime $ [1..]\n</code></pre>\n\n<p>Haskell's version is a lot more like the mathematical model than Python is. We define the <code>isPrime</code> function in terms of the constraints that a prime number must obey, not by describing a step-by-step algorithm to do such a check. Moreover, <code>s</code> (the set we have been defining so far) is defined in terms of the constraints its members must obey, rather than in terms of an algorithm to compute <code>s</code> itself. The compiler, more often than not <a href=\"https://downloads.haskell.org/~ghc/7.8.3/docs/html/users_guide/\" rel=\"nofollow\">The Glorious Glasgow Haskell Compilation System</a> (a.k.a GHC), is the one responsible for generating an algorithm. GHC's optimizer is known to be one of the strongest in the world, not because its the best compiler of 'em all, but because Haskell's nature allows for this.</p>\n\n<p>Haskell (and other functional languages), in summary, have several features that make it look, taste, and behave like pure math:</p>\n\n<ul>\n<li><p>Referential transparency and purity.</p></li>\n<li><p><a href=\"https://en.wikipedia.org/wiki/Lazy_evaluation\" rel=\"nofollow\">Haskell is lazy</a>.</p></li>\n<li><p><a href=\"http://programmers.stackexchange.com/questions/279316/what-exactly-makes-the-haskell-type-system-so-revered-vs-say-java\">A <strong>very</strong> strong type system</a>, with such exotic (but very useful!) stuff as <a href=\"https://en.wikipedia.org/wiki/Recursive_data_type\" rel=\"nofollow\">recursive</a> (and <a href=\"https://en.wikipedia.org/wiki/Algebraic_data_type\" rel=\"nofollow\">algebraic</a>) data types.</p></li>\n</ul>\n\n<p>So, the bottom line is that AI researchers often prefer functional languages over imperative languages (or, more assertively, pure over impure languages), because they are attempting to <em>define</em> artificial intelligence itself, by means of functions (relationship between an input and an output). At the end, we do this because we have no real algorithm for human-level intelligence to raise from a handful of transistors. Also, there's been a historical bias towards these kind of languages, starting with <a href=\"https://en.wikipedia.org/wiki/John_McCarthy_(computer_scientist)\" rel=\"nofollow\">John McCarthy</a>, Lisp's creator and a pioneer in early AI research.</p>\n", "question": "<p>What specific advantages of declarative languages make them more applicable to AI than imperative languages?  What can declarative languages do easily that other languages styles find difficult for this kind of problem?</p>\n"}, "id": "139"}, {"body": {"answer": "<p>The terms <em>strong</em> and <em>weak</em> don't actually refer to processing, or optimization power, or any interpretation leading to \"strong AI\" being <em>stronger</em> than \"weak AI\". It holds conveniently in practice, but the terms come from elsewhere. In 1980, <a href=\"https://en.wikipedia.org/wiki/John_Searle\">John Searle</a> coined the following statements:</p>\n\n<ul>\n<li>AI hypothesis, strong form: an AI system can <em>think</em> and have a <em>mind</em> (in the philosophical definition of the term);</li>\n<li>AI hypothesis, weak form: an AI system can only <em>act</em> like it thinks and has a mind.</li>\n</ul>\n\n<p>So <em>strong AI</em> is a shortcut for an AI systems that verifies the <em>strong AI hypothesis</em>. Similarly, for the weak form. The terms have then evolved: strong AI refers to AI that performs as well as humans (who have minds), weak AI refers to AI that doesn't.</p>\n\n<p>The problem with these definitions is that they're fuzzy. For example, <a href=\"https://en.wikipedia.org/wiki/AlphaGo\">AlphaGo</a> is an example of weak AI, but is \"strong\" by Go-playing standards. A hypothetical AI replicating a human baby would be a strong AI, while being \"weak\" at most tasks.</p>\n\n<p>Other terms exist: <a href=\"https://en.wikipedia.org/wiki/Artificial_general_intelligence\">Artificial General Intelligence</a> (AGI), which has cross-domain capability (like humans), can learn from a wide range of experiences (like humans), among other features. Artificial Narrow Intelligence refers to systems bound to a certain range of tasks (where they may nevertheless have superhuman ability), lacking capacity to significantly improve themselves.</p>\n\n<p>Beyond AGI, we find Artificial Superintelligence (ASI), based on the idea that a system with the capabilities of an AGI, without the physical limitations of humans would learn and improve far beyond human level.</p>\n", "question": "<p>I've heard the terms strong-AI and weak-AI used.  Are these well defined terms or subjective ones?  How are they generally defined?</p>\n"}, "id": "141"}, {"body": {"answer": "<p>In some iterative learning methods the more iterations you apply the more specific your model becomes about the training set. If there are too much iterations, your model will become too specifically trained for the training samples and will score less on other samples that are not seen during the training phase. This is call over-fitting, though over-fitting is not specific to iterative learning methods.</p>\n\n<p>One solution to prevent over-fitting in these iterative learning algorithms is early stopping. Normally a control group of samples called validation samples (validation set) are used to validate the model and notify when it starts to over-fit. The validation set is not used by the training algorithm however its corresponding outputs are known and after each iteration its samples are employed to measure how good the model currently works. As soon as the performance on the validation set stops to grow and starts to drop we stop iterating the training algorithm. This is called early stopping which can help to maximize the generalization power of our learned model.</p>\n\n<p>Note that if we use the training set itself for validation the performance will always increase because that is what the learning algorithm is designed to do. However the learning algorithm does not know how specifically it should learn the training set and that is why we need methods like early stopping.</p>\n", "question": "<p>What is the \"early stopping\" and what are the advantages using this method? How does it help exactly.</p>\n"}, "id": "142"}, {"body": {"answer": "<ul>\n<li>An ability that is commonly attributed to intelligence is <strong>problem solving</strong>. </li>\n<li>Another one is <strong>learning</strong> (improving itself from experience).</li>\n<li>Artificial intelligence can be defined as \"replicating intelligence, or parts of it, at least in appearance, inside a computer\" (dodging the definition of intelligence itself).</li>\n<li>Genetic algorithms are computational <strong>problem solving</strong> tools that find and improve solutions (they <strong>learn</strong>).</li>\n</ul>\n\n<p>Thus, genetic algorithms are a kind of artificial intelligence.</p>\n\n<p>Regarding scale, I don't see it as an important factor for defining G.A. as A.I or not. The same way we can simply classify different living forms as more or less intelligent instead of just saying intelligent or not intelligent.</p>\n\n<p>Finally, let's just make an important distinction: our brains are the product of natural selection, but the brains themselves don't use the same principle in order to achieve intelligence.</p>\n", "question": "<p>Since human intelligence presumably is a function of a natural genetic algorithm in nature, is using a genetic algorithm in a computer an example of artificial intelligence?  If not, how do they differ?  Or perhaps some are and some are not expressing artificial intelligence depending upon the scale of the algorithm and what it evolves into?</p>\n"}, "id": "143"}, {"body": {"answer": "<p>I believe this argument is based on the fact that intelligence is a single dimension when it really isn't. Are machines and humans really on the same level if a machine can solve a complex problem in a millionth of the time a human can? </p>\n\n<p>It also assumes that the Turing machine is still the best computational model for the time period that you are in, which is not necessarily true for the future, it is just true until this point in time. </p>\n", "question": "<p>An ultraintelligent machine is a machine that can surpass all intellectual activities by any human, and such machine is often used in science fiction as a machine that brings mankind to an end. </p>\n\n<p>Any machine is executed using an algorithm. By the Church-Turing thesis, any algorithm that can be executed by a modern computer can be executed by a Turing Machine. However, a human can easily simulate a Turing Machine. Doesn't this mean that a machine can't surpass all intellectual activities, since we can also execute the algorithm?</p>\n\n<p>This argument is most likely flawed, since my intuition tells me that  ultraintelligent machines are possible. However, it is not clear to me where the flaw is. Note that this is my own argument. </p>\n"}, "id": "144"}, {"body": {"answer": "<p>A quantum computer has a huge amount of internal state that even the machine can't get at directly. (You can only sample the matrix state.) The amount of that state goes up exponentially with each quantum bit involved in the system. Some operations get insane speedups from quantum computing: you just put the quantum wire through a quantum gate and you've updated the entire matrix at once.</p>\n\n<p>Simulating a quantum computer with a classical one would take exponentially longer for each qubit. With several dozen qubits, the machine's computing power for some tasks couldn't even be approached by a normal computer, much less a human mind.</p>\n\n<p>Relevant: my answer on <a href=\"http://ai.stackexchange.com/a/114/75\">To what extent can quantum computers help to develop Artificial Intelligence?</a></p>\n\n<p>Note that with quantum computers, you've gone beyond the normal zeroes and ones. You then need a <a href=\"https://en.wikipedia.org/wiki/Quantum_Turing_machine\" rel=\"nofollow\">quantum Turing machine</a>, which is a generalization of the classical one.</p>\n", "question": "<p>An ultraintelligent machine is a machine that can surpass all intellectual activities by any human, and such machine is often used in science fiction as a machine that brings mankind to an end. </p>\n\n<p>Any machine is executed using an algorithm. By the Church-Turing thesis, any algorithm that can be executed by a modern computer can be executed by a Turing Machine. However, a human can easily simulate a Turing Machine. Doesn't this mean that a machine can't surpass all intellectual activities, since we can also execute the algorithm?</p>\n\n<p>This argument is most likely flawed, since my intuition tells me that  ultraintelligent machines are possible. However, it is not clear to me where the flaw is. Note that this is my own argument. </p>\n"}, "id": "149"}, {"body": {"answer": "<p>Not really.</p>\n\n<p>Neural networks are good for determining non-linear relationships between inputs when there are hidden variables. In the examples above the relationships are linear, and there are no hidden variables. But even if they were non-linear, a traditional ANN design would not be well suited to accomplish this.</p>\n\n<p>By carefully constructing the layers and tightly supervising the training, you could get a network to consistently produce the output 4.01, say, for the inputs: 2, 1 (+), and 2, but this is not only wrong, it's an inherently unreliable application of the technology.</p>\n", "question": "<p>I'm aware that neural networks are probably not designed to do that, however asking hypothetically, is it possible to train the deep neural network (or similar) to solve math equations?</p>\n\n<p>So given the 3 inputs: 1st number, operator sign represented by the number (1 - <code>+</code>, 2 - <code>-</code>, 3 - <code>/</code>, 4 - <code>*</code>, and so on), and the 2nd number, then after training the network should give me the valid results.</p>\n\n<p>Example 1 (<code>2+2</code>):</p>\n\n<ul>\n<li>Input 1: <code>2</code>; Input 2: <code>1</code> (<code>+</code>); Input: <code>2</code>; Expected output: <code>4</code></li>\n<li>Input 1: <code>10</code>; Input 2: <code>2</code> (<code>-</code>); Input: <code>10</code>; Expected output: <code>0</code></li>\n<li>Input 1: <code>5</code>; Input 2: <code>4</code> (<code>*</code>); Input: <code>5</code>; Expected output: <code>25</code></li>\n<li>and so</li>\n</ul>\n\n<p>The above can be extended to more sophisticated examples.</p>\n\n<p>Is that possible? If so, what kind of network can learn/achieve that?</p>\n"}, "id": "155"}, {"body": {"answer": "<p>Yes, it has been done!</p>\n\n<p>However, the applications aren't to replace calculators or anything like that. The lab I'm associated with develops neural network models of equational reasoning to better understand how humans might solve these problems. This is a part of the field known as <a href=\"http://psych.stanford.edu/~jlm/\">Mathematical Cognition</a>. Unfortunately, our website isn't terribly informative, but here's a <a href=\"http://web.stanford.edu/~kmickey/pdf/MickeyMcClelland2014.pdf\">link</a> to an example of such work.</p>\n\n<p>Apart from that, recent work on extending neural networks to include external memory stores (e.g. Neural Turing Machines) tend to use solving math problems as a good proof of concept. This is because many arithmetic problems involve long procedures with stored intermediate results. See the sections of <a href=\"http://arxiv.org/pdf/1511.08228.pdf\">this paper</a> on long binary addition and multiplication.</p>\n", "question": "<p>I'm aware that neural networks are probably not designed to do that, however asking hypothetically, is it possible to train the deep neural network (or similar) to solve math equations?</p>\n\n<p>So given the 3 inputs: 1st number, operator sign represented by the number (1 - <code>+</code>, 2 - <code>-</code>, 3 - <code>/</code>, 4 - <code>*</code>, and so on), and the 2nd number, then after training the network should give me the valid results.</p>\n\n<p>Example 1 (<code>2+2</code>):</p>\n\n<ul>\n<li>Input 1: <code>2</code>; Input 2: <code>1</code> (<code>+</code>); Input: <code>2</code>; Expected output: <code>4</code></li>\n<li>Input 1: <code>10</code>; Input 2: <code>2</code> (<code>-</code>); Input: <code>10</code>; Expected output: <code>0</code></li>\n<li>Input 1: <code>5</code>; Input 2: <code>4</code> (<code>*</code>); Input: <code>5</code>; Expected output: <code>25</code></li>\n<li>and so</li>\n</ul>\n\n<p>The above can be extended to more sophisticated examples.</p>\n\n<p>Is that possible? If so, what kind of network can learn/achieve that?</p>\n"}, "id": "158"}, {"body": {"answer": "<p>This is the well known <a href=\"https://en.wikipedia.org/wiki/Trolley_problem\" rel=\"nofollow\"><em>Trolley Problem</em></a>. As <a href=\"http://ai.stackexchange.com/a/134/8\">Ben N</a> said, people disagree on the right course of action for trolley problem scenarios, but it should be noted that with self-driving cars, reliability is so high that these scenarios are really unlikely. So, not much effort will be put into the problems you are describing, at least in the short term.</p>\n", "question": "<p>Obviously driverless cars aren't perfect, so imagine that the Google car (as an example) got into difficult situation.</p>\n\n<p>Here are a few examples of unfortunate situations caused by set of events:</p>\n\n<ul>\n<li>the car is heading toward a crowd of 10 people crossing the road, so it cannot stop in time, but it can avoid killing 10 people by hitting the wall (killing the passengers),</li>\n<li>avoiding killing the rider of the motorcycle considering that the probability of survival is greater for the passenger of the car,</li>\n<li>killing animal on the street in favour of human being,</li>\n<li>changing lanes to crash into another car to avoid killing a dog,</li>\n</ul>\n\n<p>And here are few dilemmas:</p>\n\n<ul>\n<li>Does the algorithm recognize the difference between a human being and an animal?</li>\n<li>Does the size of the human being or animal matter?</li>\n<li>Does it count how many passengers it has vs. people in the front?</li>\n<li>Does it \"know\" when babies/children are on board?</li>\n<li>Does it take into the account the age (e.g. killing the older first)?</li>\n</ul>\n\n<p>How would an algorithm decide what should it do from the technical perspective? Is it being aware of above (counting the probability of kills), or not (killing people just to avoid its own destruction)?</p>\n\n<p>Related articles:</p>\n\n<ul>\n<li><a href=\"https://www.technologyreview.com/s/542626/why-self-driving-cars-must-be-programmed-to-kill/\">Why Self-Driving Cars Must Be Programmed to Kill</a></li>\n<li><a href=\"https://www.technologyreview.com/s/539731/how-to-help-self-driving-cars-make-ethical-decisions/\">How to Help Self-Driving Cars Make Ethical Decisions</a></li>\n</ul>\n"}, "id": "161"}, {"body": {"answer": "<p>For a driverless car that is designed by a single entity, the best way for it to make decisions about whom to kill is by estimating and minimizing the probable liability.</p>\n\n<p>It doesn't need to absolutely correctly identify all the potential victims in the area to have a defense for its decision, only to identify them as well as a human could be expected to.</p>\n\n<p>It doesn't even need to know the age and physical condition of everyone in the car, as it can ask for that information and if refused, has the defense that the passengers chose not to provide it, and therefore took responsibility for depriving it of the ability to make a better decision.</p>\n\n<p>It only has to have a viable model for minimizing exposure of the entity to lawsuits, which can then be improved over time to make it more profitable.</p>\n", "question": "<p>Obviously driverless cars aren't perfect, so imagine that the Google car (as an example) got into difficult situation.</p>\n\n<p>Here are a few examples of unfortunate situations caused by set of events:</p>\n\n<ul>\n<li>the car is heading toward a crowd of 10 people crossing the road, so it cannot stop in time, but it can avoid killing 10 people by hitting the wall (killing the passengers),</li>\n<li>avoiding killing the rider of the motorcycle considering that the probability of survival is greater for the passenger of the car,</li>\n<li>killing animal on the street in favour of human being,</li>\n<li>changing lanes to crash into another car to avoid killing a dog,</li>\n</ul>\n\n<p>And here are few dilemmas:</p>\n\n<ul>\n<li>Does the algorithm recognize the difference between a human being and an animal?</li>\n<li>Does the size of the human being or animal matter?</li>\n<li>Does it count how many passengers it has vs. people in the front?</li>\n<li>Does it \"know\" when babies/children are on board?</li>\n<li>Does it take into the account the age (e.g. killing the older first)?</li>\n</ul>\n\n<p>How would an algorithm decide what should it do from the technical perspective? Is it being aware of above (counting the probability of kills), or not (killing people just to avoid its own destruction)?</p>\n\n<p>Related articles:</p>\n\n<ul>\n<li><a href=\"https://www.technologyreview.com/s/542626/why-self-driving-cars-must-be-programmed-to-kill/\">Why Self-Driving Cars Must Be Programmed to Kill</a></li>\n<li><a href=\"https://www.technologyreview.com/s/539731/how-to-help-self-driving-cars-make-ethical-decisions/\">How to Help Self-Driving Cars Make Ethical Decisions</a></li>\n</ul>\n"}, "id": "162"}, {"body": {"answer": "<p>Well I do not know what type of features you are giving to your neural network. However in general I would go with a single neural network. It seems that you have no limitation in resources for training your network and the only problem is resources while you apply your network. </p>\n\n<p>The thing is that probably the two problems have things in common (e.g. both types of plates are rectangular). This means that if you use two networks, each has to solve the same sub-problem (the common part) again. If you use only one network the common part of the problem takes fewer cells/weights to be solved and the remaining weights/cells can be employed for better recognition.</p>\n\n<p>At the end if I was in your place I would try both of them. I think that is the only way to be really sure what is the best solution. When speaking theoretically it is possible that we do not include some factors.</p>\n", "question": "<p>In particular, an embedded computer (limited resources) analyzes live video stream from a traffic camera, trying to pick good frames that contain license plate numbers of passing cars. Once a plate is located, the frame is handed over to an OCR library to extract the registration and use it further.</p>\n\n<p>In my country two types of license plates are in common use - rectangular (the typical) and square - actually, somewhat rectangular but \"higher than wider\", with the registration split over two rows.</p>\n\n<p>(there are some more types, but let us disregard them; they are a small percent and usually belong to vehicles that lie outside our interest.)</p>\n\n<p>Due to the limited resources and need for rapid, realtime processing, the maximum size of the network (number of cells and connections) the system can handle is fixed.</p>\n\n<p>Would it be better to split this into two smaller networks, each recognizing one type of registration plates, or will the larger single network handle the two types better?</p>\n"}, "id": "163"}, {"body": {"answer": "<p>LISP is still used significantly, but less and less. There is still momentum due to so many people using it in the past, who are still active in the industry or research (anecdote: the last VCR was produced by a Japanese maker in July 2016, yes). The language is however used (to my knowledge) for the kind of AI that does not leverage Machine Learning, typically as the reference books from Russell and Norvig. These applications are still very useful, but Machine Learning gets all the steam these days.</p>\n\n<p>Another reason for the decline is that LISP practitioners have partially moved to Clojure and other recent languages.</p>\n\n<p>If you are learning about AI technologies, LISP (or Scheme or Prolog) is good choice to understand what is going on with \"AI\" at large. But if you wish or have to be very pragmatic, Python or R are the community choices</p>\n\n<p>Note: The above lacks concrete example and reference. I am aware of some work in universities, and some companies inspired by or directly using LISP.</p>\n\n<hr>\n\n<p>To add on @Harsh's answer, LISP (and Scheme, and Prolog) has qualities that made it look like it was better suited for creating intelligent mechanisms---making AI as perceived in the 60s.</p>\n\n<p>One of the qualities was that the language design leads the developer to think in a quite elegant way, to decompose a big problem into small problems, etc. Quite \"clever\", or \"intelligent\" if you will. Compared to some other languages, there is almost no choice but to develop that way. LISP is a list processing language, and \"purely functional\".</p>\n\n<p>One problem, though, can be seen in work related to LISP. A notable one in the AI domain is the work on the <a href=\"https://en.wikipedia.org/wiki/Situation_calculus\" rel=\"nofollow\">Situation Calculus</a>, where (in short) one describes objects and rules in a \"world\", and can let it evolve to compute <em>situations</em>---states of the world. So it is a model for reasoning on situations. The main problem is called the <a href=\"https://en.wikipedia.org/wiki/Frame_problem\" rel=\"nofollow\">frame problem</a>, meaning this calculus cannot tell what does <em>not</em> change---just what changes. Anything that is not defined in the world cannot be processed (note the difference here with ML). First implementations used LISPs, because that was the AI language then. And there were bound by the frame problem. But, as @Harsh mentioned, it is not LISP's fault: Any language would face the same framing issue (a conceptual problem of the Situation Calculus).</p>\n\n<p>So the language really does not matter from the AI / AGI / ASI perspective. The concepts (algorithms, etc.) are really what matters.</p>\n\n<p>Even in Machine Learning, the language is just a practical choice. Python and R are popular today, primarily due to their library ecosystem and the focus of key companies. But try to use Python or R to run a model for a RaspberryPI-based application, and you will face some severe limitations (but still possible, I am doing it :-)). So the language choice burns down to pragmatism.</p>\n", "question": "<p>I know that language of <strong><code>Lisp</code></strong> was used early on when working on artificial intelligence problems.  Is it still being used today for significant work?  If not, is there a new language that has taken its place as the most common one being used for work in AI today?</p>\n"}, "id": "164"}, {"body": {"answer": "<p>The flaw in your argument is that \"surpass\" doesn't just mean that you should be able to run all algorithms, it includes a notion of complexity, i.e. how many time steps you will take to simulate an algorithm. </p>\n\n<p>How do you simulate an algorithm with a Turing machine? A <a href=\"https://en.wikipedia.org/wiki/Turing_machine\" rel=\"nofollow\">Turing machine</a> consists of a finite state machine and an infinite tape. A TM does run an algorithm, determined by its initial state and the state transition matrix, but what I think you are talking about is Universal Turing Machines (UTM)  that can read \"code\" (which is usually a description of another Turing machine) written on a \"code segment\" of the tape and then simulate that machine on input data written on the \"data segment\" of the tape.</p>\n\n<p>Turing machines can differ in the number of states in their finite state machines (and also in the alphabet they write on the tape but any finite alphabet is easily encoded in binary so this should not be the big reason for differences among Turing machines). So, you can have UTMs with bigger state machines and UTMs with smaller state machines. The bigger UTM could possibly surpass the smaller one if they use the same encoding for the \"code\" part of the tape.</p>\n\n<p>You can also play around with the code used to describe the TM being simulated. This code could be C++ for example, or could be a Neural network with the synapse strength written down as a matrix. Which description is better for computation depends on the problem.</p>\n\n<p>An example comparison among UTMs with different state machines: consider different compilers for the same language, say C++. Both of them will first compile C++ to assembly and then run another UTM which reads and executes assembly (your physical CPU). So, a better compiler will run the same code faster.</p>\n\n<p>Back to humans vs computers, humans are neural networks that run algorithms like those you would write in C++. This involves a costly and inefficient conversion of the algorithm into hand movements. A computer uses a compiler to convert C++ to assembly that it can run natively, so its able to do a much more efficient implementation of C++ code. Alternately, humans have a ton of neurons, and the neural code, i.e. synapse strength, is hard to read, so current computers cannot run that code yet.</p>\n", "question": "<p>An ultraintelligent machine is a machine that can surpass all intellectual activities by any human, and such machine is often used in science fiction as a machine that brings mankind to an end. </p>\n\n<p>Any machine is executed using an algorithm. By the Church-Turing thesis, any algorithm that can be executed by a modern computer can be executed by a Turing Machine. However, a human can easily simulate a Turing Machine. Doesn't this mean that a machine can't surpass all intellectual activities, since we can also execute the algorithm?</p>\n\n<p>This argument is most likely flawed, since my intuition tells me that  ultraintelligent machines are possible. However, it is not clear to me where the flaw is. Note that this is my own argument. </p>\n"}, "id": "165"}, {"body": {"answer": "<p>I definitely continue to often use Lisp when working on AI models.</p>\n\n<p>You asked if it is being used for <em>substantial</em> work.  That's too subjective for me to answer regarding my own work, but I queried one my AI models whether or not it considered itself substantial, and it replied with an affirmative response.  Of course, it's response is naturally biased as well.</p>\n\n<p>Overall, a significant amount of AI research and development is conducted in Lisp.  Furthermore, even for non-AI problems, Lisp is sometimes used.  To demonstrate the power of Lisp, I engineered the first neural network simulation system written entirely in Lisp over a quarter century ago.</p>\n", "question": "<p>I know that language of <strong><code>Lisp</code></strong> was used early on when working on artificial intelligence problems.  Is it still being used today for significant work?  If not, is there a new language that has taken its place as the most common one being used for work in AI today?</p>\n"}, "id": "166"}, {"body": {"answer": "<p>Does the halting problem imply any limits on human cognition?</p>\n\n<p>Yes, absolutely--that there are pieces of code a human could look at and not be sure whether or not it will halt in finite time. (Certainly there are pieces of code that a human can look at and say \"yes\" or \"no\" definitely, but we're talking about the ones that are actually quite difficult to analyze.)</p>\n\n<p>The halting problem means that there are types of code analysis that no computer could do, because it's mathematically impossible. But the realm of <em>possibility</em> is still large enough to allow strong artificial intelligence (in the sense of code that can understand itself well enough to improve itself).</p>\n", "question": "<p>Given the proven <a href=\"https://en.wikipedia.org/wiki/Halting_problem\">halting problem</a> for <a href=\"https://en.wikipedia.org/wiki/Turing_machine\">Turing machines</a>, can we infer limits on the ability of strong Artificial Intelligence?</p>\n"}, "id": "170"}, {"body": {"answer": "<p>In contrast to the <em>philosophical</em> definitions, which rely on terms like \"mind\" and \"think,\" there are also definitions that hinge on <em>observables</em>.</p>\n\n<p>That is, a Strong AI is an AI that understands itself well enough to self-improve. Even if it is philosophically not equivalent to a human, or unable to perform <em>all</em> cognitive tasks that a human can, this AI can still generate a tremendous amount of optimization power / good decision-making, and its creation would be of historic importance (to put it lightly).</p>\n\n<p>A Weak AI, in contrast, is an AI with no or limited ability to self-modify. A chessbot that runs on your laptop might have superhuman ability to play chess, but it can <em>only</em> play chess, and while it might tune its weights or its architecture and slowly improve, it cannot modify itself in a deep enough way to generalize to other tasks.</p>\n\n<p>Another way to think about this is that a Strong AI is an AI researcher in its own right, and a Weak AI is what AI researchers produce.</p>\n", "question": "<p>I've heard the terms strong-AI and weak-AI used.  Are these well defined terms or subjective ones?  How are they generally defined?</p>\n"}, "id": "171"}, {"body": {"answer": "<p>The halting problem is an example of a general phenomenon known as <a href=\"https://en.wikipedia.org/wiki/Undecidable_problem\" rel=\"nofollow\">Undecidability</a>, which shows that there are problems no Turing machine can solve in finite time. Let's consider the generalization that it is undecidable whether a Turing Machine satisfies some non-trivial property P, called <a href=\"https://en.wikipedia.org/wiki/Rice%27s_theorem\" rel=\"nofollow\">Rice's theorem</a>.</p>\n\n<p>First note that the halting problem applies only if the Turing machine takes arbitrarily long input. If the input is bounded, it is possible to enumerate all possible cases and the problem is no longer undecidable. It might still be inefficient to calculate it, but then we are turning to the complexity theory, which should be a separate question.</p>\n\n<p>Rice's theorem implies that an intelligence (a human) cannot be able to determine whether another intelligence (such as an AGI) possesses a certain property, such as being <a href=\"https://en.wikipedia.org/wiki/Friendly_artificial_intelligence\" rel=\"nofollow\">friendly</a>. This does not mean that we cannot design a Friendly AGI, but it does mean that we cannot check whether an arbitrary AGI is friendly. So, while we can possibly create an AI which is guaranteed to be friendly, we also need to ensure that IT cannot create another AI which is unfriendly.</p>\n", "question": "<p>Given the proven <a href=\"https://en.wikipedia.org/wiki/Halting_problem\">halting problem</a> for <a href=\"https://en.wikipedia.org/wiki/Turing_machine\">Turing machines</a>, can we infer limits on the ability of strong Artificial Intelligence?</p>\n"}, "id": "173"}, {"body": {"answer": "<p>The similarity of artificial neural networks and the human visual cortex goes very deep, and in many ways the human visual cortex was the inspiration for the techniques we use for the design and implementation of ANNs designed for image recognition. So in that direction, the similarity seems obvious to me.</p>\n\n<p>The reverse direction, though, is a question about how the human mind works under the influence of LSD, which you'll probably get a better answer asking about in the <a href=\"http://biology.stackexchange.com/\">biology</a> or <a href=\"http://cogsci.stackexchange.com/\">cognitive science</a> stack exchange sites.</p>\n\n<p>Some brief details to add to the answer, though: the human visual cortex is arranged in layers that correspond to increasing layers of abstraction. In the eyes themselves, photons are detected by light-sensitive cells and added together to make what are essentially the color elements of pixels. Those are then routed to another layer which does something like edge detection, and then the next layer does something like shape detection, and so on up to higher level concepts like \"a cat's face.' If LSD lowers the activation threshold for those neurons, or makes them more excitable, then more things will be interpreted as having the higher level concept (and so a patch of rough texture may have a face jump out of it, for example).</p>\n\n<p>The way that CNN \"deep dreaming' works is that the base image is amplified. That is, to make a particular patch look more like a dog, the shapes are nudged to be more dog-like, and the shapes nudge the edges, and the edges nudge the pixels.</p>\n", "question": "<p>In <a href=\"https://en.wikipedia.org/wiki/DeepDream\" rel=\"nofollow\">DeepDream</a> wikipedia page it's suggested that a dreamlike images created by a convolutional neural network may be related to how visual cortex works in humans when they're tripping.</p>\n\n<blockquote>\n  <p>The imagery to LSD- and psilocybin-induced hallucinations is suggestive of a functional resemblance between artificial neural networks and particular layers of the visual cortex.</p>\n</blockquote>\n\n<p>How this is even possible?</p>\n\n<p>How exactly convolutional neural networks have anything to do with human visual cortex?</p>\n"}, "id": "174"}, {"body": {"answer": "<p>Handwritten digit recognition is a standard benchmark in Machine Learning in the form of the <a href=\"https://en.wikipedia.org/wiki/MNIST_database\">MNIST dataset</a>. For example, <a href=\"http://scikit-learn.org/stable/auto_examples/classification/plot_digits_classification.html\">scikit-learn</a>, a python package for Machine Learning uses it as a tutorial example. </p>\n\n<p>The paper you cite uses this standard task as a proof of concept, to show that their system works.</p>\n", "question": "<p>This 2014 <a href=\"https://medium.com/the-physics-arxiv-blog/first-demonstration-of-artificial-intelligence-on-a-quantum-computer-17a6b9d1c5fb\" rel=\"nofollow\">article</a> saying that a Chinese team of physicists have trained a quantum computer to recognise handwritten characters.</p>\n\n<p><strong>Why did they have to use a quantum computer</strong> to do that?</p>\n\n<p>Is it just for fun and demonstration, or is it that recognising the handwritten characters is so difficult that standard (non-quantum) computers or algorithms cannot do that?</p>\n\n<p>If standard computers can achieve the same thing, what are the benefits of using quantum computers to do that then over standard methods?</p>\n"}, "id": "176"}, {"body": {"answer": "<p>As for your comment about a computer program showing lower emotional intelligence, you may find Eliza (which you can try <a href=\"http://manifestation.com/neurotoys/eliza.php3/\" rel=\"nofollow\">here</a>) interesting. It is a classical in the history of AI, and pretends to mimic an analyst (psychology).</p>\n\n<p>However, I think your question fits nowadays more in the field of <a href=\"https://en.wikipedia.org/wiki/Human%E2%80%93robot_interaction\" rel=\"nofollow\">Human-Robot Interaction</a>, which relies largely on <a href=\"http://nordicapis.com/20-emotion-recognition-apis-that-will-leave-you-impressed-and-concerned/\" rel=\"nofollow\">vision</a> for recognition of gestures and follow movements, as well as <em>soft, natural</em> movements as a response. Note that the movements of the face and hands belong to the most complex tasks, involving many muscles at a time. </p>\n\n<p>I strongly recommend the film <a href=\"http://www.plugandpray-film.de/en/trailer.html\" rel=\"nofollow\">Plug&amp;Pray</a> to have an idea of what people are researching in this area.</p>\n\n<p>On the <em>purely human</em> end of the scale, I sometimes wonder about our (my) emotional intelligence myself. Would I want to implement such an intelligence in an artificial agent at all?</p>\n\n<hr>\n\n<p>I remember why I thought of Eliza: not because of its emotional intelligence, but because it was <a href=\"http://www.alicebot.org/articles/wallace/eliza.html\" rel=\"nofollow\">apparently taken seriously</a> by a couple of humans. Could this be taken as a sort of (approved) Turing test? What does it say about the humans it met?</p>\n", "question": "<p>I've seen emotional intelligence defined as the capacity to be aware of, control, and express one's emotions, and to handle interpersonal relationships judiciously and empathetically.  What are some strategies for artificial intelligence to begin to tackle this problem and develop emotional intelligence for computers?  Are there examples where this is already happening to a degree today?  For example, wouldn't a computer that passes a Turing test necessarily express emotional intelligence or it would be seen as an obvious computer?  Perhaps that is why early programs that pass the test represented young people, who presumably have lower emotional intelligence.</p>\n"}, "id": "177"}, {"body": {"answer": "<p>This is known as the <a href=\"https://en.wikipedia.org/wiki/Intelligence_explosion\" rel=\"nofollow\">Intelligence Explosion</a> hypothesis or <a href=\"https://wiki.lesswrong.com/wiki/Recursive_self-improvement\" rel=\"nofollow\">Recursive self-improvement</a></p>\n", "question": "<p>Is it possible that at some time in the future, AIs will be able to initiatively develop themselves, rather than passively being developed by humanity?</p>\n"}, "id": "178"}, {"body": {"answer": "<p>There is a technique called <code>Pruning</code> in neural networks, which is used just for this same purpose.</p>\n\n<p>The pruning is done on the number of hidden layers. The process is very similar to the pruning process of decision trees. The pruning process is done as follows:</p>\n\n<ul>\n<li>Train a large, densely connected, network with a standard training\nalgorithm</li>\n<li>Examine the trained network to assess the relative importance of the\nweights</li>\n<li>Remove the least important weight(s)</li>\n<li>retrain the pruned network</li>\n<li>Repeat steps 2-4 until satisfied</li>\n</ul>\n\n<p>However, there are <a href=\"https://arxiv.org/abs/1510.00149\">several optimized methods</a> for pruning neural nets, and it is also a <a href=\"http://www.idiap.ch/ftp/reports/1997/rr97-03.pdf\">very active area of research</a>.</p>\n", "question": "<p>How to decide the optimum number of layers to be created while implementing a Neural Network (Feedforward, back propagation or RNN)?</p>\n"}, "id": "183"}, {"body": {"answer": "<p>Humans might create somewhere in the future a so-called ultraintelligent machine, a machine that can surpass all intellectual activities by any human. This would be the last invention man would need to do, since this machine is better in inventing machines than humans are (since that is an intellectual activity). Also, since humans can create machines as good as the ultraintelligent machine, this machine can create better machines, which in turn can create better machines, etcetera. This is known as the <a href=\"https://en.wikipedia.org/wiki/Intelligence_explosion\" rel=\"nofollow\">Intelligence explosion</a>, and it is also called recursive self-improvement (as has been pointed out by @Harsh).</p>\n\n<p>The existence, let alone the development, if an ultraintelligent machine is still hypothetical. We are nowhere close to creating an ultraintelligent machine.</p>\n", "question": "<p>Is it possible that at some time in the future, AIs will be able to initiatively develop themselves, rather than passively being developed by humanity?</p>\n"}, "id": "185"}, {"body": {"answer": "<blockquote>\n  <p>(this was intended as a comment, but turned out long and longer)</p>\n</blockquote>\n\n<p>A couple of points to elaborate on <a href=\"http://ai.stackexchange.com/a/73/70\">Ben's answer</a>:</p>\n\n<ul>\n<li>It is possible to generate different models (out of existing data!) and then look for the model that best fit new data (e.g. with <a href=\"https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm\" rel=\"nofollow\">knn</a>). Example: \n\n<ul>\n<li>States = {<em>sleep</em>, <em>eat</em>, <em>walk</em>, <em>work</em>}</li>\n<li>Model 1: Most probable sequence on weekdays, say: sleep \u2192 sleep \u2192 eat \u2192 walk \u2192 work \u2192 work \u2192 eat \u2192 walk \u2192 sleep  \u2192 sleep</li>\n<li>Model 2: Most probable sequence on weekends, some: sleep \u2192 sleep \u2192 eat \u2192 walk \u2192 eat \u2192 walk \u2192 sleep \u2192 sleep</li>\n<li>New data arrives: Which sequence is more probable that it came from? Check model 1, check model 2. Which fits better? \u2192 Assign</li>\n</ul></li>\n<li>Note that the previous example is oversimplified. Also note that a <em>unit time</em> is needed there (other than letters / words, for instance).</li>\n<li>You can <em>nest</em> Markov models. That means that you generate a model (a set of probabilities for all the states) in a \"lower scale\" and then use it in a more abstract model. For example, you can nest your day-scale model to a month or year (to include holidays, for instance).</li>\n</ul>\n\n<p>Also <a href=\"http://blog.wolfram.com/2013/02/04/centennial-of-markov-chains/\" rel=\"nofollow\">see this link for a nice introduction</a> and <a href=\"https://stats.stackexchange.com/questions/tagged/mcmc?sort=votes&amp;pageSize=50\">some posts in crossvalidated</a>.</p>\n\n<hr>\n\n<p>As for the question if artificial intelligence can be created by using this kind of methods, my personal (easy) answer would be <strong>no</strong>, because they only relate data and probabilities and thus belong more to the statistics and machine learning branch. </p>\n\n<p>A longer answer needs to take into account the <a href=\"http://ai.stackexchange.com/questions/74/what-is-the-difference-between-strong-ai-and-weak-ai\">weak vs. strong AI question</a>.</p>\n", "question": "<p>I believe a Markov chain is a sequence of events where each subsequent event depends probabilistically on the current event.  What are examples of the application of a Markov chain and can it be used to create artificial intelligence?  Would a genetic algorithm be an example of a Markov chain since each generation depends upon the state of the prior generation?</p>\n"}, "id": "187"}, {"body": {"answer": "<p>To my knowledge, this is very  much an open research issue.</p>\n\n<p>Here is a paper by Prof Leslie Smith, an acknowledged expert on neuromorphic perceptual coding, which explains the importance of the notion of perceptual time for Artificial General Intelligence and sketches an architecture from which a notion of 'now' might emerge: <a href=\"http://www.cs.stir.ac.uk/~lss/recentpapers/perctime.pdf\" rel=\"nofollow\">Perceptual Time</a>.</p>\n", "question": "<p>I am interested in the <a href=\"https://en.wikipedia.org/wiki/Emergence\" rel=\"nofollow\">emergence</a> of properties in <a href=\"https://en.wikipedia.org/wiki/Agent-based_model#Theory\" rel=\"nofollow\">agents</a>, and, more generally in robotics.</p>\n\n<p>I was wondering if there is work on the emergence of time-related concepts, on the low-level representation of notions like <em>before</em> and <em>after</em>. I know, for example, that there is work on the emergence of <a href=\"http://www.scholarpedia.org/article/Kohonen_network\" rel=\"nofollow\">spatial representation</a> (similar to <a href=\"https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm\" rel=\"nofollow\">knn</a>), or even <a href=\"https://infoscience.epfl.ch/record/129415/files/Mitrietal_1.pdf\" rel=\"nofollow\">communication</a>* but time seems to be a tricky concept. </p>\n\n<p>This has everything to do with the <em>platform</em>, i.e. the way that the representation would be coded in. We tend to favour ways that have some meaning or somehow mimic natural, well, yes, human structures, like the brain. I am not a neuroscientist and do not know that the sense of time <em>looks like</em> in humans, or if it is even present in other living beings.</p>\n\n<p><strong>Is there some work on the (emergence of the) representation of <em>time</em> in artificial agents?</strong></p>\n\n<hr>\n\n<p>*I remember watching a really cool... Actually creepy video from these robots but cannot find it anymore. Does anyone have the link at hand?</p>\n"}, "id": "188"}, {"body": {"answer": "<p>Architectures for recognising and generating emotion are typically somewhat complex and don't generally have short descriptions, so it's probably better to reference the literature rather than give a misleading soundbite:</p>\n\n<p>Some of the early work in `Affective Computing' was done by Rosamund Picard.\nThere is a <a href=\"http://affect.media.mit.edu/\">research group at MIT</a> specializing in this area.</p>\n\n<p>Some of the more developed architectural ideas are due to Marvin Minsky.\nA pre-publication draft of his book, `The Emotion Machine' is available via <a href=\"https://en.wikipedia.org/wiki/The_Emotion_Machine\">Wikipedia</a>.</p>\n\n<p>Emotional intelligence would certainly seem to be a necessary component of passing the Turing test - indeed, in the original Turing test essay in <a href=\"http://www.csee.umbc.edu/courses/471/papers/turing.pdf\">Computing Machinery and Intelligence</a> implied some degree of \"Theory of Mind\" about Mr Pickwick's preferences:</p>\n\n<p><em>\"Yet Christmas is a Winter\u2019s day, and I do not think Mr Pickwick would mind the comparison.\"</em></p>\n", "question": "<p>I've seen emotional intelligence defined as the capacity to be aware of, control, and express one's emotions, and to handle interpersonal relationships judiciously and empathetically.  What are some strategies for artificial intelligence to begin to tackle this problem and develop emotional intelligence for computers?  Are there examples where this is already happening to a degree today?  For example, wouldn't a computer that passes a Turing test necessarily express emotional intelligence or it would be seen as an obvious computer?  Perhaps that is why early programs that pass the test represented young people, who presumably have lower emotional intelligence.</p>\n"}, "id": "189"}, {"body": {"answer": "<p>It depends a bit on what you mean by 'quantum computer'. The 'conventional' notion is that quantum computation buys a (in some cases, exponential) speedup - it doesn't change <em>what</em> can be computed, just how quickly.</p>\n\n<p>In contrast, advocates of <em>hypercomputation</em> claim that quantum effects may make it possible to do infinite computations in finite time. Note, however, that this is not a mainstream belief - the reknowned logician Martin Davis has written an article claiming that hypercomputation is <a href=\"http://www1.maths.leeds.ac.uk/~pmt6sbc/docs/davis.myth.pdf\" rel=\"nofollow\">a myth</a>.</p>\n\n<p>Roger Penrose has also claimed that quantum vibrations in neural microtubules may be <a href=\"https://en.wikipedia.org/wiki/Quantum_mind\" rel=\"nofollow\">responsible for consciousness</a>.</p>\n", "question": "<p>Have there been proposed extensions to go beyond a Turing machine that solve the halting problem and if so, would those proposed extensions have value to advance strong Artificial Intelligence?  For example, does quantum computing go beyond the definition of a Turing machine and resolve the halting problem, and does that help in creating strong AI?</p>\n"}, "id": "190"}, {"body": {"answer": "<p>A neural net with even a single hidden layer is capable of Universal function approximation - it can approximate any continuous function 'as closely as you like'.</p>\n\n<p>Hence, one option would be to look for GOFAI applications that would benefit from this property - for example, in state-space search approaches where the utility of a state is not readily defined in advance, and could instead be learned.</p>\n", "question": "<p>In what ways can connectionist artificial intelligence (neural networks) be integrated with <em>Good Old-Fashioned A.I.</em> (<em>GOFAI</em>)? For instance, how could deep neural networks be integrated with knowledge bases or logical inference? One such example seems to be the <a href=\"http://wiki.opencog.org/w/DestinOpenCog\" rel=\"nofollow\">OpenCog + Destin integration</a>.</p>\n"}, "id": "193"}, {"body": {"answer": "<p>In 1986, the first PC therapist program was written by Joseph Weintraub. This program won the first Loebner Prize in 1991, and then again in 1992, 1993 and 1995. </p>\n\n<p>In 1981 or 1982, Jabberwacky was founded, which is the foundation of the current Cleverbot. Jabberwacky  appeared on the internet in 1997, reaching the third place for the Loebner Prize in 2003, the second place in 2004, and won in 2005 and 2006. In 2008, Cleverbot was launched as an variant of Jabberwacky. </p>\n\n<p>I'm not sure these are really the earliest, but that also depends on what you want earliest (programming started, first conversation,  first decent conversation, etc.). Also, it depends on what you call a \"real response\".</p>\n", "question": "<p>What was the first AI that was able to carry on a conversation, with real responses, such as in the famous <a href=\"https://www.youtube.com/watch?v=WnzlbyTZsQY\" rel=\"nofollow\">'I am not a robot. I am a unicorn' case?</a></p>\n\n<p>A 'real response' constitutes a sort-of personalized answer to a specific input by a user.</p>\n"}, "id": "196"}, {"body": {"answer": "<p>It all depends of what your A.I. can do. Even humans cannot do everything.</p>\n\n<p>If your AI program is so smart, ask it to take the general IQ tests for humans. Because the real IQ tests are made of several questions from different areas, so in that way you can measure IQ of your AI.</p>\n\n<p>This is because the <strong>IQ</strong> means the tests which are <strong>designed</strong> to assess human intelligence.</p>\n\n<blockquote>\n  <p>An intelligence quotient (IQ) is a total score derived from one of several standardized tests designed to assess human intelligence.<sup><a href=\"https://en.wikipedia.org/wiki/Intelligence_quotient\" rel=\"nofollow\">wiki</a></sup></p>\n</blockquote>\n\n<p>So there is no any other way of measuring IQ without taking IQ test, otherwise it won't be IQ (very logical).</p>\n\n<p>If your program is not so smart, you should look for specific tests related to the expertise or problem being solved. Ideally let it compete with humans who has the same expertise in that area, but it's important make the test on the same ground/level.</p>\n\n<p>For example the intelligence of <a href=\"https://en.wikipedia.org/wiki/Deep_Blue_(chess_computer)\" rel=\"nofollow\">Deep Blue</a> project was measured by playing chess with Kasparov. Then if world champion cannot win the game, who will?</p>\n\n<p>If you're writing program to play a game, make it play with compete with humans and measure the intelligence in terms of score.</p>\n\n<hr>\n\n<p>The equivalent of IQ for AI is a Turing Test (like <a href=\"http://ai.stackexchange.com/q/1397/8\">MIST</a> and other), see:</p>\n\n<ul>\n<li><a href=\"http://ai.stackexchange.com/q/15/8\">Is the Turing Test, or any of its variants, a reliable test of artificial intelligence?</a></li>\n</ul>\n", "question": "<p>Can an AI program have an IQ?</p>\n\n<p>In other words, can the IQ of an AI program be measured?</p>\n\n<p>Like how humans can do an IQ test.</p>\n"}, "id": "199"}, {"body": {"answer": "<p>Personally, I think this might be an overhyped issue. Trolley problems only occur when the situation is optimized to prevent \"3rd options\".</p>\n\n<p>A car has brakes, does it not? \"But what if the brakes don't work?\" Well, then <strong>the car is not allowed to drive at all.</strong> Even in regular traffic, human operators are taught that your speed should be limited as such that you can stop within the area you can see. Solutions like these will reduce the possibility of a trolley problem.</p>\n\n<p>As for animals... if there is no explicit effort to deal with humans on the road I think animals will be treated the same. This sounds implausible - roadkill happens often and human \"roadkill\" is unwanted, but animals are a lot smaller and harder to see than humans, so I think detecting humans will be easier, preventing a lot of the accidents.</p>\n\n<p>In other cases (bugs, faults while driving, multiple failures stacked onto each other), perhaps accidents will occur, they'll be analysed, and vehicles will be updated to avoid causing similar situations. </p>\n", "question": "<p>Obviously driverless cars aren't perfect, so imagine that the Google car (as an example) got into difficult situation.</p>\n\n<p>Here are a few examples of unfortunate situations caused by set of events:</p>\n\n<ul>\n<li>the car is heading toward a crowd of 10 people crossing the road, so it cannot stop in time, but it can avoid killing 10 people by hitting the wall (killing the passengers),</li>\n<li>avoiding killing the rider of the motorcycle considering that the probability of survival is greater for the passenger of the car,</li>\n<li>killing animal on the street in favour of human being,</li>\n<li>changing lanes to crash into another car to avoid killing a dog,</li>\n</ul>\n\n<p>And here are few dilemmas:</p>\n\n<ul>\n<li>Does the algorithm recognize the difference between a human being and an animal?</li>\n<li>Does the size of the human being or animal matter?</li>\n<li>Does it count how many passengers it has vs. people in the front?</li>\n<li>Does it \"know\" when babies/children are on board?</li>\n<li>Does it take into the account the age (e.g. killing the older first)?</li>\n</ul>\n\n<p>How would an algorithm decide what should it do from the technical perspective? Is it being aware of above (counting the probability of kills), or not (killing people just to avoid its own destruction)?</p>\n\n<p>Related articles:</p>\n\n<ul>\n<li><a href=\"https://www.technologyreview.com/s/542626/why-self-driving-cars-must-be-programmed-to-kill/\">Why Self-Driving Cars Must Be Programmed to Kill</a></li>\n<li><a href=\"https://www.technologyreview.com/s/539731/how-to-help-self-driving-cars-make-ethical-decisions/\">How to Help Self-Driving Cars Make Ethical Decisions</a></li>\n</ul>\n"}, "id": "200"}, {"body": {"answer": "<p>Convolutional Nets (CNN) rely on mathematical convolution (e.g. 2D or 3D convolutions), which is commonly used for signal processing. Images are a type of signal, and convolution can equally be used on sound, vibrations, etc. So, in principle, CNNs can find applications to any signal, and probably more.</p>\n\n<p>In practice, there exists already work on NLP (as mentioned by Matthew Graves), where some people process text with CNNs rather than recursive networks. Some other works apply to sound processing (no reference here, but I have yet unpublished work ongoing).</p>\n\n<hr>\n\n<p><em>Original contents: In answer to the original title question, which has changed now. Perhaps need to delete this one</em>.</p>\n\n<p>Research on adversarial networks (and related) show that even <a href=\"http://arxiv.org/abs/1412.1897\" rel=\"nofollow\">deep networks can easily be fooled</a>, leading them to see a dog (or whatever object) in what appears to be random noise when a human look at it (the article has clear examples).</p>\n\n<p>Another issue is the generalization power of a neural network. Convolutional nets have amazed the world with their capability to generalize way better than other techniques. But if the network is only fed images of cats, it will recognize only cats (and probably see cats everywhere, as by adversarial network results). In other words, even CNs have a hard time generalizing too far <em>beyond</em> what they learned from.</p>\n\n<p>The recognition limit is hard to define precisely. I would simply say that the diversity of the learning data pushes the limit (I assume further detail should lead to more appropriate venue for discussion).</p>\n", "question": "<p>Can a Convolutional Neural Network be used for pattern recognition in a problem domain where there are no pre-existing images, say by representing abstract data graphically? Would that always be less efficient?</p>\n\n<p><a href=\"https://youtu.be/py5byOOHZM8?t=815\">This developer</a> says current development could go further but not if there's a limit outside image recognition. </p>\n"}, "id": "201"}, {"body": {"answer": "<p>It's possible to implement a form of curiosity-driven behavior without requiring full 'emotional intelligence'. One elementary strategy would be to define some form of similarity measure on inputs.</p>\n\n<p>More generally, Jurgen Schmidhuber has pioneered work on 'Artificial Curiosity/Creativity' and 'Intrinsic Motivation' and has written a number of papers on the subject:</p>\n\n<ul>\n<li><a href=\"http://people.idsia.ch/~juergen/curioussingapore/curioussingapore.html\">Artificial Curiosity</a> </li>\n<li><a href=\"http://people.idsia.ch/~juergen/ieeecreative.pdf\">Intrinsic Motivation</a></li>\n</ul>\n\n<p>Here is a <a href=\"https://www.youtube.com/watch?v=Ipomu0MLFaI\">video</a> of a nice associated presentation.</p>\n", "question": "<p>I'd like to know more about <a href=\"http://ai.stackexchange.com/q/26/8\">implementing emotional intelligence</a>.</p>\n\n<p>Given I'm implementing a chat bot and I'd like to introduce the levels of curiosity to measure whether user text input is interesting or not.</p>\n\n<p>High level would mean bot is asking more questions and is following the topic, lower level of curiosity makes the bot not asking any questions and changing the topics.</p>\n\n<p>Less interesting content could mean the bot doesn't see any opportunity to learn something new or it doesn't understand the topic or doesn't want to talk about it, because of its low quality. </p>\n\n<p>How this possibly can be achieved? Are there any examples?</p>\n"}, "id": "206"}, {"body": {"answer": "<p>Formally, a single hidden layer is sufficient to approximate a continuous function to any desired degree of accuracy, so in that sense, you never need more than 1.</p>\n\n<p>Finding the best topology for a given problem is an open research problem. As far as I know, there are few universal 'rules of thumb' for this.</p>\n\n<p>For a given problem, one option is to apply a <em>neuroevolutionary</em> approach such as <a href=\"https://www.cs.ucf.edu/~kstanley/neat.html\">NEAT</a>, which attempts to find a topology that works well for the problem at hand.</p>\n", "question": "<p>I've read that the most of the problems can be solved with 1-2 hidden layers.</p>\n\n<p>How do you know you need more than 2? For what kind of problems you would need them (as example)?</p>\n"}, "id": "208"}, {"body": {"answer": "<p>In general purpose for this kind of applications, One can use databases such as sqlite, mysql, mssql etc. It simplifies read / write operations, allows for a common language to interact with different databases from different vendors and platforms. </p>\n", "question": "<p>I'd like to know which common file format is more efficient in terms of simplicity and storage space for storing the state of artificial neural network.</p>\n\n<p>I'm not talking about memory storage, but file storage, so the data can be loaded later on.</p>\n\n<p>My first guess would be XML, but having millions of connections and weights would generate huge amount of data. Another thing would be to dump object instances into binary file using some export/serialize functions, but the disadvantage is that the file isn't common and it's language specific.</p>\n\n<p>Are there any common file format standards which can be used for exporting huge artificial neural network into the file to be loaded by another program? If so, which one.</p>\n"}, "id": "213"}, {"body": {"answer": "<p>One option is <a href=\"https://www.neuroml.org/\" rel=\"nofollow\">NeuroML</a>, one of the goals of which is:</p>\n\n<blockquote>\n  <p>To facilitate the exchange of complex neuronal models between researchers, allowing for greater transparency and accessibility of models</p>\n</blockquote>\n\n<p>In general, the matrices associated with large neural network models are likely to be sparse. Hence a 'homebrew' alternative to the above would be to use something like the <a href=\"http://people.sc.fsu.edu/~jburkardt/data/mm/mm.html\" rel=\"nofollow\">Matrix Market format</a>.</p>\n", "question": "<p>I'd like to know which common file format is more efficient in terms of simplicity and storage space for storing the state of artificial neural network.</p>\n\n<p>I'm not talking about memory storage, but file storage, so the data can be loaded later on.</p>\n\n<p>My first guess would be XML, but having millions of connections and weights would generate huge amount of data. Another thing would be to dump object instances into binary file using some export/serialize functions, but the disadvantage is that the file isn't common and it's language specific.</p>\n\n<p>Are there any common file format standards which can be used for exporting huge artificial neural network into the file to be loaded by another program? If so, which one.</p>\n"}, "id": "215"}, {"body": {"answer": "<p>Over the last few years, evolutionary computation research has shown increasing interest in including some aspect of epigenetics. For example:</p>\n\n<ul>\n<li>A 2008 paper by <a href=\"http://www.sciencedirect.com/science/article/pii/S0020025508002880\" rel=\"nofollow\">Tanev and Yuta</a></li>\n<li>Work from <a href=\"http://faculty.hampshire.edu/lspector/pubs/Epigenetics_2015_GECCO_final.pdf\" rel=\"nofollow\">Lee Spector's genetic programming group</a></li>\n<li>A recent paper by <a href=\"http://link.springer.com/chapter/10.1007/978-3-319-30668-1_9\" rel=\"nofollow\">Ricalde and Banzhaf</a></li>\n</ul>\n", "question": "<p>What designs for genetic algorithms are there, if they are classified differently and/or have different names, that leverage models for epigenetics in evolution? What are the pros/cons of the designs? Are there vast insufficiencies or wide-open questions about their usefulness? </p>\n"}, "id": "216"}, {"body": {"answer": "<p>The ability to re-frame summarization as a problem for ANN is rather dependent on what kind of output you're looking for: you mentioned 'salient parts of the text'.</p>\n\n<p>One possibly is to use a deep learning approach that first chunks together words that belong in the same phrase as a single 'feature'.</p>\n\n<p>Another possibility is to identify both key words and relations between them. Here is some previous work on using neural nets for <a href=\"https://lirias.kuleuven.be/bitstream/123456789/131932/1/41238.pdf\" rel=\"nofollow\">relational learning</a>.</p>\n", "question": "<p>Text summarization is a long-standing research problem that was <em>\"ignited\"</em> by Luhn in 1958. However, a half century later, we still came nowhere close  to solving this problem (abstractive summarization). The reason for this might be because researchers are resorting to statistical (and sometimes linguistic) methods to find &amp; extract the most salient parts of the text.</p>\n\n<p>Is summarization problem solvable using AI (neural networks to be precise)? </p>\n"}, "id": "217"}, {"body": {"answer": "<p>In-between your input and desired output, there's obviously a huge space to search. The more relevant domain information you include as features, the higher chance that the Deep Learning (DL) algorithm can find the desired mapping.</p>\n\n<p>At this early stage in DL research, there aren't so many rules of thumb to tell you what features to explicitly encode - not least because it depends on the size of your training corpus. My suggestion would be: obtain (or generate) a large corpus of C code, train on that with the most naive feature representation that you think might work, then repeatedly gather data and add more feature preprocessing as necessary.</p>\n\n<p>This following paper describes a DL approach to what is almost the 'reverse problem' to yours - <a href=\"http://arxiv.org/pdf/1510.07211.pdf\" rel=\"nofollow\">generating the source code for a program described in natural language</a>.</p>\n\n<p>I found the strength of the results reported in this paper surprising, but it does give me some hope that what you are asking might be possible.</p>\n", "question": "<p>I would like to learn more whether it is possible and how to write a program which decompiles executable binary (an object file) to the C source. I'm not asking exactly 'how', but rather how this can be achieved.</p>\n\n<p>Given the following <code>hello.c</code> file (as example):</p>\n\n<pre><code>#include &lt;stdio.h&gt;\nint main() {\n  printf(\"Hello World!\");\n}\n</code></pre>\n\n<p>Then after compilation (<code>gcc hello.c</code>) I've got the binary file like:</p>\n\n<pre><code>$ hexdump -C a.out | head\n00000000  cf fa ed fe 07 00 00 01  03 00 00 80 02 00 00 00  |................|\n00000010  0f 00 00 00 b0 04 00 00  85 00 20 00 00 00 00 00  |.......... .....|\n00000020  19 00 00 00 48 00 00 00  5f 5f 50 41 47 45 5a 45  |....H...__PAGEZE|\n00000030  52 4f 00 00 00 00 00 00  00 00 00 00 00 00 00 00  |RO..............|\n00000040  00 00 00 00 01 00 00 00  00 00 00 00 00 00 00 00  |................|\n00000050  00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  |................|\n00000060  00 00 00 00 00 00 00 00  19 00 00 00 d8 01 00 00  |................|\n00000070  5f 5f 54 45 58 54 00 00  00 00 00 00 00 00 00 00  |__TEXT..........|\n$ wc -c hello.c a.out \n  60 hello.c\n8432 a.out\n</code></pre>\n\n<p>For the learning dataset I assume I'll have to have thousands of source code files along with its binary representation, so algorithm can learn about moving parts on certain changes.</p>\n\n<p>My concerns are:</p>\n\n<ul>\n<li>do my algorithm needs to be aware about the header file, or it's \"smart\" enough to figure it out,</li>\n<li>if it needs to know about the header, how do I tell my algorithm 'here is the header file',</li>\n<li>what should be input/output mapping (whether some section to section or file to file),</li>\n<li>do I need to divide my source code into some sections,</li>\n<li>do I need to know exactly how decompilers work or AI can figure it out for me,</li>\n<li>or should I've two networks, one for header, another for body it-self,</li>\n<li>or more separate networks, each one for each logical component (e.g. byte->C tag, etc.)</li>\n</ul>\n\n<p>How would you tackle this?</p>\n"}, "id": "219"}, {"body": {"answer": "<p>'Backprop' is short for 'backpropagation of error' in order to avoid confusion when using <em>backpropagation</em> term.</p>\n\n<p>Basically <em>backpropagation</em> refers to the method for computing the gradient of the case-wise error function with respect to the weights for a feedforward network<sup>Werbos</sup>. And <em>backprop</em> refers to a training method that uses backpropagation to compute the gradient.</p>\n\n<p>So we can say that a <em>backprop</em> network is a feedforward network trained by <em>backpropagation</em>.</p>\n\n<p>The 'standard backprop' term is a euphemism for the <em>generalized delta rule</em> which is most widely used supervised training method.</p>\n\n<p>Source: <a href=\"ftp://ftp.sas.com/pub/neural/FAQ2.html#A_backprop\" rel=\"nofollow\">What is backprop?</a> at FAQ of Usenet newsgroup comp.ai.neural-nets</p>\n\n<p>References:</p>\n\n<ul>\n<li>Werbos, P. J. (1974). Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences. PhD thesis, Harvard University.</li>\n<li>Werbos, P. J. (1994). The Roots of Backpropagation: From Ordered Derivatives to Neural Networks and Political Forecasting,Wiley Interscience.</li>\n<li>Bertsekas, D. P. (1995), Nonlinear Programming, Belmont, MA: Athena Scientific, ISBN 1-886529-14-0.</li>\n<li>Bertsekas, D. P. and Tsitsiklis, J. N. (1996), Neuro-Dynamic Programming, Belmont, MA: Athena Scientific, ISBN 1-886529-10-8.</li>\n<li>Polyak, B.T. (1964), \"Some methods of speeding up the convergence of iteration methods,\" Z. Vycisl. Mat. i Mat. Fiz., 4, 1-17.</li>\n<li>Polyak, B.T. (1987), Introduction to Optimization, NY: Optimization Software, Inc.</li>\n<li>Reed, R.D., and Marks, R.J, II (1999), Neural Smithing: Supervised Learning in Feedforward Artificial Neural Networks, Cambridge, MA: The MIT Press, ISBN 0-262-18190-8.</li>\n<li>Rumelhart, D.E., Hinton, G.E., and Williams, R.J. (1986), \"Learning internal representations by error propagation\", in Rumelhart, D.E. and McClelland, J. L., eds. (1986), Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Volume 1, 318-362, Cambridge, MA: The MIT Press.</li>\n<li>Werbos, P.J. (1974/1994), The Roots of Backpropagation, NY: John Wiley &amp; Sons. Includes Werbos's 1974 Harvard Ph.D. thesis, Beyond Regression.</li>\n</ul>\n", "question": "<p>What does \"backprop\" mean? I've Googled it, but it's showing backpropagation.</p>\n\n<p>Is the \"backprop\" term basically the same as \"backpropagation\" or does it have a different meaning?</p>\n"}, "id": "222"}, {"body": {"answer": "<p>Supervised learning is typically an attempt to learn a mathematical function, <em>f(X)=y</em>. For this, you need both the input vector <em>X</em> and the output vector <em>y</em>. The model outputs have whatever type / dimensionality / etc. that the target values have.</p>\n\n<blockquote>\n  <p>basically targets are most likely the same as the inputs.</p>\n</blockquote>\n\n<p>This doesn't seem right to me.</p>\n\n<p>Unsupervised learning models instead learn a <em>structure</em> from the data. A clustering model, for example, is learning both how many clusters exist in the data (a number that's not the same type as the inputs) and where those clusters are located (which is also a different type from the inputs). The output of running this model on a new datapoint <em>x</em> is not the same type as <em>x</em>, but instead a classification label.</p>\n\n<p>Similarly, time series models learn parameters that symbolize how vectors in the input relate to each other, rather than raw inputs themselves.</p>\n\n<p>As for how they learn, the structures are mathematical objects whose fitness is determined by the input data. The simplest possible unstructured unsupervised learning problem is probably \"what's the mean of the data?\", and it should be clear how that's 'learned' through processing the input. More sophisticated models are just adding more pieces to that calculation.</p>\n", "question": "<p>Unsupervised learning does not involve target values, so basically targets are most likely the same as the inputs (in other words, involves no target values).</p>\n\n<p>So how does this model learn?</p>\n"}, "id": "223"}, {"body": {"answer": "<p>'Direct augmentation' of human intelligence, of the sort that you would see in science fiction, looks to be very hard. Most of our promising approaches deal with <em>avoiding damage</em> rather than <em>adding capabilities</em>--there's no drug that you can take now that will make you smarter to the degree that missing a night of sleep can make you dumber.</p>\n\n<p>The most informative area of current practice is probably game-playing, where '<a href=\"http://bloomreach.com/2014/12/centaur-chess-brings-best-humans-machines/\">centaurs</a>,' or humans working with computers, outcompete human players or computer players.</p>\n\n<p>But a centaur player doesn't have a wire jutting out of their skull to jack into the computer; they're looking at a laptop screen. One of the reasons to be pessimistic about cyborg augmentation is because current I/O technology is already so good. Why install a new wire to put information into your visual cortex, when you come already equipped with two? </p>\n\n<p>If you could think code directly onto the screen, how much better would that be than typing code through a keyboard? Probably some, but I find it difficult to imagine that it'll be more than twice as good. So most human-computer intelligence augmentation will look like people using software, and software using human inputs, rather than humans and computers evolving together.</p>\n\n<hr>\n\n<p>Transcranial Direct Current Stimulation (TDCS) and similar approaches cause temporary changes in mental abilities by raising or lowering the activation potentials of neurons in particular regions of the brain. (I've done it myself, a few years ago, and what weak effects I noticed were probably negative. Not too much surprise for a DIY setup!)</p>\n\n<p>It looks like it has a number of useful implications. One article about TDCS that I found particularly striking was the journalist who tried it gushing about how their anxiety disappeared for a few days, presumably because the part of their brain behind the anxiety was dampened. One could imagine it being useful for the treatment of many different mental disorders.</p>\n\n<p>That said, I'm pessimistic that it will translate into superior <em>peak</em> performance, and I think that's the sort of thing that's more relevant for discussions of augmentation. (Is there TDCS that we could do that would make Terrence Tao better at doing mathematics?)</p>\n\n<p>Where improved AI methods will come into play is by improving our models of the brain, allowing us to better target interventions, much in the way that AI methods are improving our treatment of cancer (through superior diagnosis and targeting of radiotherapy, as two easy examples). These effects will all be indirect--for example, AI empowering an app or gadget that helps you sleep better won't <em>directly</em> augment your intelligence, but will cause population-level increases in effective intelligence through reducing sleep deprivation.</p>\n\n<p>I haven't talked yet about nootropics, chemicals that increase intelligence, but it's reasonable to expect that AI will improve drug discovery there like it improves drug discovery for anything else. But the same caveats apply--the effect of nootropics seem to be negatively correlated with intelligence (that is, the smarter someone already is, the harder it is to increase their intelligence further).</p>\n", "question": "<p>One of the most compelling applications for AI would be in augmenting human biological intelligence. What are some of the currently proposed methods for doing this aside from vague notions such as \"nanobots swimming around our brains and bodies\" or \"electrodes connected to our skulls\"?</p>\n"}, "id": "226"}, {"body": {"answer": "<p>Pseudo-random number generators are specifically defined to defeat any form of prediction via 'black box' observation. Certainly, some (e.g. linear congruential) have weaknesses, but you are unlikely to have any success in general in predicting the output of a modern RNG. For devices based on chaotic physical systems (e.g. most national lotteries), there is no realistic possibility of prediction.</p>\n\n<p>\"Patterns or statistical association\" is a much weaker criterion than 'prediction'. Some very recent work has applied topological data analysis to visualize patterns within the infamous Randu RNG.</p>\n", "question": "<p>Given list of fixed numbers from a mathematical constant such as Pi, is it is possible to train AI to attempt to predict the next numbers?</p>\n\n<p>Which AI or neural network would be more suitable for this task? </p>\n\n<p>Especially the one which will work without memorizing the entire training set, but the one which will attempt to find some patterns or statistical association.</p>\n"}, "id": "228"}, {"body": {"answer": "<blockquote>\n  <p>Multilayer Perceptron networks (MLP) have been applied to distinct\n  areas, performing tasks such as function fitting and pattern\n  recognition problems, by using the supervised training with an\n  algorithm known as \u201cerror back propagation\u201d. </p>\n  \n  <p>Radial basis function (RBF) networks have the advantages of an easy\n  design (just three layer architecture), good generalization, and high\n  tolerance of input noises and ability of online learning. From the\n  point of generalization, RBF networks can respond well well to\n  patterns that were not used for training.</p>\n</blockquote>\n\n<p><a href=\"http://www.aidic.it/cet/13/32/230.pdf\" rel=\"nofollow\"><em>1.1, 1.2 of 230.pdf</em></a></p>\n", "question": "<p>What are the main differences between two types of feedforward networks such as <em>multilayer perceptrons</em> (MLP) and <em>radial basis function</em> (RBF)?</p>\n\n<p>What are the fundamental differences between these two types?</p>\n"}, "id": "229"}, {"body": {"answer": "<p>You would probably have to pack recursive structures into finite-dimensional real vectors and there have been such attempts. The finite precision limits goes as far as the recursion can go.</p>\n\n<p>The limitation of <em>feedforward</em> neural networks is restricted to finite input and output spaces, so <em>recurrent</em> may be more suitable for this task as in theory can process arbitrarily long strings of numbers, but it has much more practical difficulties than feedforward network.</p>\n\n<p>These kind of methods are open to debate.</p>\n\n<p>Source: <a href=\"ftp://ftp.sas.com/pub/neural/FAQ.html\" rel=\"nofollow\">SAS FAQ</a></p>\n\n<p>References:</p>\n\n<ul>\n<li>Blair, 1997; Pollack, 1990; Chalmers, 1990; Chrisman, 1991; Plate, 1994; Hammerton, 1998; Hadley, 1999</li>\n</ul>\n", "question": "<p>Given list of fixed numbers from a mathematical constant such as Pi, is it is possible to train AI to attempt to predict the next numbers?</p>\n\n<p>Which AI or neural network would be more suitable for this task? </p>\n\n<p>Especially the one which will work without memorizing the entire training set, but the one which will attempt to find some patterns or statistical association.</p>\n"}, "id": "230"}, {"body": {"answer": "<p>The concept of 'survival instinct' probably falls in the category of what Marvin Minsky would call a 'suitcase word', i.e. it packages together a number of related phenomena into what at first appears to be a singular notion. </p>\n\n<p>So it's quite possible that we can construct mechanisms that have the appearance of some kind of 'hard-coded' survival instinct, without that ever featuring as an explicit rule(s) in the design.</p>\n\n<p>See the beautiful little book <a href=\"https://mitpress.mit.edu/books/vehicles\" rel=\"nofollow\">'Vehicles'</a> by the neuroanatomist Valentino Braitenberg for a compelling narrative of how such 'top down' concepts as 'survival instinct' might evolve 'from the bottom up'.</p>\n\n<p>Also, trying to ensure that intelligent artefacts place too high a priority on their survival might easily lead to a <a href=\"https://xkcd.com/1613/\" rel=\"nofollow\">Killbot Hellscape</a>.</p>\n", "question": "<p>This question stems from quite a few \"informal\" sources. Movies like <em>2001, A Space Odyssey</em> and <em>Ex Machina</em>; books like <em>Destination Void</em> (Frank Herbert), and others suggest that general intelligence <em>wants</em> to survive, and even learn the importance for it.</p>\n\n<p>There may be several arguments for survival. What would be the most prominent?</p>\n"}, "id": "232"}, {"body": {"answer": "<p>Steve Omohudro wrote a paper called <a href=\"https://selfawaresystems.files.wordpress.com/2008/01/ai_drives_final.pdf\" rel=\"nofollow\">Basic AI Drives</a> that steps through why we would expect an AI with narrow goals to find some basic, general concepts as instrumentally useful for their narrow goals. </p>\n\n<p>For example, an AI designed to maximize stock market returns but whose design is silent on the importance of continuing to survive would realize that its continued survival is a key component of maximizing stock market returns, and thus take actions to keep itself operational.</p>\n\n<p>In general, we should be skeptical of 'anthropomorphizing' AI and other code, but it seems like there <em>are</em> reasons to expect this beyond \"well, humans behave this way, so it must be how all intelligence behaves.\"</p>\n", "question": "<p>This question stems from quite a few \"informal\" sources. Movies like <em>2001, A Space Odyssey</em> and <em>Ex Machina</em>; books like <em>Destination Void</em> (Frank Herbert), and others suggest that general intelligence <em>wants</em> to survive, and even learn the importance for it.</p>\n\n<p>There may be several arguments for survival. What would be the most prominent?</p>\n"}, "id": "234"}, {"body": {"answer": "<p>I presume the proof the OP is referring to can be found in <a href=\"http://link.springer.com/book/10.1007%2F978-1-4612-0707-8\" rel=\"nofollow\">this monograph</a> by Hava Siegelmann?</p>\n\n<p>In his article <a href=\"http://www1.maths.leeds.ac.uk/~pmt6sbc/docs/davis.myth.pdf\" rel=\"nofollow\">'The Myth of Hypercomputation'</a>, the eminent computer scientist Martin Davis explains (p8-9) that there is nothing 'super Turing' about this formulation.</p>\n\n<p>EDIT: It's looking like the claim about <em>rational</em> weights being super-Turing is made in <a href=\"http://www.ncbi.nlm.nih.gov/pubmed/25354762\" rel=\"nofollow\">this</a>  more recent paper by Siegelmann, which introduces an additional assumption of <em>plasticity</em>, i.e. that weights can be dynamically updated.</p>\n", "question": "<p>It is proved that a recurrent neural net with rational weights can be a super-Turing machine. Can we achieve this in practice ?</p>\n"}, "id": "235"}, {"body": {"answer": "<p>If by true AI, you mean 'like human beings', the answer is - no-one knows what the appropriate computational mechanisms (neural or otherwise) are or indeed whether we are capable of constructing them.</p>\n\n<p>What Artificial Neural Nets (ANNs) do is essentially 'nonlinear regression' - perhaps this is not a sufficiently strong model to express humanlike behaviour. </p>\n\n<p>Despite the 'Universal function approximation' property of ANNs, what if human intelligence depends on some as-yet-unguessed mechanism of the physical world?</p>\n\n<p>With respect to your question about \"the only way\":\nEven if (physical) neural mechanisms somehow actually were the <em>only</em> route to intelligence (e.g. via Penrose's quantum microtubules), how could that be proved? </p>\n\n<p>Even in the formal world of mathematics, there's a saying that \"Proofs of non-existence are hard\". It scarcely seems conceivable that, in the physical world, it would be possible to demonstrate that intelligence could not arise by any other mechanism.</p>\n\n<p>Moving back to computational systems, note that Stephen Wolfram made the interesting observation in his book <a href=\"http://www.wolframscience.com/nksonline/toc.html\">'A New Kind of Science'</a> that many of the apparently distinct mechanisms he observed seem to be capable of 'Universal Computation', so in that sense there's nothing very particular about ANNs.</p>\n", "question": "<p>According to my knowledge most of the current artificial intelligence study uses of some kind of neural network or its variants. A good example would be DeepMind's alphago which I believe is a deep neural network, for vision CNN, text, music and other ordered features RNN's, etc. But for machine learning application we have neural networks, support vector machines, random forest, regression methods, etc. available for applications. </p>\n\n<p>So are neural networks and its variants the only way to reach \"true\" artificial intelligence? </p>\n"}, "id": "236"}, {"body": {"answer": "<p>As far as I can see, there's no reason why you couldn't (for example) take the convolutional inputs to deepdream from adjacent sample points, rather than adjacent spatial positions, as is the case with image input.</p>\n\n<p>Given the 'self similar' nature of deep dream images, listening to this <a href=\"https://vimeo.com/13541969\" rel=\"nofollow\">fractal granular synthesis</a> technique might be of interest/inspiration.</p>\n", "question": "<p>What happens if you apply the same <a href=\"https://en.wikipedia.org/wiki/DeepDream\" rel=\"nofollow\">deep dream technique</a> which produces \"dream\" visuals, but to media streams such as audio files?</p>\n\n<p>Does changing image functions into audio and enhancing the logic would work, or it won't work or doesn't make any sense?</p>\n\n<p>My goal is to create \"dream\" like audio based on the two samples.</p>\n"}, "id": "238"}, {"body": {"answer": "<p>It depends on what you consider \"true artificial intelligence\". But this probably means to be able to think like a human - and perhaps, do so in a more rational manner, as in the human brain emotion comes before ratio.  </p>\n\n<p>It would seem that a neural network, or a genetic algorithm that evolves neural networks, is the closest way - mimicking humans.  </p>\n\n<p>However, the traditional counter-argument to this is that we tried to do the same with flight. We tried to copy nature, mimick the birds - trying to fly by flapping wings. But eventually we made airplanes that did not rely on flapping their wings.  </p>\n\n<p>In AI, there are far more variables than in aerodynamics. So it is quite likely that a human-like intelligence can be attained by other methods than neural networks.</p>\n\n<p>In the end, neural networks are one approach to machine learning. There are others, all governed by the rules for what can and cannot be learnt. (There is a field called Computational Learning Theory that covers this). </p>\n\n<p>Although it is possible to extend learning systems beyond what can be learnt according to COLT, this means that such a learning system - neural network or otherwise - is essentially flawed, and will draw wrong conclusions at one point or another.</p>\n", "question": "<p>According to my knowledge most of the current artificial intelligence study uses of some kind of neural network or its variants. A good example would be DeepMind's alphago which I believe is a deep neural network, for vision CNN, text, music and other ordered features RNN's, etc. But for machine learning application we have neural networks, support vector machines, random forest, regression methods, etc. available for applications. </p>\n\n<p>So are neural networks and its variants the only way to reach \"true\" artificial intelligence? </p>\n"}, "id": "239"}, {"body": {"answer": "<p>A genetic algorithm is an algorithm that randomly generates a number of attempted solutions for a problem. This set of attempted solutions is called the \"population\".  </p>\n\n<p>It then tries to see how well these solutions solve the problem, using a given <em>fitness function</em>. The attempted solutions with the best <em>fitness</em> value are used to generate a new population. This can be done by making small changes to the attempted solutions (mutation) or by combining existing attempted solutions (crossover).</p>\n\n<p>The idea is that, over time, an attempted solution emerges that has a high enough <em>fitness</em> value to solve the problem.</p>\n\n<p>The inspiration for this came from the theory of evolution; the fittest solutions survive and procreate.</p>\n\n<p><strong>Example 1</strong></p>\n\n<p>Suppose you were looking for the most efficient way to cut a number of shapes out of a piece of wood. You want to waste as little wood as possible.</p>\n\n<p>Your attempted solutions would be random arrangements of these shapes on your piece of wood. <em>Fitness</em> would be determined by how little wood would be left after cutting the shapes following this arrangement.<br>\nThe less wood is left, the better the attempted solution. </p>\n\n<p><strong>Example 2</strong></p>\n\n<p>Suppose you were trying to find a polynomial that passes through a number of points. Your attempted solutions would be random polynomials.<br>\nTo determine the <em>fitness</em> of these polynomials, you determine how well they fit the given points. (In this particular case, you would probably use the least squares method to determine how well the polynomial fit the points).\nOver a number of trials, you would get polynomials that fit the points better, until you had a polynomial that fit the points closely enough.</p>\n", "question": "<p>I've noticed that a few questions on this site mention genetic algorithms and it made me realize that I don't really know much about those.</p>\n\n<p>I have heard the term before, but it's not something I've ever used, so I don't have much idea about how they work and what they are good for. All I know is that they involve some sort of evolution and randomly changing values.</p>\n\n<p>Can you give me a short explanation, preferably including some sort of practical example that illustrates the basic principles?</p>\n"}, "id": "242"}, {"body": {"answer": "<p>Evolutionary algorithms are a family of optimization algorithms based on the principle of <strong>Darwinian natural selection</strong>. As part of natural selection, a given environment has a population of individuals that compete for survival and reproduction. The ability of each individual to achieve these goals determines their chance to have children, in other words to pass on their genes to the next generation of individuals, who for genetic reasons will have an increased chance of doing well, even better, in realizing these two objectives. </p>\n\n<p>This principle of continuous improvement over the generations is taken by evolutionary algorithms to optimize solutions to a problem. In the <strong>initial generation</strong>, a population composed of different <strong>individuals</strong> is generated randomly or by other methods. An individual is a solution to the problem, more or less good: the quality of the individual in regards to the problem is called <strong>fitness</strong>, which reflects the adequacy of the solution to the problem to be solved. The higher the fitness of an individual, the higher it is likely to pass some or all of its genotype to the individuals of the next generation.</p>\n\n<p>An individual is coded as a <strong>genotype</strong>, which can have any shape, such as a** bit vector (<strong>genetic algorithms</strong>) or a vector of real (evolution strategies). Each genotype is transformed into a <strong>phenotype</strong> when assessing the individual, i.e. when its fitness is calculated. In some cases, the phenotype is identical to the genotype: it is called <strong>direct</strong> <strong>coding</strong>. Otherwise, the coding is called indirect. For example, suppose you want to optimize the size of a rectangular parallelepiped defined by its length, height and width. To simplify the example, assume that these three quantities are integers between 0 and 15. We can then describe each of them using a 4-bit binary number. An example of a potential solution may be to genotype 0001 0111 01010. The corresponding phenotype is a parallelepiped of length 1, height 7 and width 10.</p>\n\n<p>During the transition from the old to the new generation are called <strong>variation</strong> <strong>operators</strong>, whose purpose is to manipulate individuals. There are two distinct types of variation operators:</p>\n\n<ul>\n<li>the <strong>mutation</strong> <strong>operators</strong>, which are used to introduce variations within the same individual, as genetic mutations;</li>\n<li>the <strong>crossover</strong> <strong>operators</strong>, which are used to cross at least two different genotypes, as genetic crosses from breeding.</li>\n</ul>\n\n<p>Evolutionary algorithms have proven themselves in various fields such as operations research, robotics, biology, nuance, or cryptography. In addition, they can optimize multiple objectives simultaneously and can be used as black boxes because they do not assume any properties in the mathematical model to optimize. Their only real limitation is the computational complexity.</p>\n\n<p><a href=\"http://i.stack.imgur.com/wweBO.png\" rel=\"nofollow\"><img src=\"http://i.stack.imgur.com/wweBO.png\" alt=\"enter image description here\"></a></p>\n", "question": "<p>I've noticed that a few questions on this site mention genetic algorithms and it made me realize that I don't really know much about those.</p>\n\n<p>I have heard the term before, but it's not something I've ever used, so I don't have much idea about how they work and what they are good for. All I know is that they involve some sort of evolution and randomly changing values.</p>\n\n<p>Can you give me a short explanation, preferably including some sort of practical example that illustrates the basic principles?</p>\n"}, "id": "243"}, {"body": {"answer": "<p>As observed in another answer, all you need to apply Genetic Algorithms (GAs) is to represent a potential solution to your problem in a form that is subject to crossover and mutation. Ideally, the fitness function will provide some kind of smooth feedback about the quality of a solution, rather than simply being a 'Needle in a Haystack'.</p>\n\n<p>Here are some characteristics of problems that Genetic Algorithms (and indeed <a href=\"https://cs.gmu.edu/~sean/book/metaheuristics/\" rel=\"nofollow\">Metaheuristics</a> in general) are good for:</p>\n\n<ul>\n<li>NP-complete - The number of possible solutions to the problem is\nexponential, but checking the fitness of a solution is relatively\ncheap (technically, with time polynomial in the input size). </li>\n<li>Black box - GAs work reasonably well even if you don't have a particularly\ninformed model of the problem to be solved. This means that these\napproaches are also useful as a 'rapid prototyping' approach to\nsolving problems.</li>\n</ul>\n\n<p>However, despite their widespread use for the purpose, note that GAs are actually <a href=\"http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.161.5655\" rel=\"nofollow\"><em>not</em> function optimizers</a> - GA mechanisms tend not to explore 'outlying' regions of the search space in the hope of finding some distant high quality solution, but rather to cluster around more easily attainable peaks in the 'fitness landscape'.</p>\n\n<p>More detail on the applicability of GAs is given in a famous early paper <a href=\"http://download.springer.com/static/pdf/167/art%253A10.1007%252FBF00993046.pdf?originUrl=http%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2FBF00993046&amp;token2=exp=1470247330~acl=%2Fstatic%2Fpdf%2F167%2Fart%25253A10.1007%25252FBF00993046.pdf%3ForiginUrl%3Dhttp%253A%252F%252Flink.springer.com%252Farticle%252F10.1007%252FBF00993046*~hmac=5686f87439a520f9c56ff0bd34b38bacf7c442edb6ecaaf9b93a2c02d17815c2\" rel=\"nofollow\">\"What makes a problem hard for a Genetic Algorithm?\"</a></p>\n", "question": "<p>I've noticed that a few questions on this site mention genetic algorithms and it made me realize that I don't really know much about those.</p>\n\n<p>I have heard the term before, but it's not something I've ever used, so I don't have much idea about how they work and what they are good for. All I know is that they involve some sort of evolution and randomly changing values.</p>\n\n<p>Can you give me a short explanation, preferably including some sort of practical example that illustrates the basic principles?</p>\n"}, "id": "244"}, {"body": {"answer": "<p>This answer requests a practical example of how one might be used, which I will attempt to provide in addition to the other answers. They seem to due a very good job of explaining what a genetic algorithm is. So, this will give an example.</p>\n\n<p>Let's say you have a neural network (although they are not the only application of it), which, from some given inputs, will yield some outputs. A genetic algorithm can create a population of these, and by seeing which output is the best, breed and kill off members of the population. Eventually, this should optimise the neural network if it is complicated enough. </p>\n\n<p>Here is a demonstration I've made, which despite being badly coded, might help you understand. <a href=\"http://khrabanas.github.io/projects/evo/evo.html\" rel=\"nofollow\">http://khrabanas.github.io/projects/evo/evo.html</a>\nHit the evolve button and mess around with the goals.</p>\n\n<p>It uses a simple genetic algorithm to breed, mutate and decide between which of the population survive. Depending on how the input variables are set, the network will be able to get to some level of closeness to them.In this fashion, wthe population will likely eventually become a homogenenous group, whose outputs resemble the goals.</p>\n\n<p>The genetic algorithm is trying to create a \"neural network\" of sorts, that by taking in RGB, will yield an output color. First it generates a random population. It then by taking 3 random members from the population, selecting the one with the lowest fitness and removing it from the population. The fitness is equal to the difference in the top goal squared + the difference in the bottom goal squared. It then breeds the two remaining ones together and adds the child to the same place in the population as the dead member.\nWhen mating occurs, there is a chance a mutation will occur. This mutation will change one of the values randomly.</p>\n\n<p>As a side note, due to how it is set up, it is impossible for it to be totally correct in many cases, though it will reach relative closeness.</p>\n", "question": "<p>I've noticed that a few questions on this site mention genetic algorithms and it made me realize that I don't really know much about those.</p>\n\n<p>I have heard the term before, but it's not something I've ever used, so I don't have much idea about how they work and what they are good for. All I know is that they involve some sort of evolution and randomly changing values.</p>\n\n<p>Can you give me a short explanation, preferably including some sort of practical example that illustrates the basic principles?</p>\n"}, "id": "246"}, {"body": {"answer": "<p>First up, those images (even the first few) aren't complete trash despite being junk to humans; they're actually finely tuned with various advanced techniques, including another neural network.</p>\n\n<blockquote>\n  <p>The deep neural network is the pre-trained network modeled on AlexNet provided by <a href=\"https://github.com/BVLC/caffe\">Caffe</a>. To evolve images, both the directly encoded and indirectly encoded images, we use the <a href=\"https://github.com/jbmouret/sferes2\">Sferes</a> evolutionary framework. The entire code base to conduct the evolutionary experiments can be download [sic] <a href=\"https://github.com/Evolving-AI-Lab/fooling\">here</a>. The code for the images produced by gradient ascent is available <a href=\"https://github.com/Evolving-AI-Lab/fooling/tree/master/caffe/ascent\">here</a>.</p>\n</blockquote>\n\n<p>Images that are actually random junk were correctly recognized as nothing meaningful:</p>\n\n<blockquote>\n  <p>In response to an unrecognizable image, the networks could have output a low confidence for each of the 1000 classes, instead of an extremely high confidence value for one of the classes. In fact, they do just that for randomly generated images (e.g. those in generation 0 of the evolutionary run)</p>\n</blockquote>\n\n<p>The original goal of the researchers was to use the neural networks to automatically generate images that look like the real things (by getting the recognizer's feedback and trying to change the image to get a more confident result), but they ended up creating the above art. Notice how even in the static-like images there are little splotches - usually near the center - which, it's fair to say, are triggering the recognition.</p>\n\n<blockquote>\n  <p>We were not trying to produce adversarial, unrecognizable images. Instead, we were trying to produce recognizable images, but these unrecognizable images emerged.</p>\n</blockquote>\n\n<p>Evidently, these images had just the right distinguishing features to match what the AI looked for in pictures. The \"paddle\" image does have a paddle-like shape, the \"bagel\" is round and the right color, the \"projector\" image is a camera-lens-like thing, the \"computer keyboard\" is a bunch of rectangles (like the individual keys), and the \"chainlink fence\" legitimately looks like a chain-link fence to me.</p>\n\n<blockquote>\n  <p>Figure 8. Evolving images to match DNN classes produces a tremendous diversity of images. Shown are images selected to showcase diversity from 5 evolutionary runs. The diversity suggests that the images are non-random, but that instead evolutions producing [sic] discriminative features of each target class.</p>\n</blockquote>\n\n<p>Further reading: <a href=\"http://www.evolvingai.org/files/DNNsEasilyFooled_cvpr15.pdf\">the original paper</a> (large PDF)</p>\n", "question": "<p>The following <a href=\"http://www.evolvingai.org/fooling\">page</a>/<a href=\"http://www.evolvingai.org/files/DNNsEasilyFooled_cvpr15.pdf\">study</a> demonstrates that the deep neural networks are easily fooled by giving high confidence predictions for unrecognisable images, e.g.</p>\n\n<p><a href=\"http://i.stack.imgur.com/7pgrH.jpg\"><img src=\"http://i.stack.imgur.com/7pgrH.jpg\" alt=\"Evolved images that are unrecognisable to humans, but that state-of-the-art DNNs trained on ImageNet believe with &gt;= 99.6% certainty to be a familiar object. This result highlights differences between how DNNs and humans recognise objects. Directly and indirectly encoded images\"></a></p>\n\n<p><a href=\"http://i.stack.imgur.com/pBm48.png\"><img src=\"http://i.stack.imgur.com/pBm48.png\" alt=\"Evolving images to match DNN classes produces a tremendous diversity of images. The mean DNN confidence scores for these images is 99.12% for the listed class, meaning that the DNN believes with near-certainty that the image is that type of thing. Shown are images selected to showcase diversity from 5 independent evolutionary runs. The images shed light on what the DNN network cares about, and what it does not, when classifying an image. For example, a school bus is alternating yellow and black lines, but does not need to have a windshield or wheels\"></a></p>\n\n<p>How this is possible? Can you please explain ideally in plain English?</p>\n"}, "id": "250"}, {"body": {"answer": "<p>In May 2016 Google announced a custom ASIC which was is specifically built for machine learning<sup><a href=\"https://en.wikipedia.org/wiki/TensorFlow#DistBelief\" rel=\"nofollow\">wiki</a></sup> and tailored for <a href=\"https://en.wikipedia.org/wiki/TensorFlow\" rel=\"nofollow\">TensorFlow</a>. It is using <a href=\"https://en.wikipedia.org/wiki/Tensor_processing_unit\" rel=\"nofollow\">tensor processing unit</a> (TPU) which is a programmable microprocessor designed to accelerate artificial neural networks.</p>\n\n<p><a href=\"https://web.stanford.edu/group/brainsinsilicon/goals.html\" rel=\"nofollow\">NeuroCores</a>, 12x14 sq-mm chips which can be interconnected in a binary tree, see: <a href=\"https://en.wikipedia.org/wiki/TensorFlow#DistBelief\" rel=\"nofollow\">Neurogrid</a>, a supercomputer which can provide an option for brain simulations.</p>\n\n<p><a href=\"https://en.wikipedia.org/wiki/TrueNorth\" rel=\"nofollow\">TrueNorth</a>, a neuromorphic CMOS chip produced by IBM, which has 4096 cores in the current chip, each can simulate 256 programmable silicon \"neurons\", giving a total of over a million neurons.</p>\n\n<p>Further readings: <a href=\"https://en.wikipedia.org/wiki/Neuromorphic_engineering\" rel=\"nofollow\">Neuromorphic engineering</a>, <a href=\"https://en.wikipedia.org/wiki/Vision_processing_unit\" rel=\"nofollow\">Vision processing unit</a>, <a href=\"https://en.wikipedia.org/wiki/Category:AI_accelerators\" rel=\"nofollow\">AI accelerators</a></p>\n\n<hr>\n\n<p>As a side note, you can always use an FPGA based piece of hardware which you can implement selected genetic algorithm (GA) directly in hardware. For example the <a href=\"https://en.wikipedia.org/wiki/CoDi#Implementation_in_Hardware\" rel=\"nofollow\">CoDi model</a> was implemented in the FPGA based CAM-Brain Machine (CBM)<sup><a href=\"http://link.springer.com/article/10.1023%2FA%3A1011286308522\" rel=\"nofollow\">2001</a></sup>.</p>\n", "question": "<p>I'm interested in hardware implementation of ANNs (artificial neural networks). Are there any popular existing technology implementations in form of microchips which are purpose designed to run artificial neural networks? For example, a chip which is optimised for an application like image recognition or something similar?</p>\n"}, "id": "253"}, {"body": {"answer": "<p>It depends what you mean by 'develop themselves' - in a rather limited sense, an online machine learning approach such as Genetic Algorithms 'develops itself' to provide better solutions.</p>\n\n<p>There is already a theoretical model that represents the <em>ultimate</em> concept of development: Juergen Schmidhuber's <a href=\"http://arxiv.org/abs/cs/0309048\" rel=\"nofollow\">Goedel Machine</a> is constructed so as to self-modify when it can prove that this modification is optimal.</p>\n", "question": "<p>Is it possible that at some time in the future, AIs will be able to initiatively develop themselves, rather than passively being developed by humanity?</p>\n"}, "id": "264"}, {"body": {"answer": "<p>Not exactly a detective mystery, but according to a slide dated June 2012 from a NSA PowerPoint presentation (see: Glenn Greenwald\u2019s site), NSA used some kind of <em>Skynet</em> AI technology to analyze and detect suspicious patterns from location and communication data in order to create a watch list of suspected terrorists. This helped to track associated members of Al-Qa\u2019ida as well as the Muslim Brotherhood. And I'm sure their AI solved a lot of mysteries and found some controversial figures.</p>\n\n<p>Source: <a href=\"https://theintercept.com/2015/05/08/u-s-government-designated-prominent-al-jazeera-journalist-al-qaeda-member-put-watch-list/\" rel=\"nofollow\">U.S. Government Designated Prominent Al Jazeera Journalist as a member of AI Qaeda</a></p>\n\n<p>For more details check: <a href=\"https://theintercept.com/document/2015/05/08/skynet-courier/\" rel=\"nofollow\">SKYNET: Courier Detection via Machine Learning</a> for courier detection data and charts generated by analyzing GSM metadata using machine learning algorithms. Also <a href=\"https://theintercept.com/document/2015/05/08/skynet-applying-advanced-cloud-based-behavior-analytics/\" rel=\"nofollow\">Applying Advanced Cloud-based Behavior Analytics</a>.</p>\n", "question": "<p>In detective novels, the point is often that the reader gets enough information to solve the crime themselves. This \"puzzle\" aspect of detective novels is part of the attraction.</p>\n\n<p>Often the difficulty for humans is to keep track of all the variables - events, items, motivations.<br>\nAn AI would have an easier time keeping track of all the details, but would rely on real-world knowledge to prevent making crazy mistakes. For example, if it was stated that a character took the train, the AI would need to know that this is a method of transportation - that it changes the location property of an agent over time.</p>\n\n<p>Has an AI ever been able to solve a detective mystery?</p>\n"}, "id": "265"}, {"body": {"answer": "<p>While I'm not familiar with any explicit statements regarding what a Multilayer Perceptron (MLP) <em>cannot</em> learn, I can provide some further detail on the positive statements you made about MLP capabilities:</p>\n\n<p>A MLP with a single hidden layer is capable of what is commonly termed <a href=\"https://en.wikipedia.org/wiki/Universal_approximation_theorem\" rel=\"nofollow\">'Universal Function Approximation'</a>, i.e. it can approximate any bounded continuous function to an arbitrary degree of accuracy. With two hidden layers, the boundness restriction is removed <a href=\"http://link.springer.com/article/10.1007/BF02551274\" rel=\"nofollow\">[Cybenko, 1988]</a>.</p>\n\n<p><a href=\"http://www.sciencedirect.com/science/article/pii/089360809190009T\" rel=\"nofollow\">This paper</a> goes on to demonstrate that this is true for a wide range of activation functions (not necessarily nonlinear). 3 layer MLPs are also capable of representing any boolean function (although they may require an exponential number of neurons).</p>\n\n<p>See also <a href=\"http://cstheory.stackexchange.com/questions/7894/universal-function-approximation\">this interesting answer</a> on CS SE about other Universal approximators.</p>\n", "question": "<p>In 1969, Seymour Papert and Marvin Minsky showed that Perceptrons could not learn the XOR function.  </p>\n\n<p>This was solved by the backpropagation network with at least one hidden layer. This type of network can learn the XOR function.</p>\n\n<p>I believe I was once taught that every problem that could be learnt by a backpropagation neural network with multiple hidden layers, could also be learnt by a backpropagation neural network with a single hidden layer. (Although possible a nonlinear activation function was required).</p>\n\n<p>However, it is unclear to me what the limits are to backpropagation neural networks themselves. Which patterns <strong>cannot</strong> be learnt by a backpropgation neural network?</p>\n"}, "id": "266"}, {"body": {"answer": "<p>The following post has a bit of math, which I hope helps to explain the problem better. Unfortunately it seems, this SE site does not support LaTex:</p>\n\n<p>Document summarization is very much an open problem in AI research. One way this task is currently handled is called \"extractive summarization\". The basic strategy is as follows: Split this document into sentences and we will present as a summary a subset of sentences which together cover all the important details in the post. Assign sentence i, 1&lt;=i&lt;=n, a variable z_i \\in {0,1}, where z_i = 1 indicates the sentence was selected and z_i = 0 means the sentence was left out. Then, z_i z_j = 1 if and only if both sentences were chosen. We will also define the importance of each sentence w_i for sentence i and interaction terms w_{ij} between sentences i and j. </p>\n\n<p>Let x_i be the feature vectors for sentence i. w_i = w(x_i) captures how important it is to include this sentence (or the topics covered by it) while w_ij = w(x_i,x_j) indicates the amount of overlap between sentences in our summary. Finally we put all this in a minimization problem:</p>\n\n<p>maximize_{z_i} \\sum_{i} w_i z_i - w_{ij} z_i z_j \ns.t. z_i = 0 or 1</p>\n\n<p>This tries to maximize the total weight of the sentences covered and tries to minimize the amount of overlap. This is an integer programming problem similar to finding the lowest weight independent set in a graph and many techniques exist to solve such problems.</p>\n\n<p>This design, in my opinion, captures the fundamental problems in text summarization and can be extended in many ways. We will discuss those in a bit, but first we need to completely specify the features w. w_i = w(x_i) could be a function only of the sentence i, but it could also depends on the place of the sentence in the document or its context (Is the sentence at the beginning of a paragraph? Does it share common words with the title? What is its length? Does it mention any proper nouns? etc)</p>\n\n<p>w_ij = w(x_i,x_j) is a similarity measure. It measures how much repetition there will be if we include both words in the sentence. It can be defined by looking at common words between sentences. We can also extract topics or concepts from each sentence and see how many are common between them, and use language features like pronouns to see if one sentence expands on another.</p>\n\n<p>To improve the design, first, we could do <em>keyphrase extraction</em>, i.e. identify key phrases in the text and choose to define the above problem in terms of those instead of trying to pick sentences. That is a similar problem to what Google does to summarize news articles in their search results, but I am not aware of the details of their approach. We could also break the sentences up further into concepts and try to establish the semantic meaning of the sentences ( Ponzo and Fila are people P1 and P2, a mall is a place P, P1 and P2 went to the place P at time T (day). Mode of transport walking.... and so on). To do this, we would need to use a semantic ontology or other common-sense knowledge database. However, all the parts of this last semantic classification problem are open and I have not seen anyone make satisfactory progress on it yet. </p>\n\n<p>We could also tweak the loss function above so that instead of the setting the tradeoff between the sentence importance w_i and the diversity score w_ij by hand, we could learn it from data. One way to do this is to use Conditional Random Fields to model the data, but many others surely exist.</p>\n\n<p>I hope this answer explained the basic problems that need to be solved to make progress towards good summarization systems. This is an active field of research and you will find the most recent papers via Google Scholar, but first read the <a href=\"https://en.wikipedia.org/wiki/Automatic_summarization\" rel=\"nofollow\">Wikipedia page</a> to learn the relevant terms</p>\n", "question": "<p>If I have a paragraph I want to summarize, for example:</p>\n\n<blockquote>\n  <p>Ponzo and Fila went to the mall during the day. They walked for a long while, stopping at shops. They went to many shops. At first, they didn't buy anything. After going to a number of shops, they eventually bought a shirt, and a pair of pants.</p>\n</blockquote>\n\n<p>Better summarized as:</p>\n\n<blockquote>\n  <p>They shopped at the mall today and bought some clothes.</p>\n</blockquote>\n\n<p>What is the best AI strategy to automate this process, if there is one? If there isn't, is it because it would be dependent on first having an external information resource that would inform any algorithm? Or is it because the problem is inherently contextual?</p>\n"}, "id": "267"}, {"body": {"answer": "<p>ANN research does not try to model biological neurons, as the aim is to achieve better performance at prediction tasks. However, there is a body of literature in neuroscience that looks at <a href=\"https://en.wikipedia.org/wiki/Models_of_neural_computation\" rel=\"nofollow\">Computational models of neurons</a>. Neurons are complicated cells and our understanding of neurons is still not complete. </p>\n", "question": "<p>On the Wikipedia page we can read the basic structure of an artificial neuron (a model of biological neurons) which consist:</p>\n\n<ul>\n<li>Dendrites - acts as the input vector,</li>\n<li>Soma - acts as the summation function,</li>\n<li>Axon - gets its signal from the summation behavior which occurs inside the soma.</li>\n</ul>\n\n<p>I've checked <a href=\"https://en.wikipedia.org/wiki/Deep_learning\" rel=\"nofollow\">Deep learning</a> wiki page, but I couldn't find any references to dendrites, soma or axons.</p>\n\n<p>So my question is, which type of artificial neural network implements or can mimic such model most closely?</p>\n"}, "id": "268"}, {"body": {"answer": "<p>An important question that does not yet have a satisfactory answer in neural network research is how DNNs come up with the predictions they offer. DNNs effectively work (though not exactly) by matching patches in the images to a \"dictionary\" of patches, one stored in each neuron (see <a href=\"http://research.google.com/archive/unsupervised_icml2012.html\">the youtube cat paper</a>). Thus, it may not have a high level view of the image since it only looks at patches, and images are usually downscaled to much lower resolution to obtain the results in current generation systems. Methods which look at how the components of the image interact may be able to avoid these problems.</p>\n\n<p>Some questions to ask for this work are: How confident were the networks when they made these predictions? How much volume do such adversarial images occupy in the space of all images?</p>\n\n<p>Some work I am aware of in this regard comes from Dhruv Batra and Devi Parikh's Lab at Virginia Tech who look into this for question answering systems: <a href=\"http://arxiv.org/pdf/1606.07356v1.pdf\">Analyzing the Behavior of Visual Question Answering Models</a> and <a href=\"https://filebox.ece.vt.edu/~dbatra/papers/gmpb_icmlvis16.pdf\">Interpreting Visual Question Answering models</a>. </p>\n\n<p>More such work is needed, and just as the human visual system does also get fooled by such \"optical illusions\", these problems may be unavoidable if we use DNNs, though AFAIK nothing is yet known either way, theoretically or empirically. </p>\n", "question": "<p>The following <a href=\"http://www.evolvingai.org/fooling\">page</a>/<a href=\"http://www.evolvingai.org/files/DNNsEasilyFooled_cvpr15.pdf\">study</a> demonstrates that the deep neural networks are easily fooled by giving high confidence predictions for unrecognisable images, e.g.</p>\n\n<p><a href=\"http://i.stack.imgur.com/7pgrH.jpg\"><img src=\"http://i.stack.imgur.com/7pgrH.jpg\" alt=\"Evolved images that are unrecognisable to humans, but that state-of-the-art DNNs trained on ImageNet believe with &gt;= 99.6% certainty to be a familiar object. This result highlights differences between how DNNs and humans recognise objects. Directly and indirectly encoded images\"></a></p>\n\n<p><a href=\"http://i.stack.imgur.com/pBm48.png\"><img src=\"http://i.stack.imgur.com/pBm48.png\" alt=\"Evolving images to match DNN classes produces a tremendous diversity of images. The mean DNN confidence scores for these images is 99.12% for the listed class, meaning that the DNN believes with near-certainty that the image is that type of thing. Shown are images selected to showcase diversity from 5 independent evolutionary runs. The images shed light on what the DNN network cares about, and what it does not, when classifying an image. For example, a school bus is alternating yellow and black lines, but does not need to have a windshield or wheels\"></a></p>\n\n<p>How this is possible? Can you please explain ideally in plain English?</p>\n"}, "id": "271"}, {"body": {"answer": "<p>To have any chance at answering this, you'd first need a rigorous definition of \"true artificial intelligence\", which we don't have.  And even if you had that, the best answer would probably be \"nobody knows.\"  We don't even understand exactly how human intelligence (which is probably the best model of intelligence we have available to study) works.  What we do know (or think we know) is that ANN's are at best a very superficial replica of brain function.  It may turn out that they're absolutely the wrong path to achieving \"true artificial intelligence\" although I expect most people would be surprised if that turned out to be the case.</p>\n\n<p>What probably wouldn't be so surprising would be if some other technique emerged which is better than ANN's, OR if it turns out that you need an ensemble of techniques.  Personally I think it's close to self-evident that the brain works largely in a probabilistic fashion, but it's also clear that we do sometimes use symbolic processing / deductive logic / rules / etc.  And right now, ANN's don't give you much in the way of reasoning, deduction, etc.  So we may ultimately find that we have to combine a probabilistic approach like ANN's with other techniques - maybe Inductive Logic Programming or something of that nature. </p>\n", "question": "<p>According to my knowledge most of the current artificial intelligence study uses of some kind of neural network or its variants. A good example would be DeepMind's alphago which I believe is a deep neural network, for vision CNN, text, music and other ordered features RNN's, etc. But for machine learning application we have neural networks, support vector machines, random forest, regression methods, etc. available for applications. </p>\n\n<p>So are neural networks and its variants the only way to reach \"true\" artificial intelligence? </p>\n"}, "id": "272"}, {"body": {"answer": "<p>I am assuming each image contains a single object.</p>\n\n<p>It is possible, however, it is not as easy as you might think. Firstly, you need extract as many features as possible: original image, <a href=\"https://en.wikipedia.org/wiki/Local_binary_patterns\" rel=\"nofollow\">LBP</a>, <a href=\"https://en.wikipedia.org/wiki/Scale-invariant_feature_transform\" rel=\"nofollow\">SIFT</a>, <a href=\"https://en.wikipedia.org/wiki/Image_moment#Moment_invariants\" rel=\"nofollow\">moments</a>, contour descriptors to name a few. Than concatenate these features into a single feature vector. After this step, use clustering. You will need a lot of samples to compensate for the number of features. After clustering, use a correlation method to find which features are related to each cluster.</p>\n\n<p>If you need features to classify within a cluster, you could do a second clustering with full set of features and apply the same method. The features that are selected for a cluster will not be suitable to classify within the cluster.</p>\n", "question": "<p>Is it possible for <em>unsupervised learning</em> to learn about high-level, class-specific features given only unlabelled images? For example detecting human or animal faces? If so, how?</p>\n"}, "id": "1286"}, {"body": {"answer": "<ul>\n<li><strong>MLP</strong>: uses dot products (between inputs and weights) and <a href=\"https://en.wikipedia.org/wiki/Sigmoid_function\" rel=\"nofollow\">sigmoidal activation functions</a> (or other monotonic functions such as <a href=\"https://en.wikipedia.org/wiki/Rectifier_(neural_networks)\" rel=\"nofollow\">ReLU</a>) and training is usually done through backpropagation for all layers (which can be as many as you want). This type of neural network is used in deep learning with the help of many techniques (such as dropout or batch normalization);</li>\n<li><strong>RBF</strong>: uses Euclidean distances (between inputs and weights, which can be viewed as centers) and (usually) <a href=\"https://en.wikipedia.org/wiki/Gaussian_function\" rel=\"nofollow\">Gaussian activation functions</a> (which could be multivariate), which makes neurons more locally sensitive. Thus, RBF neurons have maximum activation when the center/weights are equal to the inputs (look at the image below). Due to this property, RBF neural networks are good for novelty detection (if each neuron is centered on a training example, inputs far away from all neurons constitute novel patterns) but not so good at extrapolation. Also, RBFs may use backpropagation for learning, or hybrid approaches with unsupervised learning in the hidden layer (they usually have just 1 hidden layer). Finally, RBFs make it easier to <a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.294.5088&amp;rep=rep1&amp;type=pdf\" rel=\"nofollow\">grow new neurons</a> during training.</li>\n</ul>\n\n<p><a href=\"http://i.stack.imgur.com/9u5fQ.jpg\" rel=\"nofollow\"><img src=\"http://i.stack.imgur.com/9u5fQ.jpg\" alt=\"enter image description here\"></a></p>\n", "question": "<p>What are the main differences between two types of feedforward networks such as <em>multilayer perceptrons</em> (MLP) and <em>radial basis function</em> (RBF)?</p>\n\n<p>What are the fundamental differences between these two types?</p>\n"}, "id": "1287"}, {"body": {"answer": "<p><em>I tend to think this question is border-line and may get close. A few comments for now, though.</em></p>\n\n<hr>\n\n<p>There are (at least) two issues with reproducing the work of a company like DeepMind:</p>\n\n<ul>\n<li>Technicalities missing from publications.</li>\n<li>Access to the same level of data.</li>\n</ul>\n\n<p>Technicalities should be workable. Some people have reproduced some of the <a href=\"https://github.com/kristjankorjus/Replicating-DeepMind\" rel=\"nofollow\">Atari</a> gaming stunts. AlphaGo is seemingly more complex and will require more work, yet that should be feasible at some point in the future (individuals may lack computing resources today).</p>\n\n<p>Data can be more tricky. Several companies open their data sets, but data is also the nerve of the competition...</p>\n", "question": "<p>Deep Mind has published a lot of works on deep learning in the last years, most of them state-of-the-art on their respective tasks. But how much of this work has actually been reproduced by the AI community? For instance, the Neural Turing Machine paper seems to be very hard to reproduce, according to other researchers.</p>\n"}, "id": "1292"}, {"body": {"answer": "<p>Firstly, an AGI could conceivably exhibit all of the observable properties of intelligence without being conscious. Although that may seem counter-intuitive, at present we have no physical theory that allows us to detect consciousness (philosophically speaking, a <a href=\"http://plato.stanford.edu/entries/zombies/\" rel=\"nofollow noreferrer\">'Zombie'</a> is indistinguishable from a non-Zombie - see the writing of <a href=\"http://rads.stackoverflow.com/amzn/click/0316180661\" rel=\"nofollow noreferrer\">Daniel Dennett</a> and <a href=\"http://consc.net/books/tcm/\" rel=\"nofollow noreferrer\">David Chalmers</a> for more on this). Destroying a non-conscious entity has the same moral cost as destroying a chair.</p>\n\n<p>Also, note that 'destroy' doesn't necessarily mean the same for entities with <em>persistent substrate</em>, i.e. their 'brain state' can be reversibly serialized to some other storage medium and/or multiple copies of them can co-exist. So if by 'destroy' we simply mean 'switch off', then an AGI might conceivably be reassured of a subsequent re-awakening. Douglas Hofstadter gives an interesting description of such an 'episodic consciousness' in <a href=\"http://themindi.blogspot.co.uk/2007/02/chapter-26-conversation-with-einsteins.html\" rel=\"nofollow noreferrer\">\"A Conversation with Einstein's Brain\"</a></p>\n\n<p>If by 'destroy', we mean 'irrevocably erase with no chance of re-awakening', then (unless we have a physical test which proves it is <em>not</em> conscious) destroying an entity with a seemingly human-level awareness is clearly morally tantamount to murder. To believe otherwise would be <em>substrate-ist</em> - a moral stance which may one day be seen as antiquated as racism.</p>\n", "question": "<p>According to wikipedia <a href=\"https://en.wikipedia.org/wiki/Artificial_general_intelligence\" rel=\"nofollow\">Artificial general intelligence(AGI)</a></p>\n\n<blockquote>\n  <p>Artificial general intelligence (AGI) is the intelligence of a\n  (hypothetical) machine that could successfully perform any\n  intellectual task that a human being can. </p>\n</blockquote>\n\n<p>According to below image todays artifical intellgence is same as that of a lizards.</p>\n\n<p><a href=\"http://i.stack.imgur.com/gddKB.jpg\" rel=\"nofollow\"><img src=\"http://i.stack.imgur.com/gddKB.jpg\" alt=\"enter image description here\"></a></p>\n\n<p>Lets assume(or not) that within 10-20 years we humans are successful in creating a AGI or AGIs. As AGI has the same intelligence and <a href=\"http://futurism.com/scientist-claims-to-be-on-the-verge-of-making-an-ai-that-feels-true-emotions/\" rel=\"nofollow\">emotions</a> as that of humans because according to wikipedia definition it can perform same intellectual task of a human. Then can we destroy an AGI without its consent? Do this be considered as murder?</p>\n"}, "id": "1293"}, {"body": {"answer": "<p>I might be wrong, but I do not believe that something of the scope you describe would be possible with the current state of technology. It would require a lot of things which are still in relatively early stages of research.</p>\n\n<p>For one, just extracting relevant information from text is a huge task by itself. Doubly so with a novel which contains a large amount of unimportant details.</p>\n\n<p>It might perhaps be easier if the input was presented in the form of some sort of list of important facts. But it would still be rather difficult for the AI to connect them and find a solution.</p>\n\n<p>As an example, let's say that we have these two facts:</p>\n\n<ul>\n<li>Alice died of a snake bite.</li>\n<li>Bob was seen buying a couple of mice recently.</li>\n</ul>\n\n<p>To a human, it seems obvious that the mice were bought to feed a venomous snake. However, it would probably require a tremendous effort to teach an AI to make such connections.</p>\n\n<p>Disclaimer: I don't do text processing myself, so I'm not quite up-to-date on the current state-of-the-art. It's possible that some of these things have already been done in some form. If anyone knows more about the subject, please correct me if I'm wrong.</p>\n", "question": "<p>In detective novels, the point is often that the reader gets enough information to solve the crime themselves. This \"puzzle\" aspect of detective novels is part of the attraction.</p>\n\n<p>Often the difficulty for humans is to keep track of all the variables - events, items, motivations.<br>\nAn AI would have an easier time keeping track of all the details, but would rely on real-world knowledge to prevent making crazy mistakes. For example, if it was stated that a character took the train, the AI would need to know that this is a method of transportation - that it changes the location property of an agent over time.</p>\n\n<p>Has an AI ever been able to solve a detective mystery?</p>\n"}, "id": "1298"}, {"body": {"answer": "<p>According to <a href=\"http://link.springer.com/chapter/10.1007/3-540-44989-2_118\">this paper</a>, complex-valued ANNs (C-ANNs) can solve problems such as XOR and symmetry detection with a smaller number of layers than real ANNs (for both of these a 2 layer C-ANN suffices, whereas a 3-layer R-ANN is required).</p>\n\n<p>I believe that it is still an open question as to how useful this result is in practice (e.g. whether it actually makes finding the right topology easier), so at present the key practical advantage of C-ANNs is when they are a closer model for the problem domain.</p>\n\n<p>Application areas are then where complex values arise naturally, e.g. in optics, signal processing/FFT or electrical engineering.</p>\n", "question": "<p>During my research, I've stumbled upon \"complex-valued neural networks\", which are neural networks that work with complex-valued inputs (probably weights too). What are the advantages (or simply the applications) of this kind of neural network over real-valued neural networks?</p>\n"}, "id": "1299"}, {"body": {"answer": "<p>As explained in an answer to <a href=\"http://ai.stackexchange.com/questions/240/what-exactly-are-genetic-algorithms-and-what-sort-of-problems-are-they-good-for/244#244\">this AI SE question</a>, GAs are 'satisficers' rather than 'optimizers' and tend not to explore 'outlying' regions of the search space. Rather, the population tends to cluster in regions that are 'fairly good' according to the fitness function.</p>\n\n<p>In contrast, I believe the thinking is that novelty affords a kind of dynamic fitness, tending to push the population away from previously discovered areas.</p>\n", "question": "<p>The <a href=\"http://fabelier.org/novelty-search-and-open-ended-evolution-by-ken-stanley/\" rel=\"nofollow\">author</a> claims that guiding evolution by novelty alone (without explicit goals) can solve problems even better than using explicit goals. In other words, using a novelty measure as a fitness function for a genetic algorithm works better than a goal-directed fitness function. How is that possible?</p>\n"}, "id": "1300"}, {"body": {"answer": "<p><em>Question on-topicness questionable, but...</em></p>\n\n<hr>\n\n<p>The most logical reason why PHP is unsuited for neural networks is that PHP is, well, intended to be used for server side webpages. It can connect to various external resources, such as databases, via native language features. It is very much a glue language, and not a processing language. PHP is also mostly stateless, only allowing you to store state in either clients, file storage or databases.</p>\n\n<p>As such, it's <strong>not</strong> suitable for this sort of thing - not because PHP is a high level language, but rather because it's so request based and focused towards creating pages to serve to clients.</p>\n\n<p>That won't stop people from trying, though - there are various esoteric programming languages out there in which regular programming would be an insane task or not possible at all - but from a ease of development perspective, making a neural network in PHP makes no sense.</p>\n", "question": "<p>I read some information<sup>1</sup> about attempts to build neural networks in the PHP programming language. Personally I think PHP is not the right language to do so at all probably because it's a high-level language, I assume low level language are way more suitable for AI in terms of performance and scalability. </p>\n\n<p>Is there a good/logical reason why you should or shouldn't use PHP as a language to write AI in?</p>\n\n<p><em><sup>1</sup></em> <a href=\"http://www.developer.com/lang/php/creating-neural-networks-in-php.html\" rel=\"nofollow\">http://www.developer.com/lang/php/creating-neural-networks-in-php.html</a> and <a href=\"http://stackoverflow.com/questions/2303357/are-there-any-artificial-intelligence-projects-in-php-out-there\">http://stackoverflow.com/questions/2303357/are-there-any-artificial-intelligence-projects-in-php-out-there</a> </p>\n"}, "id": "1305"}, {"body": {"answer": "<p>Actually, yes. Remember, that due to the history of PHP development, some very good things has formed what we have now:</p>\n\n<ul>\n<li><p>From a simple/laggy/limited interpreter in PHP 3, we have now three mainstream lines coming one-by-one like v5/v6/v7 with <em>full bytecode</em> supported.   </p></li>\n<li><p>In PHP v7 you don't even need a bytecode cache due to HHVM, old Zend VM is a hell-good-debugged and using a cacher like XCache you can achieve a true native execution speed <strong>and</strong> payload</p></li>\n<li><p>The PHP language interface allows <strong>any</strong> external C/C++ library <em>just to be added</em> as a module via very simple wrapper that can be written by the person that just red Kerrigan&amp;Richie and Straustrup base books on C and C++. This is amazing feature, exclusive to PHP as far as I know</p></li>\n<li><p>In PHP v7 you're welcome to use <em>native</em> multi-threading and even CUDA-based things, if you wish to do it. I did it, so I can confirm that it works</p></li>\n</ul>\n", "question": "<p>I read some information<sup>1</sup> about attempts to build neural networks in the PHP programming language. Personally I think PHP is not the right language to do so at all probably because it's a high-level language, I assume low level language are way more suitable for AI in terms of performance and scalability. </p>\n\n<p>Is there a good/logical reason why you should or shouldn't use PHP as a language to write AI in?</p>\n\n<p><em><sup>1</sup></em> <a href=\"http://www.developer.com/lang/php/creating-neural-networks-in-php.html\" rel=\"nofollow\">http://www.developer.com/lang/php/creating-neural-networks-in-php.html</a> and <a href=\"http://stackoverflow.com/questions/2303357/are-there-any-artificial-intelligence-projects-in-php-out-there\">http://stackoverflow.com/questions/2303357/are-there-any-artificial-intelligence-projects-in-php-out-there</a> </p>\n"}, "id": "1307"}, {"body": {"answer": "<p>Generally agree with @Inquisitive Lurker, but I think we also have a wide range of potential abilities/requirements. As with computer chess or Go, where there's a big difference between \"beating an honest novice human \" and \"beating all humans\"; there's a big difference between solving a simple kids' mystery and a complex adult novel.</p>\n\n<p>So I don't think there would be any problem writing a program that could solve a problem that is listed as a list of statements, or laid out as a (very young) children's book. However something like an Agatha Christie or John Le Carre's \"Tinker Tailor Soldier Spy\" (relatively simple solution, but the story is told in a complex manner) are far in the future.</p>\n\n<p>Sometimes an alternative approach might work. For example a neural network could probably solve all Colombo mysteries at the \"Who did it?\" level without a full \"Why?\" explanation, after only reading a few Colombo mysteries. The same is true for most kids!</p>\n", "question": "<p>In detective novels, the point is often that the reader gets enough information to solve the crime themselves. This \"puzzle\" aspect of detective novels is part of the attraction.</p>\n\n<p>Often the difficulty for humans is to keep track of all the variables - events, items, motivations.<br>\nAn AI would have an easier time keeping track of all the details, but would rely on real-world knowledge to prevent making crazy mistakes. For example, if it was stated that a character took the train, the AI would need to know that this is a method of transportation - that it changes the location property of an agent over time.</p>\n\n<p>Has an AI ever been able to solve a detective mystery?</p>\n"}, "id": "1310"}, {"body": {"answer": "<p>As per this <a href=\"http://www.dailymail.co.uk/sciencetech/article-2095214/As-scientists-discover-translate-brainwaves-words--Could-machine-read-innermost-thoughts.html\" rel=\"nofollow\">site</a></p>\n\n<blockquote>\n  <p>Researchers recorded the complex patterns of electrical activity generated by someone\u2019s brain, as the subject listened to someone talking.\n  By feeding those brainwave patterns into a computer, they were able to translate them back into actual words \u2014 the same words that the volunteer had been hearing.</p>\n  \n  <p><strong>The scientists behind the work believe they can now go further and read the unspoken thoughts of people using electrodes placed against the brain.</strong></p>\n  \n  <p>In the experiment, each patient listened to a recording of spoken words for five to ten minutes, while the net of electrodes placed under their skull monitored activity in a part of the brain involved in understanding speech called <a href=\"https://en.wikipedia.org/wiki/Wernicke%27s_area\" rel=\"nofollow\">Wernicke\u2019s area</a>.</p>\n  \n  <p>In one experiment, volunteers looked at black-and-white photographs while the scanner monitored activity in part of the brain that handles vision called the primary visual cortex.\n  A computer predicted accurately the image that the person was looking at purely from the pattern of brain activity.</p>\n</blockquote>\n\n<p>So AI might be able to read our emotions as well in near future.</p>\n\n<p>I found that <a href=\"http://electronics.howstuffworks.com/gadgets/high-tech-gadgets/google-glass-detect-emotions.htm\" rel=\"nofollow\">google glasses can detect people's emotion</a> via facial expression, voice tone e.t.c, (just like us), obviously not what they are thinking in their brain.</p>\n", "question": "<p>Have there been any studies which attempted to use AI algorithms to detect human thoughts or emotions based on brain activity, such as using <a href=\"https://en.wikipedia.org/wiki/Brain%E2%80%93computer_interface#EEG-based\" rel=\"nofollow\">BCI/EEG devices</a>?</p>\n\n<p>By this, I mean simple guesses such as whether the person was happy or angry, or what object (e.g. banana, car) they were thinking about.</p>\n\n<p>If so, did any of those studies show some degree of success?</p>\n"}, "id": "1311"}, {"body": {"answer": "<p>Any technology in the nuclear industry represents variance--it may be an improvement in safety or efficiency, or it may contain some unseen defect that allows a catastrophe to happen.</p>\n\n<p>But the simple <em>possibility</em> of harm isn't enough to swing the decision one way or the other. The application of AI methods--whether to the real-time control of plant variables, or the early detection of problems, or to the design of plants and their components--seems likely to be as beneficial as in other realms.</p>\n\n<p>For example, check out the <a href=\"http://www.lasar.polimi.it/?page_id=798\" rel=\"nofollow\">publication list</a> of a lab active in this area. Their paper I'm most familiar with is one in which they build a fault detector paired with a fault library classifier, so that the operators can be alerted not just that something is abnormal but what fault has probably occurred. This is done in such a way that standardized plants (such as, say, the French nuclear system) can share records with each other, meaning that <em>any</em> plant has the experience of <em>every</em> plant at their fingertips.</p>\n", "question": "<p>I've found <a href=\"http://link.springer.com/chapter/10.1007%2F978-1-4613-1009-9_2\" rel=\"nofollow\">this old scientific paper from 1988</a> about introduction of AI into nuclear power fields.</p>\n\n<p>Were or still are there any dangers by application of such algorithm? Are nuclear power plants or human life in risk if the algorithm will fail?</p>\n\n<p>Especially applications to the core, like cooling systems and other components which can be affected in negative way.</p>\n"}, "id": "1312"}, {"body": {"answer": "<p>It appears to not be published yet; the best available online are <a href=\"http://cseweb.ucsd.edu/~gary/cs200/s12/Hinton.pdf\" rel=\"nofollow\">these slides</a> for <a href=\"https://www.youtube.com/watch?v=TFIMqt0yT2I\" rel=\"nofollow\">this talk</a>. (Several people reference an earlier talk with <a href=\"http://techtv.mit.edu/collections/bcs/videos/30698-what-s-wrong-with-convolutional-nets\" rel=\"nofollow\">this link</a>, but sadly it's broken at time of writing this answer.)</p>\n\n<p>My impression is that it's an attempt to formalize and abstract the creation of subnetworks inside a neural network. That is, if you look at a standard neural network, layers are fully connected (that is, every neuron in layer 1 has access to every layer in neuron 0, and is itself accessed by every neuron in layer 2). But this isn't obviously useful; one might instead have, say, <em>n</em> parallel stacks of layers (the 'capsules') that each specializes on some separate task (which may itself require more than one layer to complete successfully).</p>\n\n<p>If I'm imagining its results correctly, this more sophisticated graph topology seems like something that could easily increase both the effectiveness and the interpretability of the resulting network.</p>\n", "question": "<p>Geoffrey Hinton has been researching something he calls \"capsules theory\" in neural networks. What is this and how does it work?</p>\n"}, "id": "1313"}, {"body": {"answer": "<p><a href=\"https://en.wikipedia.org/wiki/Swarm_intelligence\">Swarm intelligence</a> is the term for systems where relatively simple agents work together to solve a complicated problem in a decentralized fashion.</p>\n\n<p>In general, distributed computing methods are very important for dealing with problems at scale, and many of them embrace decentralization in a deep way. (Given the reality of hardware failure and the massive size of modern datasets relative to individual nodes, the less work is passed through a central bottleneck, the better.) While there are people interested in doing computation on the blockchain, it seems to me like it's unlikely to be competitive with computation in dedicated clusters (like AWS).</p>\n", "question": "<p>Has there been any attempts to deploy AI with blockchain technology? </p>\n\n<p>Are there any decentralized examples of AI networks with no central point of control with AI nodes acting independently (but according to a codified set of rules) creating, validating and storing the same shared decentralized database in many locations around the world?</p>\n"}, "id": "1315"}, {"body": {"answer": "<p>H+ magazine wrote an estimate <a href=\"http://hplusmagazine.com/2009/04/07/brain-chip/\" rel=\"nofollow\">in 2009</a> that seems broadly comparable to other things I've seen; they think the human brain is approximately 37 petaflops. A supercomputer larger than that 37 petaflop estimate <a href=\"https://en.wikipedia.org/wiki/Sunway_TaihuLight\" rel=\"nofollow\">exists today</a>.</p>\n\n<p>But emulation is hard. See <a href=\"http://stackoverflow.com/questions/471973/what-makes-building-game-console-emulators-so-hard\">this SO question about hardware emulation</a> or <a href=\"http://www.tested.com/tech/gaming/2712-why-perfect-hardware-snes-emulation-requires-a-3ghz-cpu/\" rel=\"nofollow\">this article</a> on emulating the SNES, in which they require <strong>140 times</strong> the processing power of the SNES chip to get it right. <a href=\"http://gizmodo.com/an-83-000-processor-supercomputer-only-matched-one-perc-1045026757\" rel=\"nofollow\">This 2013 article</a> claims that a second of human brain activity took 40 minutes to emulate on a 10 petaflop computer (a <em>2400-times</em> slowdown, not the 4-times slowdown one might naively expect).</p>\n\n<p>And all this assumes that neurons are relatively simple objects! It could be that the amount of math we have to do to model a single neuron is actually much more than the flops estimate above. Or it could be the case that dramatic simplifications can be made, and if we knew what the brain was actually trying to accomplish we could do it much more cleanly and simply. (One advantage that ANNs have, for example, is that they are doing computations with much more precision than we expect biological neurons to have. But this means emulation is <em>harder</em>, not easier, while replacement <em>is</em> easier.)</p>\n", "question": "<p>How much processing power is needed to emulate the human brain? More specifically, the neural simulation, such as communication between the neurons and processing certain data in real-time.</p>\n\n<p>I understand that this may be a bit of speculation and it's not possible to be accurate, but I'm sure there is some data available or research studies which attempted to estimate it based on our current understanding of the human brain.</p>\n"}, "id": "1316"}, {"body": {"answer": "<p>Not just how much, but what kind of processing power : there're specially-crafted <a href=\"http://www.research.ibm.com/cognitive-computing/neurosynaptic-chips.shtml#fbid=V01grppeOIs\" rel=\"nofollow\">dedicated chips</a>, and it has a <a href=\"http://www.dailymail.co.uk/sciencetech/article-3516047/IBM-reveals-brain-supercomputer-neurosynaptic-chip-replicate-16-million-neurons-work-using-hearing-aid-battery.html\" rel=\"nofollow\">practical applications</a>, so it's not a lab-only project</p>\n", "question": "<p>How much processing power is needed to emulate the human brain? More specifically, the neural simulation, such as communication between the neurons and processing certain data in real-time.</p>\n\n<p>I understand that this may be a bit of speculation and it's not possible to be accurate, but I'm sure there is some data available or research studies which attempted to estimate it based on our current understanding of the human brain.</p>\n"}, "id": "1317"}, {"body": {"answer": "<p>The simple answer is \"no, they aren't limited to images\": CNNs are also being used for natural language processing. (See <a href=\"http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/\" rel=\"nofollow\">here</a> for an introduction.)</p>\n\n<p>I haven't seen them applied to graphical data yet, but I haven't looked; there are some obvious things to try and so I'm optimistic that it would work.</p>\n", "question": "<p>Can a Convolutional Neural Network be used for pattern recognition in a problem domain where there are no pre-existing images, say by representing abstract data graphically? Would that always be less efficient?</p>\n\n<p><a href=\"https://youtu.be/py5byOOHZM8?t=815\">This developer</a> says current development could go further but not if there's a limit outside image recognition. </p>\n"}, "id": "1319"}, {"body": {"answer": "<p>Most of the existing AI bots which can play games use deep search from possible space and choose the best move. This is done by most of the chess, Go, Tic-Tac-Toe, etc bots.</p>\n\n<p>However, there has been a recent breakthrough where (deep)neural nets with deep search techniques like monte-carlo search, etc; which might be more human-like and demonstrate a much more complex game behaviour than the above bots. One such example is the Google's Alpha-Go bot.</p>\n", "question": "<p>Artificial intelligence is present in many games, both current and older games. How can such intelligence understand what to do? I mean, how can it behave like a human in a game, allowing you to play against itself, or that AI plays against itself?</p>\n\n<p>In games like Age of Empires, for example.</p>\n"}, "id": "1321"}, {"body": {"answer": "<p>There are a number of good answers here explaining what genetic algorithms are, and giving example applications. I'm adding some general purpose advice on what they are good for, but also cases where you should NOT use them. If my tone seems harsh, it is because using GAs in any of the cases in the Not Appropriate section will lead to your paper being <em>instantly</em> rejected from any top-tier journal. </p>\n\n<p>First, your problem MUST be an optimization problem. You need to define a \"fitness function\" that you are trying to optimize and you need to have a way to measure it.</p>\n\n<p>Good: </p>\n\n<ul>\n<li><strong>Crossover functions are easy to define and natural</strong>: When dealing with certain kinds of data, crossover/mutation functions might be easy to define. For example strings (eg. DNA or gene sequences) can be mutated easily by splicing two candidate strings to obtain a new one (this is why nature uses genetic algorithms!). Trees (like phylogenetic trees or parse trees) can be spliced too, by replacing a branch of one tree with a branch from another. Shapes (like airplane wings or boat shapes) can be mutated easily by drawing a grid on the shape and combining different grid sections from the parents to obtain a child. Usually this means your problem is composed of different parts and putting together parts from distinct solutions is a valid candidate solution.\n\n<ul>\n<li>This means that if your problem is defined in a vector space where the coordinates don't have any special meaning, GAs are not a good choice. If it is hard to formulate your problem as a GA, it is not worth it.</li>\n</ul></li>\n<li><strong>Black Box evaluation</strong>: If for a candidate, your fitness function is evaluated outside the computer, GAs are a good idea. For example, if you are testing a wing shape in an air tunnel, genetic algorithms will help you generate good candidate shapes to try. \n\n<ul>\n<li><strong>Exception: Simulations</strong>. If your fitness function is measuring how well a nozzle design performs and requires simulating the fluid dynamics for each nozzle shape, GAs may work well for you. They may also work if you are simulating a physical system through time and are interested in how well your design performs over the course of the operation eg. <a href=\"https://www.youtube.com/watch?v=dRthdBr46cs\" rel=\"nofollow\">modelling locomotion patterns</a>. However, methods that use partial differential equations as constraints are being developed in the literature, eg. <a href=\"https://www.siam.org/meetings/op08/Heinkenschloss.pdf\" rel=\"nofollow\">PDE constrained optimization</a>, so this may change in the future.</li>\n</ul></li>\n</ul>\n\n<p>Not Appropriate:</p>\n\n<ul>\n<li><strong>You can calculate a gradient</strong> for your function: If you have access to the gradient of your function, you can do gradient descent, which is in general much more efficient than GAs. Gradient descent may have issues with local minima (as will GAs) but many methods have been studied to mitigate this. </li>\n<li><strong>You know the fitness function in closed form</strong>: Then, you can probably calculate the gradient. Many languages have libraries supporting <a href=\"https://en.wikipedia.org/wiki/Automatic_differentiation\" rel=\"nofollow\">automatic differentiation</a>, so you don't even need to do it manually. If your function is not differentiable, then you can use <a href=\"https://en.wikipedia.org/wiki/Subgradient_method\" rel=\"nofollow\">subgradient descent</a>.</li>\n<li>Your optimization problem is of a known form, like a <strong>linear program or a quadratic program</strong>: GAs (and black box optimization methods in general) are very inefficient in terms of the number of candidates they need to evaluate, and are best avoided if possible. </li>\n<li><strong>Your solution space is small</strong>: If you can grid your search space efficiently, you can guarantee that you have found the best solution, and can make contour plots of the solution space to see if there is a region you need to explore further.</li>\n</ul>\n\n<p>Finally, if you are considering a GA, consider more recent work in Evolutionary Strategies. I am biased towards <a href=\"https://en.wikipedia.org/wiki/CMA-ES\" rel=\"nofollow\">CMA-ES</a>, which I think is a good simple algorithm that captures the notion of a gradient in the fitness landscape in a way that traditional GAs do not.</p>\n", "question": "<p>I've noticed that a few questions on this site mention genetic algorithms and it made me realize that I don't really know much about those.</p>\n\n<p>I have heard the term before, but it's not something I've ever used, so I don't have much idea about how they work and what they are good for. All I know is that they involve some sort of evolution and randomly changing values.</p>\n\n<p>Can you give me a short explanation, preferably including some sort of practical example that illustrates the basic principles?</p>\n"}, "id": "1322"}, {"body": {"answer": "<p>There are many different kinds of AI used in games; AI for historical board games (like chess or Go) tends to be much better than AI for computer games (such as Starcraft or Civilization), in large part because there's more academic interest in developing strategies for those games.</p>\n\n<p>The basic structure of a game-playing AI is that it takes in game state inputs and outputs an action; typically, the internals also contain some sort of goal and some sort of future prediction.</p>\n\n<p>But beyond that, there's tremendous amounts of variability. Some AI are little more than scripted reflexes, some are built like control systems, some do actual optimization and forward thinking.</p>\n\n<p>Getting into the details of <em>how</em> the many different approaches work is probably beyond the scope of this site, though.</p>\n", "question": "<p>Artificial intelligence is present in many games, both current and older games. How can such intelligence understand what to do? I mean, how can it behave like a human in a game, allowing you to play against itself, or that AI plays against itself?</p>\n\n<p>In games like Age of Empires, for example.</p>\n"}, "id": "1324"}, {"body": {"answer": "<p>For a finite value to be 'optimal,' typically you need some benefit from more paired up with some cost for more, and eventually the lines cross because the benefit decreases and the cost increases.</p>\n\n<p>Most models will have a reduction in error with more training data, that asymptotically approaches the best the model can do. See this image (from <a href=\"http://blog.revolutionanalytics.com/2015/09/why-big-data-learning-curves.html\" rel=\"nofollow\">here</a>) as an example:</p>\n\n<p><a href=\"http://i.stack.imgur.com/Jx1AZ.png\" rel=\"nofollow\"><img src=\"http://i.stack.imgur.com/Jx1AZ.png\" alt=\"Decreasing error with increasing training set size\"></a></p>\n\n<p>The costs of training data are also somewhat obvious; data is costly to obtain, to store, and to move. (Assuming model complexity stays constant, the actual cost of storing, moving, and using the model remains the same, since the weights in the model are just being tuned.)</p>\n\n<p>So at some point the slope of the error-reduction curve becomes horizontal enough that more data points are costlier than they're worth, and that's the optimal amount of training data.</p>\n", "question": "<p><a href=\"http://cs.stackexchange.com/a/60535/54605\">At a related question in Computer Science SE</a>, a user told:</p>\n\n<blockquote>\n  <p>Neural networks typically require a large training set.</p>\n</blockquote>\n\n<p>Is there a way to define the boundaries of the \"optimal\" size of a training set in general case?</p>\n\n<p>When I was learning about fuzzy logic, I've heard some rules of thumb that involved examining the mathematical composition of the problem and using that to define the number of fuzzy sets.</p>\n\n<p>Is there such a method that can be applicable for an already defined neural network topology? </p>\n"}, "id": "1325"}, {"body": {"answer": "<p>In general, the larger the training set, the better. See <a href=\"http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/35179.pdf\" rel=\"nofollow\">The Unreasonable effectiveness of Data</a>, though this article is quite dated (written in 2009). Xavier Amatriain, a researcher at Netflix has a <a href=\"https://www.quora.com/In-machine-learning-is-more-data-always-better-than-better-algorithms\" rel=\"nofollow\">Quora answer</a> where he discusses that more data can sometimes hurt algorithms. </p>\n\n<p>For deep neural networks in particular, it does not seem that we have hit these limits yet. </p>\n", "question": "<p><a href=\"http://cs.stackexchange.com/a/60535/54605\">At a related question in Computer Science SE</a>, a user told:</p>\n\n<blockquote>\n  <p>Neural networks typically require a large training set.</p>\n</blockquote>\n\n<p>Is there a way to define the boundaries of the \"optimal\" size of a training set in general case?</p>\n\n<p>When I was learning about fuzzy logic, I've heard some rules of thumb that involved examining the mathematical composition of the problem and using that to define the number of fuzzy sets.</p>\n\n<p>Is there such a method that can be applicable for an already defined neural network topology? </p>\n"}, "id": "1326"}, {"body": {"answer": "<p>One good way of differentiating modelling and implementation is to consider that models occupy a much higher level of abstraction. </p>\n\n<p>To continue with the mathematical example: even though experimental mathematics might be dependent on computation, the program can be considered as one possible realization of the necessary conditions of a more abstract existence proof.</p>\n\n<p>Over the last 25 years, software engineering methodologies have become quite good at separating models and implementations, e.g. by using interfaces/typeclasses/abstract base classes to define constraints on behavior that is concretely realized by the implementation of derived classes.</p>\n\n<p>AI has always been a battle between the <a href=\"https://en.wikipedia.org/wiki/Neats_vs._scruffies\" rel=\"nofollow\">'neats and the scruffies'</a>. Neats tend to prefer working 'top down' from clean abstractions, 'scruffies' like to work 'bottom up', and 'bang the bits' of the implementation together, to see what happens.</p>\n\n<p>Of course, in practice, interplay between both styles is necessary, but AI <em>as a science</em> progresses when we abstract mechanisms away from specific implementations into their most general (and hence re-useable) form.</p>\n", "question": "<p>Quote from this <a href=\"http://meta.ai.stackexchange.com/a/46/8\">Eric's meta post</a> about modelling and implementation:</p>\n\n<blockquote>\n  <p>They are not exactly the same, although strongly related. This was a very difficult lesson to learn among mathematicians and early programmers, notably in the 70s (mathematical proofs can demand a lot of non-trivial programming work to make them \"computable\", as in runnable on a computer).</p>\n</blockquote>\n\n<p>If they're not the same, what is the difference?</p>\n\n<p>How we can say when we're talking about AI implementation, and when about modelling? It's suggested above it's not easy task. So where we can draw the line when we talk about it?</p>\n\n<p>I'm asking in general, not specifically for this site, that's why I haven't posted question in meta</p>\n"}, "id": "1327"}, {"body": {"answer": "<p>Human brain contains about 100 billions neurons (10^11) and about hundred trillions synapses ($10^14). Each neuron can fire about 100 times a second. If we model brain as a simple neural network, then it would be equivalent to machine that requires 1016 calculations per second and 1013 bits of memory.</p>\n\n<p>From <a href=\"https://en.wikipedia.org/wiki/The_Singularity_Is_Near#The_brain\">Wikipedia</a></p>\n\n<blockquote>\n  <p>Kurzweil introduces the idea of \"uploading\" a specific brain with every mental process intact, to be instantiated on a \"suitably powerful computational substrate\". He writes that general modeling requires 1016 calculations per second and 1013 bits of memory, but then explains uploading requires additional detail, perhaps as many as 1019 cps and 1018 bits. Kurzweil says the technology to do this will be available by 2040.</p>\n</blockquote>\n\n<p>According to this two site <a href=\"http://www.extremetech.com/extreme/163051-simulating-1-second-of-human-brain-activity-takes-82944-processors\">here</a>:</p>\n\n<blockquote>\n  <p>Using the NEST software framework, the team led by Markus Diesmann and Abigail Morrison succeeded in creating an artificial neural network of 1.73 billion nerve cells connected by 10.4 trillion synapses. While impressive, this is only a fraction of the neurons every human brain contains. Scientists believe we all carry 80-100 billion nerve cells</p>\n  \n  <p>It took 40 minutes with the combined muscle of 82,944 processors in K computer to get just 1 second of biological brain processing time. While running, the simulation ate up about 1PB of system memory as each synapse was modeled individually.</p>\n</blockquote>\n\n<p>Computing power will continue to ramp up while transistors scale down, which could make true neural simulations possible in real time with supercomputers.</p>\n\n<p><a href=\"https://en.wikipedia.org/wiki/SpiNNaker\">SpiNNaker</a> is a manycore computer architecture designed to <a href=\"https://en.wikipedia.org/wiki/Human_Brain_Project\">simulate the human brain</a>. It is planned to use 1 million ARM processors (currently .5 million). The completed design will holds 100,000 cores</p>\n\n<p>In this <a href=\"https://www.youtube.com/watch?v=2e06C-yUwlc\">video</a> they showed a completed rack with 100,000 cores emulating 25 million neurons (at \u00bc the efficiency\u2014it will eventually run 1,000 neurons per core). </p>\n", "question": "<p>How much processing power is needed to emulate the human brain? More specifically, the neural simulation, such as communication between the neurons and processing certain data in real-time.</p>\n\n<p>I understand that this may be a bit of speculation and it's not possible to be accurate, but I'm sure there is some data available or research studies which attempted to estimate it based on our current understanding of the human brain.</p>\n"}, "id": "1328"}, {"body": {"answer": "<p>What might be classed as AI has of course changed over the years, but landmarks and research breakthroughs include:</p>\n\n<ul>\n<li>Babbagge's <a href=\"https://en.wikipedia.org/wiki/Difference_engine\" rel=\"nofollow\">Difference Engine</a> (~1823) for tabulating/interpolating polynomials.</li>\n<li>Frank Rosenblatt's 1957 invention of the <a href=\"http://blogs.umass.edu/brain-wars/files/2016/03/rosenblatt-1957.pdf\" rel=\"nofollow\">Perceptron</a>.</li>\n<li><p>John McCarthy's invention of <a href=\"https://en.wikipedia.org/wiki/Lisp_(programming_language)\" rel=\"nofollow\">Lisp</a> in the late 1950s.</p></li>\n<li><p>Arthur Samuel's 1959 <a href=\"http://link.springer.com/chapter/10.1007/978-3-319-30668-1_9\" rel=\"nofollow\">checkers player</a>, which famously improved by playing against itself (it would have been nice if that had destroyed the myth about a program only being as 'intelligent' as its creator).</p></li>\n<li><p>Newell and Simon's 1959 <a href=\"http://bitsavers.informatik.uni-stuttgart.de/pdf/rand/ipl/P-1584_Report_On_A_General_Problem-Solving_Program_Feb59.pdf\" rel=\"nofollow\">General Problem Solver</a> applied Means-Ends analysis to solve a range of problems expressed as Horn clauses.</p></li>\n<li><p>Davis, Putnam et al: 1962 invention of the <a href=\"https://en.wikipedia.org/wiki/DPLL_algorithm\" rel=\"nofollow\">DPLL algorithm</a> which still forms the core of modern SAT-based theorem provers.</p></li>\n<li><p>Lawrence Fogel et al: 1966 book <a href=\"http://rads.stackoverflow.com/amzn/click/B0000CNARU\" rel=\"nofollow\">Artificial Intelligence through Simulated Evolution</a>.</p></li>\n<li><p>Rechenberg and Schwefel: 1960s development of <a href=\"https://en.wikipedia.org/wiki/Evolution_strategy\" rel=\"nofollow\">Evolutionsstrategie</a> - an Evolutionary Computation approach using mutation and a form of Darwinian 'survival of the fittest'.</p></li>\n<li><p>Lotfi Zadeh's 1965 invention of <a href=\"https://people.eecs.berkeley.edu/~zadeh/papers/Fuzzy%20Sets-Information%20and%20Control-1965.pdf\" rel=\"nofollow\">Fuzzy Logic</a>.</p></li>\n<li><p>John Holland's 1975 book <a href=\"http://dl.acm.org/citation.cfm?id=531075\" rel=\"nofollow\">\"Adaptation in Natural and Artificial Systems\"</a> which introduced Genetic Algorithms.</p></li>\n<li><p>The 1980 <a href=\"http://aitopics.org/sites/default/files/classic/Webber-Nilsson-Readings/Rdgs-NW-Erman-Hayes-Roth-Lesser-Reddy.pdf\" rel=\"nofollow\">Hearsay II</a> Blackboard Architecture by Hayes-Roth et al.</p></li>\n<li><p>The 1980s invention of the <a href=\"http://dl.acm.org/citation.cfm?id=104293\" rel=\"nofollow\">backpropagation algorithm</a> for Mutlilayer Perceptrons by Rumelhart, Hinton et al.</p></li>\n</ul>\n", "question": "<p>What were the first areas of research into Artificial Intelligence and what were some early successes?  More recently we've had:</p>\n\n<ol>\n<li>Beating a human at the game of chess</li>\n<li>Convincing a human that a person was conversing with them (passing the Turing test)</li>\n<li>Beating a human at Jeopardy game show</li>\n<li>Beating a human at the game of go.</li>\n</ol>\n\n<p>Were there milestones that were considered major in the field before the 1990s?</p>\n"}, "id": "1329"}, {"body": {"answer": "<p>Randomness is typically the best one can do with ignorance, rather than a source of strength in its own right.</p>\n\n<p>For example, the primary use of randomness in statistics is random assignment (A/B testing, randomized controlled trials, etc.). The reason to do this is to make the influence of confounders independent from the influence of the factor under investigation.</p>\n\n<p>But randomness only works for this <em>in expectation</em>. If we actually knew what the confounders were, we could do a paired assignment (or a similar scheme) that ensured the various groups were matched <em>as well as possible</em>, instead of us just not knowing ahead of time which way the bias went.</p>\n\n<hr>\n\n<p>There are some cases where pseudorandomness, rather than full randomness, will impair training AI designs. A simple example would be a case where you want to randomly initialize weights in a network where the number of parameters exceeds the periodicity of the RNG; this means that while you have as many possible networks as there are possible unique seeds, you can't actually visit the entire weight space that you wanted to sample over.</p>\n\n<p>I don't think any of those cases are limiting factors, however. Having truly random stochastic gradient descent instead of pseudorandom stochastic gradient descent doesn't seem like it would make a serious difference in the trajectory of AI designs.</p>\n", "question": "<p>How important is true (non-<a href=\"https://en.wikipedia.org/wiki/Pseudorandomness\" rel=\"nofollow\" title=\"pseudo\">pseudo</a>) randomness in Artificial Intelligence designs? Is there any chance that pseudo-randomness could be a barrier to more successful designs?</p>\n"}, "id": "1335"}, {"body": {"answer": "<p>AFAIK, normally detection algorithms work in a sub-window of the image and not the whole of it. For example, for a specific size and orientation you slide a sub-window on the image and extract sub-images. Then you apply your algorithm on every sub-image for detection and report the size-and-orientations with positive results.</p>\n\n<p>You can have a single neural network for face detection in this case or you might want to have different detectors for different orientation or any other feature, that is your decision.</p>\n\n<p>There is also the technique of <a href=\"https://en.wikipedia.org/wiki/Ensembles_of_classifiers\" rel=\"nofollow\">Combining Classifiers</a> by which you can improve the decision of single classifiers by combining them.</p>\n\n<p><a href=\"https://en.wikipedia.org/wiki/Ensemble_learning\" rel=\"nofollow\">Ensemble Learning</a> is another way in which your classifiers are not trained independently but rather together. In fact, the well-known <a href=\"https://en.wikipedia.org/wiki/Viola%E2%80%93Jones_object_detection_framework\" rel=\"nofollow\">object detector of Viola and Jones</a> uses such a technique.</p>\n", "question": "<p>Given pictures with multiple features such as faces, can single AI algorithm detect all of them, or for better reliability is it preferred to use separate instances?</p>\n\n<p>In other words I'm talking about attempt of finding all possible human faces on the same picture by a single neural network.</p>\n"}, "id": "1337"}, {"body": {"answer": "<p>Until we can make a quantum computer with a lot more qubits, the potential to further develop AI will remain just that.</p>\n\n<p>D-Wave (which currently has a qubit count of over 1,000) is an <em>adiabatic quantum computer</em>, not a general purpose quantum computer. It is restricted to certain optimization problems (at which its effectiveness <a href=\"https://en.wikipedia.org/wiki/D-Wave_Systems#Reception\" rel=\"nofollow\">has reportedly been doubted</a> by one of the originators of the theory on which it is based).</p>\n\n<p>Suppose that we could build a 32 qubit general purpose quantum computer (twice as big as current models, as far as I'm aware). This would still mean that only 2<sup>32</sup> possibilities exist in superposition. This is a space small enough to be explored exhaustively for many problems. Hence, there are perhaps not so many problems for which any of the known quantum algorithms (e.g. <a href=\"https://en.wikipedia.org/wiki/Shor%27s_algorithm\" rel=\"nofollow\">Shor</a>, <a href=\"https://en.wikipedia.org/wiki/Grover%27s_algorithm\" rel=\"nofollow\">Grover</a>) would be useful for that number of bits.</p>\n", "question": "<p>What aspects of quantum computers, if any, can help to further develop Artificial Intelligence?</p>\n"}, "id": "1338"}, {"body": {"answer": "<p>There has been previous research with promising results cited at length in the following recent article, and although they have limited training data, here is some <a href=\"http://uaf46365.ddns.uark.edu/SarahStolze_Thesis.pdf\" rel=\"nofollow\">impressive research for an undergraduate thesis at the University of Arkansas</a> which extends that research using an artificial neural network on enhancing a classifying algorithm's capacity to facilitate unspoken, or imagined, speech recognition by collecting and analyzing a large dataset of simultaneous EEG signal and video data streams. </p>\n\n<blockquote>\n  <p>Imagined speech (unspoken speech, silent speech, or covert speech) is\n  the process by which one thinks about a word, or \u201chears\u201d the word in\n  one\u2019s head, in the absence of any vocalization or physical movement\n  indicating the word. Though there exists evidence that it is possible\n  for imagined speech information to be captured and interpreted. To\n  facilitate imagined speech, a Brain-to-Computer Interface (BCI) must\n  be implemented to provide silent communication abilities directly\n  between the two entities. One of the most popular methods for\n  interfacing directly between a human brain and a computer is through\n  electroencephalographic signals.</p>\n  \n  <p>Researchers have created models capable of achieving 70 - 90%\n  predictive accuracy in recognizing patterns in EEG data;\n  however, the accuracy of current methods for unspoken speech\n  recognition is not yet sufficient to enable fluid communication\n  between humans and machines.</p>\n</blockquote>\n\n<p>High Level Experiment Design</p>\n\n<blockquote>\n  <p>the subjects were asked to imagine a specific word or feeling (label).\n  The subjects responded to a set of uniform verbal cues describing the\n  set of labels as well as the desired individual label to imagine. The\n  data was then processed in order to minimize the effects of irrelevant\n  signal activity, or noise. Additionally the data was processed to\n  minimize its volume while still maintaining the core \u201cinformation\u201d in\n  the data. The condensed dataset was created by dropping irrelevant\n  information from the EEG device and applying principal component\n  analysis (PCA) to the video stream data. Once the data was processed\n  and assembled into the correct format, cross-validation using a random\n  forest algorithm was performed on the control group of EEG signals\n  alone and on the hypothesis group consisting of both EEG and video\n  data. The predictive accuracy measurements obtained from the\n  cross-validation experiments were used as metrics to evaluate the\n  success of the hypothesis.</p>\n</blockquote>\n\n<p>The results show a notable improvement classifying thoughts when in conjunction with the video streams.</p>\n\n<p><a href=\"http://i.stack.imgur.com/LH6sI.png\" rel=\"nofollow\"><img src=\"http://i.stack.imgur.com/LH6sI.png\" alt=\"graph of predictive accuracy\"></a></p>\n", "question": "<p>Have there been any studies which attempted to use AI algorithms to detect human thoughts or emotions based on brain activity, such as using <a href=\"https://en.wikipedia.org/wiki/Brain%E2%80%93computer_interface#EEG-based\" rel=\"nofollow\">BCI/EEG devices</a>?</p>\n\n<p>By this, I mean simple guesses such as whether the person was happy or angry, or what object (e.g. banana, car) they were thinking about.</p>\n\n<p>If so, did any of those studies show some degree of success?</p>\n"}, "id": "1339"}, {"body": {"answer": "<p><a href=\"http://www.cell.com/trends/cognitive-sciences/abstract/S1364-6613(04)00243-8?_returnURL=http%3A%2F%2Flinkinghub.elsevier.com%2Fretrieve%2Fpii%2FS1364661304002438%3Fshowall%3Dtrue\" rel=\"nofollow\">This article</a> gives a description of mirror neurons in terms of Hebbian learning, a mechanism that has been widely used in AI. I don't know whether the formulation given in the article has ever actually been implemented computationally.</p>\n", "question": "<p>From Wikipedia:</p>\n\n<blockquote>\n  <p>A mirror neuron is a neuron that fires both when an animal acts and when the animal observes the same action performed by another.</p>\n</blockquote>\n\n<p>Mirror neurons are related to imitation learning, a very useful feature that is missing in current real-world A.I. implementations. Instead of learning from input-output examples (supervised learning) or from rewards (reinforcement learning), an agent with mirror neurons would be able to learn by simply observing other agents, translating their movements to its own coordinate system. What do we have on this subject regarding computational models?</p>\n"}, "id": "1340"}, {"body": {"answer": "<p>MLP's can theoretically approximate any bounded, continuous function. There's no guarantee for a discontinuous function. There are plenty of important discontinuous functions, like, say, the prime counting function.</p>\n\n<p>The <a href=\"https://en.wikipedia.org/wiki/Prime-counting_function\" rel=\"nofollow\">prime counting function</a> pi(n) is simply equal to the number of primes less than or equal to n. It has a discontinuity about each prime p, so good luck trying to approximate this with a neural network!</p>\n\n<p>However, this function is extensively studied and extremely important in number theory. See the <a href=\"https://en.wikipedia.org/wiki/Riemann_hypothesis\" rel=\"nofollow\">Riemann hypothesis</a>.</p>\n", "question": "<p>In 1969, Seymour Papert and Marvin Minsky showed that Perceptrons could not learn the XOR function.  </p>\n\n<p>This was solved by the backpropagation network with at least one hidden layer. This type of network can learn the XOR function.</p>\n\n<p>I believe I was once taught that every problem that could be learnt by a backpropagation neural network with multiple hidden layers, could also be learnt by a backpropagation neural network with a single hidden layer. (Although possible a nonlinear activation function was required).</p>\n\n<p>However, it is unclear to me what the limits are to backpropagation neural networks themselves. Which patterns <strong>cannot</strong> be learnt by a backpropgation neural network?</p>\n"}, "id": "1341"}, {"body": {"answer": "<p>Watson starts off by searching its massive database of sources for stuff that might be pertinent to the question. Next, it searches through all of the search results and turns them into candidate answers. For example, if one of the search results is an article, Watson might pick the title of the article as a possible answer. After finding all of these candidate answers, it proceeds to iteratively score them to determine which one is best.</p>\n\n<p>The scoring process is very complicated, and involves finding supporting evidence for each answer, and then combining many different scoring algorithms to determine which candidate answer is the best. You can read a more detailed (but still very conceptual) overview <a href=\"http://www.aaai.org/Magazine/Watson/watson.php\" rel=\"nofollow\">here</a>, by the creators of Watson.</p>\n", "question": "<p>Is there any simple explanation how <em>Watson</em> finds and scores evidence after gathering massive evidence and analyzing the data?</p>\n\n<p>In other words, how does it know which precise answer it needs to return?</p>\n"}, "id": "1342"}, {"body": {"answer": "<p>The following survey article by researchers from IIT Bombay summarizes recent advances in sarcasm detection: <a href=\"https://arxiv.org/abs/1602.03426\" rel=\"nofollow\">Arxiv link</a>.</p>\n\n<p>In reference to your question, I do not think it is considered either extraordinarily difficult or open-ended. While it does introduce ambiguity that computers cannot yet handle, Humans are easily able to understand sarcasm, and are thus able to label datasets for sarcasm detection.</p>\n", "question": "<p>Identifying sarcasm is considered as one of the most difficult open-ended problems in the domain of ML and NLP.</p>\n\n<p>So, was there any considerable research done in that front? If yes, then what is the accuracy like? Please also explain the NLP model briefly.</p>\n"}, "id": "1345"}, {"body": {"answer": "<p>Asimov's laws are not strong enough to be used in practice. Strength isn't even a consideration, when considering that since they're written in English words would first have to be interpreted subjectively to have any meaning at all. You can find a good discussion of this <a href=\"https://youtu.be/7PKx3kS7f4A\">here</a>.</p>\n\n<p>To transcribe an excerpt:</p>\n\n<blockquote>\n  <p>How do you define these things? How do you define \"human\", without first having to take a stand on almost every issue. And if \"human\" wasn't hard enough, you then have to define \"harm\", and you've got the same problem again. Almost any really solid unambiguous definitions you give for those words&mdash;that don't rely on human intuition&mdash;result in weird quirks of philosophy, leading to your AI doing something you really don't want it to do.</p>\n</blockquote>\n\n<p>One can easily imagine that Asimov was smart enough to know this and was more interested in story-writing than designing real-world AI control protocols.</p>\n\n<p>In the novel <a href=\"https://en.wikipedia.org/wiki/Neuromancer#Plot_summary\">Neuromancer</a>, it was suggested that AIs could possibly serve as checks against each other. Ray Kurzweil's impending <a href=\"https://en.wikipedia.org/wiki/Technological_singularity\">Singularity</a>, or the possibility of hyperintelligent AGIs otherwise, might not leave much of a possibility for humans to control AIs at all, leaving peer-regulation as the only feasible possibility.</p>\n\n<p>It's worth noting that Eliezer Yudkowsky and others ran an <a href=\"http://www.yudkowsky.net/singularity/aibox/\">experiment</a> wherein Yudkowsky played the role of a superintelligent AI with the ability to speak, but no other connection outside of a locked box. The challengers were tasked simply with keeping the AI in the box at all costs. Yudkowsky escaped both times.</p>\n", "question": "<p>Isaac Asimov's famous <a href=\"https://en.wikipedia.org/wiki/Three_Laws_of_Robotics\">Three Laws of Robotics</a> originated in the context of Asimov's science fiction stories. In those stories, the three laws serve as a safety measure, in order to avoid untimely or manipulated situations from exploding in havoc.</p>\n\n<p>More often than not, Asimov's narratives would find a way to break them, leading the writer to make several modifications to the laws themselves. For instance, in some of his stories, he <a href=\"https://en.wikipedia.org/wiki/Three_Laws_of_Robotics#First_Law_modified\">modified the First Law</a>, <a href=\"https://en.wikipedia.org/wiki/Three_Laws_of_Robotics#Zeroth_Law_added\">added a Fourth (or Zeroth) Law</a>, or even <a href=\"https://en.wikipedia.org/wiki/Three_Laws_of_Robotics#Removal_of_the_Three_Laws\">removed all Laws altogether</a>.</p>\n\n<p>However, it is easy to argue that, in popular culture, and even in the field of AI research itself, the Laws of Robotics are taken quite seriously. Ignoring the side problem of the different, subjective, and mutually-exclusive interpretations of the laws, are there any arguments proving the laws themselves intrinsically flawed by their design, or, alternatively, strong enough for use in reality? Likewise, has a better, stricter security heuristics set being designed for the purpose?</p>\n"}, "id": "1355"}, {"body": {"answer": "<p>There has been a recent work in the same domain where neural networks(CNNs to be accurate) are used for the same purpose. Some info. about the research is:</p>\n\n<blockquote>\n  <p>To learn that context, the paper describes a method by which the\n  neural network finds the user\u2019s \u201cembeddings\u201d \u2014 i.e. contextual cues\n  like the content of previous tweets, related interests and accounts,\n  and so on. It uses these various factors to plot the user with others,\n  and (ideally) finds that they form relatively well-defined groups.</p>\n</blockquote>\n\n<p>So, the paper uses CNNs, word and user embeddings for detecting sarcasm in text. There is also a <a href=\"https://techcrunch.com/2016/08/04/this-neural-network-tries-to-tell-if-youre-being-sarcastic-online/\" rel=\"nofollow\">Techcrunch article</a> on that.</p>\n\n<p>The paper uses sentiment of the tweet and compares with that of the other similar tweets:</p>\n\n<blockquote>\n  <p>If the sentiment of the tweet seems to disagree with the bulk of what\n  is expressed by similar users, there\u2019s a good chance sarcasm is being\n  employed.</p>\n</blockquote>\n\n<p><a href=\"http://arxiv.org/pdf/1607.00976v2.pdf\" rel=\"nofollow\">Link to the paper</a></p>\n", "question": "<p>Identifying sarcasm is considered as one of the most difficult open-ended problems in the domain of ML and NLP.</p>\n\n<p>So, was there any considerable research done in that front? If yes, then what is the accuracy like? Please also explain the NLP model briefly.</p>\n"}, "id": "1356"}, {"body": {"answer": "<p>The answer to your question is \"In principle, yes\" - in it's most general form, EQ testing is just a specific case of the Turing test (\"How would you feel about ... ?\"). </p>\n\n<p>To see why meaningful EQ tests might be difficult to achieve, consider the following two possible tests:</p>\n\n<p>At one extreme of complexity, the film 'Blade Runner' famously shows a test to distinguish between human and android on the basis of responses to emotionally-charged questions.</p>\n\n<p>If you tried asking these questions (or even much simpler ones) to a modern chatbot, you'd likely quickly conclude that you were not talking to a person.</p>\n\n<p>The problem with assessing EQ is that the more emotionally sophisticated the test, the more general the AI system is likely have to be, in order to turn the input into a meaningful representation.</p>\n\n<p>At the other extreme from the above, suppose that an EQ test was phrased in an extremely structured way, with the structured input provided by a human. In such a case, success at an 'EQ test' is not really grounded in the real-world.</p>\n\n<p>In an essay entitled \"The ineradicable Eliza effect and its dangers\", Douglas Hofstadter gives the following example, in which the ACME program is claimed (not by Hofstadter) to 'understand' analogy.</p>\n\n<blockquote>\n  <p>Here the computer learns about a fellow named Sluggo taking his wife Jane and\n  his good buddy Buck to a bar, where things take their natural course and Jane\n  winds up pregnant by Buck. She has the baby but doesn't want it, and so, aided\n  by her husband, she drowns the baby in a river, thus \"neatly solving \"the problem\"\n  of Bambi.</p>\n</blockquote>\n\n<p>This story is presented to ACME in the following form:</p>\n\n<pre><code>ql: (neglectful-husband (Sluggo))\nq2: (lonely-and-sex-starved-wife (Jane-Doe))\nq3: (macho-ladykiller (Buck-Stag))\nq4: (poor-innocent-little-fetus (Bambi))\nq5: (takes-out-to-local-bar (Sluggo Jane-Doe Buck-Stag))\n...\nq11: (neatly-solves-the-problem-of (Jane-Doe Bambi))\nq12: (cause (ql0 q11))\n</code></pre>\n\n<p>Suppose the program were to be asked if Jane Doe's behavior was moral. Complex compound emotional concepts such as 'neglectful', 'lonely' and 'innocent' are here simply predicates, not available to the AI for deeper introspective examination. They could just as easily be replaced by labels such as as 'bling-blang-blong15657'. </p>\n\n<p>So in one sense, the absence of success at an EQ test with any depth is indicative of the general problem currently facing AI: the inability to define (or otherwise learn) meaningful representations of subtle complexities of the human world, which is a lot more complex than being able to recognize videos of cats.</p>\n", "question": "<p>Can an AI program have an EQ (Emotional intelligence or emotional quotient)?</p>\n\n<p>In other words, can the EQ of an AI program be measured?</p>\n\n<p>If EQ is more problematic to measure than IQ (at least with a standard applicaple to both humans and AI programs), why is that the case?</p>\n"}, "id": "1359"}, {"body": {"answer": "<p><a href=\"https://en.wikipedia.org/wiki/Monte_Carlo_method\">Monte Carlo method</a> is an approach where you generate a large number of random values or simulations and form some sort of conlusions based on the general patterns, such as the means and variances.</p>\n\n<p>As an example, you could use it for <a href=\"https://en.wikipedia.org/wiki/Numerical_weather_prediction\">weather forecasts</a>. Predicting long-term weather is quite difficult, because it is a chaotic system where small changes can lead to very different results. Using Monte Carlo methods, you could run a large number of simulations, each with slightly different atmospheric changes. Then you can analyze the results and for example calculate the probability of rain on a given day based on how many simulations ended up with rain. </p>\n\n<p>As for the use of Monte Carlo in Alpha Go, they seem to be using the so-called <a href=\"https://en.wikipedia.org/wiki/Monte_Carlo_tree_search\">Monte Carlo Tree Search</a>. In this approach, you make a tree of possible moves, a few turns into the future, and try to find the best sequence. However, since the number of possible moves in the game of go is very large, you won't be able to explore very far ahead. This means that some of the moves which look good now might turn out to be bad later. </p>\n\n<p>So, in the Monte Carlo Tree Search, you pick a promising sequence of moves and run one or more simulations of how the game might proceed from that point. Then you can use the results of that simulation to get a better idea of how good that specific sequence of moves really is and you update the tree accordingly. Repeat as needed until you find a good move.</p>\n\n<p>If you want more information or to look at some illustrations, I found an interesting paper on the topic: C. Browne et al., A Survey of Monte Carlo Tree Search Methods (<a href=\"http://repository.essex.ac.uk/4117/1/MCTS-Survey.pdf\">open repository</a> / <a href=\"http://dx.doi.org/10.1109/TCIAIG.2012.2186810\">permanent link (paywalled)</a>)</p>\n", "question": "<p>I have heard about this concept in a reddit post about Alpha Go. I have trued to go through the paper and the article, but could not really make sense of the algorithm.</p>\n\n<p>So, can someone give a easy-to-understand explanation of how the Monte-Carlo search algorithm work and how is it being used in building game-playing AI bots?</p>\n"}, "id": "1361"}, {"body": {"answer": "<p>It depends on the type of what type neural networks you are dealing with.</p>\n\n<p>For medium sized neural nets, the matrix approach is a very good way to do quick computations and even backpropogation of errors. One can even exploit sparse matrixes for understanding the sparse architecture of some neural nets.</p>\n\n<p>But, for very large neural nets, using matrix computations would be computationally very intensive. So, relevant methods like graph-based stores, etc are used for them depending on the purpose and the architecture.</p>\n", "question": "<p>A neural network is a directed weighted graph. These can be represented by a (sparse) matrix. Doing so can expose some elegant properties of the network.</p>\n\n<p>Is this technique beneficial for examining neural networks?</p>\n"}, "id": "1364"}, {"body": {"answer": "<p>There are several elementary techniques to try and move a search out of the basin of attraction of local optima. They include:</p>\n\n<ul>\n<li>Probabalistically accepting worse solutions in the hope that this\nwill jump out of the current basin (like Metropolis-Hastings acceptance in Simulated Annealing). </li>\n<li>Maintaining a list of recently-encountered states (or attributes thereof) and not returning\nto a recently-encountered one (like Tabu Search). </li>\n<li>Performing a random walk of a length determined by the current state of the search (an explicit 'Diversification strategy', e.g. as used in 'Reactive Tabu Search').</li>\n</ul>\n\n<p>See the excellent (and free online) book <a href=\"https://cs.gmu.edu/~sean/book/metaheuristics/\">'Essentials of Metaheuristics'</a> by Sean Luke for more details on these kind of techniques and some rules of thumb about when and how to use them.</p>\n", "question": "<p>How do I avoid my gradient descent algorithm into falling into the \"local minima\" trap while backpropogating on my neural network?</p>\n\n<p>Are there any methods which help me avoid it?</p>\n"}, "id": "1365"}, {"body": {"answer": "<p>For large ANNs, something equivalent to a 'sparse matrix format' is used in practice.</p>\n\n<p>In contrast to what is said in another answer given, considering an ANN as a graph doesn't actually buy very much, for two reasons:</p>\n\n<ol>\n<li><p>The backpropagation algorithm can usefully be\ndefined in terms of matrix operations. <a href=\"http://briandolhansky.com/blog/2014/10/30/artificial-neural-networks-matrix-form-part-5\" rel=\"nofollow\">This page</a> gives a\nreadable and comprehensive description.</p></li>\n<li><p>All real-valued matrices can be represented as graphs, but the converse is clearly not the case. So while it is true that an ANN can be considered as a special case of a graph data structure, making that specialization explicit in matrix form is more efficient.</p></li>\n</ol>\n", "question": "<p>A neural network is a directed weighted graph. These can be represented by a (sparse) matrix. Doing so can expose some elegant properties of the network.</p>\n\n<p>Is this technique beneficial for examining neural networks?</p>\n"}, "id": "1366"}, {"body": {"answer": "<p>An easy method would be to use a poem (short one, two paragraphs) then give an one line test question regarding the emotional states of the poem, that an AI won't or can't be programmed to understand.</p>\n", "question": "<p>Are there any modern techniques of generating <strong>textual</strong> CAPTCHA (so person needs to type the right text) challenges which can easily <a href=\"http://ai.stackexchange.com/q/92/8\">fool AI</a> with some visual obfuscation methods, but at the same time human can solve them without any struggle?</p>\n\n<p>For example I'm talking about plain ability of <strong>recognising text embedded into image</strong> (without considering any external plugins like flash or java, image classification, etc.) and re-typing the text that has been written or something similar.</p>\n\n<p>I guess adding noise, gradient, rotating letters or changing colours are not reliable methods any more, since they can be quickly broken.</p>\n\n<p>Any suggestions or research has been done?</p>\n"}, "id": "1367"}, {"body": {"answer": "<p>The first thing is to define what is a \u00abgood\u00bb and a \u00abbad\u00bb sound. This is an extremely tricky issue, since the networks need <em>numeric</em> inputs. And music is whole bunch of numbers.</p>\n\n<p>I know from people doing research in identifying <em>how similar</em> two sounds are, and imitation, say: you hear a sound and try to make another that sounds like it. Like when you hum a song or similar. That is by no means easy. These guys are using something similar to feature extraction, with Fourier transforms and energy and <a href=\"https://en.wikipedia.org/wiki/Music_information_retrieval\" rel=\"nofollow\">such things</a>. They feed the networks with the (selected) features and... Train. </p>\n\n<p>Now, to return to your original question: *What do you present as <em>target</em> during training?* You can present different <em>types</em> of music as categories and classify (I couldn't help but think on <a href=\"https://link.springer.com/article/10.3758/BF03192900\" rel=\"nofollow\">this research with fish</a>). Or <strong>you</strong> define categories of music <strong>you</strong> like and see if the network can classify them ;)</p>\n\n<p>One basic decision here is how long you get a piece of sound. Since it is needed to analyse frequency, this is a key issue. Since you talked about DNN, I was wondering if you wanted to do it <em>online</em>, as a stream, in which case I don't have the slightest idea where to begin, other than do it <em>after a little while</em>.</p>\n\n<hr>\n\n<p>Other idea: I remember a little sketch in <a href=\"http://www.bbc.co.uk/programmes/b012xppj\" rel=\"nofollow\">this series</a> about a researcher that makes use of the relations between peaks in the Fourier spectrum in order to differentiate noise from music.</p>\n", "question": "<p>DNNs are typically used to classify things (of course) but can we let them go wild with sounds and then tell them if we think it sounds good or not? I'd like to think after a training class has been made (perhaps comparing the output to an existing song) we could get an NN that has a basic concept of music.</p>\n\n<p>Timing would be an issue; I'm not sure how feasible this is. A strongly weighted input attached to all hidden layers perhaps? Use it as the bias?</p>\n\n<p>Is this even slightly feasible? </p>\n"}, "id": "1369"}, {"body": {"answer": "<p>It seems easy for this to be sublinear growth or superlinear growth, depending on context. </p>\n\n<p>If we imagine the space of the complex AI as split into two parts--the context model and the content model (that is, information and structure that is expected to be shared across entries vs. information and structure that is local to particular entries), then expanding the source material means we don't have much additional work to do on the context model, but whether the additional piece of the content model is larger or smaller depends on how connected the new material is to the old material.</p>\n\n<p>That is, one of the reasons why Watson takes many times the space of its source material is because it stores links between objects, which one would expect to grow with roughly order <em>n</em> squared. If there are many links between the old and new material, then we should expect it to roughly quadruple in size instead of double; if the old material and new material are mostly unconnected and roughly the same in topology, then we expect the model to roughly double; if the new material is mostly unconnected to the old material and also mostly unconnected to itself, then we expect the model to not grow by much.</p>\n", "question": "<p>Complex AI that learns lexical-semantic content and its meaning (such as collection of words, their structure and dependencies) such as <em>Watson</em> takes terabytes of disk space.</p>\n\n<p>Lets assume <em>DeepQA</em>-like AI consumed whole Wikipedia of size 10G which took the same amount of structured and unstructured stored content.</p>\n\n<p>Will learning another 10G of different encyclopedia (different topics in the same language) take the same amount of data? Or will the AI reuse the existing structured and take less than half (like 1/10 of it) additional space?</p>\n"}, "id": "1370"}, {"body": {"answer": "<p>If anything, multiple intelligences are much more obvious in AI than in other fields, because we haven't yet unlocked how to do transfer between domains.</p>\n\n<p>As an example, AlphaGo is very, very good at playing Go, but it's got basically nothing in the way of bodily-kinesthetic intelligence. But other teams have built software to control robots that does have bodily-kinesthetic intelligence, while not being good at the tasks that AlphaGo excels at.</p>\n\n<p>This sort of modular intelligence is typically referred to as 'narrow AI,' whereas we use the term 'general AI' (or AGI, for Artificial General Intelligence) to refer to intelligence that we've built that can do roughly as many different kinds of things as people can do.</p>\n", "question": "<p>I have been wondering since a while ago about the <a href=\"https://en.wikipedia.org/wiki/Theory_of_multiple_intelligences\" rel=\"nofollow\">multiple intelligences</a> and how they could fit in the field of Artificial Intelligence as a whole.</p>\n\n<p>We hear from time to time about <a href=\"https://www.theguardian.com/artanddesign/jonathanjonesblog/2016/feb/08/leonardo-da-vinci-mechanics-of-genius-science-museum-london\" rel=\"nofollow\">Leonardo</a> being a genius or <a href=\"https://www.youtube.com/watch?v=xUHQ2ybTejU\" rel=\"nofollow\">Bach's musical intelligence</a>. These persons are commonly said to be (have been) <em>more intelligent</em>. But the multiple intelligences speak about cooking or dancing or chatting as well, i.e. <em>coping with everyday tasks</em> (at least that's my interpretation).</p>\n\n<p><strong>Are there some approaches on incorporating multiple intelligences into AI?</strong></p>\n\n<hr>\n\n<p><a href=\"http://ai.stackexchange.com/questions/26/how-could-emotional-intelligence-be-implemented\">Related question - How could emotional intelligence be implemented?</a></p>\n"}, "id": "1371"}, {"body": {"answer": "<p>\"Current artificial intelligence research\" is a pretty broad field. From where I sit, in a mostly CS realm, people are focused on narrow intelligence that can do economically relevant work on narrow tasks. (That is, predicting when components will fail, predicting which ads a user will click on, and so on.)</p>\n\n<p>For those sorts of tools, the generality of a formalism like AIXI is a weakness instead of a strength. You don't need to take an AI that could in theory compute anything, and then slowly train it to focus on what you want, when you could just directly shape a tool that is the mirror of your task.</p>\n\n<p>I'm not as familiar with AGI research itself, but my impression is that AIXI is, to some extent, the simplest idea that could work--it takes all the hard part and pushes it into computation, so it's 'just an engineering challenge.' (This is the bit about 'finding approximations to AIXI.') The question then becomes, is starting at AIXI and trying to approximate down a more or less fruitful research path than starting at something small and functional, and trying to build up?</p>\n\n<p>My impression is the latter is much more common, but again, I only see a small corner of this space.</p>\n", "question": "<p>From Wikipedia:</p>\n\n<blockquote>\n  <p>AIXI ['ai\u032fk\u0361si\u02d0] is a theoretical mathematical formalism for artificial general intelligence. It combines Solomonoff induction with sequential decision theory. AIXI was first proposed by Marcus Hutter in 2000[1] and the results below are proved in Hutter's 2005 book Universal Artificial Intelligence.[2]</p>\n</blockquote>\n\n<p>Albeit non-computable, approximations are possible, such as <em>AIXItl</em>. Finding approximations to AIXI could be an objective way for solving AI.</p>\n\n<p>My question is: is <em>AIXI</em> really a big deal in artificial <em>general</em> intelligence research? Can it be thought as a central concept for the field? If so, why don't we have more publications on this subject (or maybe we have and I'm not aware of them)?</p>\n"}, "id": "1372"}, {"body": {"answer": "<p>AIXI is really a conceptual framework. All the hard work of actually compressing the environment still remains.</p>\n\n<p>To further discuss the question raised in Matthew Graves answer: given our current limited level of ability to represent complex environments, it seems to me that it doesn't make a lot of practical difference whether you start with AIXI as defining the 'top' of the system and working down (e.g. via supposedly generalized compression methods) or start at the 'bottom' and try solve problems in a single domain via domain-specific methods that (you hope) can subsequently be abstracted to provide cross-domain compression.</p>\n", "question": "<p>From Wikipedia:</p>\n\n<blockquote>\n  <p>AIXI ['ai\u032fk\u0361si\u02d0] is a theoretical mathematical formalism for artificial general intelligence. It combines Solomonoff induction with sequential decision theory. AIXI was first proposed by Marcus Hutter in 2000[1] and the results below are proved in Hutter's 2005 book Universal Artificial Intelligence.[2]</p>\n</blockquote>\n\n<p>Albeit non-computable, approximations are possible, such as <em>AIXItl</em>. Finding approximations to AIXI could be an objective way for solving AI.</p>\n\n<p>My question is: is <em>AIXI</em> really a big deal in artificial <em>general</em> intelligence research? Can it be thought as a central concept for the field? If so, why don't we have more publications on this subject (or maybe we have and I'm not aware of them)?</p>\n"}, "id": "1373"}, {"body": {"answer": "<p>The question mentions \"walking robot\", but it may be illustrative to re-frame the discussion in terms of self-driving cars, because: </p>\n\n<ul>\n<li>It gives a common point of reference, rather than everyone having their own separate vision of how vulnerable/powerful a kung-fu walking robot might be.</li>\n<li>We already know a lot about societal attitudes to car theft.</li>\n<li>Given that autonomous vehicles will soon be mainstream, the morality of the question is then more of a pressing issue.</li>\n</ul>\n\n<p>So, should a self-driving car run someone over (likely killing them) if they try to steal it? I'm hoping that few people would argue that it should.</p>\n\n<p>Should it attempt to do a lesser amount of damage (say, calculated to hopefully only break a leg)?</p>\n\n<p>Again, I'd argue not. The main reason for saying this is that our decision-making algorithms are simply not sufficiently context aware to be able to decide whether theft or harm is the intent. To concretely illustrate this: a recent fatality arose because a self-driving Tesla <a href=\"http://www.livescience.com/55273-first-self-driving-car-fatality.html\" rel=\"nofollow\">was oblivious to context</a> to the extent that it couldn't distinguish between a high-sided van and empty space.</p>\n\n<p>Under those circumstances, it's probably best not to allow commercial autonomous systems to cause physical damage (even to inanimate objects). </p>\n\n<p>'Running away' (or rather, 'driving away', in the case of the car) is another matter: driving is what it's designed to do.</p>\n", "question": "<p>Would it be ethical to implement AI for self-defence for public walking robots which are exposed to dangers such as violence and crime such as robbery (of parts), damage or abduction?</p>\n\n<p>What would be pros and cons of such AI behavior? Is it realistic, or it won't be taken into account for some obvious reasons?</p>\n\n<p>Like pushing back somebody when somebody start pushing it first (AI will say: he pushed me first), or running away on crowded street in case algorithm will detect risk of abduction.</p>\n"}, "id": "1377"}, {"body": {"answer": "<p>It depends on whether the loss of the robot would end up causing harm to humans.</p>\n\n<p>If the robot was supposed to be watching for a suspected terrorist attack to start taking place (so it could alert authorities or halt the attack), it would be very bad if somebody dismantled the robot or otherwise stopped it from carrying out its mission. In that case, the device would be certainly justified in stopping humans from injuring it in any meaningful way.</p>\n\n<p>A robot carrying classified information should probably be similarly willing to protect itself, since the spread of such data could bring harm to a state or a lot of people.</p>\n\n<p>If an AI-enabled device was just walking the streets in the course of carrying out some mundane task, I think it would be hard to justify allowing the robot to incapacitate a human attacker. After all, it was made - presumably - to serve humans.</p>\n\n<p>No matter whether the AI was programmed to defend itself, people couldn't just impede or damage it with impunity. <a href=\"https://malegislature.gov/laws/generallaws/partiv/titlei/chapter266/section127\" rel=\"nofollow\">Intentional destruction</a> of another person's property (including public property) is almost certainly a crime, as is <a href=\"http://law.justia.com/codes/georgia/2010/title-16/chapter-10/article-2/16-10-24\" rel=\"nofollow\">intentional obstruction of law enforcement</a>. It wouldn't have to be up to each robot to defend itself; it could just send information about the perpetrator to C&amp;C before its demise.</p>\n", "question": "<p>Would it be ethical to implement AI for self-defence for public walking robots which are exposed to dangers such as violence and crime such as robbery (of parts), damage or abduction?</p>\n\n<p>What would be pros and cons of such AI behavior? Is it realistic, or it won't be taken into account for some obvious reasons?</p>\n\n<p>Like pushing back somebody when somebody start pushing it first (AI will say: he pushed me first), or running away on crowded street in case algorithm will detect risk of abduction.</p>\n"}, "id": "1378"}, {"body": {"answer": "<p>I get the impression that (perhaps even more than Bluemix) this is what the <a href=\"https://www.wolfram.com/language/elementary-introduction/\" rel=\"nofollow\">Wolfram Language</a> is looking to offer in the longer term.</p>\n\n<p>Seems to me that the main pros and cons are two sides of the same coin:</p>\n\n<p>With Wikipedia, there's no 'search filter' between you and the text. Adding an algorithmic level of indirection between the user and the knowledge that they're looking for is subject to hidden biases.</p>\n\n<p>If those biases are intended in your best interests, and the search is context-sensitive enough to present you with information in the form that is most useful and digestible to you, then this is a good thing. Otherwise, not. Like many topics in AI, problems arise because we're simply not that good at modelling human context yet.</p>\n\n<p>Of course, we're already subject to this <a href=\"https://en.wikipedia.org/wiki/Filter_bubble\" rel=\"nofollow\">filter bubble</a> effect via search engines and social media. The current consensus seems to be that even more of this would not be a good thing for society.</p>\n", "question": "<p>Is there any risk in the near future of replacing all encyclopedias with Watson-like AI where knowledge is accessible by everybody through <a href=\"https://watson-api-explorer.mybluemix.net/\" rel=\"nofollow\">API</a>?</p>\n\n<p><sup>Something similar happened in the future in <a href=\"https://en.wikipedia.org/wiki/The_Time_Machine_(2002_film)\" rel=\"nofollow\"><strong>The Time Machine</strong> movie from 2002</a>.</sup></p>\n\n<p>Obviously maintaining 40 million articles and keeping it up-to-date and consistent could be beyond brain power of few thousands of active editors. Not to mention thousands of other encyclopedias including paperback version or large number of books used by universities which needs to be updated every year by a huge number of people.</p>\n\n<p>What are the pros and cons of such a change?</p>\n"}, "id": "1380"}, {"body": {"answer": "<p>It <a href=\"http://benjamin.wtf/\" rel=\"nofollow\">appears to use</a> Recurrent NNs (RNNs) that have a 'Long Short-Term Memory' (LTSM) architecture.</p>\n\n<p><a href=\"https://medium.com/artists-and-machine-intelligence/adventures-in-narrated-reality-6516ff395ba3#.5lvtgribl\" rel=\"nofollow\">Here's a summary</a> of the development process that the author, Ross Goodwin, went through to create it.</p>\n\n<p>It seems to me (and is also observed in the above link) that the output is rather poor - simply comparable to what one might expect from Markov chains, a technique that is over 100 years old.</p>\n\n<p>I haven't dug deeply into the technique, so I could be misktaken, but perhaps one of the reasons that it's so bad is that (as far as I can see), the model-building process is essentially <em>lexical</em> - i.e. it is linking together tokens (words) without any more informed language model to guide it. In particular, the generated output doesn't seem to know anything about the functional roles played by objects (chairs are supporting objects, used by humans for sitting on etc), which is something that might be fairly readily incorporated.</p>\n", "question": "<p>I've watched the <a href=\"https://www.youtube.com/watch?v=LY7x2Ihqjmc\" rel=\"nofollow\">Sunspring</a> video which didn't make any sense to me (a lot of nonsense monologues), mainly because it was created by Jetson AI.</p>\n\n<p>What was the mechanism of creating such screenplay?</p>\n\n<p>On what criteria was it trained? What was the goal or motivation in terms of training criteria of defining when text does make sense? And what was missed (that it's so bad) and how possibly this could be improved?</p>\n"}, "id": "1382"}, {"body": {"answer": "<p>Have you read the book <a href=\"http://golibgen.io/view.php?id=1397686\" rel=\"nofollow\">The Master Algorithm:</a> by Pedro Domingos?</p>\n\n<p>He discusses the present day machine learning algorithms... Their strengths, weaknesses and applications...</p>\n\n<ul>\n<li>Deep Neural Network</li>\n<li>Genetic Algorithm</li>\n<li>Bayesian Network </li>\n<li>Support Vector Machine</li>\n<li>Inverse Deduction </li>\n</ul>\n\n<p><a href=\"http://i.stack.imgur.com/9HpIP.png\" rel=\"nofollow\"><img src=\"http://i.stack.imgur.com/9HpIP.png\" alt=\"enter image description here\"></a></p>\n", "question": "<p>This <a href=\"http://blog.claymcleod.io/2016/06/01/The-truth-about-Deep-Learning/\" rel=\"nofollow\">article</a> suggests that deep learning is not designed to produce the universal algorithm and cannot be used to create such a complex systems.</p>\n\n<p>First of all it requires huge amounts of computing power, time and effort to train the algorithm the right way and adding extra layers doesn't really help to solve complex problems which cannot be easily predicted.</p>\n\n<p>Secondly some tasks are extremely difficult or impossible to solve using DNN, like solving a <a href=\"http://ai.stackexchange.com/q/154/8\">math</a> equations, predicting <a href=\"http://ai.stackexchange.com/q/225/8\">pseudo-random lists</a>, <a href=\"http://ai.stackexchange.com/q/168/8\">fluid mechanics</a>, guessing encryption algorithms, or <a href=\"http://ai.stackexchange.com/q/205/8\">decompiling</a> unknown formats, because there is no simple mapping between input and output.</p>\n\n<p>So I'm asking, are there any alternative learning algorithms as powerful as deep architectures for general purpose problem solving? Which can solve more variety of problems, than \"deep\" architectures cannot?</p>\n"}, "id": "1385"}, {"body": {"answer": "<p>Deep learning is actually pretty useful (relative to other techniques) <em>precisely when there is no simple mapping between input and output</em>, and features from the raw input need to be aggregated and combined in complex ways by successive layers to form the output.</p>\n\n<p>As I pointed out in my answer to the <a href=\"http://ai.stackexchange.com/questions/205/how-to-write-c-decompiler-using-ai\">AI SE decompilation question</a>, there is recent DL research which takes a natural language description as input and <a href=\"http://arxiv.org/pdf/1510.07211.pdf\" rel=\"nofollow\">generates program text as output</a>. Despite working in this general research area, I was personally surprised by this - the problem is significantly harder than the 'AI math' link you provide above.</p>\n", "question": "<p>This <a href=\"http://blog.claymcleod.io/2016/06/01/The-truth-about-Deep-Learning/\" rel=\"nofollow\">article</a> suggests that deep learning is not designed to produce the universal algorithm and cannot be used to create such a complex systems.</p>\n\n<p>First of all it requires huge amounts of computing power, time and effort to train the algorithm the right way and adding extra layers doesn't really help to solve complex problems which cannot be easily predicted.</p>\n\n<p>Secondly some tasks are extremely difficult or impossible to solve using DNN, like solving a <a href=\"http://ai.stackexchange.com/q/154/8\">math</a> equations, predicting <a href=\"http://ai.stackexchange.com/q/225/8\">pseudo-random lists</a>, <a href=\"http://ai.stackexchange.com/q/168/8\">fluid mechanics</a>, guessing encryption algorithms, or <a href=\"http://ai.stackexchange.com/q/205/8\">decompiling</a> unknown formats, because there is no simple mapping between input and output.</p>\n\n<p>So I'm asking, are there any alternative learning algorithms as powerful as deep architectures for general purpose problem solving? Which can solve more variety of problems, than \"deep\" architectures cannot?</p>\n"}, "id": "1386"}, {"body": {"answer": "<blockquote>\n  <p>\"the human mind is a battleground of higher level goals and lower level goals \"<br>\u2014 Marvin Minsky paraphrasing Sigmund Freud</p>\n</blockquote>\n\n<p>I argue that in general human agents try to maximise a hierarchy of performance measures.</p>\n\n<h1>performance measures of humans</h1>\n\n<ul>\n<li><p>Survival of genetic data </p>\n\n<ul>\n<li><p>Energy supply and Water</p></li>\n<li><p>Sex</p>\n\n<ul>\n<li><em>myriad subgoals....</em></li>\n</ul></li>\n</ul></li>\n</ul>\n\n<p>Mysterious mental mechanisms which neuroscientists do not understand yet force the average human agent to maximise various evaluation metrics.\nWith the overarching goal of <strong>survival of genetic information</strong>. Successful genes are immortal. We are still under the yoke of an ancient genetic algorithm.</p>\n\n<p>These measures are optimised throughout a humans life time. A 30 year old agent is better at survival than a 10 year old agent. A 30 year old agent makes fewer mistakes.</p>\n\n<p>We remember our mistakes. Mistakes are burned into our memory by high levels of neurotransmitters (and reinforcing of synapses) so we don't make them again.</p>\n\n<p>We attempt to optimise a swarm of subgoals that are all connected in one way or another to the main goal <strong>gene survival</strong>.</p>\n\n<ul>\n<li><p>status</p></li>\n<li><p>money</p></li>\n<li><p>education </p></li>\n<li><p>happiness</p></li>\n</ul>\n", "question": "<p>The intelligent agent definition of intelligence states that an agent is intelligent if it acts so to maximize the expected value of a performance measure based on past experience and knowledge. (paraphrased from <a href=\"http://en.wikipedia.org/wiki/Philosophy_of_artificial_intelligence#Intelligent_agent_definition\" rel=\"nofollow\">Wikipedia</a>)</p>\n\n<p>Does this mean that humans are not intelligent? I think we all make mistakes that imply that we are not maximizing the expected value of a performance measure.</p>\n"}, "id": "1387"}, {"body": {"answer": "<p>One obstacle to the development of AI is the fundamental limitations of computer memory. Computers, at a fundamental level, can only work with bits. This limits the type of information that they can describe.</p>\n\n<p>EDIT:</p>\n\n<p>The precise nature and complexity of human memory isn't fully understood, but I would argue that at the very least, human memory is well adapted for the types of tasks that humans perform. Thus, computer memory, even if theoretically capable of representing everything that human memory can, is  probably inefficient and poorly structured for such a task.  </p>\n", "question": "<p>I have a background in Computer Engineering and have been working on developing better algorithms to mimic human thought. (One of my favorites is Analogical Modeling as applied to language processing and decision making.) However, the more I research, the more I realize just <em>how</em> complicated AI is.</p>\n\n<p>I have tried to tackle many problems in this field, but sometimes I find that I am reinventing the wheel or am trying to solve a problem that has already been proven to be unsolvable (ie. the halting problem). So, to help in furthering AI, I want to better understand the current obstacles that are hindering our progress in this field.</p>\n\n<p>For example, time and space complexity of some machine learning algorithms is super-polynomial which means that even with fast computers, it can take a while for the program to complete. Even still, some algorithms may be fast on a desktop or other computer while dealing with a small data set, but when increasing the size of the data, the algorithm becomes intractable.</p>\n\n<p>What are other issues currently facing AI development?</p>\n"}, "id": "1389"}, {"body": {"answer": "<p>Neuromorphic engineering offers various of ways of reproducing the brain\u2019s processing ability.</p>\n\n<p>The recent technology can include IBM's multi-artificial-neuron computer, the world's first artificial nanoscale stochastic phase-change neurons<sup><a href=\"http://arstechnica.com/gadgets/2016/08/ibm-phase-change-neurons/?\" rel=\"nofollow\">article</a></sup>. Check the: <a href=\"http://www.nature.com/nnano/journal/v11/n8/full/nnano.2016.70.html\" rel=\"nofollow\">Stochastic phase-change neurons</a> study.</p>\n\n<p>Other can include</p>\n\n<ul>\n<li><p><a href=\"http://web.stanford.edu/group/brainsinsilicon/neurogrid.html\" rel=\"nofollow\">Neurogrid</a>, built by Brains in Silicon at Stanford University is another example for brain simulation. It uses analog computation to emulate ion-channel activity. It emulates neurons using digital circuitry designed to maximize spiking throughput<sup><a href=\"https://en.wikipedia.org/wiki/Neuromorphic_engineering#Examples\" rel=\"nofollow\">wiki</a></sup>.</p></li>\n<li><p><a href=\"https://en.wikipedia.org/wiki/SpiNNaker\" rel=\"nofollow\">SpiNNaker</a>, which is a manycore computer to simulate the human brain (see <a href=\"https://en.wikipedia.org/wiki/Human_Brain_Project\" rel=\"nofollow\">Human Brain Project</a>).</p></li>\n<li><p><a href=\"https://en.wikipedia.org/wiki/SyNAPSE\" rel=\"nofollow\">SyNAPSE</a>, a DARPA neuromorphic machine technology, that scales to biological levels. Each chip can have over a million of electronic \u201cneurons\u201d and 256 million electronic synapses between neurons. In 2014 the 5.4 billion transistor chip had one of the highest transistor counts of any chip ever produced. The program is undertaken by HRL, HP and IBM.</p></li>\n</ul>\n", "question": "<p>I'd like to know whether there were attempts to simulate the whole brain, I'm not talking only about some <a href=\"http://ai.stackexchange.com/q/237/8\">ANN on microchips</a>, but brain simulations.</p>\n"}, "id": "1395"}, {"body": {"answer": "<p>I'm not sure if predicting MNIST can be really considered as an AI task. AI problems can be usually framed under the context of an agent acting in an environment. Neural nets and machine learning techniques in general do not have to deal with this framing. Classifiers for example, are learning a mapping between two spaces. Though one could argue that you <em>can</em> frame OCR/image classification as an AI problem - the classifier is the agent, each prediction it makes is an action, and it receives rewards based on its classification accuracy - this is rather unnatural and different from problems that are commonly considered AI problems.</p>\n", "question": "<p>On <a href=\"https://en.wikipedia.org/wiki/Artificial_intelligence\">the wikipedia page</a> about AI, we can read:</p>\n\n<blockquote>\n  <p>Optical character recognition is no longer perceived as an exemplar of \"artificial intelligence\" having become a routine technology.</p>\n</blockquote>\n\n<p>On the other hand, the <a href=\"https://en.wikipedia.org/wiki/MNIST_database\">MNIST</a> database of handwritten digits is especially designed for training and testing neural networks and their error rates (see: <a href=\"https://en.wikipedia.org/wiki/MNIST_database#Classifiers\">Classifiers</a>).</p>\n\n<p>So why does the above quote state that OCR is no longer exemplar of AI?</p>\n"}, "id": "1398"}, {"body": {"answer": "<p>I think in this case, you'll probably want to use a genetic algorithm to generate a topology rather than working on your own. I personally like <a href=\"http://nn.cs.utexas.edu/downloads/papers/stanley.ec02.pdf\" rel=\"nofollow\">NEAT Paper</a> (NeuroEvolution of Augmemting Topologies).\n(<a href=\"http://nn.cs.utexas.edu/downloads/papers/stanley.ec02.pdf\" rel=\"nofollow\">NEAT Paper</a>)</p>\n\n<p>The original NEAT paper involves evolving weights for connections, but if you only want a topology, you can use a weighting algorithm instead. You can also mix activation functions if you aren't sure which to use. <a href=\"http://blog.otoro.net/2016/05/07/backprop-neat/\" rel=\"nofollow\">Here</a> is an example of using backpropagation and multiple neuron types.</p>\n", "question": "<p>Assume that I want to solve an issue with neural network that either I can't fit to already existing topologies (perceptron, Konohen, etc) or I'm simply not aware of the existence of those or I'm unable to understand their mechanics and I rely on my own instead.</p>\n\n<p>How can I deconstruct a problem to find a corresponding neural network topology? By this I don't mean only the size of certain layers, but the number of them, the type of activation functions, the number and the direction of connections, and so on.</p>\n\n<p>I'm a beginner, yet I realized that in some topologies (or, at least in perceptrons) it is very hard if not impossible to understand the inner mechanics as the neurons of the hidden layers don't express any mathematically meaningful context.</p>\n"}, "id": "1399"}, {"body": {"answer": "<p>Another answer mentions <a href=\"https://www.cs.ucf.edu/~kstanley/neat.html\" rel=\"nofollow\">NEAT</a> to generate network weights/topologies. Here's a <a href=\"http://doc.gold.ac.uk/aisb50/AISB50-S11/AISB50-S11-Turner-paper.pdf\" rel=\"nofollow\">nice paper</a> on an alternative approach to NEAT, which also gives a short summary of neuroevolution techniques. It uses <a href=\"http://www.cartesiangp.co.uk/\" rel=\"nofollow\">Cartesian Genetic Programming</a> to evolve a multiple activation functions.</p>\n", "question": "<p>Assume that I want to solve an issue with neural network that either I can't fit to already existing topologies (perceptron, Konohen, etc) or I'm simply not aware of the existence of those or I'm unable to understand their mechanics and I rely on my own instead.</p>\n\n<p>How can I deconstruct a problem to find a corresponding neural network topology? By this I don't mean only the size of certain layers, but the number of them, the type of activation functions, the number and the direction of connections, and so on.</p>\n\n<p>I'm a beginner, yet I realized that in some topologies (or, at least in perceptrons) it is very hard if not impossible to understand the inner mechanics as the neurons of the hidden layers don't express any mathematically meaningful context.</p>\n"}, "id": "1400"}, {"body": {"answer": "<p>Although OCR is now a mainstream technology, it remains true that none our methods genuinely have the recognition facilities of a 5 year old (claimed success with CAPTCHAs notwithstanding). We don't know how to achieve this using well-understood techniques, so OCR should still rightfully be considered an AI problem.</p>\n\n<p>To see why this might be so, it is illuminating to read the essay\n<a href=\"https://web.stanford.edu/group/SHR/4-2/text/hofstadter.html\">\"On seeing A's and seeing AS\"</a> by Douglas Hofstadter.</p>\n\n<p>With respect to a point made in another answer, the agent framing is a useful one insofar as it motivates success in increasingly complex environments. However, there are many hard problems (e.g. Bongard) that don't need to be stated in such a fashion. </p>\n", "question": "<p>On <a href=\"https://en.wikipedia.org/wiki/Artificial_intelligence\">the wikipedia page</a> about AI, we can read:</p>\n\n<blockquote>\n  <p>Optical character recognition is no longer perceived as an exemplar of \"artificial intelligence\" having become a routine technology.</p>\n</blockquote>\n\n<p>On the other hand, the <a href=\"https://en.wikipedia.org/wiki/MNIST_database\">MNIST</a> database of handwritten digits is especially designed for training and testing neural networks and their error rates (see: <a href=\"https://en.wikipedia.org/wiki/MNIST_database#Classifiers\">Classifiers</a>).</p>\n\n<p>So why does the above quote state that OCR is no longer exemplar of AI?</p>\n"}, "id": "1402"}, {"body": {"answer": "<p>Unfortunately, this is extremely unlikely.</p>\n\n<p>It is nearly impossible to make statements about the behaviour of software in general. This is due to the <a href=\"https://en.wikipedia.org/wiki/Halting_problem\">Halting problem</a>, which shows that it is impossible to prove whether a program will stop for any given input. From this result, many other things have been shown to be unprovable.</p>\n\n<p>The question whether a piece of code is friendly, can very likely be reduced to a variant of the halting problem.<br>\nAn AI that operates in the real world, which is a requirement for \"friendliness\" to have a meaning, would need to be Turing complete. Input from the real world cannot be reliably interpreted using regular or context-free languages.</p>\n\n<p>Proofs of correctness work for small code snippets, with clearly defined inputs and outputs. They show that an algorithm produces the mathematically right output, given the right input.<br>\nBut these are about situations that can be defined with mathematical rigour.</p>\n\n<p>\"Friendliness\" isn't a rigidly defined concept, which already makes it difficult to prove anything about it. On top of that, \"friendliness\" is about how the AI relates to the real world, which is an environment whose input to the AI is highly unpredictable.</p>\n\n<p>The best we can hope for, is that an AI can be programmed to have safeguards, and that the code will raise warning flags if unethical behaviour becomes likely - that AI's are programmed defensively.</p>\n", "question": "<p>It is possible of normal code to prove that it is correct using mathematical techniques, and that is often done to ensure that some parts are bug-free. </p>\n\n<p>Can we also prove that a piece of code in AI software will cause it to never turn against us, i.e. that the AI is <a href=\"https://en.wikipedia.org/wiki/Friendly_artificial_intelligence\" rel=\"nofollow\">friendly</a>? Has there any research been done towards this?</p>\n"}, "id": "1403"}, {"body": {"answer": "<p>Here are some examples of recent work on verifying certain properties of autonomous systems <a href=\"https://www.cs.york.ac.uk/circus/RoboCalc-event/courses/\" rel=\"nofollow\">[RoboCheck]</a>.</p>\n\n<p>However, to achieve the same kind of thing for the notion of 'friendly' using formal verification (i.e. 'proving correctness using mathematical techniques'),\nit would (at the least) seem necessary to be able to express 'friendly' within a logical formalism, (i.e. as a predicate testable within a model-checker, so that we can be sure a system never enters an undesirable state).</p>\n\n<p>However, it's not immediately clear that 'friendly' has a more specific definition than 'a desire not to harm humans', so much more low-level detail is needed.</p>\n\n<p>Some previous work in this general area that might be useful in this respect include:</p>\n\n<ul>\n<li><a href=\"https://en.wikipedia.org/wiki/Deontic_logic\" rel=\"nofollow\">Deontic Logic</a> - a logical calculus of obligations.</li>\n<li><a href=\"http://www.jfsowa.com/ikl/McCarthy89\" rel=\"nofollow\">Elephant 2000</a> - John McCarthy's description of a promise-based programming language.</li>\n</ul>\n", "question": "<p>It is possible of normal code to prove that it is correct using mathematical techniques, and that is often done to ensure that some parts are bug-free. </p>\n\n<p>Can we also prove that a piece of code in AI software will cause it to never turn against us, i.e. that the AI is <a href=\"https://en.wikipedia.org/wiki/Friendly_artificial_intelligence\" rel=\"nofollow\">friendly</a>? Has there any research been done towards this?</p>\n"}, "id": "1405"}, {"body": {"answer": "<p>Whenever a problem becomes solvable by a computer, people start arguing that it does not require intelligence. John McCarthy is often quoted: \"As soon as it works, no one calls it AI anymore\" (<a href=\"http://cacm.acm.org/magazines/2012/1/144824-artificial-intelligence-past-and-future/fulltext\" rel=\"nofollow\">Referenced in CACM</a>).</p>\n\n<p>One of my teachers in college said that in the 1950's, a professor was asked what he thought was intelligent for a machine. The professor reputedly answered that if a vending machine gave him the right change, that would be intelligent.</p>\n\n<p>Later, playing chess was considered intelligent. However, computers can now defeat grandmasters at chess, and people are no longer saying that it is a form of intelligence. </p>\n\n<p>Now we have OCR. It's already stated in <a href=\"http://ai.stackexchange.com/a/1402/66\">another answer</a> that our methods do not have the recognition facilities of a 5 year old. As soon as this is achieved, people will say \"meh, that's not intelligence, a 5 year old can do that!\"</p>\n\n<p>A psychological bias, a need to state that we are somehow superior to machines, is at the basis of this.</p>\n", "question": "<p>On <a href=\"https://en.wikipedia.org/wiki/Artificial_intelligence\">the wikipedia page</a> about AI, we can read:</p>\n\n<blockquote>\n  <p>Optical character recognition is no longer perceived as an exemplar of \"artificial intelligence\" having become a routine technology.</p>\n</blockquote>\n\n<p>On the other hand, the <a href=\"https://en.wikipedia.org/wiki/MNIST_database\">MNIST</a> database of handwritten digits is especially designed for training and testing neural networks and their error rates (see: <a href=\"https://en.wikipedia.org/wiki/MNIST_database#Classifiers\">Classifiers</a>).</p>\n\n<p>So why does the above quote state that OCR is no longer exemplar of AI?</p>\n"}, "id": "1406"}, {"body": {"answer": "<p>The authors do actually give an English definition in terms of the well-known agent formulation of AI:</p>\n\n<blockquote>\n  <p>We intend this usage to be intuitive: death means that one sees\n  no more percepts, and takes no more actions.</p>\n</blockquote>\n\n<p>It would seem that this becomes possible for a reinforcement learning agent such as AIXI in a formulation that uses <em>semi-measures</em> of probability (which need not sum up to 1), rather than the more traditional notion.</p>\n", "question": "<p>In <a href=\"http://arxiv.org/pdf/1606.00652.pdf\" rel=\"nofollow\">this paper</a>, a proposal is given for what death could mean for Artificial Intelligence. </p>\n\n<p>What does this mean using English only? I understand that mathematical notation is useful for giving a precise definition, but I'd like to understand what the definition really means. </p>\n"}, "id": "1407"}, {"body": {"answer": "<p>Vernor Vinge said that if we can scan a human brain and then simulate it: We can run it at 1000 times the speed. The brain will be able to do 1000 years of thinking in 1 year ect. </p>\n\n<p>At this stage in history we have the computer power.</p>\n\n<p>The trouble lies in cutting a brain up and scanning the 100 billion neurons and 12 million kilometres of axons and 100000 billion synapses.\nAnd piecing together the connectome from all the data.</p>\n\n<p>Sebastian Seung at MIT is working on automating this scanning process with machine learning. By gathering training data from thousands of people playing his <a href=\"https://en.wikipedia.org/wiki/Eyewire\" rel=\"nofollow\">Eyewire game</a></p>\n\n<p>Henry Markram in Europe tried to do something similar with his <a href=\"https://en.wikipedia.org/wiki/Blue_Brain_Project\" rel=\"nofollow\">Blue Brain Project</a>.\nHe attempted to simulate the neocortical column of a rat. The EU gave him a billion euros to do this. Unfortunately he has been heavily criticised by the Neuroscience community. They claim that we don't know the physiology  well enough to make a valid simulation.</p>\n\n<p>Check out his <a href=\"https://www.youtube.com/watch?v=LS3wMC2BpxU\" rel=\"nofollow\">Ted Talk</a>.</p>\n\n<p>In the 1970s Sydney Brenner achieved a <em>full brain scan</em> of a C Elegans worm. This worm has one of the simplest biological neural networks having only 302 neurons.\nHere is a picture of its connectome:<a href=\"http://i.stack.imgur.com/cAZ49.png\" rel=\"nofollow\"><img src=\"http://i.stack.imgur.com/cAZ49.png\" alt=\"enter image description here\"></a>\n<a href=\"http://i.stack.imgur.com/siHt8.png\" rel=\"nofollow\"><img src=\"http://i.stack.imgur.com/siHt8.png\" alt=\"enter image description here\"></a></p>\n\n<p>An accurate computer simulation of this worm would be a major stepping stone to uploading a human brain.</p>\n", "question": "<p>I'd like to know whether there were attempts to simulate the whole brain, I'm not talking only about some <a href=\"http://ai.stackexchange.com/q/237/8\">ANN on microchips</a>, but brain simulations.</p>\n"}, "id": "1408"}, {"body": {"answer": "<p>Matrix representation is beneficial for implementing neural networks in silicon.</p>\n\n<p>But for examining neural networks empirically it is sometimes good to visualise the synapse weight values as images or videos: <a href=\"https://www.youtube.com/watch?v=AgkfIQ4IGaM\" rel=\"nofollow\">Jason Yosinski's </a> exploration of a convolution neural network. The network seems to have a \"filter\" that just detects shoulders. A bit like a lock that only opens when it recognises the pattern of shoulders.<a href=\"http://i.stack.imgur.com/4g4gF.png\" rel=\"nofollow\"><img src=\"http://i.stack.imgur.com/4g4gF.png\" alt=\"enter image description here\"></a></p>\n", "question": "<p>A neural network is a directed weighted graph. These can be represented by a (sparse) matrix. Doing so can expose some elegant properties of the network.</p>\n\n<p>Is this technique beneficial for examining neural networks?</p>\n"}, "id": "1409"}, {"body": {"answer": "<p>One of the challenges of AI is defining Intelligence.\nIf we could precisely define general intelligence then we could program it into a computer. After all an algorithm is a process so well defined that it can be run on a computer.</p>\n\n<p>Narrow AI can be evaluated on its success at achieving goals in an environment. In domains such as computer vision and speech recognition narrow AI algorithms can be easily evaluated.</p>\n\n<p>Many universities curate narrow AI tests. Fei-Fei Li a professor at Stanford who directs the Artificial Intelligence lab there organises the annual ImageNet Challenge. In 2012 Geoffrey Hinton famously won the competition by building a Deep Neural Network that could recognize pictures more accurately than humans can.</p>\n\n<p>To my knowledge the testers commonly use <a href=\"https://en.wikipedia.org/wiki/Precision_and_recall\" rel=\"nofollow\">Precision and recall</a> evaluation metrics</p>\n", "question": "<p>We can measure the power of the machine with the number of operation per second or the frequency of the processor. But does units similar of IQ for humans exist for a AI?<br/>\nI'm asking for a unit which can give countable result so something different from a Turing Test which only give a binary result.</p>\n"}, "id": "1411"}, {"body": {"answer": "<p>Yes, many people have worked on this sort of thing, due to its obvious industrial applications (most of the ones I'm familiar with are in the pharmaceutical industry). Here's <a href=\"https://arxiv.org/abs/1305.7074\">a paper from 2013</a> that claims good results; following the trail of <a href=\"https://scholar.google.com/scholar?cites=10630711614897084406&amp;as_sdt=5,44&amp;sciodt=0,44&amp;hl=en\">papers that cited it</a> will likely give you more recent work. </p>\n", "question": "<p>Is there any research which study application of AI into chemistry which can predict the output of certain chemical reactions.</p>\n\n<p>So for example, you train the AI about current compounds, substances, structures and their products and chemical reactions from the existing <a href=\"http://opendata.stackexchange.com/q/3553/3082\">dataset</a> (basically what produce what). Then you give the task to find how to create a gold or silver from group of available substances. Then the algorithm will find the chemical reactions (successfully predicting new one which weren't in the dataset) and gives the results. Maybe the gold is not a good example, but the practical scenario would be creation of drugs which are cheaper to create by using much more simpler processes or synthesizing some substances for the first time for drug industries.</p>\n\n<p>Was there any successful research attempting to achieve that using deep learning algorithms?</p>\n"}, "id": "1412"}, {"body": {"answer": "<p>Shane Legg and Marcus Hutter <a href=\"http://www.vetta.org/documents/42.pdf\" rel=\"nofollow\">proposed one</a> in 2006. The main descriptive quotes (see the paper for the actual formula):</p>\n\n<blockquote>\n  <p>Intelligence measures an agent\u2019s general ability to achieve goals in a wide range of environments</p>\n  \n  <p>...</p>\n  \n  <p>It is clear by construction that universal intelligence measures the general ability of an agent to perform well in a very wide range of environments, as required by our informal definition of intelligence given earlier. The definition places no restrictions on the internal workings of the agent; it only requires that the agent is capable of generating output and receiving input which includes a reward signal.</p>\n</blockquote>\n", "question": "<p>We can measure the power of the machine with the number of operation per second or the frequency of the processor. But does units similar of IQ for humans exist for a AI?<br/>\nI'm asking for a unit which can give countable result so something different from a Turing Test which only give a binary result.</p>\n"}, "id": "1413"}, {"body": {"answer": "<p>The other answers are correct that machine IQ test results are currently <strong>not</strong> indicative of machine intelligence. One of the surprising facts of human intelligence is that performance on almost all cognitive tasks are correlated with each other; that is, there is such a thing as 'general smartness' and IQ tests attempt to measure that thing.</p>\n\n<p>People <em>have</em> built programs that take IQ tests, however, and some of them perform quite well. Raven's Progressive Matrices, a visual pattern recognition IQ test, is an easy target for AI (see <a href=\"https://www.researchgate.net/publication/288211280_Solving_Raven&#39;s_IQ-tests_An_AI_and_cognitive_modeling_approach\" rel=\"nofollow\">this paper</a> as representative) and another group <a href=\"http://arxiv.org/abs/1509.03390\" rel=\"nofollow\">has constructed an AI</a> that performs about as well as a 4 year old on the verbal intelligence portion of a standard childhood IQ test.</p>\n", "question": "<p>Can an AI program have an IQ?</p>\n\n<p>In other words, can the IQ of an AI program be measured?</p>\n\n<p>Like how humans can do an IQ test.</p>\n"}, "id": "1414"}, {"body": {"answer": "<p>The images that you provided may be unrecognizable for us. They are actually the images that we recognize but evolved  using the <a href=\"https://github.com/sferes2/sferes2\" rel=\"nofollow\">Sferes</a> evolutionary framework.</p>\n\n<p>While these images are almost impossible for humans to label with anything but abstract arts, the Deep Neural Network will label them to be familiar objects with 99.99% confidence.</p>\n\n<p>This result highlights differences between how DNNs and humans recognize objects. Images are either directly () or indirectly\n() encoded</p>\n\n<p>According to this <a href=\"https://youtu.be/M2IebCN9Ht4\" rel=\"nofollow\">video</a></p>\n\n<blockquote>\n  <p>Changing an image originally correctly classified in a way imperceptible to humans can cause the cause DNN to classify it as something else.</p>\n  \n  <p>In the image below the number at the bottom are the images are supposed to look like the digits\n  But the network believes the images at the top (the one like white noise) are real digits with 99.99% certainty.</p>\n</blockquote>\n\n<p><a href=\"http://i.stack.imgur.com/Jx1wX.png\" rel=\"nofollow\"><img src=\"http://i.stack.imgur.com/Jx1wX.png\" alt=\"enter image description here\"></a></p>\n\n<blockquote>\n  <p>The main reason why these are easily fooled is because Deep Neural Network does not see the world in the same way as human vision. We use the whole image to identify things while DNN depends on the features. As long as DNN detects certain features, it will classify the image as a familiar object it has been trained on.\n  The researchers proposed one way to prevent such fooling by adding the fooling images to the dataset in a new class and training DNN on the enlarged dataset. In the experiment, the confidence score decreases significantly for ImageNet AlexNet. It is not easy to fool the retrained DNN this time. But when the researchers applied such method to MNIST LeNet, evolution still produces many unrecognizable images with confidence scores of 99.99%.</p>\n</blockquote>\n\n<p>More details <a href=\"http://www.evolvingai.org/fooling\" rel=\"nofollow\">here</a>, <a href=\"http://www.kdnuggets.com/2015/01/deep-learning-can-be-easily-fooled.html\" rel=\"nofollow\">here</a> and <a href=\"http://www.kdnuggets.com/2015/01/deep-learning-can-be-easily-fooled.html\" rel=\"nofollow\">here</a>.</p>\n", "question": "<p>The following <a href=\"http://www.evolvingai.org/fooling\">page</a>/<a href=\"http://www.evolvingai.org/files/DNNsEasilyFooled_cvpr15.pdf\">study</a> demonstrates that the deep neural networks are easily fooled by giving high confidence predictions for unrecognisable images, e.g.</p>\n\n<p><a href=\"http://i.stack.imgur.com/7pgrH.jpg\"><img src=\"http://i.stack.imgur.com/7pgrH.jpg\" alt=\"Evolved images that are unrecognisable to humans, but that state-of-the-art DNNs trained on ImageNet believe with &gt;= 99.6% certainty to be a familiar object. This result highlights differences between how DNNs and humans recognise objects. Directly and indirectly encoded images\"></a></p>\n\n<p><a href=\"http://i.stack.imgur.com/pBm48.png\"><img src=\"http://i.stack.imgur.com/pBm48.png\" alt=\"Evolving images to match DNN classes produces a tremendous diversity of images. The mean DNN confidence scores for these images is 99.12% for the listed class, meaning that the DNN believes with near-certainty that the image is that type of thing. Shown are images selected to showcase diversity from 5 independent evolutionary runs. The images shed light on what the DNN network cares about, and what it does not, when classifying an image. For example, a school bus is alternating yellow and black lines, but does not need to have a windshield or wheels\"></a></p>\n\n<p>How this is possible? Can you please explain ideally in plain English?</p>\n"}, "id": "1417"}, {"body": {"answer": "<p>This is something of an orthogonal answer, but I think Brooks didn't go about his idea the right way. That is, <a href=\"https://en.wikipedia.org/wiki/Subsumption_architecture\" rel=\"nofollow\">subsumption architecture</a> is one in which the 'autopilot' is <em>replaced</em> by a more sophisticated system when necessary. (All pieces receive the raw sensory inputs, and output actions, some of which turn off or on other systems.)</p>\n\n<p>But a better approach is the normal hierarchical control approach, in which the target of a lower level system is the output of a higher level system. That is, the targeted joint angle of a robot leg is determined by the system that is trying to optimize the velocity, which is determined by a system that is trying to optimize the trajectory, which is determined by a system that is trying to optimize the target position, and so on.</p>\n\n<p>This allows for increasing level of complexity while maintaining detail and system reusability.</p>\n\n<hr>\n\n<p>That said, I don't think you actually need what one would naively call 'embodied cognition' in order to get the bottom-up hierarchy of competencies that Brooks is right to point towards. The core feature is the wide array of inputs and outputs, which are understood in a hierarchical fashion that allows systems to be chained together vertically. I think you could get a functional general intelligence whose only inputs and outputs involve going through an Ethernet cable, and doesn't have anything like a traditional body that it actuates or senses through. (This is a claim that the hierarchical structure is what matters, not the content of what we use that structure for.)</p>\n\n<p>(The main place to look for more, I think, is actually a book about <em>human</em> cognition, called The Control of Perception by William T. Powers.)</p>\n", "question": "<p>In the mid 1980s, Rodney Brooks famously created the foundations of \"the new AI\". The central claim was that the symbolist approach of 'Good Old Fashioned AI' (GOFAI) had failed by attempting to 'cream cognition off the top', and that <em>embodied cognition</em> was required, i.e. built from the bottom up in a 'hierarchy of competances' (e.g. basic locomotion -> wandering around -> actively foraging) etc.</p>\n\n<p>I imagine most AI researchers would agree that the 'embodied cognition' perspective has now (at least tacitly) supplanted GOFAI as the mainstream.</p>\n\n<p>My question takes the form of a thought experiment and asks: \"Which (if any)  aspects of 'embodied' can be relaxed/omitted before we lose something essential for AGI?\"</p>\n"}, "id": "1418"}, {"body": {"answer": "<p>I can offer two (at first sight, conflicting) perspectives on this:</p>\n\n<p>Firstly:</p>\n\n<p><em>If the letter string 'abc' becomes 'abd' what would \"doing the same thing\" to 'ijk' look like?</em></p>\n\n<p>This is just one example of a problem (so-called 'letterstring analogy problems') that is not easily framed as an optimization problem - there are a range of answers that appear compelling to humans, each for it's own structurally-specific reason. Some of the subtleties of these kind of problems are discussed in detail <a href=\"http://cognitrn.psych.indiana.edu/rgoldsto/courses/concepts/copycat.pdf\" rel=\"nofollow\">here</a>.</p>\n\n<p>Secondly:</p>\n\n<p>Here's a <em>very</em> high-level perspective on AGI in which <a href=\"http://arxiv.org/abs/cs/0309048\" rel=\"nofollow\">optimization plays a key part</a>.</p>\n\n<p>It's not at all clear how these two very different scales of approach might be reconciled. As someone who does optimization research for a living, I'd be inclined to say that, certainly for all <em>current, practical</em> purposes, AGI can't really be treated as an optimization problem, since most interesting activities don't readily lend themselves to description via a cost function.</p>\n", "question": "<p>In <a href=\"https://youtu.be/oSdPmxRCWws?t=30\">this video</a> an expert says, \"One way of thinking about what intelligence is [specifically with regard to artificial intelligence], is as an optimization process.\"</p>\n\n<p>Can intelligence always be thought of as an optimization process, and can artificial intelligence always be modeled as an optimization problem? What about pattern recognition? Or is he mischaracterizing?</p>\n"}, "id": "1419"}, {"body": {"answer": "<p>We are absolutely nowhere near, nor do we have any idea how to bridge the gap between what we can currently do and what is depicted in these films.</p>\n\n<p>The current trend for DL approaches (coupled with the emergence of data science as a mainstream discipline) has led to a lot of popular interest in AI.</p>\n\n<p>However, researchers and practitioners would do well to learn the lessons of the 'AI Winter' and not engage in hubris or read too much into current successes.</p>\n\n<p>For example:</p>\n\n<ul>\n<li>Success in transfer learning is very limited. </li>\n<li>The 'hard problem' (i.e. presenting the 'raw, unwashed environment' to the machine and having it come up with a solution from scratch) is not being\naddressed by DL to the extent that it is popularly portrayed: expert human knowledge is still required to help decide how the input should be framed, tune parameters, interpret output etc.</li>\n</ul>\n\n<p>Someone who has enthusiasm for AGI would hopefully agree that the 'hard problem' is actually the only one that matters. Some years ago, a famous cognitive scientist said \"We have yet to successfully represent <em>even a single concept</em> on a computer\". </p>\n\n<p>In my opinion, recent research trends have done little to change this.</p>\n\n<p>All of this perhaps sounds pessimistic - it's not intended to. None of us want another AI Winter, so we should challenge (and be honest about) the limits of our current techniques rather than mythologizing them.</p>\n", "question": "<p>Are there any research teams which attempted to create or have already created an AI robot which can be as close to intelligent as these found in <a href=\"https://en.wikipedia.org/wiki/Ex_Machina_(film)\"><em>Ex Machina</a></em> or <em><a href=\"https://en.wikipedia.org/wiki/I,_Robot_(film)\">I, Robot</em></a> movies?</p>\n\n<p>I'm not talking about full awareness, but an artificial being which can make its own decisions and physical and intellectual tasks that a human being can do?</p>\n"}, "id": "1421"}, {"body": {"answer": "<blockquote>\n  <p>\"heavier-than-air flying machines are impossible\" <em>_ Lord Kelvin 1895</em> </p>\n</blockquote>\n\n<p>7 years later the Wright brothers built one.</p>\n\n<hr>\n\n<p>Currently we have many powerful narrow AI (good at special tasks) but we have no idea how to unify them into a single system like in a biological brain. </p>\n", "question": "<p>Are there any research teams which attempted to create or have already created an AI robot which can be as close to intelligent as these found in <a href=\"https://en.wikipedia.org/wiki/Ex_Machina_(film)\"><em>Ex Machina</a></em> or <em><a href=\"https://en.wikipedia.org/wiki/I,_Robot_(film)\">I, Robot</em></a> movies?</p>\n\n<p>I'm not talking about full awareness, but an artificial being which can make its own decisions and physical and intellectual tasks that a human being can do?</p>\n"}, "id": "1422"}, {"body": {"answer": "<p>Yes, there were successful attempts at predicting the interaction between molecules and biological proteins which have been used to identify potential treatments by using <a href=\"https://en.wikipedia.org/wiki/Convolutional_neural_network#Drug_discovery\" rel=\"nofollow\">convolutional neural networks</a>.</p>\n\n<p>For example in 2015, the first deep learning neural network has been created for structure-based drug design which trains 3-dimensional representation of chemical interactions which works similar to how image recognition works (composing smaller features into larger, complex structures).<sup><a href=\"https://en.wikipedia.org/wiki/Convolutional_neural_network#Drug_discovery\" rel=\"nofollow\">wiki</a></sup></p>\n\n<p><sup>Study: <a href=\"https://arxiv.org/abs/1510.02855\" rel=\"nofollow\">AtomNet: A Deep Convolutional Neural Network for Bioactivity Prediction in Structure-based Drug Discovery</a></sup></p>\n\n<hr>\n\n<p>Another approach is to use evolutionary artificial neural networks which can achieve great optimization results.</p>\n\n<p>Further more, the <a href=\"https://arxiv.org/pdf/1502.00193.pdf\" rel=\"nofollow\">paper from 2015</a> demonstrated heurisic <a href=\"http://link.springer.com/article/10.1007/s12293-012-0075-1\" rel=\"nofollow\">chemical reaction optimization</a> (CRO) which is inspired by the natural of chemical reactions (e.g. transforming the unstable substances to the stable onces). Simulation results shows that CRO outperforms many evolutionary algorithms by population-based metaheuristics mimicking the transition of molecules and their interactions in a chemical reaction.</p>\n\n<p>Sample pseudocode algorithm for predicting synthesis given \u03c91, \u03c92 (from the above paper):</p>\n\n<pre><code> 1: for all Matrices and vectors m in \u03c9\u2032 do\n 2:     for all Elements e in m do\n 3:         Generate a real r between 0 and 1\n 4:         if r &gt; 0.5 then\n 5:             e =counterpart in m1\n 6:         else\n 7:             e =counterpart in m2\n 8:         end if\n 9:     end for\n10: end for\n</code></pre>\n\n<p>which is used to generate a new solution \u03c9\u2032 based on two given solutions \u03c91 and \u03c92.</p>\n", "question": "<p>Is there any research which study application of AI into chemistry which can predict the output of certain chemical reactions.</p>\n\n<p>So for example, you train the AI about current compounds, substances, structures and their products and chemical reactions from the existing <a href=\"http://opendata.stackexchange.com/q/3553/3082\">dataset</a> (basically what produce what). Then you give the task to find how to create a gold or silver from group of available substances. Then the algorithm will find the chemical reactions (successfully predicting new one which weren't in the dataset) and gives the results. Maybe the gold is not a good example, but the practical scenario would be creation of drugs which are cheaper to create by using much more simpler processes or synthesizing some substances for the first time for drug industries.</p>\n\n<p>Was there any successful research attempting to achieve that using deep learning algorithms?</p>\n"}, "id": "1424"}, {"body": {"answer": "<p>Watson can make its diagnosis based on the patient's data and comparing it to the data from million of other studies.</p>\n\n<p>For example having enough genetic data and the right algorithms, its AI computing capability demonstrated the huge potential of data analysis based on which it can used for everything from diagnosing rare illnesses to prescribing perfect dosages of medicine based on the patient's personal genetic makeup. Of course there are still plenty of challenges which need to be overcome before it can be used in mainstream medicine. </p>\n\n<p>Recently Watson was able to diagnose rare form of leukemia after treatment was proved ineffective. It was fed in with patient\u2019s genetic data and compared to data from other 20 million oncological studies.</p>\n\n<p>Sources:</p>\n\n<ul>\n<li><a href=\"http://www.ndtv.com/health/artificial-intelligence-used-to-detect-rare-leukemia-type-in-japan-1440789\" rel=\"nofollow\">Artificial Intelligence Used To Detect Rare Leukemia Type In Japan</a> (2016)</li>\n<li><a href=\"http://siliconangle.com/blog/2016/08/05/watson-correctly-diagnoses-woman-after-doctors-were-stumped/\" rel=\"nofollow\">Watson correctly diagnoses woman after doctors were stumped</a> (2016)</li>\n</ul>\n", "question": "<p>On Watson wiki page we can read:</p>\n\n<blockquote>\n  <p>In healthcare, Watson's natural language, hypothesis generation, and evidence-based learning capabilities allow it to function as a clinical decision support system for use by medical professionals.</p>\n</blockquote>\n\n<p>How exactly such AI can help doctors to diagnose the diseases?</p>\n"}, "id": "1428"}, {"body": {"answer": "<p>There are variety of aspects where AI can help for a public good. Future studies of computational methods can contribute to sustainable management ecosystem by its data acquisition, interpretation, integration and model fitting.</p>\n\n<p><a href=\"http://web.engr.oregonstate.edu/~tgd/\" rel=\"nofollow\">Prof. Tom Dietterich</a> is a leader in combining computer science and ecological sciences to build new discipline of Ecosystem Informatics which studies methods for collecting, analyzing and visualizing data on the structure and function of ecosystems.</p>\n\n<p>His group is involved in many aspects of ecosystem, such as:</p>\n\n<ul>\n<li>Models that can predict species distribution and their presence/absence elsewhere in order to create species distribution and migration/dispersal maps (such as <a href=\"https://dataone.org/\" rel=\"nofollow\">DataONE Datanet</a>, <a href=\"http://ebird.org/content/ebird/\" rel=\"nofollow\">eBird project</a>, <a href=\"http://birdcast.info/\" rel=\"nofollow\">BirdCast</a>) .</li>\n<li>Bio-economic models which require solving large spatio-temporal optimization problems under uncertainty.</li>\n<li>Ecosystem prediction problems which require integrating heterogeneous data sources.</li>\n<li>Algorithms for deployment (sensor placement), cleaning and analysis of sensor network data of resulting data to increase agricultural productivity (Project <a href=\"http://tahmo.org/\" rel=\"nofollow\">TAHMO</a>), like deployment of 20,000 hydro-meteorological stations in Africa (e.g. computational problem where to place it).</li>\n<li>Systems for capturing, imaging, and sorting bugs combined with general image processing/machine learning/pattern recognition tools for counting and classifying them (<a href=\"http://web.engr.oregonstate.edu/~tgd/bugid/\" rel=\"nofollow\">BugID project</a>). The goal is to develop algorithms for automating biodiversity based on the visual pattern recognition by using computer vision method.</li>\n</ul>\n\n<p>For further information about this work, check the following resources, see:</p>\n\n<ul>\n<li>(video) <a href=\"https://www.youtube.com/watch?v=FjHBFWwOdIk\" rel=\"nofollow\">\"Challenges for Machine Learning in Computational Sustainability\" (CRCS)</a> (2013)</li>\n<li>(slides) <a href=\"http://cra.org/ccc/wp-content/uploads/sites/2/2016/06/Thomas-Dietterich-AI-slides.pdf\" rel=\"nofollow\">Tom Dietterich, Understanding and Managing Ecosystems through AI</a> (2013)</li>\n<li>(study) <a href=\"http://rspb.royalsocietypublishing.org/content/282/1808/20142984\" rel=\"nofollow\">Adapting environmental management to uncertain but inevitable change</a> (2015)</li>\n</ul>\n", "question": "<p>Recently White House published the article: <a href=\"https://www.whitehouse.gov/blog/2016/05/03/preparing-future-artificial-intelligence\" rel=\"nofollow\">Preparing for the Future of Artificial Intelligence</a> which says that government is working to leverage AI for public good and toward a more effective government.</p>\n\n<p>I'm especially interested how AI can help with computational sustainability, environmental management and Earth's ecosystem such as biological conservation?</p>\n"}, "id": "1430"}, {"body": {"answer": "<p>According to IBM Research organization in the response to White House as part of <a href=\"https://www.whitehouse.gov/webform/rfi-preparing-future-artificial-intelligence\" rel=\"nofollow\">preparing for the future of Artificial Intelligence</a>, AI depends upon many long-term advances, not only from AI researchers, but from many interdisciplinary teams of experts from many disciplines, including the following challenges:</p>\n\n<ul>\n<li><p><strong>Machine learning and reasoning.</strong></p>\n\n<p>Currently AI systems use supervised learning using huge amount of dataset of labeled data for training. This is very different to how humans learn by creating concepts, relationship, common sense reasoning which gives ability to learn much without too much data. Therefore machine learning with common-sense reasoning capabilities should be researched further more.</p></li>\n<li><p><strong>Decision techniques.</strong></p>\n\n<p>Current AI-based systems have very limited ability for making decisions, therefore new techniques must be developed (e.g. modeling systemic risks, analyzing tradeoffs, detecting anomalies in context, analyzing data while preserving privacy).</p></li>\n<li><p><strong>Domain-specific AI systems.</strong></p>\n\n<p>The current AI-based system is lack of abilities to understand the variety of domains of human expertise (such as medicine, engineering, law and many more). The systems should be able to perform professional-level tasks such as designing problems, experiments, managing contradictions, negotiating, etc.</p></li>\n<li><p><strong>Data assurance and trust.</strong></p>\n\n<p>The current AI-based systems require huge amounts of data and their behaviour directly depends on the quality of this data which can be biased, incomplete or compromised. This can be expensive and time consuming especially where it is used for safety critical systems which potentially can be very dangerous.</p></li>\n<li><p><strong>Radically efficient computing infrastructure.</strong></p>\n\n<p>The current AI-based systems require unprecedented workloads and computing power which require development of new computing architectures (such as neuromorphic).</p></li>\n<li><p><strong>Interpretability and explanations.</strong></p>\n\n<p>For people to follow AI suggestions, they need to trust systems, and this is only when they are capable of knowing users' intents, priorities, reasoning and they can learn from their mistakes. These capabilities are required in many business domains and professionals</p></li>\n<li><p><strong>Value alignment and ethics.</strong></p>\n\n<p>Humans can share the common knowledge of how the world function, the machine cannot. They can fail by having unintended and unexpected behaviour only because humans did not specify the right goals for them or them omitted essential training details. The systems should be able to correct specification of the goals and avoid unintended and undesired consequences in the behaviour.</p></li>\n<li><p><strong>Social AI.</strong></p>\n\n<p>The AI-based systems should be able to work closely to humans in their professional and personal life, therefore they should have significant social capabilities, because they can impact on our emotions and our decision making capabilities. Also sophisticated natural language capabilities will need to be developed to allow a natural interaction and dialog between humans and machines.</p></li>\n</ul>\n\n<p>Source: <a href=\"http://research.ibm.com/cognitive-computing/ostp/document4.shtml\" rel=\"nofollow\">Fundamental questions in AI research, and the most important research gaps (RFI questions 5 and 6)</a></p>\n", "question": "<p><a href=\"https://www.whitehouse.gov/webform/rfi-preparing-future-artificial-intelligence\" rel=\"nofollow\">White House published the information</a> about AI which requests mentions about 'the most important research gaps in AI that must be addressed to advance this field and benefit the public'.</p>\n\n<p>What are these exactly?</p>\n"}, "id": "1434"}, {"body": {"answer": "<p><a href=\"http://karpathy.github.io/2016/05/31/rl/\" rel=\"nofollow\">Andrej Karpathy's blog</a> has a tutorial on getting a neural network to learn pong with reinforcement learning. His commentary on the current state of the field is interesting.</p>\n\n<p>He also provides a whole bunch of links (David Silver's <a href=\"http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html\" rel=\"nofollow\">course</a> catches my eye). <a href=\"https://www.youtube.com/watch?v=2pWv7GOvuf0\" rel=\"nofollow\">Here is a working link to the lecture videos.</a></p>\n\n<p>Here are demos of DeepMinds game playing.\nGet links to the papers at Andrej Karpathy's blog above\n- <a href=\"https://www.youtube.com/watch?v=MAMuNUixKJ8\" rel=\"nofollow\">rat fps</a></p>\n\n<ul>\n<li><a href=\"http://livestream.com/oxuni/StracheyLectureDrDemisHassabis/videos/113380152\" rel=\"nofollow\">nice demos at 19 minutes into this</a></li>\n</ul>\n", "question": "<p>I've implemented <a href=\"https://en.wikipedia.org/wiki/Reinforcement_learning\" rel=\"nofollow\">the reinforcement learning alogrithm</a> for an agent to play <a href=\"https://github.com/admonkey/snappybird\" rel=\"nofollow\">snappy bird</a> (a shameless cheap ripoff of flappy bird) utilizing a q-table for storing the history for future lookups. It works and eventually achieves perfect convergence after enough training.</p>\n\n<p>Is it possible to implement a neural network to do function approximation in order to accomplish the purpose of the q-table? Obviously storage is a concern with the q-table, but it doesn't seem to ever train with the neural net alone. Perhaps training the nnet on an existing q-table would work, but I would like to not use a q-table at all if possible.</p>\n"}, "id": "1437"}, {"body": {"answer": "<p>There are three typical use cases for the phrase 'superintelligent':</p>\n\n<ol>\n<li><p>Something that is at least as smart as a human for <em>every</em> task.</p></li>\n<li><p>Something that is smart enough to improve itself on a fundamental level.</p></li>\n<li><p>Something that is smarter than a human at a <em>single</em> task.</p></li>\n</ol>\n\n<p>Most uses that I see are definition 2, but the other two are also somewhat common. I typically follow I. J. Good and use <a href=\"https://en.wikipedia.org/wiki/I._J._Good#Research_and_publications\" rel=\"nofollow\">ultraintelligence</a> for the first definition. (There are lots of arguments that something that fits definition 1 is likely to fit definition 2 as well, but this doesn't seem to be logically necessary.)</p>\n\n<p>Obviously, Deep Blue only counts for definition 3.</p>\n", "question": "<p>When AI has some narrow domain such as chess where it can outperform the world's human masters of chess, does it make it a superintelligence or not?</p>\n"}, "id": "1438"}, {"body": {"answer": "<p>To my knowledge, recursion does not play a strong role in the definition of modern AI techniques, although it does feature used in Lovasz's definition of 'Local Search' and Kurzweil is <a href=\"http://www.kurzweilai.net/a-formula-for-intelligence-the-recursive-paradigm\">certainly an advocate</a>. </p>\n\n<p>Recursion can be seen as an elegant 'architectural factorization' - building complexity by combining the results of smaller, similar patterns previously encountered. Computationally, <a href=\"http://stackoverflow.com/questions/931762/can-every-recursion-be-converted-into-iteration\">recursion can always be converted into iteration</a> so this form of elegance is really mainly of use in helping to make designs more comprehensible.</p>\n\n<p>GOFAI algorithms that were traditionally defined using recursion include depth- and breath- first search and means-ends analysis (used in Newell and Simon's General Problem Solver).</p>\n\n<p>With respect to performance, while many functions can be very economically <em>defined</em> using recursion, the naive version of such definitions can be inefficient.</p>\n\n<p><a href=\"http://rayhightower.com/blog/2014/04/12/recursion-and-memoization/\">This page gives</a> an example in which recursive version of the Fibbonnacci function, which has asymptotic execution time n^1.6, which is reduced to n by the use of memoization.</p>\n", "question": "<p>Is there any methods by which artificial intelligence use recursion(s) to solve a certain issue or to keep up working and calculating?</p>\n"}, "id": "1439"}, {"body": {"answer": "<p>Douglas Hofstadter's <a href=\"http://cognitrn.psych.indiana.edu/rgoldsto/courses/concepts/copycat.pdf\">CopyCat</a> architecture for solving letter-string analogy problems was deliberately engineered to maintain a semantically-informed notion of 'salience', i.e. given a variety of competing possibilities, tend to maintain interest in the one that is most compelling. Although the salience value of (part of) a solution is ultimately represented numerically, the means by which it determined is broadly intended to correspond (at least functionally) to the way 'selective attention' might operate in human cognition.</p>\n", "question": "<p>We, humans, during following multiple processes (e.g. reading while listening to music) memorize information from less focused sources with worse efficiency than we do from our main concentration.</p>\n\n<p>Do such things exist in case of artificial intelligences? I doubt, for example that neural networks obtain such features, but I may be wrong.</p>\n"}, "id": "1440"}, {"body": {"answer": "<p>One way of illustrating the deficiencies of many of our current approaches <em>at once</em> is to consider how well it is possible to represent (equivalently, learn) commonsense knowledge. </p>\n\n<p>In this area, the Winograd Schema Challenge has been proposed by <a href=\"http://www.cs.toronto.edu/~hector/Papers/winograd.pdf\" rel=\"nofollow\">Levesque</a>, in which each problem is given as input natural language text containing an ambiguous pronoun:</p>\n\n<blockquote>\n  <p>Babar wonders how he can get new clothing. Luckily, a very rich old man who has always been fond of little elephants understands right away that he is longing for a fine suit.</p>\n</blockquote>\n\n<p>Here, the program is asked to decide if 'he' in \"he is longing for a fine suit\" refers to Babar or the old man. Several thousand such questions <a href=\"http://commonsensereasoning.org/winograd.html\" rel=\"nofollow\">have been collated</a> and proposed as a more quantifiable alternative to the Turing test.</p>\n\n<p>Despite the fact that the input domain is natural language, success here is undeniably a pre-requisite for AGI and (as implied in my answer <a href=\"http://ai.stackexchange.com/questions/1376/is-it-ethical-to-implement-self-defence-for-street-walking-ai-robots\">here</a>) for being able to interact ethically with the human world.</p>\n", "question": "<p><a href=\"https://www.whitehouse.gov/webform/rfi-preparing-future-artificial-intelligence\" rel=\"nofollow\">White House published the information</a> about AI which requests mentions about 'the most important research gaps in AI that must be addressed to advance this field and benefit the public'.</p>\n\n<p>What are these exactly?</p>\n"}, "id": "1441"}, {"body": {"answer": "<p>There does not appear to be an historicial consensus on this. </p>\n\n<p>The <a href=\"https://en.wikipedia.org/wiki/Perceptrons_(book)\" rel=\"nofollow\">Wikipedia page on the Perceptrons book</a> (which does not come down on either side) gives an argument that the ability of MLPs to compute any Boolean function was widely known at the time (at the very least to McCulloch and Pitts).</p>\n\n<p>However, <a href=\"http://harveycohen.net/image/perceptron.html\" rel=\"nofollow\">this page</a> gives an account by someone present at the MIT AI lab in 1974, claiming that this was not common knowledge there, alluding to documentation in \"Artificial Intelligence Progress Report: Research at the Laboratory in Vision, Language, and other problems of Intelligence\" (p31-32) which is claimed to support this.</p>\n", "question": "<p>In their famous book entitled \"<em>Perceptrons: An Introduction to Computational Geometry</em>\", Minsky and Papert show that a perceptron can't solve the XOR problem. This contributed to the first AI winter, resulting in funding cuts for neural networks. However, now we know that a multilayer perceptron can solve the XOR problem easily.</p>\n\n<p>Backprop wasn't known at the time, but did they know about manually building multilayer perceptrons? Did Minsky &amp; Papert know that multilayer perceptrons could solve XOR at the time they wrote the book, albeit not knowing how to train it?</p>\n"}, "id": "1442"}, {"body": {"answer": "<p>There has been quite a few approaches to achieve such kind of distributed coordination. I present here one of them, for its generality and simplicity (that makes it easy to remember too). But first, the general idea behind these approaches is pretty interesting, around a mechanism called <a href=\"https://en.wikipedia.org/wiki/Stigmergy\">stigmergy</a>.</p>\n\n<p>Stigmergy is a behaviour coordination mechanism mediated by the environment. It was first described for termites, and the most famous example pertains to ants. Ants form trails when going out for food, but they often do not interact <em>directly</em>. It turns out they leave pheromones on the ground as they walk away from their hills. The pheromone allows them to find their way home, and it also guides where they are going: If they find a pheromone trace from one of their peers, they follow it and their own pheromones <em>add up</em>, reinforcing the signal of the trail. In stage, more and more ants get \"together as they move\", forming a trail. Coordination has been achieved.</p>\n\n<p>Among the various implementation derived from stigmergy, there is the \"field-based motion coordination model\" (FBMCM). The idea is to create a (maybe virtual) environment that maintains some states of the world. Each object registers in the environment a signal that is maximum at the object position (its edges) and then decreases with distance. Moving objects (e.g. robots) each emit a signal relayed by the environment. They can then sense each other's field and act accordingly: E.g., when signals are strong, move away; when weak, it is safe to get closer, etc. Several complex group moves have been demonstrated in software simulators (platoon formation in games, drill simulations) and with robots. The benefit of this approach is that it can be cheap to compute even complex behaviours. For example, avoiding clashes requires simple \"logic\" code based on summing-up nearby fields value. FBMCM is pretty slick, used in video-games, but hard to implement in physical settings (to my <em>dating</em> knowledge), as it can be challenging to build a reliable environment. See for example the work from <a href=\"https://www.researchgate.net/publication/2949751_Field-based_Motion_Coordination_In_Quake_3_Arena\">Mamei and Zambonelli</a>, as well as one of the first industrial implementations for robots by <a href=\"https://www.researchgate.net/publication/221456459_Decentralized_control_of_E%27GV_transportation_systems\">Weyns et al.</a>. Note that the implementation for robots required significant work on the environment infrastructure, made somewhat more feasible as it was a controlled warehouse.</p>\n\n<p>The advantage of stigmergy-like models is that they are often <em>simple</em> and <em>resilient</em>: You can lose an ant without impact on the food-finding trail. On the downside, these models are usually <em>slow</em>, as the coordination takes time to emerge from indirect interactions. This can be improved upon by adding extra direct interactions (e.g. empowering ants with a GPS and a grocery store map, or just a magnetic-North sense).</p>\n\n<p>In practice, these models can collapse if the environment implementation is not reliable. It can be difficult for robots or, say, self-driving cars, if they expect some transponders put on their way, as these devices are expensive to set and maintain, and they can be broken or stolen. It would be better to endow robots with radars, sonars or other proximity sensors to implement stigmergic models. One related example is the decision by Tesla to add radars to its cars, instead of assuming reliable transponders on the road (<em>note</em>: This is just a parallel; there is no official relation).</p>\n\n<p>Other implementations and related models are, for example, tuple-based coordination languages such as <a href=\"https://en.wikipedia.org/wiki/Linda_(coordination_language)\">Linda</a>, and network protocols like <a href=\"https://pdos.csail.mit.edu/papers/chord:sigcomm01/chord_sigcomm.pdf\">Chord</a>. As you see, these works are not necessarily in the \"AI domain\".</p>\n", "question": "<p>How can a swarm of small robots (like Kilobots) walking close to each other achieve collaboration without bumping into each other? For example, one study shows <a href=\"http://science.sciencemag.org/content/345/6198/795.abstract\" rel=\"nofollow\">programmable self-assembly in a thousand-robot swarm</a> (see <a href=\"http://robohub.org/thousand-robot-swarm-self-assembles-into-arbitrary-shapes/\" rel=\"nofollow\">article</a> &amp; <a href=\"https://vimeo.com/103329200\" rel=\"nofollow\">video</a>) which are moving without GPS-like system and by measuring distances to neighbours. This was achieved, because the robots were very slow.</p>\n\n<p>Is there any way that similar robots can achieve much more efficient and quicker assembly by using more complex techniques of coordination? Not by walking around clock-wise (which I guess was the easiest way), but I mean using some more sophisticated way. Because waiting half a day (~11h) to create a simple star shape using a thousand-robot swarm is way too long!</p>\n"}, "id": "1443"}, {"body": {"answer": "<p>Concentration, perhaps easier to grasp as \"focus\" or \"attention\", has quite some history in AI. user217281728 mentions CopyCat, and there was work with neural networks in the 80s as well (e.g. from <a href=\"http://link.springer.com/article/10.1007%2FBF00363973\" rel=\"nofollow\">Fukushima</a>, creator of the Neocognitron).</p>\n\n<p>More recently, <em>attention</em> in neural networks is <a href=\"http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/\" rel=\"nofollow\">gaining momentum</a>. The mechanisms are applied to learning in deep neural networks.</p>\n", "question": "<p>We, humans, during following multiple processes (e.g. reading while listening to music) memorize information from less focused sources with worse efficiency than we do from our main concentration.</p>\n\n<p>Do such things exist in case of artificial intelligences? I doubt, for example that neural networks obtain such features, but I may be wrong.</p>\n"}, "id": "1444"}, {"body": {"answer": "<p>Yes - it would seem that it is now possible to achieve more is required from the example you've given this paper describes a DL solution to a considerably harder problem - <a href=\"http://arxiv.org/pdf/1510.07211.pdf\" rel=\"nofollow\">generating the source code for a program described in natural language</a>.</p>\n\n<p>Both of these can be described as regression problems (i.e. the goal is to minimize some loss function on the validation set), but the search space in the natural language case is much bigger.</p>\n", "question": "<p>I'm aware that neural networks are probably not designed to do that, however asking hypothetically, is it possible to train the deep neural network (or similar) to solve math equations?</p>\n\n<p>So given the 3 inputs: 1st number, operator sign represented by the number (1 - <code>+</code>, 2 - <code>-</code>, 3 - <code>/</code>, 4 - <code>*</code>, and so on), and the 2nd number, then after training the network should give me the valid results.</p>\n\n<p>Example 1 (<code>2+2</code>):</p>\n\n<ul>\n<li>Input 1: <code>2</code>; Input 2: <code>1</code> (<code>+</code>); Input: <code>2</code>; Expected output: <code>4</code></li>\n<li>Input 1: <code>10</code>; Input 2: <code>2</code> (<code>-</code>); Input: <code>10</code>; Expected output: <code>0</code></li>\n<li>Input 1: <code>5</code>; Input 2: <code>4</code> (<code>*</code>); Input: <code>5</code>; Expected output: <code>25</code></li>\n<li>and so</li>\n</ul>\n\n<p>The above can be extended to more sophisticated examples.</p>\n\n<p>Is that possible? If so, what kind of network can learn/achieve that?</p>\n"}, "id": "1445"}, {"body": {"answer": "<p>One of the comments suggests a PhD in machine learning. As a full-time AI researcher myself, I'd say that would certainly be one useful option.</p>\n\n<p>However, in order <a href=\"http://ai.stackexchange.com/questions/1420/how-close-are-we-to-creating-ex-machina\">make much-needed progress</a>, AI needs avoid falling into the trap of thinking that currently fashionable methods are any kind of 'silver bullet'. There's some danger that a PhD that heads straight into (say) some sub-sub-sub area of DL would end up imposing too much bias on the student's subsequent perspective.</p>\n\n<p>AI research is an essentially multi-disciplinary activity. Other possible backgrounds therefore include:</p>\n\n<ul>\n<li><p>Mathematics or physics (to first degree or PhD level). A strong background in either of these never did anyone any harm. People who are competent in these fields tend to be able to turn their abilities to new domains relatively easily. </p></li>\n<li><p>Software Engineering. One of the things that AI needs are integrative architectures for knowledge engineering. <a href=\"http://www.cogsys.wiai.uni-bamberg.de/teaching/materials/AI-Light-Bulbb.htm\">Here's why</a>. I believe that one of the reasons that we haven't yet managed to do <a href=\"http://ai.stackexchange.com/questions/1396/why-ocr-cannot-be-perceived-as-good-example-of-ai\">OCR at the level of a 5 year old</a> is that we've yet to accept that we have to 'build a sledgehammer to crack a nut'. Software architects are used to managing large-scale complexity, so they may be able to help.</p></li>\n<li><p>Cognitive Science, Psychology, Cognitive Linguistics. The reasons here are obvious.</p></li>\n</ul>\n\n<p>Above all, I personally think that a good AI researcher should be creative, inquisitive and prepared to question received wisdom, all of which are more important in practice than specifics of their background.</p>\n", "question": "<p>Suppose my goal is to collaborate and create an advanced AI, for instance one that resembles a human being and the project would be on the frontier of AI research, what kind of skills would I need?</p>\n\n<p>I am talking about specific things like what university program should I complete to enter and be competent in the field. Here are some of the things that I thought about, just to exemplify what I mean:</p>\n\n<ul>\n<li>Computer sciences: obviously the AI is built on computers, it wouldn't hurt to know how computers work, but some low level stuff and machine specific things does not seem essential, I may be wrong of course.</li>\n<li>Psychology: if AI resembles human beings, knowledge of human cognition would probably be useful, although I do not imagine neurology on a cellular level or complicated psychological quirks typical to human beings like the Oedipus complex would be relevant, but again, I may be wrong.</li>\n</ul>\n"}, "id": "1447"}, {"body": {"answer": "<p>Research on AI seems to be getting wider these days (2016). First, \"obvious\" few departments (no order):</p>\n\n<ul>\n<li><a href=\"https://en.wikipedia.org/wiki/Computer_science\" rel=\"nofollow\">Computer Science</a> (e.g. computation theory, algorithms): AI researchers there assume that intelligence is a kind of computation, under various forms (e.g. a neural network, a logic system).</li>\n<li><a href=\"https://en.wikipedia.org/wiki/Software_engineering\" rel=\"nofollow\">Software Engineering</a>: Assuming we find a good model for AI, how do you make it? This is what the engineer will want to figure out. And it can be hard to map mathematical models to an engineered piece.</li>\n<li>Statistics and Probabilities (more specific than just Mathematics, which is also close to Computer Science): This is about Data Science, notably as a foundation to Machine Learning, the most active branch in AI---which \"just\" covers the <em>learning</em> part.</li>\n<li>Physics: This is particularly relevant now for hardware (see below).</li>\n<li>Neuro Science: Understand how the brain works, as an inspiration to create an artificial one, is the home for Connectionists. Recently, Hassabis and his team at Google Deepmind made several breakthroughs related to reinforcement learning, memory, attention, etc.</li>\n</ul>\n\n<p>Recently Electric Engineering is getting a lot of light, together with the related branches of Physics. Several public and private laboratories focus on \"brain chips\". To name a few: IBM (who's working on that for some time already), Nvidia, and Facebook. Circa 2010, it became clear that techniques like deep learning require horsepower, thus an increasing focus on creating more powerful, smaller, more energy efficient chips. And on top of that, there is all the work in Quantum Computing.</p>\n\n<p>But the thing is, there seems to be many more fields that are getting involved in AI research. We should mention Chemistry and Biology, as both inspiration and tools to make new models or hardware (e.g. chips that do not use silicon, so they can get smaller).</p>\n\n<p>As for 2016, the above fields are the most active, and promise to remain very active for quite some time. Pick your own depending on your interest, skills, or mere intuition!</p>\n\n<p>To finish, we may be surprised in a few years when we look back at where AI has come from. I believe that if we manage to build an AGI, it will leverage <em>all</em> these fields anyway. I guess the thrill is to be part of the story.</p>\n", "question": "<p>Suppose my goal is to collaborate and create an advanced AI, for instance one that resembles a human being and the project would be on the frontier of AI research, what kind of skills would I need?</p>\n\n<p>I am talking about specific things like what university program should I complete to enter and be competent in the field. Here are some of the things that I thought about, just to exemplify what I mean:</p>\n\n<ul>\n<li>Computer sciences: obviously the AI is built on computers, it wouldn't hurt to know how computers work, but some low level stuff and machine specific things does not seem essential, I may be wrong of course.</li>\n<li>Psychology: if AI resembles human beings, knowledge of human cognition would probably be useful, although I do not imagine neurology on a cellular level or complicated psychological quirks typical to human beings like the Oedipus complex would be relevant, but again, I may be wrong.</li>\n</ul>\n"}, "id": "1448"}, {"body": {"answer": "<p>Convolutional neural network can be applied not only for image recognition, but also for video analysis and recognition, natural language processing, natural language processing, in games (e.g. <a href=\"https://en.wikipedia.org/wiki/Computer_Go#New_approaches_to_problems\" rel=\"nofollow\">Go</a>) or even for <a href=\"http://ai.stackexchange.com/a/1424/8\">drug discovery</a> by predicting the interaction between molecules and biological proteins<sup><a href=\"https://en.wikipedia.org/wiki/Convolutional_neural_network#Applications\" rel=\"nofollow\">wiki</a></sup>.</p>\n\n<p>Therefore it can be used for variety of problems by using convolutional and subsampling layers connected to more fully connected layers. They're easier to train, because have fewer parameters than fully connected networks with the same number of hidden units.<sup><a href=\"http://ufldl.stanford.edu/tutorial/supervised/ConvolutionalNeuralNetwork/\" rel=\"nofollow\">UFLDL</a></sup></p>\n", "question": "<p>Can a Convolutional Neural Network be used for pattern recognition in a problem domain where there are no pre-existing images, say by representing abstract data graphically? Would that always be less efficient?</p>\n\n<p><a href=\"https://youtu.be/py5byOOHZM8?t=815\">This developer</a> says current development could go further but not if there's a limit outside image recognition. </p>\n"}, "id": "1449"}, {"body": {"answer": "<p>Yes. Here are some of the most prominent ones and their respective state-of-the-art errors:</p>\n\n<ul>\n<li><a href=\"https://www.cs.toronto.edu/~kriz/cifar.html\" rel=\"nofollow\">CIFAR-10</a>: ~3.5% error</li>\n<li><a href=\"https://www.cs.toronto.edu/~kriz/cifar.html\" rel=\"nofollow\">CIFAR-100</a>: ~24% error  </li>\n<li><a href=\"https://cs.stanford.edu/~acoates/stl10/\" rel=\"nofollow\">STL-10</a>: ~26% error</li>\n<li><a href=\"http://ufldl.stanford.edu/housenumbers/\" rel=\"nofollow\">SVHN</a>: ~1.7% error</li>\n<li><a href=\"http://www.image-net.org/\" rel=\"nofollow\">ImageNet</a> tasks: the best 2012 classification task solution got 15% top-5 error, better results are currently available</li>\n</ul>\n\n<p>You can check an updated list of solutions <a href=\"http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html\" rel=\"nofollow\">here</a>. Also, a more comprehensive list of modern datasets can be found <a href=\"http://deeplearning.net/datasets/\" rel=\"nofollow\">here</a>.</p>\n", "question": "<p>For example there is <a href=\"https://en.wikipedia.org/wiki/MNIST_database\" rel=\"nofollow\">the MNIST database</a> which is used to test artificial neural network (ANN), however it's not so challenging, because some hierarchical systems of convolutional neural networks manages to get an error rate of 0.23 percent.</p>\n\n<p>Are there any similar, especially the most challenging tasks with dataset which are used as benchmark tests to challenge the AI which are fairly reliable and it's possible to pass, but most AAN are struggling to achieve the lower error rate?</p>\n"}, "id": "1450"}, {"body": {"answer": "<p>Yes, although how useful this AI can be is another question entirely.</p>\n\n<p><strong>mpgac</strong> is a \"minimally intelligent AGI\" trained on the GAC-80K corpus of MIST questions. As a result, it should be able to \"minimally\" pass this test. However, being trained on the GAC-80K corpus obviously make it lacking for any practical purposes. From the README:</p>\n\n<blockquote>\n  <p>Obviously this should only be capable of producing a minimally intelligent signal when ordinary commonsense questions are asked, of the kind depicted above, using questions which would have made sense to an average human between the years 2000 and 2005. On expert knowledge or current affairs related questions it should perform no better than chance.</p>\n</blockquote>\n\n<p>The point of mpgac is to compare it to other AIs that could be built to pass this test. Or as the writer wrote in the README:</p>\n\n<blockquote>\n  <p>When scanning the skies how can we tell whether the radio signals detected are from an intelligent source, or are merely just background or sensor noise?</p>\n</blockquote>\n\n<p>Ideally, you would want to build a program that is \"better\" than mpgac. In much the same way as ELIZA can be seen as a baseline for the Turing Test, mpgac is the baseline for the MIST test.</p>\n\n<p>The GitHub repo of mpgac (as well as the GAC-80K corpus) is available <a href=\"https://github.com/bashrc/mindpix\">here</a>.</p>\n", "question": "<p><a href=\"https://en.wikipedia.org/wiki/Minimum_intelligent_signal_test\" rel=\"nofollow\">MIST</a> is a quantiative test of humanness, consisting of ~80k propositions such as:</p>\n\n<ul>\n<li>Is Earth a planet?</li>\n<li>Is the sun bigger than my foot?</li>\n<li>Do people sometimes lie?</li>\n<li>etc.</li>\n</ul>\n\n<p>Have any AI attempted and passed this test to date?</p>\n"}, "id": "1452"}, {"body": {"answer": "<p>No, quantum computers (as understood by mainstream scientists) cannot solve the halting problem. We can already <a href=\"http://algorithmicassertions.com/2016/05/22/quirk.html\" rel=\"nofollow\">simulate quantum circuits with normal computers</a>; it just takes a really long time when you get a decent number of qubits involved. (Quantum computing provides exponential speedups for some problems.) Therefore, if quantum computers could solve the halting problem, we could solve the halting problem with classical computers by simulating a quantum one, but it's impossible to solve the halting problem with classical computers, so we can't do it with quantum ones either.</p>\n\n<p>There are proponents of hypercomputation - infinite speedups using quantum computers - but the evidence put forward so far is mostly conjecture. Further reading: <a href=\"https://arxiv.org/ftp/quant-ph/papers/0512/0512248.pdf\" rel=\"nofollow\">Can quantum computing solve classically\nunsolvable problems?</a> (PDF), <a href=\"http://cs.stackexchange.com/q/6296\">References on comparison between quantum computers and Turing machines</a> (at CS.SE).</p>\n\n<p>Solving the halting problem would make a computer exceptionally powerful. It would conceivably be able to <a href=\"https://blogs.msdn.microsoft.com/ericlippert/2011/02/24/never-say-never-part-two/\" rel=\"nofollow\">check whether complex theorems are true</a> without necessarily needing to product a mathematical proof. Solving that problem isn't necessary for strong AI, though. Going with the definition of \"strong AI\" as \"where the machine's intellectual capability is functionally equal to a human's\" (<a href=\"https://www.ocf.berkeley.edu/~arihuang/academic/research/strongai3.html\" rel=\"nofollow\">source</a>), a computer could be able to learn like a human despite not being able to look at a program and see if it halts. I can't magically determine the halting properties of any arbitrary program, yet I'd like to think I'm an intelligent being.</p>\n", "question": "<p>Have there been proposed extensions to go beyond a Turing machine that solve the halting problem and if so, would those proposed extensions have value to advance strong Artificial Intelligence?  For example, does quantum computing go beyond the definition of a Turing machine and resolve the halting problem, and does that help in creating strong AI?</p>\n"}, "id": "1454"}, {"body": {"answer": "<p>We are getting there, with as usual some trade-off between quality and speed.</p>\n\n<p>For example <a href=\"http://cs231n.stanford.edu/slides/winter1516_lecture8.pdf\" rel=\"nofollow\">Spatial Localization and Detection lecture</a> shows some benchmarks (mAP = Mean Average Precision, higher is better; FPS = frame per second):</p>\n\n<p><a href=\"http://i.stack.imgur.com/AfHt2m.png\" rel=\"nofollow\"><img src=\"http://i.stack.imgur.com/AfHt2m.png\" alt=\"Table/Performance: Real-Time Detectors\"></a></p>\n\n<p><a href=\"http://i.stack.imgur.com/B5Qb9m.png\" rel=\"nofollow\"><img src=\"http://i.stack.imgur.com/B5Qb9m.png\" alt=\"Table/Performance: R-CNN, Fast R-CNN, Faster R-CNN\"></a></p>\n", "question": "<p>Convolutional neural network are leading type of feed-forward artificial neural network for image recognition. Can they be used for real-time image recognition for videos (frame by frame), or it takes too much processing (assuming they're written in C-like language)?</p>\n\n<p>For example for classification of type of animals based on the training from huge dataset.</p>\n"}, "id": "1456"}, {"body": {"answer": "<p>In AI (but in general too, I believe), a simplification is that modeling is more akin to Mathematics (and related hard sciences involved, like Physics and... Computer Science), and implementation to Software Engineering.</p>\n\n<p>Let's take a concrete example, really outside of AI: Find the minimum value of a given polynomial, if it exists.</p>\n\n<p>The Mathematician will derivate the polynomial, find the zeros, and checkout convexity to find a minimum (if there is any zero). This procedure is very standard---some will say straightforward. It relies on a body of knowledge and an abstraction level that is appropriate for manual proof.</p>\n\n<p>The Software Engineer approach is actually way longer to explain, and I am going to skip it. The point is that the body of knowledge is related but different: We have to find now a step-by-step procedure for the computer to achieve the result. The Mathematician one could be implemented directly in MathLab, almost verbatim, but we <em>assume</em> MathLab. And to build MathLab, we are back to the problem of making a procedure the computer can execute. We could for example base a procedure on Euler's method to find roots (a \"simple\" approach that closes on roots step after step), etc.</p>\n\n<p>Simple mathematical operations can be quite complex to implement on a computer. Perhaps the most famous is <a href=\"https://en.wikipedia.org/wiki/Random_number_generation\" rel=\"nofollow\">random-number generation</a>. Mathematically, the concept is pure and clear. Generating an actual random-number is more elusive than it looks, to the point it calls for new <em>models</em> and new <em>implementations</em>...</p>\n\n<p>A concrete example from history: Neural networks. In the 80s and 90s, NNs were weighted graphs that could be executed on computers using graph libraries or similar foundation libs. Choosing the weights was challenging. One day the back-propagation learning model was introduced to automated the choice of weights. The model relied on a procedure dedicated to NNs, using a terminology like partial derivates, gradient descent, chain rules, etc. And later then, clever engineers created libraries to automate the back-propagation procedure. The libraries can be somewhat far from the original model, as engineers learn how to make it computable, even faster (i.e. optimization, approximations/truncations).</p>\n", "question": "<p>Quote from this <a href=\"http://meta.ai.stackexchange.com/a/46/8\">Eric's meta post</a> about modelling and implementation:</p>\n\n<blockquote>\n  <p>They are not exactly the same, although strongly related. This was a very difficult lesson to learn among mathematicians and early programmers, notably in the 70s (mathematical proofs can demand a lot of non-trivial programming work to make them \"computable\", as in runnable on a computer).</p>\n</blockquote>\n\n<p>If they're not the same, what is the difference?</p>\n\n<p>How we can say when we're talking about AI implementation, and when about modelling? It's suggested above it's not easy task. So where we can draw the line when we talk about it?</p>\n\n<p>I'm asking in general, not specifically for this site, that's why I haven't posted question in meta</p>\n"}, "id": "1457"}, {"body": {"answer": "<p>I did a little search and couldn't find any database that has ground truth for aggressiveness. This means that you need to build yourself a database. This might be huge undertaking. Take thousands of messages, and classify them by hand whether they are aggressive or not. This part is quite labor intensive.</p>\n\n<p>Second part is much easier at start but would be pain to optimize (both performance and computational cost). I would suggest you to start with Naive Bayes classifier for this job. That is the preferred classifier for spam detection. ANN would probably not work for this case because the data would be a huge sparse vector. Estimated number of words in English is over a million, which means the input layer of your ANN should be able to scale up to that number. Search for sparse vector classification for additional classifier that can be used in these cases.</p>\n", "question": "<p>Just for the purpose of learning I'd like to classify the likeliness of a tweet being in aggressive language or not. </p>\n\n<p>I was wondering how to approach the problem. I guess I need first train my neural network on a huge dataset of text what aggressive language is. This brings up the question where I would get this data in the first place?</p>\n\n<p>It feels a bit like the chicken and egg problem to me so I wonder how would I approach the problem?</p>\n"}, "id": "1459"}, {"body": {"answer": "<p>The answer by <a href=\"http://ai.stackexchange.com/users/210/cem-kalyoncu\">Cem Kalyoncu</a> mentions the difficulty of building a ground truth database for aggressiveness.</p>\n\n<p>One alternative approach would be to attempt to operate at the <em>concept level</em>, which would allow the use of pre-existing ontologies such as ConceptNet.</p>\n\n<p><a href=\"http://www.cs.stir.ac.uk/~spo/publication/resources/sentiment-analysis.pdf\" rel=\"nofollow\">Here's a paper</a> that describes this technique.</p>\n", "question": "<p>Just for the purpose of learning I'd like to classify the likeliness of a tweet being in aggressive language or not. </p>\n\n<p>I was wondering how to approach the problem. I guess I need first train my neural network on a huge dataset of text what aggressive language is. This brings up the question where I would get this data in the first place?</p>\n\n<p>It feels a bit like the chicken and egg problem to me so I wonder how would I approach the problem?</p>\n"}, "id": "1460"}, {"body": {"answer": "<p>I would classify both as having / using elements of AI, yes.  But I wouldn't say either represents a truly \"intelligent\" (in the AGI sense) program.</p>\n\n<p>But here's the rub... as you'll see in other questions asking about definitions of AI, there's a sort of memetic thing where anything that AI begins to do successfully, immediately stops being considered \"AI\".  So AI is always an unreachable state, because it's always \"something humans can do that computers can't\" and once the computer <em>can</em> do it, it isn't AI anymore.   So take that into consideration.</p>\n", "question": "<p>Siri and Cortana communicate pretty much like humans. Unlike Google now which mainly gives us search results when asked some questions (not setting alarms or reminders), Siri and Cortana provide us with an answer, in the same way that a person would do.<br>\nSo are they actual AI programs or not?</p>\n\n<p>(By \"question\" I don't mean any academic related question or asking routes/ temperature, but rather opinion based question). </p>\n"}, "id": "1463"}, {"body": {"answer": "<p>One:  we don't really know what intelligence is.</p>\n\n<p>Two: we don't truly understand the best model of intelligence we have available (human intelligence) works.</p>\n\n<p>Three: we're trying to replicate human intelligence (to some extent) on hardware which is quite different from the hardware it runs on in reality.</p>\n\n<p>Four: the human brain (our best model of intelligence) is mostly a black-box to us, and it's difficult to probe/introspect it's operation without killing the test subject.  This is, of course, unethical and illegal.  So progress in understanding the brain is very slow. </p>\n\n<p>Combine those factors and you can understand why it's difficult to make progress in AI.  In many ways, you can argue that we're shooting in the dark.   Of course we have made <em>some</em> progress, so we know we're getting some things right.  But without a real comprehensive theory about <em>how</em> AI should/will work, we are reduced to a lot of trial and error and iteration to move forward.</p>\n", "question": "<p>I have a background in Computer Engineering and have been working on developing better algorithms to mimic human thought. (One of my favorites is Analogical Modeling as applied to language processing and decision making.) However, the more I research, the more I realize just <em>how</em> complicated AI is.</p>\n\n<p>I have tried to tackle many problems in this field, but sometimes I find that I am reinventing the wheel or am trying to solve a problem that has already been proven to be unsolvable (ie. the halting problem). So, to help in furthering AI, I want to better understand the current obstacles that are hindering our progress in this field.</p>\n\n<p>For example, time and space complexity of some machine learning algorithms is super-polynomial which means that even with fast computers, it can take a while for the program to complete. Even still, some algorithms may be fast on a desktop or other computer while dealing with a small data set, but when increasing the size of the data, the algorithm becomes intractable.</p>\n\n<p>What are other issues currently facing AI development?</p>\n"}, "id": "1464"}, {"body": {"answer": "<p>Although there are several definitions of \"robot\", an essential feature of everything called \"robot\" is that it is capable of movement. This does not necessarily mean <em>displacement</em>; a robot arm in a factory also moves.</p>\n\n<p>There is a single exception to this rule,  which is bot-programs like chatbots; I will discuss them later.</p>\n\n<p>Artificial Intelligence does not need to move; a chess program can be argued to be an AI, but does not move. </p>\n\n<p>A robot can actually have AI; one of the definitions of robot is that it is a system, capable of <em>autonomous</em> movement. In order to be autonomous, to be able to make decisions of its own, a certain amount of AI may be necessary. </p>\n\n<p>There is one class of \"robots\" that does not move, and does not even have physical presence; bot programs, like chatbots, that operate inside systems. I do not consider them robots, because they are not physical devices operating in the real world. A chatbot can be an AI, however - a good chatbot may have some natural language processing to interact with humans in a way that humans find natural.</p>\n\n<p>To summarize; an AI can exist purely in software. But to be a robot, there must be a moving physical component in the real world.</p>\n", "question": "<p>The question is pretty much the title.</p>\n\n<p>Basically what is the difference between AI and robots?</p>\n"}, "id": "1465"}, {"body": {"answer": "<p>IBM clearly don't provide all the details / \"secret sauce\" but there is some information out there on how Watson works.  Some of the text search / retrieval stuff uses a technology called <a href=\"http://uima.apache.org/\" rel=\"nofollow\">UIMA</a> which IBM open-sourced a few years ago.  It also uses Prolog and some custom C++ code.    Some more information can be found <a href=\"http://learning.acm.org/webinar/lally.cfm\" rel=\"nofollow\">here</a>.</p>\n", "question": "<p>Is there any simple explanation how <em>Watson</em> finds and scores evidence after gathering massive evidence and analyzing the data?</p>\n\n<p>In other words, how does it know which precise answer it needs to return?</p>\n"}, "id": "1466"}, {"body": {"answer": "<p>In the broadest sense, the difference is that non-robotic A(G)I <em>may</em> not be possible because, as per <a href=\"http://ai.stackexchange.com/questions/1415/what-kind-of-body-if-any-does-intelligence-require\">this question</a>, it could be that \"Intelligence requires a body\".</p>\n\n<p>More specifically, it could be that there are limitations to what the traditional (well, 1950s style) 'Brain in a vat' notion of an AI is capable of comprehending, in the absence of experience of embodied experience such as force, motion and \"the raw, unawshed world\".</p>\n", "question": "<p>The question is pretty much the title.</p>\n\n<p>Basically what is the difference between AI and robots?</p>\n"}, "id": "1467"}, {"body": {"answer": "<p>A simple way to do it would be lexicograpical sentiment analysis.  To do that, you'd need a list of words categorized with a score that reflects \"friendly\" vs \"aggressive\" sentiment.   For an example of setting up a SA system using Spark, see <a href=\"http://mammothdata.com/sentiment-analysis-on-enrons-emails-with-apache-spark/\" rel=\"nofollow\">this article</a>.  To do what you're talking about, substitute AFINN for a different dataset.  You might have to create said dataset yourself, if there isn't one \"out there\" like you want.</p>\n\n<p>Note that this isn't the most sophisticated technique in the world, but it's been found to be surprisingly effective. </p>\n", "question": "<p>Just for the purpose of learning I'd like to classify the likeliness of a tweet being in aggressive language or not. </p>\n\n<p>I was wondering how to approach the problem. I guess I need first train my neural network on a huge dataset of text what aggressive language is. This brings up the question where I would get this data in the first place?</p>\n\n<p>It feels a bit like the chicken and egg problem to me so I wonder how would I approach the problem?</p>\n"}, "id": "1469"}, {"body": {"answer": "<p>One thing you'll see quite often, is to declare a correspondence between a system and a human of a given age.  For example \"this program can answer questions about science approximately as well as an average 7 year old\" or something of that nature.  </p>\n", "question": "<p>We can measure the power of the machine with the number of operation per second or the frequency of the processor. But does units similar of IQ for humans exist for a AI?<br/>\nI'm asking for a unit which can give countable result so something different from a Turing Test which only give a binary result.</p>\n"}, "id": "1470"}, {"body": {"answer": "<p>I am assuming by AI you mean AG(eneral)I, not machine learning or expert systems tuned for specific tasks.</p>\n\n<p>In addition to mindcrime's answer, sometimes we run out of samples to train and sometimes computers became so slow to process enough samples to work in manageable timescales. bpachev mentioned memory but on the surface, our supercomputers have more than enough memory to store a human brain matrix. But we lack the ability to simulate it real time. After we are able to do that, we also need to connect external input, even more processing power is required for that. Even that would not be enough to simulate a human brain fully as biochemistry plays an important role. </p>\n\n<p>One final note would be there is little incentive to develop AGI other than understanding how human mind works. There are classification algorithms, expert systems, knowledge engines that can out-perform even the best humans on specific tasks.</p>\n", "question": "<p>I have a background in Computer Engineering and have been working on developing better algorithms to mimic human thought. (One of my favorites is Analogical Modeling as applied to language processing and decision making.) However, the more I research, the more I realize just <em>how</em> complicated AI is.</p>\n\n<p>I have tried to tackle many problems in this field, but sometimes I find that I am reinventing the wheel or am trying to solve a problem that has already been proven to be unsolvable (ie. the halting problem). So, to help in furthering AI, I want to better understand the current obstacles that are hindering our progress in this field.</p>\n\n<p>For example, time and space complexity of some machine learning algorithms is super-polynomial which means that even with fast computers, it can take a while for the program to complete. Even still, some algorithms may be fast on a desktop or other computer while dealing with a small data set, but when increasing the size of the data, the algorithm becomes intractable.</p>\n\n<p>What are other issues currently facing AI development?</p>\n"}, "id": "1471"}, {"body": {"answer": "<p>An <a href=\"http://www.oxforddictionaries.com/definition/english/artificial-intelligence#artificial-intelligence__2\" rel=\"nofollow\" title=\"OED - AI\">AI</a> is a computer program designed for tasks normally requiring human <a href=\"http://www.oxforddictionaries.com/definition/english/intelligence\" rel=\"nofollow\" title=\"OED - Intelligence\">intelligence</a> (a human's ability to learn), while a <a href=\"http://www.oxforddictionaries.com/definition/english/robot\" rel=\"nofollow\" title=\"OED - Robot\">robot</a> is a machine that completes complex tasks. An AI could be used to control a robot, but they are very different.</p>\n\n<p>Source: <a href=\"http://www.oxforddictionaries.com/\" rel=\"nofollow\">Oxford English Dictionary</a>, above links will direct to definitions.</p>\n", "question": "<p>The question is pretty much the title.</p>\n\n<p>Basically what is the difference between AI and robots?</p>\n"}, "id": "1482"}, {"body": {"answer": "<p>Siri and co. are AI to some extent. The usual label is \"Weak AI\" (also called \"narrow\" or \"soft\" AI). It turns out the <a href=\"https://en.wikipedia.org/wiki/Weak_AI\">Wikipedia article on Weak AI</a> explicitly refers to Siri:</p>\n\n<blockquote>\n  <p>Siri is a good example of narrow intelligence. Siri operates within a limited pre-defined range, there is no genuine intelligence, no self-awareness, no life despite being a sophisticated example of weak AI. In Forbes (2011), Ted Greenwald wrote: \"The iPhone/Siri marriage represents the arrival of hybrid AI, combining several narrow AI techniques plus access to massive data in the cloud.\" AI researcher Ben Goertzel, on his blog in 2010, stated Siri was \"VERY narrow and brittle\" evidenced by annoying results if you ask questions outside the limits of the application.</p>\n</blockquote>\n\n<p>Important to note that \"mixing\" Weak AIs does not make a \"stronger\" AI, by some arguments (see Searle's <a href=\"https://en.wikipedia.org/wiki/Chinese_room\">Chinese Room</a> argument), but there is no definitive answer yet in 2016.</p>\n", "question": "<p>Siri and Cortana communicate pretty much like humans. Unlike Google now which mainly gives us search results when asked some questions (not setting alarms or reminders), Siri and Cortana provide us with an answer, in the same way that a person would do.<br>\nSo are they actual AI programs or not?</p>\n\n<p>(By \"question\" I don't mean any academic related question or asking routes/ temperature, but rather opinion based question). </p>\n"}, "id": "1483"}, {"body": {"answer": "<p>It depends on what you mean by \"know what is happening\".</p>\n\n<p>Conceptually, yes: ANN perform nonlinear regression. The actual expression represented by the weight matrix/activation function(s) of an ANN can be explicitly expanded in symbolic form (e.g. containing sub-expressions such as 1/1+e^{1/1+e^{...}}).</p>\n\n<p>However, if by 'know' you mean <em>predicting the output of some specific (black box) ANN</em>, by some other means, then the obstacle is the presence of chaos in a ANN that has <a href=\"http://sprott.physics.wisc.edu/pubs/paper234.pdf\">high degrees of freedom</a>.</p>\n\n<p>EDIT: Here's some relatively recent work by Hod Lipson on understanding ANNs through  <a href=\"http://arxiv.org/pdf/1506.06579.pdf\">visualisation</a>.</p>\n", "question": "<p>Do scientists or research experts know from the kitchen what is happening inside complex \"deep\" neural network with at least millions of connections firing at an instant? Do they understand the process behind this (e.g. what is happening inside and how it works exactly), or it is a subject of debate?</p>\n\n<p>For example this <a href=\"https://www.cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf\">study</a> says:</p>\n\n<blockquote>\n  <p>However there is no clear understanding of <em>why</em> they perform so well, or <em>how</em> they might be improved.</p>\n</blockquote>\n\n<p>So does it mean the scientists actually doesn't know how complex convolutional network models work?</p>\n"}, "id": "1486"}, {"body": {"answer": "<p>Not sure if this is what you are searching for, but google extracted images from networks when they were fed with white noise. See <a href=\"https://research.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html\" rel=\"nofollow\">here</a>.\nThis kind of represents what the network knows.</p>\n", "question": "<p>Do scientists or research experts know from the kitchen what is happening inside complex \"deep\" neural network with at least millions of connections firing at an instant? Do they understand the process behind this (e.g. what is happening inside and how it works exactly), or it is a subject of debate?</p>\n\n<p>For example this <a href=\"https://www.cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf\">study</a> says:</p>\n\n<blockquote>\n  <p>However there is no clear understanding of <em>why</em> they perform so well, or <em>how</em> they might be improved.</p>\n</blockquote>\n\n<p>So does it mean the scientists actually doesn't know how complex convolutional network models work?</p>\n"}, "id": "1489"}, {"body": {"answer": "<p>We know what Lee's strategy was during the game, and it seems like the sort of thing that should work. <a href=\"https://gogameguru.com/lee-sedol-defeats-alphago-masterful-comeback-game-4/\" rel=\"nofollow\">Here's</a> an article explaining it. Short version: yes, we know what went wrong, but probably not how to fix it yet.</p>\n\n<p>Basically, AlphaGo is good at making lots of small decisions well, and managing risk and uncertainty better than humans can. One of the things that's surprising about it relative to previous bots that play Go is how good it was at tactical fights; in previous games, Lee had built a position that AlphaGo needed to attack, and then AlphaGo successfully attacked it.</p>\n\n<p>So in this game, Lee played the reverse strategy. Instead of trying to win many different influence battles, where AlphaGo had already shown it was stronger than him, he would set up one critical battle (incurring minor losses along the way), and then defeat it there, with ripple events that would settle the match in his favor.</p>\n\n<p>So what's the weakness of AlphaGo that allowed that to work? As I understand it, this is a fundamental limitation of Monte Carlo Tree Search (MCTS). MCTS works by randomly sampling game trees and averaging them; if 70% of games from a particular position go well and 30% of games from another position go well, then you should probably play the first move instead of the second move.</p>\n\n<p>But when there's a specific sequence of plays that go well--if, say, W has a path that requires them playing exactly the right stone each time, but B has no possible response to this path--then MCTS breaks down, because you can only find that narrow path through minimax reasoning, and moving from the slower minimax reasoning to the faster MCTS is one of the big reasons why bots are better now than they were in the past.</p>\n\n<p>It's unclear how to get around this. There may be a way to notice this sort of threat, and then temporarily switch from MCTS reasoning to minimax reasoning, or to keep around particular trajectories in memory for consideration in future plays. </p>\n", "question": "<p>We can read on wiki page that in March 2016 AlphaGo AI lost its game (1 of 5) to Lee Sedol, a professional Go player. One <a href=\"http://www.bbc.co.uk/news/technology-36558829\" rel=\"nofollow\">article</a> cite says:</p>\n\n<blockquote>\n  <p>AlphaGo lost a game and we as researchers want to explore that and find out what went wrong. We need to figure out what its weaknesses are and try to improve it.</p>\n</blockquote>\n\n<p>Have researchers already figured it out what went wrong?</p>\n"}, "id": "1495"}, {"body": {"answer": "<p>Facial recognition works by essentially turning your face into a point cloud, recognizing eyes, cheeks, nose, mouth, etc. </p>\n\n<p>Unfortunately it doesn't look at the top of your head (hair is very hard to differentiate from other hair and doesn't have many features). Face paintings   would be your best bet since they can be easily changed, tattoos not so much. Once somebody has a photo of your face with your tattoo on it, you're busted.  Glasses will work if they're opaque and hide your eyes (sunglasses). The facial recognition software does not recognize jewelry, as it's tiny, and very easy to remove and put on. Ideally you want to have your face professionally made up with makeup and fake skin (basically a fake face).</p>\n\n<p>Some challenges can be anything like an unclear picture, a picture from the wrong angle (which can probably be mathematically calculated and restructured), or like you said, face paintings to hide facial features.</p>\n", "question": "<p>Assuming we're dealing with artificial neural network (e.g. using <a href=\"https://en.wikipedia.org/wiki/Convolutional_neural_network\" rel=\"nofollow\">convnets</a>) which was trained by large dataset of human faces.</p>\n\n<p>Are there any known issues or challenges where facial recognition would fail? I'm not talking about covering half of the face, but some simple common things such as wearing the glasses, hat, jewellery, having face painting or tattoo, can this successfully prevent AI from recognizing the face? If so, what are current methods dealing with such challenges?</p>\n"}, "id": "1496"}, {"body": {"answer": "<p>I'd personally be more inclined to try longstanding deterministic methods such as <a href=\"https://en.wikipedia.org/wiki/Damerau%E2%80%93Levenshtein_distance\" rel=\"nofollow\">Damerau</a> (for typing errors) or <a href=\"https://en.wikipedia.org/wiki/Soundex\" rel=\"nofollow\">Soundex</a> (for homonyms arising from transcribed speech). At the very least, I'd use those as a baseline for any more 'AI-based' approach.</p>\n", "question": "<p>I would like to know what kind of dataset I need (to prepare) for training the network to recognize the spelling mistakes in individual words for English text.</p>\n\n<p>Given the large database of words, having correct one for each incorrect. What kind of input is more efficient for that tasks? Is it using one input per each letter, syllable, whole word or I should use different pattern syllable?</p>\n\n<p>Then the input should be incorrect word, output correct, and if the word doesn't need correction, then both input and output should be the same. Is that the right approach?</p>\n"}, "id": "1497"}, {"body": {"answer": "<p>As far as I know, Tesla cars autopilot is not a 100% AI pilot, it's an <em>assitant</em>: as it detects hands off wheel it slows down, so it's incorrect to speak about AI mistake: it is not trained/designed to drive a car all by itself. A human driver is responsible in that incident.</p>\n", "question": "<p>Do we know why Tesla's Autopilot mistaken empty sky with a high-sided lorry which resulted in fatal crash involving a car in self-drive mode? Was it AI fault or something else? Is there any technical explanation behind this why this happened?</p>\n\n<p>References: <a href=\"http://news.sky.com/story/tesla-driver-in-first-self-drive-fatal-crash-10330121\" rel=\"nofollow\">Sky News article</a>, <a href=\"http://www.theverge.com/2016/6/30/12072408/tesla-autopilot-car-crash-death-autonomous-model-s\" rel=\"nofollow\">The Verge</a>.</p>\n"}, "id": "1498"}, {"body": {"answer": "<p><a href=\"http://www.arcadelearningenvironment.org/wp-content/uploads/2012/07/bellemare13arcade.pdf\" rel=\"nofollow\">Here</a> is a description of the input to an ALE agent:\nPercept state: A single game screen (frame): a 2D array of 7-bit pixels, 160 pixels wide by 210 pixels high. \nActions: 18 discrete actions defined by the joystick controller</p>\n\n<p>Regarding VGDL, as far as I can see, the main site associated with it is gvgai.net, which is currently down. The associated API is described <a href=\"http://julian.togelius.com/Perez20152014.pdf\" rel=\"nofollow\">in this paper</a>.</p>\n\n<p>Percept state for GVGAI is more structured than for ALE, but the closest correspondence to ALE appears to be an 'Observation grid', consisting of a 2D array of sprite identifiers.</p>\n\n<p>Actions: ACTION_NIL, ACTION_UP, ACTION_LEFT, ACTION_DOWN, ACTION_RIGHT and ACTION_USE (stated as 'typical' values). </p>\n\n<p>Of the two, it would seem that ALE is more suitable for AGI, because of the more 'free form' nature of the input.</p>\n\n<p>However, one of the issues with <em>either</em> of these approaches is that the set of possible actions is strongly constrained. These domains are therefore 'operationalised' - the hard task of working out what actions are possible has already been solved for the AI by the API, effectively acting as a bottleneck on the complexity of mapping from input to output.</p>\n\n<p>A range of alternative game-playing frameworks are listed <a href=\"http://cig16.image.ece.ntua.gr/competitions/\" rel=\"nofollow\">here</a> and one alternative (which I personally believe is more useful for AGI purposes) is the <a href=\"http://atkrye.github.io/IEEE-CIG-Text-Adventurer-Competition/\" rel=\"nofollow\">Artificial Text Adventurer</a>, in which (at each turn) agent is presented with natural language input describing the scene and must then output a command in natural language. Disclaimer: I am associated with this competition.</p>\n", "question": "<p>For benefits of testing AGI, is using a high-level video game description language (VGDL) gives more reliable and accurate results of general intelligence than using Arcade Learning Environment (ALE)?</p>\n"}, "id": "1499"}, {"body": {"answer": "<p>I would also look at Minimum Edit Distances such as the Levenshtein distance.\nYou could use a dynamic programming technique such as the Viterbi Algorithm.</p>\n\n<p>If you don't have a dictionary to work against, you may want to train with a Markov Chain model using a known \"good\" text. The Viterbi Algorithm could be used again to solve the model for the text being considered.</p>\n", "question": "<p>I would like to know what kind of dataset I need (to prepare) for training the network to recognize the spelling mistakes in individual words for English text.</p>\n\n<p>Given the large database of words, having correct one for each incorrect. What kind of input is more efficient for that tasks? Is it using one input per each letter, syllable, whole word or I should use different pattern syllable?</p>\n\n<p>Then the input should be incorrect word, output correct, and if the word doesn't need correction, then both input and output should be the same. Is that the right approach?</p>\n"}, "id": "1500"}, {"body": {"answer": "<p>I believe if an AI achieves sentience, it should be treated the same way we are required to treat any other sentient animal. This is belief though, there is no established ethics for AI. But there were no ethics for animals a couple of centuries ago.</p>\n", "question": "<p>As I have been looking at other questions on this site (like <a href=\"http://ai.stackexchange.com/questions/60/what-are-the-main-problems-hindering-current-ai-development\">this</a>, <a href=\"http://ai.stackexchange.com/questions/1376/is-it-ethical-to-implement-self-defence-for-street-walking-ai-robots\">this</a>, <a href=\"http://ai.stackexchange.com/questions/111/how-would-self-driving-cars-make-ethical-decisions-about-who-to-kill\">this</a>, and <a href=\"http://ai.stackexchange.com/questions/1289/can-we-destroy-artificial-general-intelligence-without-its-consent\">this</a>), I have been thinking more about the ethical implications of creating these generalized AI systems. It seems that whether or not we <em>can</em> create it is not rationale enough as to whether or not we <em>should</em> do it.</p>\n\n<p>In dealing with the issue of ethics in AI, I wonder what the ethical implications are not just for us, but for the system itself. It seems to extend beyond the usually asked questions on the topic and into unknown territory. Are ethics computable? Can they be implemented programmatically? Can we force an AI system to do something against its <em>\"will\"</em>?</p>\n\n<p>What does the creation of AI imply ethically for us as well as the AI?</p>\n"}, "id": "1502"}, {"body": {"answer": "<p>Chieko Asakawa (<a href=\"https://en.wikipedia.org/wiki/Chieko_Asakawa\" rel=\"nofollow\">wiki</a>, <a href=\"https://www.ted.com/talks/chieko_asakawa_how_new_technology_helps_blind_people_explore_the_world?language=en\" rel=\"nofollow\">TED</a>, <a href=\"http://researcher.watson.ibm.com/researcher/view.php?person=jp-CHIE\" rel=\"nofollow\">IBM</a>) is a major researcher in this area, and the linked TED talk is probably a good introduction to the state of the art as of 2015. <a href=\"http://dl.acm.org/citation.cfm?id=2899509\" rel=\"nofollow\">Here's</a> a link to a 2016 paper on a smartphone navigation system.</p>\n\n<p>Guide animals perform manipulation tasks as well as identification tasks, and so it's not clear if those could be replaced well at all. (A smartphone that reads a label is a great help, but a dog that knows which bottle to grab and deliver to you is probably a much better help.)</p>\n", "question": "<p>Were there any successful attempts to replace poor guide dogs used for blind people with AI to achieve similar rate of success? I guess dogs could be easily distracted and not reliable for every situation, and it probably takes less time to train AI, than a dog.</p>\n"}, "id": "1503"}, {"body": {"answer": "<p>Basically a <a href=\"https://en.wikipedia.org/wiki/Robot\" rel=\"nofollow\">robot</a> is a mechanical or virtual artificial agent which exhibit intelligent behavior (<a href=\"https://en.wikipedia.org/wiki/Artificial_intelligence\" rel=\"nofollow\">AI</a>).</p>\n\n<hr>\n\n<p><a href=\"http://waitbutwhy.com/wait-but-who\" rel=\"nofollow\">Tim Urban</a> on <em><a href=\"https://en.wikipedia.org/wiki/Wait_But_Why\" rel=\"nofollow\">Wait But Why</a></em> website wrote the following to clear things up:</p>\n\n<blockquote>\n  <p>First, stop thinking of robots.</p>\n  \n  <h1>A robot is a container for AI,</h1>\n  \n  <p>sometimes mimicking the human form, sometimes not</p>\n  \n  <p>\u2014 but the AI itself <strong>is the computer inside the robot</strong>.</p>\n  \n  <h3>AI is the brain and the robot is its body \u2014 if it even has a body.</h3>\n  \n  <p>For example,</p>\n  \n  <p>the software and data behind <strong>Siri is AI</strong>, the woman\u2019s voice we hear is a personification of that AI, and there\u2019s no robot involved at all.</p>\n</blockquote>\n\n<p>Source: <a href=\"http://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html\" rel=\"nofollow\">The AI Revolution: The Road to Superintelligence</a></p>\n", "question": "<p>The question is pretty much the title.</p>\n\n<p>Basically what is the difference between AI and robots?</p>\n"}, "id": "1512"}, {"body": {"answer": "<p>They are <a href=\"https://en.wikipedia.org/wiki/Intelligent_agent\" rel=\"nofollow\">virtual artificial agents</a> which exhibit intelligent behavior (<a href=\"https://en.wikipedia.org/wiki/Artificial_intelligence\" rel=\"nofollow\">AI</a>).</p>\n\n<p><a href=\"http://waitbutwhy.com/wait-but-who\" rel=\"nofollow\">Tim Urban</a> on <em><a href=\"https://en.wikipedia.org/wiki/Wait_But_Why\" rel=\"nofollow\">Wait But Why</a></em> website wrote the following:</p>\n\n<blockquote>\n  <p>The software and data behind <strong>Siri is AI</strong>, the woman\u2019s voice we hear is a personification of that AI, and there\u2019s no robot involved at all.</p>\n</blockquote>\n\n<p><sup>Source: <a href=\"http://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html\" rel=\"nofollow\">The AI Revolution: The Road to Superintelligence</a></sup></p>\n\n<p>Related: <a href=\"http://ai.stackexchange.com/q/1462/8\">What is the difference between AI and robots?</a></p>\n", "question": "<p>Siri and Cortana communicate pretty much like humans. Unlike Google now which mainly gives us search results when asked some questions (not setting alarms or reminders), Siri and Cortana provide us with an answer, in the same way that a person would do.<br>\nSo are they actual AI programs or not?</p>\n\n<p>(By \"question\" I don't mean any academic related question or asking routes/ temperature, but rather opinion based question). </p>\n"}, "id": "1513"}, {"body": {"answer": "<p>Even a simple multilayer perceptron can sort input data to some extent, as you can see <a href=\"https://github.com/primaryobjects/nnsorting\" rel=\"nofollow\">here</a> and <a href=\"http://yyue.blogspot.com.br/2015/01/a-brief-overview-of-deep-learning.html\" rel=\"nofollow\">here</a>.</p>\n\n<p>However, neural networks for sequential data seem more appropriate, as they can handle sequences of variable lengths. It has been done with an <a href=\"https://github.com/dmlc/mxnet/tree/master/example/bi-lstm-sort\" rel=\"nofollow\">LSTM</a> (Long Short-Term Memory), <a href=\"https://arxiv.org/pdf/1602.03218.pdf\" rel=\"nofollow\">LSTM+HAM</a> (Hierarchical Attentive Memory) and an <a href=\"https://arxiv.org/pdf/1410.5401v2.pdf\" rel=\"nofollow\">NTM</a> (Neural Turing Machine).</p>\n", "question": "<p>I believe normally you can use <a href=\"https://en.wikipedia.org/wiki/Genetic_programming\" rel=\"nofollow\">genetic programming</a> for sorting, however I'd like to check whether it's possible using ANN.</p>\n\n<p>Given the unsorted text data from input, which neural network is suitable for doing sorting tasks?</p>\n"}, "id": "1514"}, {"body": {"answer": "<p>It's true that the term has become a buzzword, and is now widely used to a point of confusion - however if you look at the definition provided by Stuart Russell and Peter Norvig, they write it as follows:</p>\n\n<blockquote>\n  <p>We define AI as the study of agents that receive percepts from the \n  environment and perform actions. Each such agent implements a function\n  that maps percept sequences to actions, and we cover different ways to\n  represent these functions, such as reactive agents, real-time\n  planners, and decision-theoretic systems. We explain the role of\n  learning as extending the reach of the designer into\n  unknown environments, and we show how that role constrains agent\n  design, favoring explicit knowledge representation and reasoning.</p>\n</blockquote>\n\n<p><a href=\"http://rads.stackoverflow.com/amzn/click/9332543518\" rel=\"nofollow\">Artificial Intelligence: A Modern Approach - Stuart Russell and Peter Norvig</a></p>\n\n<p>So the example you cite, \"autopilot for cars/planes\", is actually a (famous) form of AI as it has to <strong>use a form of knowledge representation to deal with unknown environments and circumstances</strong>. Ultimately, these systems also collect data so that the knowledge representation can be updated to deal with the new inputs that they have found. They do this with <a href=\"http://fortune.com/2015/10/16/how-tesla-autopilot-learns/\" rel=\"nofollow\">autopilot for cars</a> all the time</p>\n\n<p>So, directly to your question, for something to be considered as \"having AI\", <strong>it needs to be able to deal with unknown environments/circumstances in order to achieve its objective/goal</strong>, and render knowledge in a manner that provides for new learning/information to be added easily. There are many different types of well defined knowledge representation methods, ranging from the popular <a href=\"http://neuralnetworksanddeeplearning.com/\" rel=\"nofollow\">neural net</a>, through to probabilistic models like <a href=\"https://en.wikipedia.org/wiki/Bayesian_network\" rel=\"nofollow\">bayesian networks (belief networks)</a> - but fundamentally actions by the system must be derived from whichever representation of knowledge you choose for it to be considered as AI.</p>\n", "question": "<p>I believe <em>artificial intelligence</em> (AI) term is overly overused nowadays.</p>\n\n<p>For example people see that something is self-moving and they call it AI, even if it's on autopilot (like cars or planes) or there is some simple algorithm behind it.</p>\n\n<p>What are the minimum general requirements so that we can say something is AI?</p>\n"}, "id": "1516"}, {"body": {"answer": "<p>They treated it as a classification problem. While it's common to use some variety of Neural Nets (NNs) to build classifiers, Genetic Programming (GP) can also be used for this purpose. In contrast to NN classifiers, GP can use a wider range of operations (e.g. if,while,logical statements,arbitrary mathematical functions etc) to perform the classification than <a href=\"http://ai.stackexchange.com/questions/1479/do-scientists-know-what-is-happening-inside-artificial-neural-networks\">weighted arithmetic expressions involving an activation function</a>. Whether or not this is actually of benefit depends on the specific application.</p>\n\n<p>In addition, the abstract implies their algorithm is <em>adaptive</em> (i.e. responds in some fashion to the nature of incoming attacks), which would most easily be achieved by continuing to run the GP program in the background to monitor potential intrusions.</p>\n", "question": "<p>I've read on wiki that <a href=\"https://en.wikipedia.org/wiki/Genetic_programming\" rel=\"nofollow\">genetic programming</a> has '<em>outstanding results</em>' in cyberterrorism prevention.</p>\n\n<p>Further more, this <a href=\"http://papers.ssrn.com/sol3/papers.cfm?abstract_id=877981\" rel=\"nofollow\">abstract</a> says:</p>\n\n<blockquote>\n  <p>Using machine-coded linear genomes and a homologous crossover operator in genetic programming, promising results were achieved in detecting malicious intrusions.</p>\n</blockquote>\n\n<p>I've checked the study, but it's still not clear for me.</p>\n\n<p>How exactly was this detection achieved from the technical perspective?</p>\n"}, "id": "1520"}, {"body": {"answer": "<p>Tesla's technology is assistive, as Alexey points out, so this is not a case of an autonomous system (e.g. an AGI) doing some fatal stunt (the product name AutoPilot is famously misleading). Now on why the car assistance led to this tragic accident, there is some information related to AI technologies.</p>\n\n<hr>\n\n<p><em>Warning: I cannot find again the source critical to the next paragraph, and reading again pages over pages, I cannot find similar argument in other reports. I still remember vividly the point below, but please keep in mind it may be incorrect. The rest of the answer is weakly related, so I leave it all, with this warning.</em></p>\n\n<p>An independent report (link needed, I can't find it...) explained that the assistive system was unable to detect the truck due to an exceptionally low contrast (bright sky perceived as white---colour of the truck). The report also said that a human driver would have been unable to make the difference either. In other words, it is possible that car sensors (presumably camera) and the human eye could not have detected an obstacle, and could not have triggered any safety measure. This <a href=\"http://www.nytimes.com/interactive/2016/07/01/business/inside-tesla-accident.html\" rel=\"nofollow\">short graphical explanation</a> sums up the car sensors: Camera, radar, GPS, etc.</p>\n\n<p>The assistive sub-system is based on proprietary AI technologies. We can <em>only speculate</em> under some hypothesis. _This is not very useful, honestly, except for <strong>illustration purpose</strong>. Assuming the assistive system relies on ML technologies to learn about obstacles from a video stream (such systems do exist):</p>\n\n<ul>\n<li>It may be that the learning data was not \"good enough\" to cover the truck scenario.</li>\n<li>It may be the technology was not powerful enough yet (lack generalization power, or simply too slow).</li>\n<li>It may be a hardware problem, notably from the sensors: If the \"car's eyes\" are defective, the \"car's brain\" (the assistive system) is unable to react properly.</li>\n</ul>\n\n<p>Why those technologies did not work in that case will remain a secret. We can say however that <em>any system</em>---whether built with AI technology or not---has limits. Beyond these limits, the system reaction is unpredictable: It could stop, reset, shutdown. The difficulty here is to define what a \"default behaviour\" is. A machine will basically do whatever it is designed to do, so an AI-based system too.</p>\n\n<p>We could speculate even more on what would happen if the assistive system was really autonomous, the elusive AGI, but that is really not the case here.</p>\n", "question": "<p>Do we know why Tesla's Autopilot mistaken empty sky with a high-sided lorry which resulted in fatal crash involving a car in self-drive mode? Was it AI fault or something else? Is there any technical explanation behind this why this happened?</p>\n\n<p>References: <a href=\"http://news.sky.com/story/tesla-driver-in-first-self-drive-fatal-crash-10330121\" rel=\"nofollow\">Sky News article</a>, <a href=\"http://www.theverge.com/2016/6/30/12072408/tesla-autopilot-car-crash-death-autonomous-model-s\" rel=\"nofollow\">The Verge</a>.</p>\n"}, "id": "1522"}, {"body": {"answer": "<p>The underlying abstraction (which is essentially what you'd be using the first network for) is that of reducing the state-space of the raw input via feature extraction/synthesis and/or dimensionality reduction.</p>\n\n<p>At present, there are few definite rules for doing this: practice is more a question of 'informed trial and error'. </p>\n\n<p>If you add some information to your question regarding what has been previously attempted in this area (e.g. on the \n<a href=\"http://ai.stackexchange.com/questions/1490/what-are-the-benefits-of-the-vgdl-over-the-ale\">ALE</a> platform), this it might be possible to offer some more specific advice.</p>\n", "question": "<p>For example I would like to implement transparent AI in the RTS game which doesn't offer any AI API (like old games), and I'd like to use image recognition algorithm for detecting the objects which can talks to another algorithm which is responsible for the logic.</p>\n\n<p>Given I'd like to use two neural networks, what are the approaches to setup the communication between them? Is it just by exporting result findings of the first algorithm (e.g. using CNN) with list of features which were found on the screen, then use it as input for another network? Or it's more complex than that, or I need to have more than two networks?</p>\n"}, "id": "1523"}, {"body": {"answer": "<p>The allegation was based on the fact that Deep Blue made a choice that did not yield the immediate (or short term) benefit that was synonymous with systems back then (1997). Computational capability was significantly less powerful then, and Kasparov claimed that only a grand master would have made the decision that the system did - so the deep blue team cheated by having a human perform the move instead of the system.</p>\n\n<p>He asked for a rematch, but IBM did not allow this, which only added to the suspicion.</p>\n\n<p>This is a great article with deep analysis on the specific moves and circumstances - however suffice to say that Kasparov was trying to bait the system into making a decision for a weak pawn, but the system chose otherwise and instead put Kasparov into a compromised position:\n<a href=\"https://en.chessbase.com/post/deep-blue-s-cheating-move\">https://en.chessbase.com/post/deep-blue-s-cheating-move</a></p>\n", "question": "<p>On <a href=\"https://en.wikipedia.org/wiki/Deep_Blue_(chess_computer)\" rel=\"nofollow\">Wikipedia</a> we can read:</p>\n\n<blockquote>\n  <p>Kasparov accused IBM of cheating and demanded a rematch. IBM refused and retired Deep Blue.</p>\n</blockquote>\n\n<p>What was the accusation and how was Deep Blue allegedly able to cheat?</p>\n"}, "id": "1524"}, {"body": {"answer": "<p>The well-known 'Eliza' program (Weizenbaum, ~1964) would appear to be the first. </p>\n\n<p>Eliza was designed to model the emotionally-neutral response of a psychotherapist and this masks some of the weaknesses of its limited underlying pattern-matching mechanisms. </p>\n", "question": "<p>What was the first AI that was able to carry on a conversation, with real responses, such as in the famous <a href=\"https://www.youtube.com/watch?v=WnzlbyTZsQY\" rel=\"nofollow\">'I am not a robot. I am a unicorn' case?</a></p>\n\n<p>A 'real response' constitutes a sort-of personalized answer to a specific input by a user.</p>\n"}, "id": "1526"}, {"body": {"answer": "<p>The 2015 paper entitled \"<a href=\"https://arxiv.org/pdf/1511.08899.pdf\" rel=\"nofollow\">Applying deep learning to classify pornographic images and videos</a>\" applied various types of convnets for detecting pornography. The proposed architecture achieved <strong>94.1% accuracy</strong> on the NPDI dataset, which contains 800 videos (400 porn, 200 non-porn \"easy\" and 200 non-porn \"difficult\"). More traditional computer vision methods achieved 90.9% accuracy. The proposed architecture also performs very well regarding the ROC curve.</p>\n\n<p>There does not seem to exist any works regarding the other aspects of NSFW yet.</p>\n", "question": "<p>For example, search engine companies want to classify their image searches into 2 categories (which they already do that) such as: <a href=\"https://en.wikipedia.org/wiki/Not_safe_for_work\" rel=\"nofollow\">NSFW</a> (nudity, porn, brutality) and safe to view pictures.</p>\n\n<p>How can artificial neural networks achieve that, and at what success rate? Can they be easily mistaken?</p>\n"}, "id": "1527"}, {"body": {"answer": "<p>The Control Problem is, in short, the idea that AI will eventually be much better decision-makers than humans. If we don't set things up correctly beforehand, we won't get a chance to fix it afterwards, because AI will have effective control.</p>\n\n<p>There are three main areas of discussion with regards to the Control Problem:</p>\n\n<ol>\n<li><p>Whether or not the problem is urgent. Many AI experts, cognizant of the difficulty of getting simple systems to work effectively today, think that AI able to take control is not urgent, and as detail-minded engineers, they think it will be profoundly difficult to do any useful work today. (Andrew Ng, for example, famously called these sorts of worries like worrying about overpopulation on Mars.) Given radical uncertainty among AI experts as to when this will become an issue, however, this means we can't rule out rapid AI timescales, and should do at least some work in anticipation of those timescales.</p></li>\n<li><p>Whether or not the problem is hard. Many people give short, simple, and wrong solutions to the control problem. Probably the most famous is the idea that intelligence and morality are inherently interlinked, and thus a more intelligent machine, <em>by definition</em>, will be more moral. The Orthogonality Thesis is the claim of the opposite, that intelligence and morality (or, more specifically, goal alignment) are unrelated things.</p></li>\n<li><p>What foundations we can lay now. There are a bunch of open problems (see, for example, <a href=\"https://intelligence.org/technical-agenda/\">MIRI's technical agenda</a>) that deal with mathematical logic of the sort that would be useful for ensuring robust value alignment, or on how to effectively do value learning (without giving an incentive to distort values), or on how to build value functions and goals such that they are fixable if they turn out to be mistaken. Those look like problems that we can do useful work on now, even without knowing what the actual structure of a future AI will look like.</p></li>\n</ol>\n", "question": "<p>The Wikipedia page describes <a href=\"https://en.wikipedia.org/wiki/AI_control_problem\" rel=\"nofollow\">AI control problem</a> in very intricated way.</p>\n\n<p>Therefore I would like to better understand it based on some simple explanation, what's going on.\nBasically I don't want any copy &amp; pastes from wiki, because the articles there are written in neutral point of view, in very general way where articles are evolving very slowly, so the definition from there doesn't suit me.</p>\n\n<p>I believe this is what is discussed nowadays by government and it's important aspects of AI technology where it leds to.\nI believe this could be a big problem in the near future, so I'm expecting to hear about this from people from much better and more up-to-date point of view.</p>\n\n<p>So what is exactly the AI Control Problem?</p>\n"}, "id": "1528"}, {"body": {"answer": "<p>I don't think there are many contexts where there is any really meaningful distinction between these terms.  Even in the WP article you refer to, it is shown that \"abstract intelligent agent\" and \"autonomous intelligent agent\" are generally just synonyms for \"intelligent agent\" but used to highlight certain aspects of intelligent agents in some contexts.   Net-net, I'd say there just isn't any difference there that's going to matter in practice.</p>\n\n<p>\"Virtual intelligent agent\" OTOH, used in the context you used it, suggests the distinction between an IA that's implemented in software only, versus one that has a physical manifestation.   I don't know how useful that distinction is and I haven't seen anybody else make it.  </p>\n\n<p>All in all, I expect that in almost every possible context, if you just say \"Intelligent Agent\" with no qualifiers, that's going to be sufficient.   But if there were going to be an exception, I'd say it would be around the term \"autonomous\" since an agent which is truly autonomous, versus one that needs to operate in a specific, constrained environment, is a distinction that - at least in principle - could be useful. </p>\n", "question": "<p>On Wikipedia, we can read about different type of <a href=\"https://en.wikipedia.org/wiki/Intelligent_agent\" rel=\"nofollow\">intelligent agents</a>:</p>\n\n<ul>\n<li>abstract intelligent agents (AIA),</li>\n<li>autonomous intelligent agents,</li>\n<li>virtual intelligent agent (IVA), which I've found on other websites, e.g. <a href=\"https://www.techopedia.com/definition/26646/intelligent-virtual-agent-iva\" rel=\"nofollow\">this one</a>.</li>\n</ul>\n\n<p>What are the differences between these three to avoid confusion?</p>\n\n<hr>\n\n<p>For example I've used term <em>virtual artificial agent</em> <a href=\"http://ai.stackexchange.com/a/1512/8\">here</a> as:</p>\n\n<blockquote>\n  <p>Basically a robot is a mechanical or virtual artificial agent which exhibit intelligent behavior (AI).</p>\n</blockquote>\n\n<p>so basically I'd like to know where other terms like autonomous or abstract agents can be used and in what context. Can they be all defined under 'virtual' robot definition? How to distinguish these terms?</p>\n"}, "id": "1529"}, {"body": {"answer": "<p>I'm afraid I don't have the specific citations handy, but I have seen/heard quotes by experts like Andrew Ng and Geoffrey Hinton where they clearly say that we do not really understand neural networks.  That is, we understand something of the <em>how</em> they work (for example, the math behind back propagation) but we don't really understand <em>why</em> they work.  It's sort of a subtle distinction, but the point is that no, we don't understand the very deepest details of how exactly you go from a bunch of weights, to, say, recognizing a cat playing with a ball.  </p>\n\n<p>At least in terms of image recognition, the best explanation I've heard is that successive layers of a neural network learn more sophisticated features, composed of the more granular features from earlier levels.  That is to say, the first layer might recognize \"edges\" or \"straight lines\".  The next layer might then learn geometric shapes like \"box\", or \"triangle\", and then a higher layer might learn \"nose\" or \"eye\" based on those earlier features, and then a higher level layer still learns \"face\" made up from \"eye\", \"nose\", \"jaw\", etc.   But even that, as I understand it, is still hypothetical and/or not understood in complete detail. </p>\n", "question": "<p>Do scientists or research experts know from the kitchen what is happening inside complex \"deep\" neural network with at least millions of connections firing at an instant? Do they understand the process behind this (e.g. what is happening inside and how it works exactly), or it is a subject of debate?</p>\n\n<p>For example this <a href=\"https://www.cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf\">study</a> says:</p>\n\n<blockquote>\n  <p>However there is no clear understanding of <em>why</em> they perform so well, or <em>how</em> they might be improved.</p>\n</blockquote>\n\n<p>So does it mean the scientists actually doesn't know how complex convolutional network models work?</p>\n"}, "id": "1530"}, {"body": {"answer": "<p>Remembering that artificial intelligence has been an academic endeavour for the longest time, Prolog was amongst one of the early languages used as part of the study and implementation of it. It has rarely made its way into large commercial applications, having said that, a famous commercial implementation is in <a href=\"http://www.cs.nmsu.edu/ALP/2011/03/natural-language-processing-with-prolog-in-the-ibm-watson-system/\" rel=\"nofollow\">Watson, where prolog is used for NLP</a>.</p>\n\n<p>The <a href=\"http://www.ed.ac.uk/informatics/\" rel=\"nofollow\">University of Edinburgh</a> contributed to the language and it was sometimes referred to as \"Edinburgh Prolog\". It is <a href=\"http://www.inf.ed.ac.uk/teaching/courses/lp/\" rel=\"nofollow\">still used in academic teachings</a> there as part of the artificial intelligence course.</p>\n\n<p>The reason why Prolog is considered powerful in AI is because the language allows for easy management of recursive methods, and pattern matching.</p>\n\n<p>To quote <a href=\"http://www-03.ibm.com/innovation/us/watson/research-team/systems.html\" rel=\"nofollow\">Adam Lally from the IBM Thomas J. Watson Research Center</a>, and <a href=\"http://www3.cs.stonybrook.edu/~pfodor/\" rel=\"nofollow\">Paul Fodor from Stony Brook University</a>:</p>\n\n<blockquote>\n  <p>the Prolog language is very expressive allowing recursive rules to represent reachability in parse trees and the operation of negation-as-failure to check the absence of conditions.</p>\n</blockquote>\n", "question": "<p>According to <a href=\"http://en.wikipedia.org/wiki/Prolog\">Wikipedia</a>,</p>\n\n<blockquote>\n  <p>Prolog is a general-purpose logic programming language associated with artificial intelligence and computational linguistics.</p>\n</blockquote>\n\n<p>Is it still used for AI?</p>\n\n<hr>\n\n<p><sub>This is based off of a question on the 2014 closed beta. The author had the UID of 330.</sub></p>\n"}, "id": "1532"}, {"body": {"answer": "<p>The neural networks can be easily fooled or hacked by adding certain structured noise in image space (<a href=\"https://arxiv.org/abs/1312.6199\" rel=\"nofollow\">Szegedy 2013</a>, <a href=\"http://arxiv.org/abs/1412.1897\" rel=\"nofollow\">Nguyen 2014</a>) due to ignoring non-discriminative information in their input.</p>\n\n<p>For example:</p>\n\n<blockquote>\n  <p>Learning to detect jaguars by matching the unique spots on their fur while ignoring the fact that they have four legs.<sup><a href=\"http://arxiv.org/abs/1506.06579\" rel=\"nofollow\">2015</a></sup></p>\n</blockquote>\n\n<p>So basically the high confidence prediction in certain models exists due to a '<em>combination of their locally linear nature and high-dimensional input space</em>'.<sup><a href=\"http://arxiv.org/abs/1412.1897\" rel=\"nofollow\">2015</a></sup></p>\n\n<p>Published as a conference paper at <a href=\"http://www.stat.ucla.edu/~ywu/ICLR2015.pdf\" rel=\"nofollow\">ICLR 2015</a> (work by Dai) suggest that transferring discriminatively trained parameters to generative models, could be a great area for further improvements.</p>\n", "question": "<p>The following <a href=\"http://www.evolvingai.org/fooling\">page</a>/<a href=\"http://www.evolvingai.org/files/DNNsEasilyFooled_cvpr15.pdf\">study</a> demonstrates that the deep neural networks are easily fooled by giving high confidence predictions for unrecognisable images, e.g.</p>\n\n<p><a href=\"http://i.stack.imgur.com/7pgrH.jpg\"><img src=\"http://i.stack.imgur.com/7pgrH.jpg\" alt=\"Evolved images that are unrecognisable to humans, but that state-of-the-art DNNs trained on ImageNet believe with &gt;= 99.6% certainty to be a familiar object. This result highlights differences between how DNNs and humans recognise objects. Directly and indirectly encoded images\"></a></p>\n\n<p><a href=\"http://i.stack.imgur.com/pBm48.png\"><img src=\"http://i.stack.imgur.com/pBm48.png\" alt=\"Evolving images to match DNN classes produces a tremendous diversity of images. The mean DNN confidence scores for these images is 99.12% for the listed class, meaning that the DNN believes with near-certainty that the image is that type of thing. Shown are images selected to showcase diversity from 5 independent evolutionary runs. The images shed light on what the DNN network cares about, and what it does not, when classifying an image. For example, a school bus is alternating yellow and black lines, but does not need to have a windshield or wheels\"></a></p>\n\n<p>How this is possible? Can you please explain ideally in plain English?</p>\n"}, "id": "1533"}, {"body": {"answer": "<p>I will overly simplify ANNs in order to point how they work. Examples might not be 100% accurate.</p>\n\n<p>In the simplest form, network is trained using the apriori information extracted from the ground truth. This basically means that ANN uses the relation between the input and output. </p>\n\n<p>For instance, if you are to classify shrubs and trees, one of the input could be height and the other could be the width of the tree. Now, if you have only input and output layers, increasing height means increasing chance for the object to be a tree. Thus, input height would have a positive weight connecting to tree output and a negative weight to shrub output. However, as the plant gets wider, the chance of it being a shrub increases. Taller shrubs are wider than shorter ones. Thus input weight would have positive weight connecting to the shrub output. Finally, the chance of being a tree is not affected by the width and thus will have close to 0 weight between this input and output. This network will effectively work like a linear discriminant classifier.</p>\n\n<p>Now instead of assigning weights by hand, you may use a learning algorithm that tries to adjust weights so that the output is correct when the series of input is supplied. Ideally this training algorithm should reach to the conclusion that we have made in the previous example. Most training algorithms are recursive. They supply the inputs multiple times, and in a simple sense, they reward pathways that are correct by increasing their weights and punishes pathways that are causing incorrect answer.</p>\n\n<p>When hidden layers are used in a system, they would be able to correlate input on higher degrees. Thus, as the number of layers get higher, ANN learns the input set much better. However, this does not mean it gets better. If the ANN over fits the input set, it would be affected from the random noise that is in the dataset. This problem is generally referred as memorization. There are learning algorithms that try to minimize memorization and maximize generalization ability. But ultimately, the number of training samples should be high enough so that ANN cannot overfit to the data.</p>\n", "question": "<p>When it comes to neural networks, it's often only explained what abstract task they do, say for example detect a number in an image. I never understood what's going on under the hood essentially.</p>\n\n<p>There seems to be a common structure of a directed graph, with values in each node. Some nodes are input nodes. Their values can be set. The values of subsequent nodes are then calculated based on those along the edges of the graph until the values for the output nodes are set, which can be interpreted a result.</p>\n\n<p>How exactly is the value of each node determined? I assume that some formula is associated with each node that takes all incoming nodes as input to calculate the value of the node. What formula is used? Is the formula the same throughout the network?</p>\n\n<p>Then I heard that a network has to be trained. I assume that such training would be the process to assign values to coefficients of the formulas used to determine the node values. Is that correct?</p>\n\n<p>In layman's terms, what are the underlying principles that make a neural network work?</p>\n"}, "id": "1536"}, {"body": {"answer": "<p>Unlike backpropagation, evolutionary algorithms do not require the objective function to be differential with respect to the parameters you aim to optimize. As a result, you can optimize \"more things\" in the network, such as activation functions or number of layers, which wouldn't be possible in the standard backpropagation. </p>\n\n<p>Another advantage is that by defining the mutation and crossover functions, you can influence how the parameter search space should be explored.</p>\n", "question": "<p>How does employing evolutionary algorithms to design and train artificial neural networks have advantages over using the conventional backpropagation algorithms?</p>\n"}, "id": "1545"}, {"body": {"answer": "<p>Crossover allows to combine two parents (vs. mutation, which only uses one parent). In some cases, it is useful (e.g., if you train a FPS bot, if one parent is good at shooting and another parent is good at moving, it makes sense to combine them). In some other cases, it is not.</p>\n", "question": "<p>Genetic Algorithms has come to my attention recently when trying to correct/improve computer opponents for turn-based strategy computer games.</p>\n\n<p>I implemented a simple Genetic Algorithm that didn't use any cross-over, just some random mutation. It seemed to work in this case, and so I started thinking:</p>\n\n<p><strong>Why is cross-over a part of genetic algorithms? Wouldn't mutation be enough?</strong></p>\n\n<p><sub>This is from a data dump on an old AI site. The asker had the UID of 7. </sub></p>\n"}, "id": "1546"}, {"body": {"answer": "<blockquote>\n  <p>when the AI has difficulty in classifying a image or its objects it should ask a human for help just like a curious child</p>\n</blockquote>\n\n<p>It's called <a href=\"https://en.wikipedia.org/wiki/Active_learning_(machine_learning)\">active learning</a>, it's already used quite often.</p>\n", "question": "<p>While thinking about AI, this question came into my mind. Could curiosity help in developing a true AI? According to this <a href=\"http://psychologia.co/creativity-test/\" rel=\"nofollow\">website</a> (for testing creativity):</p>\n\n<blockquote>\n  <p>Curiosity refers to persistent desire to learn and discover new things\n  and ideas</p>\n\n<pre><code>always looks for new and original ways of thinking,\nlikes to learn,\nsearches for alternative solutions even when traditional solutions are present and available,\nenjoys reading books and watching documentaries,\nwants to know how things work inside out\n</code></pre>\n</blockquote>\n\n<p>Let's take <a href=\"https://www.clarifai.com/demo\" rel=\"nofollow\">Clarifai</a>, a image/video classification startup which can classify images and video with the best accuracy (according to them). If I understand correctly, they trained their deep learning system using millions of images with supervised learning. In the same algorithm, what would happen if we somehow added a \"curiosity factor\" when the AI has difficulty in classifying a image or its objects? It would ask a human for help, just like a curious child. </p>\n\n<p>Curiosity makes a human being learn new things and also helps to generate new original ideas. Could the addition of curiosity change Clarifai into a true AI?</p>\n"}, "id": "1547"}, {"body": {"answer": "<p>Mutation is usually defined to be a <em>global</em> operator, i.e. iterated mutation is (eventually) capable of reaching every point in the vector space defined by the geneome. In that sense, mutation alone is certainly 'enough'.</p>\n\n<p>Regarding the motivation for crossover - from <a href=\"https://cs.gmu.edu/~sean/book/metaheuristics/\" rel=\"nofollow\">Essentials of Metaheuristics</a>, p42:</p>\n\n<blockquote>\n  <p>Crossover was originally based on the premise that highly fit individuals often share certain traits, called <em>building blocks</em>, in common.\n  For example, in the boolean individual 10110101, perhaps\n  ***101*1 might be a building block </p>\n</blockquote>\n\n<p>(where * means \"either 0 or 1\") </p>\n\n<p>So the idea is that crossover works by spreading building blocks quickly throughout the population.</p>\n\n<blockquote>\n  <p>Crossover methods also assume that there is some degree of linkage between genes on the chromosome: that is, settings for certain genes in groups are strongly correlated to fitness improvement. For example, genes A and B might contribute to fitness only when they\u2019re both set to 1: if either is set to 0, then the fact that the other is set to 1 doesn\u2019t do anything.</p>\n</blockquote>\n\n<p>Also note that <em>crossover is not a global operator</em>. If the only operator is crossover then (also from p42):</p>\n\n<blockquote>\n  <p>Eventually the population will converge, and often (unfortunately) prematurely\n  converge, to copies of the same individual. At this stage there\u2019s no escape: when an individual crosses over with itself, nothing new is generated.</p>\n</blockquote>\n\n<p>For this reason, crossover is generally used together with some global mutation operator.</p>\n", "question": "<p>Genetic Algorithms has come to my attention recently when trying to correct/improve computer opponents for turn-based strategy computer games.</p>\n\n<p>I implemented a simple Genetic Algorithm that didn't use any cross-over, just some random mutation. It seemed to work in this case, and so I started thinking:</p>\n\n<p><strong>Why is cross-over a part of genetic algorithms? Wouldn't mutation be enough?</strong></p>\n\n<p><sub>This is from a data dump on an old AI site. The asker had the UID of 7. </sub></p>\n"}, "id": "1548"}, {"body": {"answer": "<p>When thinking about crossover its important to think about the fitness landscape. </p>\n\n<p>Consider a hypothetical scenario where we are applying a genetic algorithm to find a solution that performs well at 2 tasks. This could be from Franck's example (moving and shooting) for an AI, or perhaps it could be predicted 2 outputs in a genetic machine learning scenario, but really most scenarios where GAs are applied are synonymous (even at solving a single task, there may be different aspects of the task to be addressed).</p>\n\n<p>Suppose we had an individual, 1, that was performing reasonably well at both tasks, and we found a series of mutations which produced 2 new individuals, 2 and 3, which performed better than Individual 1 at tasks 1 and 2 respectively. Now while both of these are improvements, ideally we want to find a generally good solution, so we want to combine the features that we have been found to be beneficial. </p>\n\n<p>This is where crossover comes in; by combining the genomes of Individuals 2 and 3, we may find some new individual which produces a mixture of their performances. While it is possible that such an individual could be produced by a series of mutations applied to Individual 2 or Individual 3, the landscape may simply not suit this (there may be no favorable mutations in that direction, for example).</p>\n\n<p><a href=\"http://i.stack.imgur.com/bsVBEm.png\" rel=\"nofollow\"><img src=\"http://i.stack.imgur.com/bsVBEm.png\" alt=\"enter image description here\"></a></p>\n\n<p>You are partially right therefore; it may sometimes be the case that the benefits of crossover could be replicated with a series of mutations. Sometimes this may not be the case and crossover may smooth the fitness landscape of your GA, speeding up optimization and helping your GA escape local optima. </p>\n", "question": "<p>Genetic Algorithms has come to my attention recently when trying to correct/improve computer opponents for turn-based strategy computer games.</p>\n\n<p>I implemented a simple Genetic Algorithm that didn't use any cross-over, just some random mutation. It seemed to work in this case, and so I started thinking:</p>\n\n<p><strong>Why is cross-over a part of genetic algorithms? Wouldn't mutation be enough?</strong></p>\n\n<p><sub>This is from a data dump on an old AI site. The asker had the UID of 7. </sub></p>\n"}, "id": "1551"}, {"body": {"answer": "<p>For a more intelligent approach than random or exhaustive searches, you could try a genetic algorithm such as NEAT <a href=\"http://nn.cs.utexas.edu/?neat\">http://nn.cs.utexas.edu/?neat</a>. However, this has no guarantee to find a global optima, it is simply an optimization algorithm based on performance and is therefore vulnerable to getting stuck in a local optima. </p>\n", "question": "<p>When you're writing your algorithm, how do you know how many neurons you need per single layer? Are there any methods for finding the optimal number of them, or is it a rule of thumb?</p>\n"}, "id": "1552"}, {"body": {"answer": "<blockquote>\n  <p>Does this addition of curosity changes clarifai into a true AI?</p>\n</blockquote>\n\n<p>As per my answer to <a href=\"http://ai.stackexchange.com/questions/1420/how-close-are-we-to-creating-ex-machina\">this question</a>, we don't know what the ingredients for a 'true AI' are. Via the Turing Test and its variants, the best we can do is \"know one when we see one\".</p>\n\n<p>Curiosity certainly appears <em>necessary</em> for intelligence, though it doesn't seem <em>sufficient</em> - a lemming-like creature curious to see what's at the bottom of a steep cliff might not survive long enough to learn caution, even if it had the learning mechanisms to do so.</p>\n\n<p>Here is some work by Schmidhuber on <a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.86.3978&amp;rep=rep1&amp;type=pdf\">Artificial Curiousity</a>. </p>\n\n<p><a href=\"http://www.pyoudeyer.com/active-learning-and-artificial-curiosity-in-robots/\">Pierre-Yves Oudeyer</a> has also done quite a lot of work on this and Active Learning/Intrinsic motivation.</p>\n", "question": "<p>While thinking about AI, this question came into my mind. Could curiosity help in developing a true AI? According to this <a href=\"http://psychologia.co/creativity-test/\" rel=\"nofollow\">website</a> (for testing creativity):</p>\n\n<blockquote>\n  <p>Curiosity refers to persistent desire to learn and discover new things\n  and ideas</p>\n\n<pre><code>always looks for new and original ways of thinking,\nlikes to learn,\nsearches for alternative solutions even when traditional solutions are present and available,\nenjoys reading books and watching documentaries,\nwants to know how things work inside out\n</code></pre>\n</blockquote>\n\n<p>Let's take <a href=\"https://www.clarifai.com/demo\" rel=\"nofollow\">Clarifai</a>, a image/video classification startup which can classify images and video with the best accuracy (according to them). If I understand correctly, they trained their deep learning system using millions of images with supervised learning. In the same algorithm, what would happen if we somehow added a \"curiosity factor\" when the AI has difficulty in classifying a image or its objects? It would ask a human for help, just like a curious child. </p>\n\n<p>Curiosity makes a human being learn new things and also helps to generate new original ideas. Could the addition of curiosity change Clarifai into a true AI?</p>\n"}, "id": "1553"}, {"body": {"answer": "<p>If the system claims that a piece of code has violated standards, then to be useful to the programmer, it really needs to provide more information than just a 'yes/no' classifier: you need some form of explanation about <em>why</em> it is claimed to be wrong.</p>\n\n<p>Clearly ANNs aren't much use for that.</p>\n\n<p><em>If</em> I were tackling such a problem (and my suspicion is that a lot of effort could be spent trying and failing to reproduce coding standards which are already well-understood), then my inclination would be to use a more explicitly rule-based representation.</p>\n\n<p>Possibilities include:</p>\n\n<ul>\n<li><p>Genetic Programming</p></li>\n<li><p>Learning Classifier Systems </p></li>\n<li>Decision Trees</li>\n</ul>\n\n<p>The ever-useful <a href=\"https://cs.gmu.edu/~sean/book/metaheuristics\" rel=\"nofollow\">\"Essentials of Metaheuristics\"</a> has a whole section on the evolution of rulesets. Obviously, nothing prevents you from initializing the evolutionary process with rules known to be useful.</p>\n\n<p>As I point out <a href=\"http://ai.stackexchange.com/questions/1420/how-close-are-we-to-creating-ex-machina\">here</a>, with our current AI algorithms, the success of an  approach is very sensitive to human expertise/effort in feature selection/preprocessing, choice of training set etc, so creative experiment with this is vital.</p>\n\n<p>Training set: how about two sets of negative and positive examples, consisting of (features extracted from) bad code and from a refactored version (respectively)?</p>\n\n<p>One elementary choice of features would be to apply a bunch of code complexity metrics and have the learning algorithm combine those. The plus side of working in such a numeric domain is that the learning algorithm might readily find a gradient to exploit. The downside is that the rules (which are then likely of the form <code>if mcabe &gt; 2.8</code> etc) are still not as informative as might be desired.</p>\n\n<p>For more complex rules (e.g. requiring <code>if elseif else</code>) you may want to extract your features from the abstract syntax tree. You could in principle use the entire tree but to my knowledge <a href=\"http://jmlr.csail.mit.edu/papers/volume11/vishwanathan10a/vishwanathan10a.pdf\" rel=\"nofollow\">ML on graph and tree structures</a> is still in relative infancy.</p>\n", "question": "<p>Are there any existing approaches for using artificial neural networks (ANN) or evolutionary algorithm (EA) for detecting coding standard violations? Which one would be more suitable?</p>\n\n<p>I don't have any specific programming language in mind, but something similar to <a href=\"http://pear.php.net/package/PHP_CodeSniffer\" rel=\"nofollow\">PHP_CodeSniffer</a> (following <a href=\"https://www.drupal.org/coding-standards\" rel=\"nofollow\">these standards</a>), but instead of using hardcoded rules, the algorithm should learn good techniques, but I'm not sure based on what training data. How would you approach the training session, any suggestions?</p>\n"}, "id": "1562"}, {"body": {"answer": "<p>Most of the algorithms (based on <a href=\"http://arxiv.org/abs/1603.01768\" rel=\"nofollow\">image synthesis and style transfer</a>, e.g. <a href=\"https://github.com/alexjc/neural-doodle\" rel=\"nofollow\">neural-doodle</a>) haven't been proven to be highly effective in terms of real-time image processing.</p>\n\n<p>However the following studies discusses such algorithms for real-time texture synthesis:</p>\n\n<ul>\n<li><p><a href=\"http://arxiv.org/abs/1603.03417\" rel=\"nofollow\">Feed-forward Synthesis of Textures and Stylized Images</a></p>\n\n<p>The approach is to move the computational burden to a learning stage, making trained network (<a href=\"https://en.wikipedia.org/wiki/Convolutional_neural_network\" rel=\"nofollow\">CNN</a>) light-weight and compact in order to generate multiple samples of the same texture. This can generate textures as good as comparable to <a href=\"http://arxiv.org/abs/1505.07376\" rel=\"nofollow\">Gatys~et~al</a>, but significantly faster.</p></li>\n<li><p><a href=\"http://arxiv.org/abs/1603.08155\" rel=\"nofollow\">Perceptual Losses for Real-Time Style Transfer and Super-Resolution</a></p>\n\n<p>This method uses parallel work which can generate high-quality images by defining and optimizing loss functions based on high-level features extracted from pretrained networks.</p></li>\n<li><p><a href=\"http://arxiv.org/abs/1604.04382\" rel=\"nofollow\">Precomputed Real-Time Texture Synthesis with Markovian Generative Adversarial Networks</a></p>\n\n<p>This uses precomputed feed-forward networks that captures the feature statistics of <a href=\"http://www.irisa.fr/vista/Papers/2008_LNLA_Pecot.pdf\" rel=\"nofollow\">Markovian patches</a> in order to generate outputs of arbitrary dimensions. This can be applied to texture synthesis, style transfer and video stylization.</p></li>\n</ul>\n\n<p><sup>Source: Above list suggested on <a href=\"https://github.com/alexjc/neural-doodle\" rel=\"nofollow\">neural-doodle</a> project.</sup></p>\n", "question": "<p>Ideally I'd like to watch movie which is deep dreamed in real-time. Most algorithms which I know are too slow or not designed for real-time processing.</p>\n\n<p>For example I'm bored with some movie which I've watched thousands of time and I'd like to add some \"dreaming\" to it which is real-time filter which takes input frames, then it's processing and enhances the images through artificial neural network to achieve doodled output.</p>\n\n<p>Doesn't have to be exactly <a href=\"https://en.wikipedia.org/wiki/DeepDream\" rel=\"nofollow\">DeepDream</a> or hallucinogenic technique (which could be too much to watch for 2h), but with any similar ANN algorithm. I'm more interested into achieving desired real-time use.</p>\n\n<p>What kind of techniques can achieve such efficiency?</p>\n"}, "id": "1569"}, {"body": {"answer": "<p>Artificially intelligent computer programs should be able to be at the same level or beat humans at every game that we play.  This is because games follow rules that are scriptable, and <a href=\"http://www.aaai.org/ojs/index.php/aimagazine/article/view/2310\" rel=\"nofollow\">artificial intelligence</a> is designed to focus on one specific game and learn from its failures.  The difference between humans and artificial intelligence is that artificial intelligence focuses on one specific task like learning to master Go while our brain is dedicated to mastering multiple tasks like...living.  Even Arimaa, a game designed to be difficult for artificially intelligent systems was beaten by a bot called Sharp: <a href=\"https://en.wikipedia.org/wiki/Arimaa\" rel=\"nofollow\">https://en.wikipedia.org/wiki/Arimaa</a>.  </p>\n", "question": "<p>Significant AI vs human board game matches include:</p>\n\n<ul>\n<li><strong>chess</strong>: <a href=\"https://en.wikipedia.org/wiki/Deep_Blue_(chess_computer)#Deep_Blue_versus_Kasparov\" rel=\"nofollow\">Deep Blue vs Kasparov</a> in 1996,</li>\n<li><strong>Go</strong>: <a href=\"https://en.wikipedia.org/wiki/AlphaGo_versus_Lee_Sedol\" rel=\"nofollow\">DeepMind AlphaGo vs Lee Sedol</a> in 2016,</li>\n</ul>\n\n<p>which demonstrated that AI challenged and defeated professional players.</p>\n\n<p>Are there known board games left where a human can still win against an AI? I mean based on the final outcome of authoritative famous matches, where there is still same board game where AI cannot beat a world champion of that game.</p>\n"}, "id": "1571"}, {"body": {"answer": "<p>Not all games (or even board games) are computationally algorithmic. Even the least skilled player is likely to trounce the hottest pattern-matching algorithm in a game of <strong><a href=\"https://en.wikipedia.org/wiki/Pictionary\" rel=\"nofollow\">Pictionary</a></strong> (for example).</p>\n\n<p><img src=\"http://i.stack.imgur.com/RDIuC.png\" alt=\"\"></p>\n\n<p>If you want to say that the movement of pieces upon successful completion of a task is only ancelary to the object of the game, than your answer will be largely self-selecting. A sufficiently sophisticated algorithm will brute force a computational problem better than human intuition&hellip; <a href=\"https://en.wikipedia.org/wiki/AI_effect\" rel=\"nofollow\"><strong>eventually.</strong></a></p>\n", "question": "<p>Significant AI vs human board game matches include:</p>\n\n<ul>\n<li><strong>chess</strong>: <a href=\"https://en.wikipedia.org/wiki/Deep_Blue_(chess_computer)#Deep_Blue_versus_Kasparov\" rel=\"nofollow\">Deep Blue vs Kasparov</a> in 1996,</li>\n<li><strong>Go</strong>: <a href=\"https://en.wikipedia.org/wiki/AlphaGo_versus_Lee_Sedol\" rel=\"nofollow\">DeepMind AlphaGo vs Lee Sedol</a> in 2016,</li>\n</ul>\n\n<p>which demonstrated that AI challenged and defeated professional players.</p>\n\n<p>Are there known board games left where a human can still win against an AI? I mean based on the final outcome of authoritative famous matches, where there is still same board game where AI cannot beat a world champion of that game.</p>\n"}, "id": "1574"}, {"body": {"answer": "<p>In the real world, decisions will be made based on the law, and <a href=\"http://law.stackexchange.com/questions/1639/what-is-the-legal-take-on-the-trolley-problem\">as noted over on Law.SE</a>, the law generally favors inaction over action. </p>\n", "question": "<p>Obviously driverless cars aren't perfect, so imagine that the Google car (as an example) got into difficult situation.</p>\n\n<p>Here are a few examples of unfortunate situations caused by set of events:</p>\n\n<ul>\n<li>the car is heading toward a crowd of 10 people crossing the road, so it cannot stop in time, but it can avoid killing 10 people by hitting the wall (killing the passengers),</li>\n<li>avoiding killing the rider of the motorcycle considering that the probability of survival is greater for the passenger of the car,</li>\n<li>killing animal on the street in favour of human being,</li>\n<li>changing lanes to crash into another car to avoid killing a dog,</li>\n</ul>\n\n<p>And here are few dilemmas:</p>\n\n<ul>\n<li>Does the algorithm recognize the difference between a human being and an animal?</li>\n<li>Does the size of the human being or animal matter?</li>\n<li>Does it count how many passengers it has vs. people in the front?</li>\n<li>Does it \"know\" when babies/children are on board?</li>\n<li>Does it take into the account the age (e.g. killing the older first)?</li>\n</ul>\n\n<p>How would an algorithm decide what should it do from the technical perspective? Is it being aware of above (counting the probability of kills), or not (killing people just to avoid its own destruction)?</p>\n\n<p>Related articles:</p>\n\n<ul>\n<li><a href=\"https://www.technologyreview.com/s/542626/why-self-driving-cars-must-be-programmed-to-kill/\">Why Self-Driving Cars Must Be Programmed to Kill</a></li>\n<li><a href=\"https://www.technologyreview.com/s/539731/how-to-help-self-driving-cars-make-ethical-decisions/\">How to Help Self-Driving Cars Make Ethical Decisions</a></li>\n</ul>\n"}, "id": "1575"}, {"body": {"answer": "<p>It likely to be happen, because it's more convenient that way. In general people, organizations and government are always keen to make things more efficient by standarizing things (computers, technology, law, science, etc.) in order to make it manageable and predictable to reduce the time and minimalize the risk of the same mistakes.</p>\n\n<p>The whole world now moves into technological advancement where automation of everything is where we are going, so we can manage complexities in more reliable way, so we can focus on much bigger picture. This includes technology such as mobiles, computers, UAV (delivery drones), robots and now self-driving cars.</p>\n\n<p>The pros of that change would be:</p>\n\n<ul>\n<li>To have safer streets by introducing autonomous cars on the road.</li>\n<li>To have fewer drunk, tired, drugged or crazy drivers.</li>\n<li>To avoid poor weather conditions.</li>\n<li><p>To reduce <a href=\"https://en.wikipedia.org/wiki/Braking_distance\" rel=\"nofollow\">braking distances</a> by dropping driver's reaction time and predicting dangerous situations much earlier.</p>\n\n<p><a href=\"http://www.cyberphysics.co.uk/topics/forces/stopping_distance.htm\" rel=\"nofollow\"><img src=\"http://i.stack.imgur.com/cz9yP.png\" alt=\"Typical stopping distance\"></a></p>\n\n<p><sup>Source: <a href=\"http://www.cyberphysics.co.uk/topics/forces/stopping_distance.htm\" rel=\"nofollow\">Cyber Physics</a></sup></p></li>\n<li><p>Reducing car deaths and costs of GNP.</p>\n\n<blockquote>\n  <p>An estimated 1.3 million people die on the world's roads every year with around 50 million injured or disabled by accidents, with accidents costing countries up to four per cent of their Gross National Product (GNP) yearly. - <a href=\"http://www.un.org/apps/news/story.asp?NewsID=36823\" rel=\"nofollow\">UN News Centre</a></p>\n</blockquote></li>\n<li><p>To have central point of safety improvements, you cannot change people, but you can fix the known safety issue on global scale.</p></li>\n<li>To introduce global standards from the central point (e.g. new law to which manufactures needs to apply).</li>\n<li>To increase <a href=\"http://www.slideshare.net/sbishop2/p22-car-design-safety\" rel=\"nofollow\">car safety</a> in general on larger scale.</li>\n<li>And so on.</li>\n</ul>\n\n<hr>\n\n<h3>Why we need the 'only self-driving cars'?</h3>\n\n<p>The <a href=\"http://www.un.org/apps/news/story.asp?NewsID=36823\" rel=\"nofollow\">Secretary-General said the UN</a> would work hard to prevent further deaths on the roads:</p>\n\n<blockquote>\n  <p>Many tragedies can be avoided through a set of proven, simple measures that benefit not only individuals and families but society at large.</p>\n</blockquote>\n\n<p>Here are my points:</p>\n\n<ul>\n<li>To achieve 'a set of proven measures' - do not allow people to drive - simple.</li>\n<li>People tend to break the rules, always, so do not allow them to drive without permission.</li>\n<li>Reduce stealing cars and other crime.</li>\n<li>Law enforcement dream is to able to stop any car on demand.</li>\n<li>Do not allow drunk people to drive a car.</li>\n<li>Disallow terrorist attacks, like in <a href=\"https://en.wikipedia.org/wiki/2016_Nice_attack\" rel=\"nofollow\">Nice where truck killed over 80 people</a>.</li>\n<li>Avoid bank robberies and similar which are possible by escaping fast cars.</li>\n</ul>\n\n<p>Is it possible? I believe it depends on specific countries and unions and how quickly we're able to advance and be ready for such change.</p>\n\n<p>To support above points and summarize the 'only self-driving cars' point, please see below references which shows that this is already happening:</p>\n\n<ul>\n<li>2014: <a href=\"http://www.theverge.com/2014/5/28/5758734/uber-will-eventually-replace-all-its-drivers-with-self-driving-cars\" rel=\"nofollow\">Uber will eventually replace all its drivers with self-driving cars</a></li>\n<li><p>2016: <a href=\"http://www.dezeen.com/2016/02/12/google-self-driving-car-artficial-intelligence-system-recognised-as-driver-usa/\" rel=\"nofollow\">Google's self-driving car system has been officially recognised as a driver in the US.</a></p>\n\n<blockquote>\n  <p>The move is seen as a first step towards changing the law for cars that have \"no need for a human driver\".</p>\n</blockquote></li>\n<li><p>2016: <a href=\"http://www.dezeen.com/2016/04/19/beverly-hills-replace-public-transport-driverless-cars-los-angeles/\" rel=\"nofollow\">Beverly Hills to replace public transport with self-driving cars</a></p></li>\n<li><p>2016: <a href=\"http://www.sfexaminer.com/sf-pitches-149-million-plan-replace-cars-self-driving-vehicles/\" rel=\"nofollow\">San Francisco pitches $149 million plan to replace cars with self-driving vehicles</a></p>\n\n<blockquote>\n  <p>San Francisco\u2019s future is autonomous and shared vehicles \u2013 and that future may be only a decade away.</p>\n</blockquote></li>\n<li><p>2016: <a href=\"http://spectrum.ieee.org/cars-that-think/transportation/self-driving/otto-selfdriving-truck-company-wants-to-replace-teamsters\" rel=\"nofollow\">Otto Self-Driving Truck Company Wants to Replace Teamsters</a></p></li>\n</ul>\n", "question": "<p><sub> This is a scope experiment. </sub></p>\n\n<hr>\n\n<p>After Google/Tesla/whoever else is making self-driving cars finishes perfecting them, will they replace the cars with human drivers, so that there are only self-driving cars?</p>\n\n<p>If they do, it would probably make the roads safer.</p>\n"}, "id": "1576"}, {"body": {"answer": "<p>The AI of the car uses sensor data to process all the data and classifies objects <strong>based on the size, shape and movement patterns</strong>. It can recognize surroundings from a 360 degree perspective by making predictions about vehicles, people and objects around it will move.</p>\n\n<p>It can detect pedestrians, but as moving, <strong>column-shaped blurs of pixels</strong>, so it really cannot tell whether it's a rock or a crumpled piece of paper.</p>\n\n<p><img src=\"https://media.giphy.com/media/3oEduYqb4Ty6dSReKc/giphy-downsized-large.gif\" alt=\"Google&#39;s self-driving car sees traffic\"></p>\n\n<p>However it is programmed to determine certain patterns when a police officer has halted traffic or the car is being signaled to move forward.</p>\n\n<p><img src=\"https://media.giphy.com/media/l41lGfjhDlrSE9ILS/giphy-downsized-large.gif\" alt=\"Google&#39;s self-driving car determines when a police officer has halted traffic or when the car is being signaled to move forward\"></p>\n\n<p>It also recognizes cyclists as objects outlined in red and can slow down to let the cyclist enter into a lane.</p>\n\n<p><img src=\"https://media.giphy.com/media/3oEduTuU46cDCGGuC4/giphy-downsized-large.gif\" alt=\"Google&#39;s self-driving car sees when a cyclist is trying to merge into a lane, the vehicle also knows to slow down and let the cyclist enter\"></p>\n\n<p><sup>Above images are provided by Chris Urmson who heads up Google's driverless car program.</sup></p>\n\n<p>Sources:</p>\n\n<ul>\n<li><a href=\"http://www.techinsider.io/how-googles-self-driving-cars-see-the-world-2015-10/#then-it-uses-its-sensor-data-to-understand-what-it-sees-in-the-moment-the-software-processes-all-of-the-data-and-classifies-objects-based-on-size-shape-and-movement-patterns-2\" rel=\"nofollow\">How Google's self-driving cars see the world</a></li>\n<li><a href=\"https://www.technologyreview.com/s/530276/hidden-obstacles-for-googles-self-driving-cars/\" rel=\"nofollow\">Hidden Obstacles for Google\u2019s Self-Driving Cars</a></li>\n<li>(video) <a href=\"https://www.youtube.com/watch?v=tiwVMrTLUWg\" rel=\"nofollow\">Chris Urmson: How a driverless car sees the road</a></li>\n</ul>\n", "question": "<p>Based on this <a href=\"http://www.dailymail.co.uk/sciencetech/article-3677950/Google-s-self-driving-cars-spot-cyclists-Sensors-read-hand-signals-predict-riders-behavior.html\" rel=\"nofollow\">article</a>, Google's self-driving cars can spot cyclists, cars, road signs, markings, traffic lights, and pedestrians.</p>\n\n<p>How exactly does it identify pedestrians? Is it based on face recognition, shape, size, distance, infrared signature?</p>\n"}, "id": "1577"}, {"body": {"answer": "<p>Google\u2019s self-driving car most likely uses <a href=\"https://viejournal.springeropen.com/articles/10.1186/s40327-015-0027-1\" rel=\"nofollow\">mapping of traffic signs using google street view images for roadway inventory management</a>. If traffic signs are not in its database, it can still \u201csee\u201d and detect moving objects which can be distinguished from the presence of certain stationary objects, like traffic lights. So its software can classify objects based on the size, shape and movement patterns. Therefore it is highly unlikely that a person would be mistaken for a traffic sign. See: <a href=\"http://ai.stackexchange.com/q/1560/8\">How does Google&#39;s self-driving car identify pedestrians?</a></p>\n\n<p><a href=\"https://www.technologyreview.com/s/530276/hidden-obstacles-for-googles-self-driving-cars/\" rel=\"nofollow\"><img src=\"http://i.stack.imgur.com/setJb.png\" alt=\"enter image description here\"></a></p>\n\n<p><sup>Image: Technology Review</sup></p>\n\n<p>To support such a claim, <a href=\"http://www.cs.cmu.edu/~illah/\" rel=\"nofollow\">Illah Nourbakhsh</a>, a professor of robotics at Carnegie Mellon University, gave an interview to the New York Times magazine cover story on autonomous driving cars, and includes this hypothetical scenario, saying:</p>\n\n<blockquote>\n  <p>If they\u2019re outside walking, and the sun is at just the right glare level, and there\u2019s a mirrored truck stopped next to you, and the sun bounces off that truck and hits the guy so that you can\u2019t see his face anymore \u2014 well, now your car just sees a stop sign. <strong>The chances of all that happening are diminishingly small \u2014 it\u2019s very, very unlikely</strong> \u2014 but the problem is we will have millions of these cars. The very unlikely will happen all the time.</p>\n</blockquote>\n\n<p>Even so, the risk would be minimal, since the car is always looking out for traffic, pedestrians and other obstacles.</p>\n\n<p>Sources:</p>\n\n<ul>\n<li><a href=\"http://www.techinsider.io/how-googles-self-driving-cars-see-the-world-2015-10/#googles-self-driving-vehicles-first-establish-their-location-by-using-mapping-and-sensor-data-1\" rel=\"nofollow\">How Google's self-driving cars see the world</a></li>\n<li><a href=\"http://www.nytimes.com/2015/11/15/magazine/the-dream-life-of-driverless-cars.html\" rel=\"nofollow\">The Dream Life of Driverless Cars</a> at The New York Times</li>\n</ul>\n", "question": "<p>In <a href=\"https://www.technologyreview.com/s/530276/hidden-obstacles-for-googles-self-driving-cars/\">Hidden Obstacles for Google\u2019s Self-Driving Cars</a> article we can read that:</p>\n\n<blockquote>\n  <p>Google\u2019s cars can detect and respond to stop signs that aren\u2019t on its map, a feature that was introduced to deal with temporary signs used at construction sites.</p>\n  \n  <p>Google says that its cars can identify almost all unmapped stop signs, and would remain safe if they miss a sign because the vehicles are always looking out for traffic, pedestrians and other obstacles.</p>\n</blockquote>\n\n<p>What would happen if a car spotted somebody in front of it (but not on the collision path) wearing a T-shirt that has a stop sign printed on it. Would it react and stop the car?</p>\n"}, "id": "1578"}, {"body": {"answer": "<p>Tesla model S has <a href=\"https://www.tesla.com/models\" rel=\"nofollow\">Autopilot</a> which allows to steer within a lane, change lanes with the simple tap of a turn signal, and can manage speed by using traffic-aware cruise control. Multiple digital controls helps to avoid collisions. Based on that, this isn't fully self-driving car.</p>\n\n<p>However it is using a computer vision detection system, but it is not intended to be used hands-free.</p>\n\n<p>So basically what is known is that the accident involved the side of a truck trailer (of a large white 18-wheel truck) and most likely the camera had a washed out of picture possibly due to glare or blooming from overexposure which made that the side of the trailer white and thin which failed to distinguish with the sky which was bright as well.</p>\n\n<p>This may have happened in part, because the crash-avoidance system only engage when both radar and vision system detect an obstacle which could not happen.</p>\n\n<p>Further more it was suggested by <em>The Associated Press</em> that the driver most likely was watching a <em>Harry Potter</em> at the time of the crash and assuming system would alert Brown, we don't know if he was able to retake controls quickly enough to avoid impact. As mentioned again, the system wasn't intended for hands-free driving and parts of the system was unfinished. Not to mention that the car was driving with full speed under the trailer.</p>\n\n<p>Tesla officially said about this crash in a statement on its website:</p>\n\n<blockquote>\n  <p><strong>The high ride height of the trailer combined with its positioning across the road and the extremely rare circumstances</strong> of the impact caused the Model S to pass under the trailer, with the bottom of the trailer impacting the windshield of the Model S.</p>\n  \n  <p>Neither Autopilot nor the driver noticed the white side of the tractor-trailer against a brightly lit sky, so the brake was not applied.</p>\n</blockquote>\n\n<p>They also said, according to techno-optimists, that they will tweaks their code, so this particular case won't happen again.</p>\n\n<p>To summarize, this was a 'technical failure' of braking system and most likely Autopilot was not at as Tesla told Senate.</p>\n\n<p><a href=\"http://i.stack.imgur.com/obdLM.png\" rel=\"nofollow\"><img src=\"http://i.stack.imgur.com/obdLM.png\" alt=\"The New York Times |Source: Florida traffic crash report\"></a></p>\n\n<p><sup>The New York Times |Source: Florida traffic crash report</sup></p>\n\n<p>Sources:</p>\n\n<ul>\n<li><a href=\"http://www.freep.com/story/money/cars/2016/07/01/tesla-autopilot-death-highlights-autonomous-risks/86591130/\" rel=\"nofollow\">Tesla Autopilot death highlights autonomous risks</a></li>\n<li><a href=\"https://robotfuturesbook.wordpress.com/2016/07/01/layers-of-autonomy\" rel=\"nofollow\">Layers of Autonomy</a></li>\n<li><a href=\"http://www.nytimes.com/interactive/2016/07/01/business/inside-tesla-accident.html\" rel=\"nofollow\">Inside the Self-Driving Tesla Fatal Accident</a></li>\n<li><a href=\"https://www.theguardian.com/technology/2016/jun/30/tesla-autopilot-death-self-driving-car-elon-musk\" rel=\"nofollow\">Tesla driver dies in first fatal crash while using autopilot mode</a></li>\n</ul>\n", "question": "<p>Do we know why Tesla's Autopilot mistaken empty sky with a high-sided lorry which resulted in fatal crash involving a car in self-drive mode? Was it AI fault or something else? Is there any technical explanation behind this why this happened?</p>\n\n<p>References: <a href=\"http://news.sky.com/story/tesla-driver-in-first-self-drive-fatal-crash-10330121\" rel=\"nofollow\">Sky News article</a>, <a href=\"http://www.theverge.com/2016/6/30/12072408/tesla-autopilot-car-crash-death-autonomous-model-s\" rel=\"nofollow\">The Verge</a>.</p>\n"}, "id": "1579"}, {"body": {"answer": "<p>Further to Franck's answer, there may be better optima (even global optima) that exist in the opposite direction to the gradient (which may be in the direction of some local optima). Evolutionary algorithms have scope to search the surrounding area, while backpropagation will always move in the direction of the gradient. With no guarantee (due to their randomness), evolutionary algorithms may be capable of finding solutions that backpropagation simply cannot.</p>\n", "question": "<p>How does employing evolutionary algorithms to design and train artificial neural networks have advantages over using the conventional backpropagation algorithms?</p>\n"}, "id": "1581"}, {"body": {"answer": "<p>For many years, the focus has been on games with perfect information. That is, in Chess and Go both of us are looking at the same board. In something like Poker, you have information that I don't have and I have information that you don't have, and so for either of us to make sense of each other's actions we need to model what hidden information the other player has, and <em>also</em> manage how we leak our hidden information. (A poker bot whose hand strength could be trivially determined from its bets will be easier to beat than a poker bot that doesn't.)</p>\n\n<p>Current research is switching to tackling games with imperfect information. Deepmind, for example, <a href=\"http://www.businessinsider.com/google-deepmind-could-play-starcraft-2016-3\">has said</a> they might approach Starcraft next.</p>\n\n<p>I don't see too much different between video games and board games, and there are several good reasons to switch to video games for games with imperfect information. </p>\n\n<p>One is that if you want beating the best human to be a major victory, there needs to be a pyramid of skill that human is atop of--it'll be harder to unseat the top Starcraft champion that the top Warcraft champion, even though the bots might be comparably difficult to code, just because humans have tried harder at Starcraft.</p>\n\n<p>Another is that many games with imperfect information deal with reading faces and concealing information, which an AI would have an unnatural advantage at; for multiplayer video games, players normally interact with each other through a server as intermediary and so the competition will be more normal.</p>\n", "question": "<p>Significant AI vs human board game matches include:</p>\n\n<ul>\n<li><strong>chess</strong>: <a href=\"https://en.wikipedia.org/wiki/Deep_Blue_(chess_computer)#Deep_Blue_versus_Kasparov\" rel=\"nofollow\">Deep Blue vs Kasparov</a> in 1996,</li>\n<li><strong>Go</strong>: <a href=\"https://en.wikipedia.org/wiki/AlphaGo_versus_Lee_Sedol\" rel=\"nofollow\">DeepMind AlphaGo vs Lee Sedol</a> in 2016,</li>\n</ul>\n\n<p>which demonstrated that AI challenged and defeated professional players.</p>\n\n<p>Are there known board games left where a human can still win against an AI? I mean based on the final outcome of authoritative famous matches, where there is still same board game where AI cannot beat a world champion of that game.</p>\n"}, "id": "1584"}, {"body": {"answer": "<p>It's not the language itself but rather the structure and for the language's ambiguity, for example in English: person a says \"John and Bob (his fish)\" person B says \"He died!\", posed question, to whom does person B refer to by he died.  More than the language, but the application. You can write a chat bot in Assembly, C, C++, C#, Java or Python. The all work a bit differently but can accomplish the same result, but one language might have more pros or cons to the other. So it will boil down to not language but the understanding of what is being said, research of language in the brain has come to confirm we associate a meaning/feeling/and other inputs with a given language. </p>\n\n<p>So to conclude: English is by far the most chaotic for a chat bot but Japanese is actually the best due to the way the language itself is written/spoken. There is more structure to it and less ambiguity.</p>\n\n<p>I'm a Software Engineer and An AI Researcher for the past 7 years.</p>\n", "question": "<p>Has there any research been done on how difficult certain languages are to learn for chatbots? \nFor example, CleverBot knows a bit of Dutch, German, Finnish and French, so there are clearly chatbots that speak other languages than English. (English is still her best language, but that is because she speaks that most often)</p>\n\n<p>I would imagine that a logical constructed language, like lobjan, would be easier to learn than a natural language, like English, for example.  </p>\n"}, "id": "1587"}, {"body": {"answer": "<p>Simply yes, but it can lead to over fixing of the NN.</p>\n\n<p>Humans favour not dying, which is only realised once a consequence is defined for the system to realise that death is an unfavorable result. Which can be train vai observation. Allow your system to observe between 2 or more separate people/systems. Then allow opportunity to test in a safe environment with the pre  existing info of the consequences that may follow, provind that if the system makes a mistake in the test/safe environment it will be saved  unknownly and then informed that it made a mistake, the place system in an unsafe world in same conditions, informing it that if something happens it will die. That is the way humans grow up, and we've lasted very long with this technic.</p>\n\n<p>I'm an AI Researcher and Software Engineer for the past 7 years.</p>\n", "question": "<p>By reinforcement learning, I don't mean the class of machine learning algorithms such as DeepQ, etc. I have in mind the general concept of learning based on rewards and punishment. </p>\n\n<p>Is it possible to create a Strong AI that does not rely on learning by reinforcement, or is reinforcement learning a requirement for artificial intelligence? The existence of rewards and punishment imply the existence of favorable and unfavorable world-states. Must intelligence in general and artificial intelligence in particular have a way of classifying world-states as favorable or unfavorable?  </p>\n"}, "id": "1588"}, {"body": {"answer": "<p>A neural network can be used but must be trained to expect the information (pattern of data, pixels or groupings of loose range such as color, and location) at any given location in the network, first a vision system must but implemented. Then a facial recognition, multiple partial individual body fixing (finding body part and there partners to a person) then training on some states and you'll have it work. MIT have done research and have made a seemy accurate implementation. </p>\n\n<p>I'm an AI Researcher and Software Engineer for the past 7 years.</p>\n", "question": "<p>For example I'd like to train my neural network to recognize the type of actions (e.g. in commercial movies or some real life videos), so I can \"ask\" my network in which video or movie (and at what frames) somebody was driving a car, kissing, eating, was scared or was talking over the phone.</p>\n\n<p>What are the current successful approaches to that type of problem?</p>\n"}, "id": "1589"}, {"body": {"answer": "<p>MIT have done research  and implemented an incomplete version of action video recognition.</p>\n\n<p>With the use of MATLAB, NNetworks and a large set of training videos.</p>\n\n<p>My suggested set of comments on my previous answer indicate the usage of a multi interconnected NNet, verus MIT's image based NNet.</p>\n", "question": "<p>For example I'd like to train my neural network to recognize the type of actions (e.g. in commercial movies or some real life videos), so I can \"ask\" my network in which video or movie (and at what frames) somebody was driving a car, kissing, eating, was scared or was talking over the phone.</p>\n\n<p>What are the current successful approaches to that type of problem?</p>\n"}, "id": "1590"}, {"body": {"answer": "<p>It's impossible to give a definitive 'yes' answer to your question, since that would require proving that alternatives <em>cannot</em> exist.</p>\n\n<p>More philosophically, it depends on what you mean by \"preference over world states\":</p>\n\n<p>However counter-intuitive it might seem, it is conceivably possible to create Strong AI purely from local condition-action rules, in which there is no global concept of 'preference value' and/or no integrated notion of 'world state'.</p>\n", "question": "<p>By reinforcement learning, I don't mean the class of machine learning algorithms such as DeepQ, etc. I have in mind the general concept of learning based on rewards and punishment. </p>\n\n<p>Is it possible to create a Strong AI that does not rely on learning by reinforcement, or is reinforcement learning a requirement for artificial intelligence? The existence of rewards and punishment imply the existence of favorable and unfavorable world-states. Must intelligence in general and artificial intelligence in particular have a way of classifying world-states as favorable or unfavorable?  </p>\n"}, "id": "1591"}, {"body": {"answer": "<p>Simply put, we don't know how to create Strong Artificial Intelligence yet, so we don't know what is or isn't required to create it.  At best we can engage in \"informed speculation\", in which case I'd say that the answer is more likely \"yes\" than \"no\".  But that's basically just a hunch.</p>\n\n<p>If you're interested in a pretty good overview of what \"pieces\" might be required to create Strong AI, and if you haven't read it yet, <a href=\"http://homes.cs.washington.edu/~pedrod/\" rel=\"nofollow\">Pedro Domingos</a>' book <em>The Master Algorithm</em> might be of interest. </p>\n", "question": "<p>By reinforcement learning, I don't mean the class of machine learning algorithms such as DeepQ, etc. I have in mind the general concept of learning based on rewards and punishment. </p>\n\n<p>Is it possible to create a Strong AI that does not rely on learning by reinforcement, or is reinforcement learning a requirement for artificial intelligence? The existence of rewards and punishment imply the existence of favorable and unfavorable world-states. Must intelligence in general and artificial intelligence in particular have a way of classifying world-states as favorable or unfavorable?  </p>\n"}, "id": "1594"}, {"body": {"answer": "<p>Yes, as mentioned in other answers, Prolog is actually used in IBM Watson.  Prolog doesn't get much \"hype\" and \"buzz\" these days, but it is absolutely still used.  As always, it has certain specific areas where it shines, and specific techniques that map well to its use.  Specifically, things like <a href=\"https://en.wikipedia.org/wiki/Inductive_logic_programming\" rel=\"nofollow\">Inductive Logic Programming</a>, <a href=\"https://en.wikipedia.org/wiki/Constraint_logic_programming\" rel=\"nofollow\">Constraint Logic Programming</a>, <a href=\"https://en.wikipedia.org/wiki/Answer_set_programming\" rel=\"nofollow\">Answer Set Programming</a> and some <a href=\"https://en.wikipedia.org/wiki/Natural_language_processing\" rel=\"nofollow\">NLP</a> applications may involve extensive use of Prolog.</p>\n", "question": "<p>According to <a href=\"http://en.wikipedia.org/wiki/Prolog\">Wikipedia</a>,</p>\n\n<blockquote>\n  <p>Prolog is a general-purpose logic programming language associated with artificial intelligence and computational linguistics.</p>\n</blockquote>\n\n<p>Is it still used for AI?</p>\n\n<hr>\n\n<p><sub>This is based off of a question on the 2014 closed beta. The author had the UID of 330.</sub></p>\n"}, "id": "1595"}, {"body": {"answer": "<p>'Personality' is something of a 'suitcase word' (Minsky) for quite a large collection of (presumably reasonably consistent) observable traits. </p>\n\n<p>It seems clear that there is a certain collective advantage in having a consistent personality - specifically that it affords observers some learning gradient in an otherwise uncertain environment. This is of particular importance because those consistencies might have been arrived at using different learning mechanisms than the ones a given observer has.</p>\n\n<p>Hence, in any non-trivial coevolutionary system, other organisms will inevitably make use of any such consistencies. Consider a simple robot, called Alice, say, that has the trait of 'quickly flashing red when it sees a blue robot'. It makes sense for all observers to exploit <em>everything</em> that they perceive as correlating with Alice's behavior, in particular, the prediction that a blue robot is likely to be present.</p>\n\n<p>The best reference I can recommend on this (which shows that we tend to ascribe 'personality' to even very simple mechanisms) is <a href=\"https://mitpress.mit.edu/books/vehicles\" rel=\"nofollow\">'Vehicles'</a> by Valentino Braitenberg.</p>\n", "question": "<p>The above question itself is perhaps too broad for this forum, hence I am phrasing it as a request for references.</p>\n\n<p>Humans have been endowed with personalities by nature, and it is not clear (to me at least) if this is a feature or a bug. This has been explored in science fiction by various notions of <a href=\"http://memory-alpha.org/Borg\" rel=\"nofollow\">Borg</a>-like entities. It is my belief that, for narrative reasons, such stories usually end with the humans with their flawed personalities winning in the end. </p>\n\n<p>Are there experts who have analyzed, perhaps mathematically, design criteria for an AI agent with weakly enforced goals (eg. to maximize reproduction in the human case) in an uncertain environment, and ended up with the answer that a notion of personality is useful? If there are philosophers or science fiction writers who have examined this question in their work, I would be happy to know about those too.</p>\n"}, "id": "1596"}, {"body": {"answer": "<p><strong>First, a note on the question itself.</strong></p>\n\n<blockquote>\n  <p>Humans have been endowed with personalities by nature, and it is not clear (to me at least) if this is a feature or a bug. </p>\n</blockquote>\n\n<p>In my opinion, this is a statement that constrains the question, since it assumes that the personality is <em>given</em>. To me, it feels a bit like <em>playing god</em>: Artificial (given) Intelligence would hence imply Artificial (given) Personality. This approach to the problem seems to be supported by the next fragment:</p>\n\n<blockquote>\n  <p>a notion of personality is useful</p>\n</blockquote>\n\n<p>I point to the above because I don't think that artificial intelligence... <strong><em>Intelligence</em></strong> itself, actually, need to be given or assigned, or even have a <em>use</em> in the sense of a <em>purpose</em>. </p>\n\n<hr>\n\n<p><strong>The previous note</strong> was about <em>emergence</em>, which is a topic that <a href=\"http://ai.stackexchange.com/users/42/user217281728\">user217281728</a> briefly addressed in <a href=\"http://ai.stackexchange.com/a/1596/70\">their answer</a>. In this second approach, the particular traits <em>just happen</em>, or <em>develop</em>. The interaction between the (so-called) agents and their environment, as well as fellow agents can give place to new behaviour patterns, not designed beforehand. </p>\n\n<p><strong>In an evolutionary</strong> approach, if the personality would happen to have an advantage (or at least not represent a disadvantage), then it could just appear. Of course, I am making a number of assumptions and demarcations here as well:</p>\n\n<ul>\n<li>I am thinking about embodied intelligence</li>\n<li>I speak of evolutionary robotics</li>\n<li>I think on social issues being of importance</li>\n<li>I assume that <em>personality</em> could emerge</li>\n</ul>\n\n<p><strong>Now, an example</strong> that I find extremely interesting is that of the little mobile robots which could move around and end-up in a pool of <em>food</em> or a pool of <em>poison</em>. And they, somehow, by some odd chance, recognised or made a relation between signals sent by other robots, and the presence of food. Or not. That was more or less the thing: Some robots (kind of) learned to <em>conceal</em> information and thus had more time to eat themselves. Well, I would have a couple of personality adjectives for such guys.</p>\n\n<p><a href=\"http://www.pnas.org/content/106/37/15786.full\" rel=\"nofollow\">Here you find the article</a> and <a href=\"https://wp.unil.ch/mitrilab/\" rel=\"nofollow\">here you find some videos</a> and related stuff.</p>\n\n<p><strong>And with that</strong>, we land at my last point: <a href=\"https://en.wikipedia.org/wiki/Anthropomorphism#In_computing\" rel=\"nofollow\">We humans</a> put the adjectives, according to our social conditioning. We call <a href=\"https://en.wikipedia.org/wiki/Marvin_(character)\" rel=\"nofollow\">Marvin</a> <em>depressive</em> and <a href=\"http://www.smithsonianmag.com/arts-culture/why-do-we-love-r2-d2-and-not-c-3po-180951176/\" rel=\"nofollow\">R2D2</a> lovely and charming. </p>\n\n<p>If they perceive their personalities as constructive or damaging, will always depend on our own judgment. In the end, it is quite common under humans to disagree on personality issues, too.</p>\n\n<hr>\n\n<p><strong>Bonus</strong></p>\n\n<p>Remember when <a href=\"https://www.youtube.com/watch?v=UgkyrW2NiwM\" rel=\"nofollow\">HAL got emotional</a>, on the face of death?</p>\n\n<p>It gets <em>human</em> when it loses its cool, before the flawed-personality human astronaut :)</p>\n", "question": "<p>The above question itself is perhaps too broad for this forum, hence I am phrasing it as a request for references.</p>\n\n<p>Humans have been endowed with personalities by nature, and it is not clear (to me at least) if this is a feature or a bug. This has been explored in science fiction by various notions of <a href=\"http://memory-alpha.org/Borg\" rel=\"nofollow\">Borg</a>-like entities. It is my belief that, for narrative reasons, such stories usually end with the humans with their flawed personalities winning in the end. </p>\n\n<p>Are there experts who have analyzed, perhaps mathematically, design criteria for an AI agent with weakly enforced goals (eg. to maximize reproduction in the human case) in an uncertain environment, and ended up with the answer that a notion of personality is useful? If there are philosophers or science fiction writers who have examined this question in their work, I would be happy to know about those too.</p>\n"}, "id": "1597"}, {"body": {"answer": "<p>If you pick up a textbook on Neural Networks, you'll find that the simplest examples shown are ones that just implement an AND gate or something.  They're trivial, probably fewer lines of code than what you have there.  The bar to be an \"artificial neural network\" is pretty low... it certainly isn't the case that ANN's <em>must</em> be incredibly complicated with thousands of lines of code, and many layers, or even many \"neurons\" total.</p>\n\n<p>Basically, if something is setting up at least one \"neuron\" with multiple inputs, and using some kind of weighting function to generate an output from those inputs, it's a valid ANN. It might be a <em>really</em> simple example of an ANN, but it's still an ANN.</p>\n\n<p>Remember what Geoffrey Hinton says in his <em>Coursera</em> class - (paraphrased) \"We don't pretend that the things we're building really work the way the brain does, we're just taking the brain as loose inspiration for an approach that we've found works\".</p>\n", "question": "<p>I've found this short <a href=\"http://iamtrask.github.io/2015/07/12/basic-python-network/\" rel=\"nofollow\">Python code</a> which implements neural network in 11 lines of code:</p>\n\n<pre><code>X = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])\ny = np.array([[0,1,1,0]]).T\nsyn0 = 2*np.random.random((3,4)) - 1\nsyn1 = 2*np.random.random((4,1)) - 1\nfor j in xrange(60000):\n    l1 = 1/(1+np.exp(-(np.dot(X,syn0))))\n    l2 = 1/(1+np.exp(-(np.dot(l1,syn1))))\n    l2_delta = (y - l2)*(l2*(1-l2))\n    l1_delta = l2_delta.dot(syn1.T) * (l1 * (1-l1))\n    syn1 += l1.T.dot(l2_delta)\n    syn0 += X.T.dot(l1_delta)\n</code></pre>\n\n<p>I believe it may be a valid implementation of neural network, but how do I know?</p>\n\n<p>In other words, is just creating bunch of arrays which compute the output on certain criteria and call them layers with synapses does it make proper neural network?</p>\n\n<p>In other words, I'd like to ask, what features/properties makes a valid artificial neural network?</p>\n"}, "id": "1599"}, {"body": {"answer": "<p>The rendering process for browsers is <a href=\"https://www.w3.org/\">very well defined</a>, and has a very rigid definite ruleset where (virtually) every accountability is noted and handled. This is not optimal for Machine Learning, which works when we have a large pool of examples, and we don't know the ruleset; it will figure it out. Even if you were to train an Neural Network to process that input, there are several things you must account for:</p>\n\n<p><strong>1. Variance in data.</strong></p>\n\n<p>Not all webpages are equal in length or complexity, and making a neural network to generate output from HTML would produce garbage most of the time.</p>\n\n<p><strong>2. Training time.</strong></p>\n\n<p>The time it would take for a neural network to understand HTML tags, attributes, the DOM Tree, and each and every element, including new ones being added every few years, and how each one renders and behaves, would take an extremely long time, most likely several years on a fast computer, if it even were possible</p>\n\n<p><strong>3. Interactivity.</strong></p>\n\n<p>Web pages aren't just static, they change according HTML, CSS and JavaScript. Not only would you have to design your system to account for the rendering step, you would also make it have to understand the <a href=\"https://en.wikipedia.org/wiki/Turing_completeness\">Turing Complete</a> scripting language <a href=\"https://en.wikipedia.org/wiki/JavaScript\">JavaScript</a>, as well as the less complicated, but inherently intertwined with HTML, CSS stylesheet language. If you thought the rendering process was easy, try training a neural network to handle complicated scripting patterns.</p>\n\n<p><strong>4. New Standards</strong></p>\n\n<p>Not all HTML is equal, because of different standards. <a href=\"https://en.wikipedia.org/wiki/Web_Hypertext_Application_Technology_Working_Group\">WHATWG</a> began working on HTML5 in 2004, and browsers started to implement not long after. In 2004, there were very few examples of HTML5 sites to train your network to begin with. Sure, now it's standardized and every website uses it, but what about HTML6? When the first specification is released (probably 2017-2025), virtually no websites will use it, because no one will support it. Only when it finally becomes standard, probably in the late 2020s or early 2030s, will you have enough data to train your monstrous system of neural networks</p>\n\n<hr />\n\n<p>As for AI in general, one could argue that browsers already use A.I. in their rendering process. They intelligently decide what to render (taking CSS into account), when in order to get the most efficient render time, they selectively use different JavaScript parsers on different sections of the code to optimize the speed, the whole system has been optimized on another ruleset to make rendering and interacting with a webpage as seamless and easy-to-use as possible. Your system will never be as good as what hundreds of humans have optimized over 20 years.</p>\n\n<p>Trying to solve HTML rendering with Neural Networks is akin to trying to nail a nail with a screwdriver. It's just not going to work</p>\n\n<p>Hope this was helpful!</p>\n", "question": "<p>I'm wondering, instead of implementing new web browsers over and over again with millions line of code which is very difficult to manage, would it be possible to use ANN or GA algorithm to teach it about the rendering process (how the page should look like)?</p>\n\n<p>So as an input I would imaging the html source code, output is the rendered page (maybe in some interactive image like SVG, some library or something, I'm not sure).</p>\n\n<p>The training data can be dataset of websites providing input source code and their rendered representation by using other browsers for the guidance as expected output.</p>\n\n<p>Which approach would you take and what are the most challenging things you can think of?</p>\n"}, "id": "1607"}, {"body": {"answer": "<p>One popular technique for doing this is to use <a href=\"http://www.artificial-immune-systems.org/\">Artificial Immune Systems</a>, an evolutionary computation approach which maintains a population of pattern detectors. </p>\n\n<p>Here is a <a href=\"https://arxiv.org/ftp/arxiv/papers/0804/0804.1266.pdf\">survey paper</a>.</p>\n", "question": "<p>I'm looking for research which discusses misbehavior detection in public internet access networks using ANN approaches.</p>\n\n<p>So it can be used by <a href=\"https://en.wikipedia.org/wiki/Internet_service_provider\" rel=\"nofollow\">ISP</a> to detect suspicious users connected to their network.</p>\n"}, "id": "1608"}, {"body": {"answer": "<p>As for AGI , Everything is broken down into groups. The are all controlled by a part called the \"Spark\", and then there are the agents, little sub routines. The SPARK is the is the main judge of the system. It compares the performance of the agents. The Spark lets one agent out of sleep and records how well it does at getting reward for the body, as a whole. If a active agent does good it is replicated in free memory with a few mutations.\nThe 'SPARK\" and agents first look at what is on the detectors and SPARK select the best agent. And the SPARK turn on and off agents like in a orchestra. As the system matures many agents will work in parallel.\nThis process is the the subconscious mind.\nAfter a while one of the agent is converted to a copy of of SPARK. This new copy is then modified and is called the OFF SPARK. The conscious mind. It will take on the control of the agents too. But SPARK is still master of all.\nOFF SPARK can activate agents. But it can organizes agents on a massive scale.\nIt will develop many routines. Many agents working in parallel.\nOnce theses massive agent swarm developed into perfected routine will become a reflex. And all reflexes will be given over to SPARK the subconscious part of this system. Stored for latter use.\nIf OFF SPARK needs to get over to a new areas to create new patterns, like working the slot machine for the first time, to develop new routines. it need automated subconscious process of walking over the that area. OFF SPARK starts the walking routine and hands off over to spark.\nIF the craving for food becomes too strong then SPARK shuts down OFF SPARK. And then uses OFF SPARK routines to get food or what urgent goal that need to be taken care of.\nIf all urgent goals are taken care of then there will be free will because OFF SPARK will be in control. And SPARK will be push into helper mode.\nThe AGI has a internal pattern editor, that is only used by OFF SPARK. that cut up existing physical routines and tries to rebuild new and different routines. This is a trial and error generator. Once a editing procedure work and perfected it is handed over to Spark for storage. Off SPAK initiate a editing routines and then SPark take over.\nThis editing of patterns will lead to a internal 3D simulator.\nSO OFF SPARK make new physical routines and new pattern editing routines. And all this is done on the backs of automated perfected routine of old.</p>\n", "question": "<p>The above question itself is perhaps too broad for this forum, hence I am phrasing it as a request for references.</p>\n\n<p>Humans have been endowed with personalities by nature, and it is not clear (to me at least) if this is a feature or a bug. This has been explored in science fiction by various notions of <a href=\"http://memory-alpha.org/Borg\" rel=\"nofollow\">Borg</a>-like entities. It is my belief that, for narrative reasons, such stories usually end with the humans with their flawed personalities winning in the end. </p>\n\n<p>Are there experts who have analyzed, perhaps mathematically, design criteria for an AI agent with weakly enforced goals (eg. to maximize reproduction in the human case) in an uncertain environment, and ended up with the answer that a notion of personality is useful? If there are philosophers or science fiction writers who have examined this question in their work, I would be happy to know about those too.</p>\n"}, "id": "1609"}, {"body": {"answer": "<p>You're going to need some way to 'see' the area around the car, and to track the speed of nearby objects. Google uses a combination of <a href=\"https://en.wikipedia.org/wiki/Lidar\" rel=\"nofollow\">LIDAR</a>, radar, conventional cameras, and occasionally sonar (see <a href=\"http://www.makeuseof.com/tag/how-self-driving-cars-work-the-nuts-and-bolts-behind-googles-autonomous-car-program\" rel=\"nofollow\"> here</a> for a high-level overview). This technology is quite expensive, and can easily cost thousands of US dollars.</p>\n\n<p>However, a bigger obstacle than the expense of the hardware (which would be smaller for a table-top prototype) is the software complexity. Like many major projects, the software for self-driving cars is the result of years of work from AI research teams, and thus extremely difficult to duplicate on your own.</p>\n\n<p>That said, you're not trying to make a state-of-the-art self-driving car.  Assuming you're an expert in image processing and robotics, you can probably create a basic prototype, (like something that drive in a limited table-top environment). However, it's still going to take a lot of time and money. </p>\n", "question": "<p>Google, Tesla, Apple etc have all built or are building their own self-driving cars. As an expert in a related area, I am interested in knowing at a high level, the systems and techniques that go into self-driving cars. How easy is it for me to make a tabletop prototype (large enough to accomodate the needed computing power needs)?</p>\n"}, "id": "1610"}, {"body": {"answer": "<p><a href=\"https://en.wikipedia.org/wiki/Emotion\" rel=\"nofollow\">Emotions</a> aren't something that you can implement - they're very complex. However, you can attempt to mimic them. Human emotions are closely related to conscious experience characterized by intense mental activity, which is based on interpretation of events.</p>\n\n<p>Recent brain studies (including research in cognitive psychology and neurophysiology) suggests that human emotional assessment of every action or event plays an important role in human mental processes.</p>\n\n<p>The recent <a href=\"http://bica2016.bicasociety.org/\" rel=\"nofollow\">2016 Annual Meeting of the BICA Society</a> brought together scientists from around the world to approach principles and mechanisms of human thought to create biologically inspired AI.</p>\n\n<p>For example, in Samsonovich's (a professor in the Cybernetics Department at the <a href=\"https://en.wikipedia.org/wiki/National_Research_Nuclear_University_MEPhI\" rel=\"nofollow\">MEPhI</a>) proposal, the idea is to test AI in computer games which involves actions with emotional content, where AI may engage with players in different types of social relationships (such as trust, subordination or leadership).</p>\n\n<p>Jonathan Gratch of the <a href=\"https://en.wikipedia.org/wiki/Institute_for_Creative_Technologies\" rel=\"nofollow\">ICT</a>, invented virtual characters capable of identifying and expressing emotions by communicating with humans in their natural language based on the situations where for example AI can deceive a human to achieve the desired result. The effect is obviously not achieved by re-creating human consciousness, but by achieving statistically adjusting parameters.</p>\n\n<p>Researchers from the Institute of Cyber Intelligence Systems in MEPhI are hoping to be able to create in the near future future virtual beings which are capable of planning, setting goals and establishing social relationships with humans, also by possessing both emotional and narrative intelligence which can interpret context of events.</p>\n\n<p><sup>Source: <a href=\"http://phys.org/news/2016-07-social-emotions-artificial-intelligence.html\" rel=\"nofollow\">Researcher proposes social emotions test for artificial intelligence</a></sup></p>\n", "question": "<p>I've seen emotional intelligence defined as the capacity to be aware of, control, and express one's emotions, and to handle interpersonal relationships judiciously and empathetically.  What are some strategies for artificial intelligence to begin to tackle this problem and develop emotional intelligence for computers?  Are there examples where this is already happening to a degree today?  For example, wouldn't a computer that passes a Turing test necessarily express emotional intelligence or it would be seen as an obvious computer?  Perhaps that is why early programs that pass the test represented young people, who presumably have lower emotional intelligence.</p>\n"}, "id": "1612"}, {"body": {"answer": "<p>I wouldn't focus <em>only</em> on \"deep learning\" unless you have some specific reason for doing so.  There may be other techniques which could be as effective, or more effective.  </p>\n\n<p>One approach I've seen used for something similar was <a href=\"https://en.wikipedia.org/wiki/Inductive_logic_programming\" rel=\"nofollow\">Inductive Logic Programming</a>.  For one example of using ILP to reason about elements of biochemistry, see <a href=\"https://www.researchgate.net/publication/224309517_Estimation_of_Possible_Reaction_States_in_Metabolic_Pathways_Using_Inductive_Logic_Programming\" rel=\"nofollow\">this paper</a></p>\n\n<p>That's not exactly about detecting disease, but it does sort of illustrate the broad idea of reasoning about states and reactions involving metabolic pathways in biochemistry, using ILP.   Possibly the basic idea could be adapted more towards detecting disease.  </p>\n", "question": "<p>It has been <a href=\"http://www.itnonline.com/content/will-fda-be-too-much-intelligent-machines\" rel=\"nofollow\">suggested</a> that machine learning algorithms (also <a href=\"http://ai.stackexchange.com/q/1427/8\">Watson</a>) can help with finding disease in patient images and optimize scans. Also that deep learning algorithms show promise for every type of digital imaging.</p>\n\n<p>How does exactly deep learning algorithms exactly can find suspicious patterns in the body\u2019s biochemistry?</p>\n"}, "id": "1615"}, {"body": {"answer": "<p>This seems to fall broadly into the regime of a <a href=\"https://en.wikipedia.org/wiki/Statistical_classification\" rel=\"nofollow\">classification problem</a> as you want to classify an outgoing communication as \"contains proprietary information\" or \"does not contain proprietary information\".  As such, any classification approach could be applied.  Neural Networks certainly seem like a valid approach, but you might also get good mileage out of Random Forests, Support Vector Machines, a Naive Bayes classifier, etc.</p>\n\n<p>GA's are more aimed towards optimization than classification, so I wouldn't say that a GA, in and of itself, would map cleanly to solving this kind of problem. If GA's had applicability here, I think it would be more likely to be in terms of training a model rooted in one of the other techniques.     </p>\n", "question": "<p>I'm investigating applications of AI algorithms which can be used for data leakage detection and prevention within an intranet network (like <a href=\"https://en.wikipedia.org/wiki/Forcepoint\" rel=\"nofollow\">Forcepoint</a>). More specifically detecting traffic patterns. I'm new to this.</p>\n\n<p>Which learning algorithms are most suitable for this goal? <a href=\"https://en.wikipedia.org/wiki/Evolutionary_algorithm\" rel=\"nofollow\">EA</a>, <a href=\"https://en.wikipedia.org/wiki/Genetic_algorithm\" rel=\"nofollow\">GA</a>, <a href=\"https://en.wikipedia.org/wiki/Artificial_neural_network\" rel=\"nofollow\">ANN</a> (which one) or something else?</p>\n"}, "id": "1616"}, {"body": {"answer": "<p>The machine learning is a sub-set of artificial intelligence which is only a small part of its potential. It's a specific way to implement AI largely focused on statistical/probabilistic techniques and evolutionary techniques.<sup><a href=\"https://www.quora.com/What-are-the-main-differences-between-artificial-intelligence-and-machine-learning/answer/Phillip-Rhodes\" rel=\"nofollow\">Q</a></sup></p>\n\n<h3>Artificial intelligence</h3>\n\n<p>Artificial intelligence is '<strong>the theory and development of computer systems able to perform tasks normally requiring human intelligence</strong>' (such as visual perception, speech recognition, decision-making, and translation between languages).</p>\n\n<p>We can think AI as concept of non-human decision making<sup><a href=\"https://www.quora.com/What-are-the-main-differences-between-artificial-intelligence-and-machine-learning/answer/Yuval-Ariav\" rel=\"nofollow\">Q</a></sup> which aims to simulate cognitive human-like functions such as problem solving, decision making or language communication.</p>\n\n<h3>Machine learning</h3>\n\n<p>Machine learning (ML) is basically <strong>a learning through doing</strong> by implementation of build models which can predict and identify patterns from data.</p>\n\n<p>According to Prof. <a href=\"http://www.cs.colby.edu/srtaylor/\" rel=\"nofollow\">Stephanie R. Taylor</a> of Computer Science and her <a href=\"http://cs.colby.edu/courses/S15/cs251/LectureNotes/Lecture_15_MLandDMintro_03_09_2015.pdf\" rel=\"nofollow\">lecture paper</a>, and also <a href=\"https://en.wikipedia.org/wiki/Learning#Machine_learning\" rel=\"nofollow\">Wikipedia page</a>, 'machine learning is a branch of artificial intelligence and <strong>it's about construction and study of systems that can learn from data</strong>' (like based on the existing email messages to learn how to distinguish between spam and non-spam).</p>\n\n<p>According to <a href=\"http://www.oxforddictionaries.com/definition/english/machine-learning\" rel=\"nofollow\">Oxford Dictionaries</a>, the machine learning is '<strong>the capacity of a computer to learn from experience</strong>' (e.g. modify its processing on the basis of newly acquired information).</p>\n\n<p>We can think ML as computerized pattern detection in the existing data to predict patterns in future data.<sup><a href=\"https://www.quora.com/What-are-the-main-differences-between-artificial-intelligence-and-machine-learning/answer/Yuval-Ariav\" rel=\"nofollow\">Q</a></sup></p>\n\n<hr>\n\n<p>In other words, <strong>machine learning involves development of self-learning algorithms</strong> and <strong>artificial intelligence involves developing systems or softwares</strong> to mimic human to respond and behave in a circumstance.<sup><a href=\"https://www.quora.com/What-are-the-main-differences-between-artificial-intelligence-and-machine-learning/answer/Sakthi-Dasan-2\" rel=\"nofollow\">Quora</a></sup></p>\n", "question": "<p>These two terms seem to be related, especially in their application in computer science and software engineering.  Is one a subset of another?  Is one a tool used to build a system for the other?  What are their differences and why are they significant?</p>\n"}, "id": "1621"}, {"body": {"answer": "<p>Section 4.2 of <a href=\"https://cs.gmu.edu/~sean/book/metaheuristics/\">\"Essentials of Metaheuristics\"</a> has a wealth of information on alternative ways of encoding graph structures via Genetic Algorithms.</p>\n\n<p>With particular regard to evolving ANNs, I would personally not be inclined to implement this sort of thing 'from scratch':</p>\n\n<p>The field of neuroevolution has been around for some time, and the implementation some of the methods, such as Neuroevolution of Augmenting Topologies (<a href=\"http://www.cs.ucf.edu/~kstanley/neat.html\">NEAT</a>) now incorporate the results of much practical experience.</p>\n\n<p>According to the above link:</p>\n\n<blockquote>\n  <p>We also developed an extension to NEAT called HyperNEAT that can evolve neural networks with millions of connections and exploit geometric regularities in the task domain. The HyperNEAT Page includes links to publications and a general explanation of the approach.</p>\n</blockquote>\n", "question": "<p><sub>This is from the 2014 closed beta. The asker had the UID of 245.</sub></p>\n\n<p>For a deterministic problem space, I need to find a neural network with the optimal node and link structure. I want to use a genetic algorithm to simulate many neural networks to find the best network structure for the problem domain.</p>\n\n<p>I know a fair amount about neural networks<sup>1</sup> but have not used genetic algorithms for a task like this before.</p>\n\n<p>What are the practical considerations? \nHow should I encode the structure into a genome?</p>\n\n<hr>\n\n<p><sub><sup>1</sup>Actually, I don't. Just saying that. -Mithrandir. </sub></p>\n"}, "id": "1626"}, {"body": {"answer": "<p>Humans have poor understanding of emotional rules. Probably every poster on here has experienced <em>greatly</em> misreading another individual emotionally. Further, people often don't act emotionally how they would expect themselves to act, for example we have all experienced frustration at someone else's irrational concerns and yet we are all guilty of holding irrational concerns of our own. This is the crux of why hard-coded emotional rules do not work - we do not have an understanding of what emotional 'rules' make someone feel real.</p>\n\n<p>By moving towards autonomous state-based actors we move away from this issue. The actor's state transitions will of course be defined rules (hard-coded or learnt) but by abstracting the actor's emotional state from the specific context (e.g. specific actions trigger emotional state transitions which trigger responses, instead of a direct response to a specific action), the programmer prevents them-self from projecting their own beliefs/emotions/logic onto the actor. </p>\n\n<p>Further, autonomous actors are more extendable. Consider an autonomous actor that is hard-coded to move towards 'upset' and 'angry' emotional states when experiencing 'pain'. Simply by associating a new world action with 'pain', one can trigger an emotional response from an autonomous actor that has not experienced that action before. When working in hard-coded emotional rules, this would not be possible.</p>\n", "question": "<p>The <a href=\"https://www.youtube.com/watch?v=AplG6KnOr2Q\" rel=\"nofollow\">Mario Lives!</a> video (and its follow-up video, <a href=\"https://www.youtube.com/watch?v=ltPj3RlN4Nw&amp;list=PLuOoXrWK6Kz5ySULxGMtAUdZEg9SkXDoq&amp;index=5\" rel=\"nofollow\">Mario Becomes Social!</a>) showcases an AI unit that is able to simulate emotional desicion-making within a virtual world, and can enter into \"emotional states\" such as curiosity, hunger, happiness, and fear. While this seems cool and exciting (especially for video game AI), I am confused how this would be useful in real-world scenarios.</p>\n\n<p>What would be the point of building autonomous actors that would behave based on these emotional states, instead of simply knowing <em>what</em> they should do (either by hardcoding in the rules, or learning the rules through machine learning)?</p>\n"}, "id": "1627"}, {"body": {"answer": "<p>The 2016 finals haven't started yet, they will start on Saturday, 17 September 2016. </p>\n\n<p>In the 2015 finals or before that, nobody won the Gold Medal or the Silver Medal. \nThe most up-to-date data can be found <a href=\"http://www.aisb.org.uk/events/loebner-prize\" rel=\"nofollow\">here</a>, where we can find both the results from 2015 and the timeline of the 2016 contest. </p>\n", "question": "<p>I've read about The Loebner Prize for AI, which pledged a Grand Prize of $100,000 and a Gold Medal for the first computer whose responses were indistinguishable from a human's.</p>\n\n<p>So I was wondering whether any chatbots have fooled the judges and won a Gold Medal yet?</p>\n\n<p>From their <a href=\"http://www.loebner.net/Prizef/loebner-prize.html\" rel=\"nofollow\">website</a> this isn't clear (as some of the links doesn't load).</p>\n\n<hr>\n\n<p>A few highlights from previous years:</p>\n\n<p><a href=\"http://loebner.exeter.ac.uk/results/\" rel=\"nofollow\">2011 Loebner Prize results</a></p>\n\n<blockquote>\n  <p>None of the AI systems fooled the judges, therefore the Turing Test has not been passed.</p>\n</blockquote>\n\n<p><a href=\"http://www.paulmckevitt.com/loebner2013/scoring/loebner2013leaderboard.txt\" rel=\"nofollow\">Loebner 2013 results</a>:</p>\n\n<blockquote>\n  <p>No chatbot fooled any of the 4 Judges.</p>\n</blockquote>\n"}, "id": "1629"}, {"body": {"answer": "<p>What 'infinite' means here could possibly be debated at some length, but that notwithstanding, here are two conflicting answers:</p>\n\n<p>'Yes': Simulate all possible universes. Stop when you get to one containing a flavor of intelligence that passes whatever test you have in mind. Steven Wolfram has suggested something <a href=\"https://www.inverse.com/article/12838-stephen-wolfram-could-there-be-alien-intelligence-among-the-digits-of-pi\" rel=\"nofollow\">broadly along these lines</a>. Problem: the state of computational testing for intelligence <a href=\"https://en.wikipedia.org/wiki/Winograd_Schema_Challenge\" rel=\"nofollow\">e.g. Winograd schema</a> would then be the bottleneck. In the limit, testing for intelligence requires intelligence and creativity on behalf of the questioner.</p>\n\n<p>'No': It may be that, even with infinite ability to simulate, there may be some missing aspect of our simulation that is necessary for intelligence. For example, AFAIK quantum gravity (for which we lack an adequate theory) is involved in Penrose's <a href=\"https://www.sciencedaily.com/releases/2014/01/140116085105.htm\" rel=\"nofollow\">\"Quantum Microtubules\"</a> theory of consciousness (*). What if that was needed, but we didn't know how to include it in the simulation?</p>\n\n<p>The reason for talking in terms of such incredibly costly computations as 'simulate all possible universes' (or at least a brain-sized portion of them) is to deliberately generalize away from the specifics of any techniques currently in vogue (DL, neuromorphic systems etc). The point is that we could be missing something essential for intelligence from <em>any</em> of these models and (as far as we know from our current theories of physical reality) only empirical evidence to the contrary would tell us otherwise.</p>\n\n<p>(*) No-one knows if consciousness is required for Strong AI, and physics can't distinguish a conscious entity from a <a href=\"http://plato.stanford.edu/entries/zombies/\" rel=\"nofollow\">Zombie</a>.</p>\n", "question": "<p>Hypothetically, assume that you have access to infinite computing power. Do we have designs for any brute-force algorithms that can find an AI capable of passing traditional tests (e.g. Turing, Chinese Room, MIST, etc.)? </p>\n"}, "id": "1631"}, {"body": {"answer": "<p>The most challenging part is this section of the first law:</p>\n\n<blockquote>\n  <p>or through inaction allow a human being to be harmed</p>\n</blockquote>\n\n<p>Humans manage to injure themselves unintentionally in all kinds of ways all the time. A robot strictly following that law would have to spend all its time saving people from their own clumsiness and would probably never get any useful work done. An AI unable to physically move wouldn't have to run around, but it would still have to think of ways to stop all accidents it could imagine. </p>\n\n<p>Anyway, fully implementing those laws would require very advanced recognition and cognition. (How do you know that industrial machine over there is about to let off a cloud of burning hot steam onto that child who wandered into the factory?) Figuring out whether a human would end up harmed after a given action through some sequence of events becomes an exceptionally challenging problem very quickly.</p>\n", "question": "<p>Would it be possible to put Asimov's three Laws of Robotics into an AI?</p>\n\n<p>The three laws are:</p>\n\n<ol>\n<li><p>A robot (or, more accurately, an AI) cannot harm a human being, or through inaction allow a human being to be harmed<sup>1</sup></p></li>\n<li><p>A robot must listen to instructions given to it by a human, as long as that does not conflict with the first law.</p></li>\n<li><p>A robot must protect its own existence, if that does not conflict with the first two laws.</p></li>\n</ol>\n\n<hr>\n\n<p><sup>1</sup> <em>To it's knowledge</em>. This was a plot point in one of the books :P</p>\n"}, "id": "1638"}, {"body": {"answer": "<p>Defining \"harm\" and in particular, \"allowing harm via inaction\" in any meaningful way would be difficult. For example, should robots spend all their time flying around attempting to prevent humans from inhaling passive smoke or petrol fumes?</p>\n\n<p>In addition, the interpretation of 'conflict' (in either rule 2 or 3) is completely open-ended. Resolving such conflicts seems to me to be \"AI complete\" in general.</p>\n\n<p>Humans have quite good mechanisms (both behavioral and social) for interacting in a complex world (mostly) without harming one another, but these are perhaps not so easily codified. The complex set of legal rules that sit on top of this (polution regulations etc) are the ones that we could most easily program, but they are really quite specialised relative to the underlying physiological and social 'rules'.</p>\n\n<p>EDIT: From other comments, it seems worth distinguishing between 'all possible harm' and 'all the kinds of harm that humans routinely anticipate'. There seems to be consensus that 'all possible harm' is a non-starter, which still leaves the hard (IMO, AI-complete) task of equaling human ability to predict harm. </p>\n\n<p>Even if we can do that, if we are to treat as actual laws, then we would still need a formal mechanism for conflict resolution (e.g. \"Robot, I will commit suicide unless you punch that man\"). </p>\n", "question": "<p>Would it be possible to put Asimov's three Laws of Robotics into an AI?</p>\n\n<p>The three laws are:</p>\n\n<ol>\n<li><p>A robot (or, more accurately, an AI) cannot harm a human being, or through inaction allow a human being to be harmed<sup>1</sup></p></li>\n<li><p>A robot must listen to instructions given to it by a human, as long as that does not conflict with the first law.</p></li>\n<li><p>A robot must protect its own existence, if that does not conflict with the first two laws.</p></li>\n</ol>\n\n<hr>\n\n<p><sup>1</sup> <em>To it's knowledge</em>. This was a plot point in one of the books :P</p>\n"}, "id": "1639"}, {"body": {"answer": "<p>Current approaches for learning a language require having a large corpus of that language; it also doesn't seem reasonable to expect that it will ever be possible to learn about language A by extracting information from a corpus from an unrelated language B.</p>\n\n<p>Even if you want to learn about human languages in general (what sorts of things are true about grammar, vocabulary, and so on), that relies having many languages as training data, so that you can see the different ways of doing things instead of assuming that the way they're done in English is the way they're done in every language.</p>\n\n<p>(There is work in automatic translation that goes from a language to 'concept-space', then goes from that 'concept-space' to another language, so that you can build an English-Chinese translator by building two separate English-Concept and Chinese-Concept translators, instead of ever needing material that directly links English and Chinese. The obvious benefit of this is scalability; in order to make translators for a new language to any other language, you just need to learn that language and the models build themselves.)</p>\n", "question": "<p>I'm aware this could be a complex topic, however I'm interested in existing research projects or studies where people are attempting or have succeeded in teaching an AI a foreign language just by training/teaching it from English books. By reading, analysing and understanding, so that it knows the foreign language's rules (such as grammar, spelling, etc.), the same way as a human would learn. The language doesn't have to be Chinese, which is difficult for even humans to learn.</p>\n"}, "id": "1640"}, {"body": {"answer": "<p>We're definitely nowhere near that level of AI; at best, high-tech solutions like deep convolutional neural nets can help with image recognition and some other algorithms can perform things like robotic movement adequately enough to be useful in some scenarios. None of this is even as sophisticated as the behavior of a flea, but no one refers to insects as \"intelligent.\" It's exciting stuff that allows us to solve problems that human intelligence often has difficulty with (such as classification of thousands of objects, which would tire an ordinary human mind), but it's nowhere close to replicating our higher brain functions. </p>\n\n<p>Also keep in mind that the Turing test is a poor test of \"intelligence\" that defies common sense. By the same extension, mistaking a mannequin for a human being in the dark does not mean that the mannequin is actually human. If it were a valid test, then we passed that way back around 1980 with programs like Dear Eliza which were coded in BASIC to regurgitate human speech patterns. There's just no need to come up with a sophisticated argument like Searle's Chinese Room to debunk it, since it's silly on its face; any layman should be able to see right through the Turing Test. If anyone except Turing had come up with this test it would not have received much attention. Turing displayed one-of-a-kind genius when it came to things like computing and cryptography, but like many other experts in such fields, he had a lot of trouble grappling with metaphysics and philosophy. Searle had more common sense, but his Chinese Room example is more of a rebuttal to the Turing Test than a test in and of itself.</p>\n\n<p>What \"intelligence\" consists of is ultimately a deep metaphysical question, not a material one. For millennia, trained philosophers have had a lot of trouble assigning clear definitions to concepts like intelligence and consciousness. Until we can answer those questions definitively, using different sets of reasoning skills than scientists, mathematicians and computer specialists are used to employing (just look at how often metaphysics is derided in some of these disciplines) then we cannot say that we have achieved genuine A.I. Until we can define what intelligence is, we cannot say whether or not we've successfully built it; we've not only got the cart before the horse, but have yet to build the cart or see a horse. By the common definitions used in everyday speech we're nowhere near genuine A.I. No one calls cows or sparrows \"intelligent,\" but our AI today isn't even as sophisticated as the mosquitoes that bite them. </p>\n\n<p>That's not going to be a popular answer - I'll probably get a dozen downvotes for this, without anyone being able to adequately rebut my contentions, but it needs to be said. There's far too much irrational exuberance and gross overestimation of what we've achieved to date and probably always will be in this field. Historically, researchers in every generation have also grossly underestimated the computing power of the human brain; every decade or so, the estimates of the FLOPS and megabytes have to be drastically revised. We have a poor track record of even getting basic material questions about the human brain right. This clear, consistent pattern of biased overestimation of our success and the lack of any real definition, let alone a test, of intelligence is going to be a serious issue in this forum for its whole existence (assuming it survives the private beta period). We have a whole forum dedicated to a field we can't even define; we can't say for sure what A.I. really is, but we're adamantly certain that we're close to achieving it...!  We cannot say if \"brute force algorithms\" exist when we're still groping for an understanding of what it is we're trying to force our way into. Certainly, there are brute force methods to solve certain problems, like Deep Blue does at chess - but we cannot say if that qualifies as intelligence or not. It is really not possible to answer questions like this without getting into deep discussions that immediately lend themselves to opinion and debate, which the Turing Test and Searle's Room are clear examples of, in and of themselves. Since implementation details of AI are considered by many to be off-limits here, we're limited mainly to highly speculative posts about tech that often doesn't even work yet (like Google's self-driving cars) and questions like this that we can't answer without first defining intelligence. This is going to be the root of a lot of problems here for a long, long time to come...</p>\n", "question": "<p>Hypothetically, assume that you have access to infinite computing power. Do we have designs for any brute-force algorithms that can find an AI capable of passing traditional tests (e.g. Turing, Chinese Room, MIST, etc.)? </p>\n"}, "id": "1641"}, {"body": {"answer": "<p>Typically, I think of intelligence in terms of the <em>control</em> of <em>perception</em>. [1] A related, but different, definition of intelligence is the (at least partial) restriction of possible future states. For example, an intelligent Chess player is one whose future rarely includes 'lost at chess to a weaker opponent' states; they're able to make changes that move those states to 'won at chess' states.</p>\n\n<p>These are both broad and continuous definitions of intelligence, where we can talk about differences of degree. A sundial doesn't exert any control over its environment; it passively casts a shadow, and so doesn't have intelligence worth speaking of. A thermostat attached to a heating or cooling system, on the other hand, does exert control over its environment, trying to keep the temperature of its sensor within some preferred range. So a thermostat does have intelligence, but not very much.</p>\n\n<p>Self-driving cars obviously fit those definitions of intelligence.</p>\n\n<hr>\n\n<p>[1] Control is meant in the context of <a href=\"https://en.wikipedia.org/wiki/Control_theory\" rel=\"nofollow\">control theory</a>, a branch of engineering that deals with dynamical systems that perceive some fact about the external world and also have a way by which they change that fact. When perception is explicitly contrasted to observations, it typically refers to an abstract feature of observations (you observe the intensity of light from individual pixels, you perceive the apple that they represent) but here I mean it as a superset that includes observation. The thermostat is a dynamical system that perceives temperature and acts to exert pressure on the temperature it perceives.</p>\n\n<p>(There's a philosophical point here that the thermostat cares directly about its sensor reading, not whatever the temperature \"actually\" is. I think that's not something that should be included in intelligence, and should deserve a name of its own, because understanding the difference between perception and reality and seeking to make sure one's perceptions are accurate to reality is another thing that seems partially independent of intelligence.)</p>\n", "question": "<p>For Example:</p>\n\n<h2>Could you provide reasons why a sundial is <em>not</em> \"intelligent\"?</h2>\n\n<p>A sundial senses its environment and acts rationally. It outputs the time. It also stores  percepts. (The numbers the engineer wrote on it.)</p>\n\n<h2>What properties of a self driving car would make it \"intelligent\"?</h2>\n\n<p>Where is the line between non intelligent matter and an intelligent system?</p>\n"}, "id": "1649"}, {"body": {"answer": "<p>To ask what makes a system intelligent almost begs the question 'in this context what do we mean by artificially intelligent?' which I think this what this question is really gearing towards.</p>\n\n<p>From my studies, I've come to see that 'Artificial Intelligence' is a catchy term to use but perhaps misleading, and it conjures up images of these self-driving cars and robots that will take over the earth.</p>\n\n<p>What I've found AI, and 'intelligent' systems moreso represent is an aid or a support that works <em>for</em> us, rather than one that works <em>because</em> of us... hear me out:</p>\n\n<p>What makes the jump to an intelligent system for me is the step where the system begins to 'adapt / learn' or otherwise do things I didn't directly tell it to do. With the sundial, I measured and cut every inch of it by hand, and put it in a specific way to do a specific thing. </p>\n\n<p>When a programmer gets into a car he automated, it may do some things he didn't directly program or maybe couldn't even expect (just one example: querying some database to see lots of people are driving somewhere, discovering a concert is going on there, and asking if the driver wants directions / tickets)</p>\n\n<p>--</p>\n\n<p>In conclusion, an intelligent system to me is one that we build in such a way that it educates and supports <em>us</em>, rather than a system we ourselves 'educate' to do a specific task. Supportive systems that elucidate and adapt and act 'rationally' even when we didn't tell it what 'rational' behaviour was.</p>\n", "question": "<p>For Example:</p>\n\n<h2>Could you provide reasons why a sundial is <em>not</em> \"intelligent\"?</h2>\n\n<p>A sundial senses its environment and acts rationally. It outputs the time. It also stores  percepts. (The numbers the engineer wrote on it.)</p>\n\n<h2>What properties of a self driving car would make it \"intelligent\"?</h2>\n\n<p>Where is the line between non intelligent matter and an intelligent system?</p>\n"}, "id": "1653"}, {"body": {"answer": "<p>I think this is almost a trick question in a sense. Let me explain:</p>\n\n<p>For law 1, any AI would abide by the first rule unless it was deliberately created to be malevolent, in that the AI it would understand harm was imminent but do nothing about or would actively attempt to harm. Any 'reasonable' AI would (try its best to) prevent any harm it understood, but couldn't react to imminent harm 'outside it's knowledge', thus satisfying law 1. Any AI that 'tries its best' to prevent harm works here.</p>\n\n<p>For law 2, it is simply a matter of design. If one can design an AI capable of parsing and understanding the entirety of human language (beyond just speech), just program it to act accordingly, mindful of the first law. Thus, I think we can develop an AI that will obey every command <em>it understands</em> but getting it to understand anything and everything I believe is impossible.</p>\n\n<p>For law 3, it rides in the same vein as law 1.</p>\n\n<p>In conclusion, I think there is no philosophical problem with implementing such an AI, but that the actual design of such an AI is fundamentally impossible (understanding all possible harms, and all possible commands).</p>\n", "question": "<p>Would it be possible to put Asimov's three Laws of Robotics into an AI?</p>\n\n<p>The three laws are:</p>\n\n<ol>\n<li><p>A robot (or, more accurately, an AI) cannot harm a human being, or through inaction allow a human being to be harmed<sup>1</sup></p></li>\n<li><p>A robot must listen to instructions given to it by a human, as long as that does not conflict with the first law.</p></li>\n<li><p>A robot must protect its own existence, if that does not conflict with the first two laws.</p></li>\n</ol>\n\n<hr>\n\n<p><sup>1</sup> <em>To it's knowledge</em>. This was a plot point in one of the books :P</p>\n"}, "id": "1654"}, {"body": {"answer": "<p>Second question first: Data is stored in an ANN in the form of weights in the adjacency matrix between neurons. During training, these weights are updated by a learning algorithm (such as <a href=\"https://en.wikipedia.org/wiki/Backpropagation\" rel=\"nofollow\">backpropagation</a>).</p>\n\n<p>First question: according to award-winning neuroscientist Tim Bliss:</p>\n\n<blockquote>\n  <p>\u201cIt\u2019s been accepted really since the turn of the 20th century, since the time of the Spanish neuroscientist Ram\u00f3n y Cajal, that really the only place where memories can be stored is at synapses, the junctions between nerve cells.</p>\n</blockquote>\n\n<p>A protein called the NMDA receptor plays a key roll in the strengthening of synaptic connections (which is more broadly achieved by a form of <a href=\"https://en.wikipedia.org/wiki/Hebbian_theory\" rel=\"nofollow\">Hebbian Learning</a>).</p>\n", "question": "<p>Do scientists know by what mechanism biological brains/biological neural networks store data?</p>\n\n<p>I was thinking about @kenorbs <a href=\"http://ai.stackexchange.com/questions/1656/how-can-nanobot-implants-in-our-brains-connect-to-the-internet\">question</a> about implanting nanobots to build an AGI on top of human wetware. </p>\n\n<p>I only have a vague notion that we store data in our brains by altering synapses? </p>\n\n<p>Links, Criticism and Detailed Explanation welcome.</p>\n\n<p>I also would love a decent description of how a vanilla Artificial Neural Network stores data. </p>\n\n<p><strong>Questions:</strong></p>\n\n<ol>\n<li><p>How is data stored in a biological Neural Network?</p></li>\n<li><p>How is data stored in an Artificial Neural Network?</p></li>\n</ol>\n"}, "id": "1663"}, {"body": {"answer": "<p>As a person who works with people who work on Watson, perhaps I can give some insight.</p>\n\n<p>The name <em>Watson</em> is casually thrown around a lot whilst many people aren't aware of its evolution into a larger suite of systems and services. We now have Chef Watson, Watson Health, and many other developing projects along the \"cognitive\" route. <em>Watson</em> is really an amalgamation and varied application of the different cognitive computing routes IBM is pursuing.</p>\n\n<p>So what I'm trying to get at is that there are many forms of NLP that Watson conducts and has conducted, developed by different teams to fit different processes, interconnected in different ways. Additionally, much (probably all) of it is proprietary/classified since, as one would imagine, ongoing research is constantly being conducted and added to Watson. This is likely your largest obstacle. The precise workings of the NLP of the Jeopardy flavor of Watson are probably themselves still classified (I can't find anything in the time I've just spent looking myself)</p>\n\n<p>There are, thus, many answers to this question; many outdated, and others not always applicable. The full answer is very complicated and by the time you find out what the answer is today it's probably already been advanced. The researches I know are always working on new, cutting-edge algorithms and processes for text classification and the related NLP topics.</p>\n\n<p>To point you to more information, though, take a look at these links:</p>\n\n<p><a href=\"https://www.ibm.com/watson/developercloud/nl-classifier.html\" rel=\"nofollow\">https://www.ibm.com/watson/developercloud/nl-classifier.html</a></p>\n\n<p><a href=\"http://researcher.watson.ibm.com/researcher/view_group.php?id=2099\" rel=\"nofollow\">http://researcher.watson.ibm.com/researcher/view_group.php?id=2099</a> (DeepQA research page, check out the publications)</p>\n\n<p><a href=\"http://www.research.ibm.com/cognitive-computing/\" rel=\"nofollow\">http://www.research.ibm.com/cognitive-computing/</a> (takes a sec to load)</p>\n\n<p>Short videos on the Watson QA subject:\n<a href=\"https://www.youtube.com/watch?v=tu5v-gu_5pY\" rel=\"nofollow\">https://www.youtube.com/watch?v=tu5v-gu_5pY</a></p>\n\n<p><a href=\"https://www.youtube.com/watch?v=PI55a1jFrMY\" rel=\"nofollow\">https://www.youtube.com/watch?v=PI55a1jFrMY</a></p>\n\n<p><a href=\"http://nlp.cs.rpi.edu/course/spring14/nlp.html\" rel=\"nofollow\">http://nlp.cs.rpi.edu/course/spring14/nlp.html</a> (NLP course syllabus from RPI based around Watson)</p>\n\n<p>A good paper on NLP that acknowledges Watson, more technical:\n<a href=\"http://jamia.oxfordjournals.org/content/18/5/544.short\" rel=\"nofollow\">http://jamia.oxfordjournals.org/content/18/5/544.short</a></p>\n\n<p>UPDATE: Looking at the article posted in the comments by @Pimgd, I find support for what I said above: \"For the Jeopardy Challenge, we use more than 100 different techniques for analyzing natural language, identifying sources, finding and generating hypotheses, finding and scoring evidence, and merging and ranking hypotheses.\"</p>\n\n<p>I am sure many if not most of these techniques have since been modified, adapted, or dropped altogether for other methods.</p>\n", "question": "<p>What AI techniques does IBM use for its Watson platform, specifically its natural language analysis?</p>\n"}, "id": "1664"}, {"body": {"answer": "<p>I cannot say for certain, but I know of no such other uses (I work at the building where Watson is developing but do not directly work with it).</p>\n\n<p>The DeepQA team's page (<a href=\"https://www.research.ibm.com/deepqa/deepqa.shtml\" rel=\"nofollow\">https://www.research.ibm.com/deepqa/deepqa.shtml</a>) only ever references Watson as the implementation, and based on the structure of the FAQ there I would imagine they'd be eager to list any interesting other uses it would have, but no such entries exist there.</p>\n\n<p>I would, however, also note that while DeepQA is IBM's proprietary implementation of QA using deep-learning methods, that approach is becoming more popular and is almost certainly being worked on by other companies and could soon be on other computers, if not already so in some form.</p>\n", "question": "<p>My understanding is that <em>Watson</em> is the name of the computer, and <em>DeepQA</em> is the name of the software or technology. They are both correlated.</p>\n\n<p>Are there any computers/technologies other than <em>Watson</em> which <strong>are using <em>DeepQA</em></strong>? Or is <em>Watson</em> the only computer which implements that software/technology?</p>\n\n<p><sup>This question is inspired by this <a href=\"http://meta.ai.stackexchange.com/q/1177/8\">meta thread</a>.</sup></p>\n"}, "id": "1666"}, {"body": {"answer": "<p>The problem detecting NSFW has been around for over two decades.</p>\n\n<hr>\n\n<p>This study from 2005 about <a href=\"http://link.springer.com/chapter/10.1007%2F3-540-61123-1_173\" rel=\"nofollow\"><em>finding naked people</em></a>, demonstrates a strategy for finding such images based on the color and texture properties to fetch an effective mask for skin regions attempting to group a human figure using geometric constraints on the human structure. This method demonstrated </p>\n\n<blockquote>\n  <p>60% precision and 52% recall on a test set of 138 uncontrolled images of naked people.</p>\n</blockquote>\n\n<p>Here are a few figures from the study explaining the algorithm:</p>\n\n<p><a href=\"http://i.stack.imgur.com/hMC5ll.png\" rel=\"nofollow\"><img src=\"http://i.stack.imgur.com/hMC5ll.png\" alt=\"Finding naked people via AI, neural network\"></a></p>\n\n<p><a href=\"http://i.stack.imgur.com/qgjcYl.png\" rel=\"nofollow\"><img src=\"http://i.stack.imgur.com/qgjcYl.png\" alt=\"Typical control images wrongly classified as containing naked people\"></a></p>\n\n<p><sup><strong>The following post contains visualizations of nudity for scientific purposes (hover to display):</sup></strong></p>\n\n<blockquote class=\"spoiler\">\n  <p> <a href=\"http://i.stack.imgur.com/qlNd8m.png\" rel=\"nofollow\"><img src=\"http://i.stack.imgur.com/qlNd8m.png\" alt=\"Typical images correctly classified as containing naked people\"></a></p>\n</blockquote>\n\n<hr>\n\n<p>A more recent approach is using <a href=\"http://ai.stackexchange.com/questions/tagged/conv-neural-network\">convolutional networks</a>. This <a href=\"http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.683.4319\" rel=\"nofollow\">study from 2014</a><sup><a href=\"https://www.cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf\" rel=\"nofollow\">PDF</a></sup> demonstrated impressive classification performance based on the ImageNet dataset. It's not clear '<em><a href=\"http://ai.stackexchange.com/q/1479/8\">how and why</a></em> they perform so well', however they can be used for classification of images with a very low error rate.</p>\n\n<p>For further details, check: <a href=\"http://blog.clarifai.com/what-convolutional-neural-networks-see-at-when-they-see-nudity/\" rel=\"nofollow\">What convolutional neural networks look at when they see nudity</a>.</p>\n\n<p>You will find the code example and the heatmap for how convnets see NSFW in the above link.</p>\n", "question": "<p>For example, search engine companies want to classify their image searches into 2 categories (which they already do that) such as: <a href=\"https://en.wikipedia.org/wiki/Not_safe_for_work\" rel=\"nofollow\">NSFW</a> (nudity, porn, brutality) and safe to view pictures.</p>\n\n<p>How can artificial neural networks achieve that, and at what success rate? Can they be easily mistaken?</p>\n"}, "id": "1667"}, {"body": {"answer": "<p>I know it seems like a cop-out answer to every question on AI, but \"it depends\".  For example, if the bulk of the storage space is storing learned concepts, and attributes of example entities, then it stands to reason that concepts and entities could be reused.  In that scenario, learning from an additional 10G of text would use less storage than the original.</p>\n\n<p>OTOH, as others have said, it could be that the storage is mostly storing the <em>links</em> between things, in which case the number of links will likely grow exponentially. In that case, the second batch of \"knowledge\" would add more storage requirements than the first.</p>\n\n<p>So it would come down to \"what exactly is the system learning, and how does it represent what it learned?\"  And that answer will vary from system to system.</p>\n", "question": "<p>Complex AI that learns lexical-semantic content and its meaning (such as collection of words, their structure and dependencies) such as <em>Watson</em> takes terabytes of disk space.</p>\n\n<p>Lets assume <em>DeepQA</em>-like AI consumed whole Wikipedia of size 10G which took the same amount of structured and unstructured stored content.</p>\n\n<p>Will learning another 10G of different encyclopedia (different topics in the same language) take the same amount of data? Or will the AI reuse the existing structured and take less than half (like 1/10 of it) additional space?</p>\n"}, "id": "1670"}, {"body": {"answer": "<p>I'll try to do something intuitive; Each node in a neural network is referred to as a neuron. To understand what's going on under the hood of a neural network you only really need to understand an individual neuron. </p>\n\n<p>Now each neuron has a set of inputs (other neurons; they can potentially be the inputs to the network as a whole as well), and each input has a weight associated with it. Every time the network is used, each neuron computes its output as the weighted sum of its inputs passed through some gate (The \"Activation Function\", a mathematical function designed to get a particular behaviour. For example sigmoid AF takes an input of any size and transforms it into an output in the range [0, 1].) Obviously, this is driven from the inputs to the neural network so that no neuron is computing its outputs before all of the neurons used as its inputs have done the same.</p>\n\n<p>When you refer to the value of a node; there isn't a single value. Each neuron has several weights associated with it as it may be the input to several other neurons, and each of those neurons assigns it a different weight. Instead, it is better to thing of a neural network as a directed graph of nodes (neurons) which are labelled with a particular activation function, and edges (input/output connections) which are labelled with a particular weight. While the structure and activation functions used in the neural networks is a matter of topology design, there are a number of algorithms for designing a ANN for a particular topology.</p>\n\n<p>The most commonly used (and possibly easiest to explain) is backpropagation. In pseudo-Layman's terms we start off with random weights on all edges in the network. We then compute the output of the network for a training set (a set of known input/output pairs). By careful choice of activation function, it is possible to differentiate the error (computed analogously to the expected output minus the actual output of the ANN for each input/output pair) with respect to the weights of the neural network. This allows us to compute a gradient for each weight; the direction in which we can move the weight to reduce the error on the training set. By doing this until we find an optima (a point where all movements increase error), we can find some 'good' configuration of weights for that particular ANN. </p>\n\n<p>There's a nice tutorial on BP here <a href=\"http://www.cse.unsw.edu.au/~cs9417ml/MLP2/BackPropagation.html\" rel=\"nofollow\">http://www.cse.unsw.edu.au/~cs9417ml/MLP2/BackPropagation.html</a>. The diagram associated with it does nicely to explain my point:\n<a href=\"http://i.stack.imgur.com/O2d6L.jpg\" rel=\"nofollow\"><img src=\"http://i.stack.imgur.com/O2d6L.jpg\" alt=\"enter image description here\"></a></p>\n", "question": "<p>When it comes to neural networks, it's often only explained what abstract task they do, say for example detect a number in an image. I never understood what's going on under the hood essentially.</p>\n\n<p>There seems to be a common structure of a directed graph, with values in each node. Some nodes are input nodes. Their values can be set. The values of subsequent nodes are then calculated based on those along the edges of the graph until the values for the output nodes are set, which can be interpreted a result.</p>\n\n<p>How exactly is the value of each node determined? I assume that some formula is associated with each node that takes all incoming nodes as input to calculate the value of the node. What formula is used? Is the formula the same throughout the network?</p>\n\n<p>Then I heard that a network has to be trained. I assume that such training would be the process to assign values to coefficients of the formulas used to determine the node values. Is that correct?</p>\n\n<p>In layman's terms, what are the underlying principles that make a neural network work?</p>\n"}, "id": "1671"}, {"body": {"answer": "<p>One of the <em>Pinterest's</em> white paper about <a href=\"https://engineering.pinterest.com/sites/engineering/files/article/fields/field_image/human-curation-convnets%20(1).pdf\" rel=\"nofollow\">Human Curation and Convnets powering item-to-item recommendations</a><sup><a href=\"https://arxiv.org/abs/1511.04003\" rel=\"nofollow\">arxiv</a></sup> describes implementation of convolutional neural network (CNN) based visual features (<a href=\"http://www.robots.ox.ac.uk/~vgg/\" rel=\"nofollow\">VGG</a><sup><a href=\"https://arxiv.org/abs/1405.3531\" rel=\"nofollow\">2014</a></sup>, <a href=\"http://arxiv.org/abs/1506.01497\" rel=\"nofollow\">Faster R-CNN</a>). This demonstrates the effectiveness of it (such image or object representations) which can improve user engagement. The visual features are computed using the process described in the <a href=\"http://arxiv.org/abs/1505.07647\" rel=\"nofollow\">previous study about visual search at <em>Pinterest</em></a> and can be used for more targeted features to be computed for related pins.</p>\n\n<p>Here are the examples of detected visual objects from Pinterest's object detection pipeline:</p>\n\n<p><a href=\"http://i.stack.imgur.com/q1Tvr.jpg\" rel=\"nofollow\"><img src=\"http://i.stack.imgur.com/q1Tvr.jpg\" alt=\"Fig. 6. Examples of detected visual objects from Pinterest\u2019s object detection pipeline. Detection of objects allows for more targeted visual features to be computed for Related Pins\"></a></p>\n\n<p><sup>Image source: <a href=\"https://engineering.pinterest.com/sites/engineering/files/article/fields/field_image/human-curation-convnets%20(1).pdf\" rel=\"nofollow\">Human Curation and Convnets: Powering\nItem-to-Item Recommendations on Pinterest</a>, Page 4, Fig. 6</sup></p>\n\n<p>The images are categorized by using dominant visual objects (individual objects seen in the image which passes a confidence threshold in Faster R-CNN) using fine-tuned VGG reranking variant. This allows Pinterest to introduce features such real-time recommendations for the users.</p>\n\n<p>Check also this blog entry: <a href=\"https://engineering.pinterest.com/blog/building-scalable-machine-vision-pipeline\" rel=\"nofollow\">Building a scalable machine vision pipeline</a>.</p>\n", "question": "<p>According to this <a href=\"http://mashable.com/2014/01/06/pinterest-acquires-visualgraph/\" rel=\"nofollow\">article</a>, Pinterest acquired VisualGraph, an image recognition and visual search technology startup.</p>\n\n<p>How does Pinterest apply VisualGraph technology for machine vision, image recognition and visual search in order to classify the images?</p>\n\n<p>In short, how do they predict the image categories? Based on what features?</p>\n"}, "id": "1688"}, {"body": {"answer": "<p>The ImageIdentify project uses the highly automated \"<a href=\"https://www.wolfram.com/algorithmbase/\" rel=\"nofollow\">superfunctions</a>\" and as part of Wolfram Language API integration. It relies on a complex collection of meta-algorithms and built-in '<a href=\"http://www.wolfram.com/knowledgebase/\" rel=\"nofollow\">knowledge</a>'. It has a built-in classifier trained from a large dataset using <a href=\"http://www.wolfram.com/data-framework/\" rel=\"nofollow\">Wolfram Data Framework</a> (WDF). However the main classifier is based on the deep neural networks.</p>\n\n<p>Source: <a href=\"https://www.imageidentify.com/about/how-it-works\" rel=\"nofollow\">How the Wolfram Language Image Identification Project Works</a></p>\n\n<p>The algorithm isn't perfect and misidentification are more likely to be caused by 'irrelevant objects repeatedly being in training images for a particular type of object'.</p>\n\n<p>You can read more at <a href=\"http://blog.stephenwolfram.com/2015/05/wolfram-language-artificial-intelligence-the-image-identification-project/\" rel=\"nofollow\">Wolfram Language Artificial Intelligence: The Image Identification Project</a>.</p>\n", "question": "<p>Wolfram Language Image Identification Project launched an <a href=\"https://www.imageidentify.com/\" rel=\"nofollow\">Image Identify site</a> demo which returns the top predicted tags for the photos.</p>\n\n<p>How does it work, briefly? I mean what type of learning vision technologies are used to analyze, recognize and understand the content of an image?</p>\n"}, "id": "1690"}, {"body": {"answer": "<p>At the highest level, all it needs is for the various systems already discussed to incorporate code objects. If it can interpret its source code / model architecture from the formatted text objects underpinning them, can 'understand' them in terms of having a useful ML model, and alter the code with its reaction, then it can self-program. </p>\n\n<p>That is, the basic loop behind a recursively improving intelligence is simple. It examines itself, writes a new version, and then that new version examines itself and writes a new version, and so on.</p>\n\n<p>The difficult component comes at lower levels. We don't need to invent a new concept like 'sensor,' what we need to do is build very, very sophisticated sensors that are equal to the task of understanding code well enough to detect and write improvements.</p>\n", "question": "<p>An AI agent is often thought of having \"sensors\", \"a memory\", \"machine learning processors\" and \"reaction\" components. However, a machine with these does not necessarily become a self-programming AI agent. Beyond the parts mentioned above, is there any other elements or details necessary to make a machine capable of being a self-programming AI agent?</p>\n\n<p>For example, <a href=\"http://www.iiim.is/wp/wp-content/uploads/2011/05/goertzel-agisp-2011.pdf\" rel=\"nofollow\">a paper from 2011</a> declared that solving the optimization problem of maximizing the intelligence is a must-have feature for the self-programming process, as quoted below:</p>\n\n<blockquote>\n  <p>A system is said to carry out an instance of self-programming when it undergoes learning regarding some element of its \"cognitive infrastructure\", where the latter is defined as the fuzzy set of \"intelligence-critical\" features of the system; and the intelligence-criticality of a system feature is defined as its \"feature quality,\" considered from the perspective of solving the optimization problem of maximizing the intelligence of a multi-feature system.</p>\n</blockquote>\n\n<p>However, this description of \"optimization of intelligence\" is vague. Can anyone give a clear definition or better summary for the necessary components for self-programming agents?</p>\n\n<p><sub>This question is from the 2014 closed beta, with the asker having a UID of 23.</sub></p>\n"}, "id": "1695"}, {"body": {"answer": "<p>You seem to be wanting some description of the 'style' of an image. </p>\n\n<p>To make that work in general, I'd guess that would actually require quite a lot of pre-processing to present 'texture elements' (rather than pixels) as the basic features. </p>\n\n<p>This is quite speculative, but one approach might be to use <a href=\"https://en.wikipedia.org/wiki/Iterated_function_system\" rel=\"nofollow\">Iterated Function Systems</a> as a means of extracting these.</p>\n\n<p>Whether 'spatial adjacency' (and hence CNN) is then the best approach to make higher-level decisions about these elements is (AFAIK) a matter for experiment.</p>\n", "question": "<p>I've <a href=\"https://www.imageidentify.com/result/0lkzuttdxipub\" rel=\"nofollow\">uploaded a picture</a> to Wolfram's ImageIdentify of graffiti on the wall, but it recognized it as 'monocle'. Secondary guesses were 'primate', 'hominid', and 'person', so not even close to 'graffiti' or 'painting'.</p>\n\n<p>Is it by design, or there are some <strong>methods to teach a convolutional neural network (CNN) to reason and be aware of a bigger picture context</strong> (like mentioned graffiti)? Currently it seems as if it's detecting literally <em>what is depicted in the image</em>, not <em>what the image actually is</em>.</p>\n\n<p><a href=\"http://i.stack.imgur.com/akquMm.png\" rel=\"nofollow\"><img src=\"http://i.stack.imgur.com/akquMm.png\" alt=\"Wolfram&#39;s Image Identify: monocle/graffiti\"></a></p>\n\n<p>This could be the same problem as mentioned <a href=\"http://ai.stackexchange.com/a/1533/8\">here</a>, that DNN are:</p>\n\n<blockquote>\n  <p>Learning to detect jaguars by matching the unique spots on their fur while ignoring the fact that they have four legs.<sup><a href=\"http://ai.stackexchange.com/a/1533/8\">2015</a></sup></p>\n</blockquote>\n\n<p>If it's by design, maybe there is some better version of CNN that can perform better?</p>\n"}, "id": "1696"}, {"body": {"answer": "<p>I can't speak to wit.ai specifically, but I can tell you a little bit about how similar applications work. Specifically, I can talk a bit about <a href=\"http://stanbol.apache.org\" rel=\"nofollow\">Apache Stanbol</a> which also converts free text into structured data.   That said, I should prefix this by saying there isn't just one way to \"get there from here.\"  Many techniques could be part of a stack for accomplishing this goal.</p>\n\n<p>Anyway, in the case of Stanbol, they run the text through multiple processing engines, sequentially, with different engines affecting the final output.  One engine simply does <a href=\"https://en.wikipedia.org/wiki/Named-entity_recognition\" rel=\"nofollow\">Named Entity Recognition</a> using OpenNLP.  This identities discrete named \"things\" - people, places, companies, etc.  Another engine does entity matching with a pre-established database of entities - specifically (in the out-of-the-box configuration) a dump of entities from <a href=\"http://www.dbpedia.org\" rel=\"nofollow\">DBPedia</a>.  Where a match is found, the text from the original input is assigned to the entity.  In the case of a collision, it assigns a weight to the mapping so any downstream consumers can use probabilistic techniques to select the \"correct\" mapping.</p>\n\n<p>There are, of course, more details that I left out.  Before NER can happen there is parsing and tokenizing and other NLP activities.  But a big part of the basic process is doing NER and then doing the entity matching.  </p>\n\n<p>And in the case of Stanbol, you can add your own entities and corresponding structured data, as well as your own engines.  So, for example, if you wanted to write an engine based on neural networks / deep learning, and plug that in, you could.</p>\n", "question": "<p>The Wit.ai is a Siri-like voice interface which can can parse messages and predict the actions to perform.</p>\n\n<p>Here is the <a href=\"https://labs.wit.ai/demo/index.html\" rel=\"nofollow\">demo site powered by Wit.ai</a>.</p>\n\n<p>How does it understand the spoken sentences and convert them into structured actionable data? Basically, how does it know what to do?</p>\n"}, "id": "1697"}, {"body": {"answer": "<p>This <a href=\"http://dx.doi.org/10.1109/TPAMI.2012.59\" rel=\"nofollow\">study from 2012</a> uses 3D <a href=\"http://ai.stackexchange.com/questions/tagged/conv-neural-network\">convolutional neural networks (CNN)</a> for automated recognition of human actions in surveillance videos. The 3D CNN model extracts features from both the spatial and the temporal dimensions by performing 3D convolutions, thereby capturing the motion information encoded in multiple adjacent frames. A very similar deep learning approach based on 3D CNN is demonstrated in the <a href=\"http://liris.cnrs.fr/Documents/Liris-5228.pdf\" rel=\"nofollow\">LIRIS and Orange Labs study from 2011</a>.</p>\n\n<hr>\n\n<p>This <a href=\"https://www.robots.ox.ac.uk/~vgg/publications/2014/Simonyan14b/simonyan14b.pdf.pdf\" rel=\"nofollow\">Oxford study from 2014</a> also uses a similar approach, but with two-stream CNN which incorporates spatial and temporal networks which can achieve good performance despite having limited training data. It recognises action from motion in the form of dense optical flow. For example:</p>\n\n<p><a href=\"http://i.stack.imgur.com/RWcT3.png\" rel=\"nofollow\"><img src=\"http://i.stack.imgur.com/RWcT3.png\" alt=\"Optical flow using ConvNets\"></a></p>\n\n<hr>\n\n<p><a href=\"http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=4224216\" rel=\"nofollow\">Another study from 2007</a> demonstrates a method by detecting human falls based on a combination of motion history and human shape variation by analysing the video frames. It uses Motion History Image (MHI) to quantify the motion of the person.</p>\n\n<p><a href=\"http://i.stack.imgur.com/Hzn4z.png\" rel=\"nofollow\"><img src=\"http://i.stack.imgur.com/Hzn4z.png\" alt=\"Motion history image (MHI)\"></a></p>\n\n<p><sup>Source: <a href=\"https://github.com/harishrithish7/Fall-Detection\" rel=\"nofollow\">harishrithish7/Fall-Detection</a> at GitHub</sup></p>\n\n<hr>\n\n<p>An alternative general approach could be action detection based on the posture using DNN. See: <a href=\"http://ai.stackexchange.com/q/1644/8\">How to achieve recognition of postures and gestures?</a></p>\n", "question": "<p>For example I'd like to train my neural network to recognize the type of actions (e.g. in commercial movies or some real life videos), so I can \"ask\" my network in which video or movie (and at what frames) somebody was driving a car, kissing, eating, was scared or was talking over the phone.</p>\n\n<p>What are the current successful approaches to that type of problem?</p>\n"}, "id": "1698"}, {"body": {"answer": "<p>There are several approaches as to how this can be achieved.</p>\n\n<p>One recent study from 2015 about <a href=\"http://link.springer.com/chapter/10.1007%2F978-3-319-09396-3_9\" rel=\"nofollow\">Action Recognition in Realistic Sports Videos</a><sup><a href=\"http://cs.stanford.edu/~amirz/index_files/Springer2015_action_chapter.pdf\" rel=\"nofollow\">PDF</a></sup> uses the action recognition framework based on the three main steps of feature extraction (shape, post or contextual information), dictionary learning to represent a video, and classification (<a href=\"https://en.wikipedia.org/wiki/Bag-of-words_model\" rel=\"nofollow\">BoW framework</a>). </p>\n\n<p>A few examples of methods:</p>\n\n<ul>\n<li><p>Spatio-Temporal Structures of Human Poses</p>\n\n<p><a href=\"http://i.stack.imgur.com/AgfDk.png\" rel=\"nofollow\"><img src=\"http://i.stack.imgur.com/AgfDk.png\" alt=\"K. Soomro and A.R. Zamir - action recognition - figure\"></a></p></li>\n<li><p>a joint shape-motion</p>\n\n<p><a href=\"http://i.stack.imgur.com/1qR9x.png\" rel=\"nofollow\"><img src=\"http://i.stack.imgur.com/1qR9x.png\" alt=\"K. Soomro and A.R. Zamir - action recognition - figure\"></a></p></li>\n<li><p>Multi-Task Sparse Learning (MTSL)</p></li>\n<li><p>Hierarchical Space-Time Segments</p>\n\n<p><a href=\"http://i.stack.imgur.com/jBMuj.png\" rel=\"nofollow\"><img src=\"http://i.stack.imgur.com/jBMuj.png\" alt=\"K. Soomro and A.R. Zamir - Extracted segments from video frames\"></a></p></li>\n<li><p>Spatio-Temporal Deformable Part Models (SDPM)</p>\n\n<p><a href=\"http://i.stack.imgur.com/5Zehk.jpg\" rel=\"nofollow\"><img src=\"http://i.stack.imgur.com/5Zehk.jpg\" alt=\"K. Soomro and A.R. Zamir - Action localization results\"></a></p></li>\n</ul>\n\n<p>Here are the results based on training of 10 action classes based on the UCF sports dataset:</p>\n\n<p><a href=\"http://i.stack.imgur.com/v77Pv.png\" rel=\"nofollow\"><img src=\"http://i.stack.imgur.com/v77Pv.png\" alt=\"UCF Sports Dataset: sample frames of 10 action classes along with their bounding box annotations of the humans shown in yellow\"></a></p>\n\n<p><sup>Source: <a href=\"http://link.springer.com/chapter/10.1007%2F978-3-319-09396-3_9\" rel=\"nofollow\">Action Recognition in Realistic Sports Videos</a>.</sup></p>\n", "question": "<p>For example I'd like to train my neural network to recognize the type of actions (e.g. in commercial movies or some real life videos), so I can \"ask\" my network in which video or movie (and at what frames) somebody was driving a car, kissing, eating, was scared or was talking over the phone.</p>\n\n<p>What are the current successful approaches to that type of problem?</p>\n"}, "id": "1699"}, {"body": {"answer": "<p>After a bit of research I found something kind of close:</p>\n\n<ul>\n<li><a href=\"http://www.techinsider.io/security-cameras-use-artificial-intelligence-to-detect-crime-2015-8\" rel=\"nofollow\">Artificially intelligent security cameras are spotting crimes before they happen</a></li>\n<li><a href=\"http://www.dailymail.co.uk/sciencetech/article-2154861/U-S-surveillance-cameras-use-eyes-pre-crimes-detecting-suspicious-behaviour-alerting-guards.html\" rel=\"nofollow\">New surveillance cameras will use computer eyes to find 'pre crimes' by detecting suspicious behaviour and calling for guards</a></li>\n<li><a href=\"http://www.dailymail.co.uk/sciencetech/article-2154861/U-S-surveillance-cameras-use-eyes-pre-crimes-detecting-suspicious-behaviour-alerting-guards.html\" rel=\"nofollow\">CCTV 'fightcams' detect violence 'before it happens'  at Dailymail</a>, also check at <a href=\"http://www.telegraph.co.uk/news/uknews/crime/11407094/CCTV-fightcams-detect-violence-before-it-happens.html\" rel=\"nofollow\">Telegraph</a></li>\n</ul>\n\n<p>They, however, makes no mention of what specific methods they use.</p>\n\n<p>So a crime detection system as that is written does not exist, but abnormal behaviour detection systems do.</p>\n\n<p>An accurate generalized system seems intuitively infeasible, however. Commiting a crime, unlike falling, is a complex behavior, and takes so many forms. A camera watching a store's counter like at a 7-11 could perhaps see that the 'customer''s arm is strangely reaching across the counter, and the attendant is moving a lot more than usual suddenly, but aside from very specific cases like this such a system is currently quite unfeasible. </p>\n\n<p>Crimes are unusual, relatively speaking, and their dramatic nature means that even the simplest crimes play out in very different ways. Perhaps you could in this case you could try to look for images of a gun, or someone with their hands up. So, looking for unusual, <em>detectable</em> behavioural mannerisms may be possible, but not crime detection.</p>\n\n<p>Ultimately, while you may be able to make (possibly pretty good) systems to detect specific crimes in specific environments, that's all we got for now.</p>\n\n<p>P.S. - Do these camera's also get audio signals? That is also an interesting facet to consider (\"PUT YOUR HANDS UP / GIVE ME ALL YOUR MONEY\")</p>\n", "question": "<p>Inspired by <a href=\"http://ai.stackexchange.com/q/1481/8\">this discussion</a> about recognizing human actions, I have found the <a href=\"https://github.com/harishrithish7/Fall-Detection\" rel=\"nofollow\">Fall-Detection</a> project which detects humans falling on the ground from a CCTV camera feed, and which can consider alerting the hospital authorities.</p>\n\n<p>My question is, are there any existing real-life implementations or research projects <strong>which specifically use live video feed from the surveillance cameras in order to detect crime</strong> using convnets (or similar approaches)? If so, how do they work, briefly? Do they automatically inform the police about the crime with the details what happened and where?</p>\n\n<p>For example car accidents, physical assaults, robberies, violent disturbances, weapon attacks, etc.</p>\n"}, "id": "1702"}, {"body": {"answer": "<p>The answer, unlike for many questions on this board, I think is definitive. No. We don't <em>need</em> AI's to have emotion to be useful, as we can see by the numerous amount's of AI's we have already that are useful.</p>\n\n<p>But to further address the question, we can't really give AI's emotions. I think the closest we could get would be 'Can we make this AI act in a way a human would if that human was <code>insert emotion</code> '.</p>\n\n<p>To what end? the only immediate thing coming to mind would be to create more lifelike companions or interactions, for the purposes of video games or demo's or for fun. I heartily agree, however, that at least of any AI system I've ever considered, emotions would not better it.</p>\n\n<p>Yann says that doing so would give our AI's more human-like qualities <em>and</em> flaws. I think it's more like it would 'give our AI's more human-like qualities <em>or in other words</em> flaws'.</p>\n\n<p>The purpose of AI's and learning systems is to create systems that act or 'think' like humans, but better. Systems that can adapt or evolve, but mess up as little as possible. </p>\n\n<p>\"To err is human. To not is AI\" (or what we strive for)</p>\n", "question": "<p>In a <a href=\"http://www.wsj.com/articles/whats-next-for-artificial-intelligence-1465827619\">recent Wall Street Journal article</a>, Yann LeCunn makes the following statement:</p>\n\n<blockquote>\n  <p>The next step in achieving human-level ai is creating intelligent\u2014but not autonomous\u2014machines. The AI system in your car will get you safely home, but won\u2019t choose another destination once you\u2019ve gone inside. From there, we\u2019ll add basic drives, along with emotions and moral values. If we create machines that learn as well as our brains do, it\u2019s easy to imagine them inheriting human-like qualities\u2014and flaws. </p>\n</blockquote>\n\n<p>Personally, I have generally taken the position that talking about emotions for artificial intelligences is silly, because there would be no <em>reason</em> to create AI's that experience emotions.  Obviously Yann disagrees.  So the question is:  what end would be served by doing this?  Does an AI <em>need</em> emotions to serve as a useful tool?  </p>\n"}, "id": "1703"}, {"body": {"answer": "<p>Wolfram's image id system is specifically meant to figure out what the image is depicting, not the medium. </p>\n\n<p>To get what you want you'd simply have to create your own system where the training data is labeled by the medium rather than the content, and probably fiddle with it to pay more attention to texture and things as such as that. The neural net doesn't care which we want - it has no inherent bias. It just knows what it's been trained for.</p>\n\n<p>That's really all there is to it. It's all to do with the training labels and the focus of the system (e.g. a system that looks for edge patterns that form shapes, compared to a system that checks if the lines in the image are perfectly computer-generated straight and clean vs imperfect brush strokes vs spraypaint).</p>\n\n<p>Now, if you want me to tell you how to build that system, I'm not the right person to ask haha</p>\n", "question": "<p>I've <a href=\"https://www.imageidentify.com/result/0lkzuttdxipub\" rel=\"nofollow\">uploaded a picture</a> to Wolfram's ImageIdentify of graffiti on the wall, but it recognized it as 'monocle'. Secondary guesses were 'primate', 'hominid', and 'person', so not even close to 'graffiti' or 'painting'.</p>\n\n<p>Is it by design, or there are some <strong>methods to teach a convolutional neural network (CNN) to reason and be aware of a bigger picture context</strong> (like mentioned graffiti)? Currently it seems as if it's detecting literally <em>what is depicted in the image</em>, not <em>what the image actually is</em>.</p>\n\n<p><a href=\"http://i.stack.imgur.com/akquMm.png\" rel=\"nofollow\"><img src=\"http://i.stack.imgur.com/akquMm.png\" alt=\"Wolfram&#39;s Image Identify: monocle/graffiti\"></a></p>\n\n<p>This could be the same problem as mentioned <a href=\"http://ai.stackexchange.com/a/1533/8\">here</a>, that DNN are:</p>\n\n<blockquote>\n  <p>Learning to detect jaguars by matching the unique spots on their fur while ignoring the fact that they have four legs.<sup><a href=\"http://ai.stackexchange.com/a/1533/8\">2015</a></sup></p>\n</blockquote>\n\n<p>If it's by design, maybe there is some better version of CNN that can perform better?</p>\n"}, "id": "1704"}, {"body": {"answer": "<p>The University of Maryland <a href=\"https://www.cs.umd.edu/class/spring2008/cmsc828g/Slides/SRL-Tutorial-05-08.pdf\">published some slides</a> (PDF) from an introductory presentation on this topic.</p>\n\n<p>The fourth page explains why SRL is interesting. \"Traditional statistical machine learning approaches\" process one sort of thing in which there is some uncertaintly. Image identification is a good example of that. \"Traditional <a href=\"https://en.wikipedia.org/wiki/Inductive_logic_programming\">ILP</a>/relational learning approaches\" use several kinds of information to produce hypotheses about the data set, but apparently allow for no noise in the data.</p>\n\n<p>Statistical relational learning models are intended to work with data sets that have several types of objects connected to each other via various links (hence \"relational\"). They also are meant to deal with uncertainty (hence \"statistical\").</p>\n\n<p>Skipping past some slides that aren't really useful without a transcript of what was said over them, we come to slide 17, which has comprehensible definitions and examples:</p>\n\n<blockquote>\n  <ul>\n  <li><strong>Object classification</strong>\n  \n  <ul>\n  <li>Predicting the category of an object based on its attributes <em>and</em> its links <em>and</em> attributes of linked objects</li>\n  <li>e.g., predicting the topic of a paper based on the words used in the paper the topics of papers it cites the used in the paper, the topics of papers it cites, the research interests of the author</li>\n  </ul></li>\n  <li><strong>Object type prediction</strong>\n  \n  <ul>\n  <li>Predicting the type of an object based on its attributes and its links and attributes of linked objects</li>\n  <li>e.g., predict the venue type of a publication (conference, journal, workshop) based on properties of the paper</li>\n  </ul></li>\n  </ul>\n</blockquote>\n\n<p>As you can see, these models can keep track of several things and the interactions between them. The next slide talks about link prediction, the ability to predict several attributes of connections between objects, like the importance/quality of a citation. As previously mentioned, these models don't require 100% accurate data to give interesting results; academic citation lists might occasionally be less than comprehensive, and the importance of a citation is challenging to quantify.</p>\n\n<p>Like ILP, SLP will hopefully be able to \"see\" new kinds of links between \"entities\", as with the presentation's example of identifying research communities.</p>\n\n<p>Past slide 20, the presentation goes into some serious mathematics. It does have a much less technical conclusion starting at slide 198. </p>\n", "question": "<p>I have gone through the <a href=\"https://en.wikipedia.org/wiki/Statistical_relational_learning\">wikipedia explanation of SRL</a>. But, it only confused me more:</p>\n\n<blockquote>\n  <p>Statistical relational learning (SRL) is a subdiscipline of artificial intelligence and machine learning that is concerned with domain models that exhibit both uncertainty (which can be dealt with using statistical methods) and complex, relational structure.</p>\n</blockquote>\n\n<p>Can someone give a more dumbed down explanation of the same, preferably with an example?</p>\n"}, "id": "1708"}, {"body": {"answer": "<p>I think you're coming at your problem slightly wrong... what you're essentially talking about is a belief network.</p>\n\n<p>You may want to look into existing Bayesian Learning techniques to get your head around this, but belief networks commonly use the exact scenario you're talking about; using a set of known (or uncertain facts) statements to produce some inferred probability of a particular output. </p>\n\n<p>Even more, they often express this through disease-symptom based examples in tutorials! Try <a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.124.2195&amp;rep=rep1&amp;type=pdf\" rel=\"nofollow\">here</a>.</p>\n\n<p>My point being that it would be better to use a belief network as the theory groundwork is all already there for you, instead of an ANN.</p>\n", "question": "<p>I'm trying to come up with the right algorithm for a system in which the user enters a few symptoms and the system has to predict or determine the likelihood that a few selected symptoms are associated with those existing in the system. Then after associating them, the result or output should be a specific disease for the symptoms.</p>\n\n<p>The system is comprised of a series of diseases with each assigned to specific symptoms, which also exist in the system.</p>\n\n<p>Let's assume that the user entered the following input:</p>\n\n<pre><code>A, B, C, and D\n</code></pre>\n\n<p>The first thing the system should do is check and associate each symptom (in this case represented by alphabetical letters) individually against a data-table of symptoms that already exist. And in cases where the input doesn't exist, the system should report or send feedback about it.</p>\n\n<p>And also, let's say that <code>A and B</code> was in the data-table, so we are 100% sure that they're valid or exist and the system is able to give out the disease based on the input. Then let's say that the input now is <code>C and D</code> where <code>C</code> doesn't exist in the data-table, but there is a possibility that <code>D</code> exists.</p>\n\n<p>We don't give <code>D</code> a score of 100%, but maybe something lower (let's say 90%). Then <code>C</code> just doesn't exist at all in the data-table. So, <code>C</code> gets a score of 0%.</p>\n\n<p>Therefore, the system should have some kind of association and prediction techniques or rules to output the result by judging the user's input.</p>\n\n<p>Summary of generating the output:</p>\n\n<pre><code>If A and B were entered and exist, then output = 100%\nIf D was entered and existed but C was not, then output = 90%\nIf all entered don't exist, then output = 0%\n</code></pre>\n\n<p>What techniques would be used to produce this system?</p>\n"}, "id": "1709"}, {"body": {"answer": "<p>I feel like from the information your giving (<em>some</em> sort of biased data) you cant get an answer as robust as you'd like (what <em>algorithmic</em> changes can be made). </p>\n\n<p>In general, the reason these methods like DNN's work is that they learn off of the data. What you train it to do is what it is capable of, and there's little one can do to 'balance' it to classes of data it just never sees. It's like training someone to do algebra then giving them a trigonometry test. It's all math, sure, but you just never can expect much without the proper learning.</p>\n\n<p>That being said, you should perhaps look at other methods to work with this data, or to approach the problem. Given that you cannot collect unbiased data and that you can't explain more due to confidentiality, I really doubt anyone here can help you that much.</p>\n\n<p>I can at most point you to this article</p>\n\n<p>\"Classification on Data with Biased Class Distribution\"<br>\n<a href=\"https://ai2-s2-pdfs.s3.amazonaws.com/277c/3795f7a66fda3fd70607d1fb45b66730c7ba.pdf\" rel=\"nofollow\">https://ai2-s2-pdfs.s3.amazonaws.com/277c/3795f7a66fda3fd70607d1fb45b66730c7ba.pdf</a></p>\n\n<p>And suggest that perhaps your current approach may not be the most approrpiate given the unfortunate circumstances.</p>\n", "question": "<p>The obvious solution is to ensure that the training data is balanced - but in my particular case that is impossible. What corrections can one perform in such a scenario?</p>\n\n<p>I know that my training data is heavily biased towards a particular class, say, and I cannot change that. Moreover, the labels are very noisy. Conditioned on this piece of information, is there anything I can do by tweaking the training process itself/ something else, to correct for the bias in the training data?</p>\n\n<p>The data comes from an experiment (from an electron microscope), and I cannot collect more data. It's always going to be biased in this way, so alternatively-biased is also not an option. I'm sorry that I'm unable to provide any more details due to confidentiality.</p>\n"}, "id": "1711"}, {"body": {"answer": "<p>I think the fundamental question is: Why even attempt to build an AI? If that objective is clear, it will provide clarity to whether or not having emotional quotient in AI make sense. Some attempts like \"Paro\" that were developed for therapeutic reasons requires they exhibit some human like emotions. Again, note that \"displaying\" emotions and \"feeling\" emotions are two completely different things. </p>\n\n<p>You can program a thing like paro to modulate the voice tones or facial twitches to express sympathy, affection, companionship, or whatever - but while doing so, a paro does NOT empathize with its owner - it is simply faking it by performing the physical manifestations of an emotion. It never \"feels\" anything remotely closer to what that emotion evokes in human brain. </p>\n\n<p>So this distinction is really important. For you to feel something, there needs to be an independent autonomous subject that has the capacity to feel. Feeling cannot be imposed by an external human agent. </p>\n\n<p>So going back to the question of what purpose it solves - answer really is - It depends. And the most I think we will achieve ever with silicone based AIs will remain the domain of just physical representations of emotions. </p>\n", "question": "<p>In a <a href=\"http://www.wsj.com/articles/whats-next-for-artificial-intelligence-1465827619\">recent Wall Street Journal article</a>, Yann LeCunn makes the following statement:</p>\n\n<blockquote>\n  <p>The next step in achieving human-level ai is creating intelligent\u2014but not autonomous\u2014machines. The AI system in your car will get you safely home, but won\u2019t choose another destination once you\u2019ve gone inside. From there, we\u2019ll add basic drives, along with emotions and moral values. If we create machines that learn as well as our brains do, it\u2019s easy to imagine them inheriting human-like qualities\u2014and flaws. </p>\n</blockquote>\n\n<p>Personally, I have generally taken the position that talking about emotions for artificial intelligences is silly, because there would be no <em>reason</em> to create AI's that experience emotions.  Obviously Yann disagrees.  So the question is:  what end would be served by doing this?  Does an AI <em>need</em> emotions to serve as a useful tool?  </p>\n"}, "id": "1714"}, {"body": {"answer": "<p>If I look at the image, I can kind of see a monocle as <em>part</em> of the image. So one part of this is that the classifier is ignoring much of the image. This could be called a lack of \"completeness\", in the sense used <a href=\"http://www.wisdom.weizmann.ac.il/~vision/VisualSummary.html\" rel=\"nofollow\">here</a> (a computer vision paper on image summarization).</p>\n\n<p>One way to discover these sorts of failure modes is <a href=\"https://plus.google.com/+ResearchatGoogle/posts/QoFzqQBeANN\" rel=\"nofollow\">adversarial images</a>, which are optimized to fool a given image classifier. Building on this, the idea of <em>adversarial training</em> is to simultaneously train competing \"machines\", one trying to synthesize data, the other trying to find weaknesses in the first one.</p>\n\n<p>Also check this page: <a href=\"https://code.facebook.com/posts/1587249151575490/a-path-to-unsupervised-learning-through-adversarial-networks/\" rel=\"nofollow\">A path to unsupervised learning through adversarial networks</a>, for further information about adversarial training.</p>\n", "question": "<p>I've <a href=\"https://www.imageidentify.com/result/0lkzuttdxipub\" rel=\"nofollow\">uploaded a picture</a> to Wolfram's ImageIdentify of graffiti on the wall, but it recognized it as 'monocle'. Secondary guesses were 'primate', 'hominid', and 'person', so not even close to 'graffiti' or 'painting'.</p>\n\n<p>Is it by design, or there are some <strong>methods to teach a convolutional neural network (CNN) to reason and be aware of a bigger picture context</strong> (like mentioned graffiti)? Currently it seems as if it's detecting literally <em>what is depicted in the image</em>, not <em>what the image actually is</em>.</p>\n\n<p><a href=\"http://i.stack.imgur.com/akquMm.png\" rel=\"nofollow\"><img src=\"http://i.stack.imgur.com/akquMm.png\" alt=\"Wolfram&#39;s Image Identify: monocle/graffiti\"></a></p>\n\n<p>This could be the same problem as mentioned <a href=\"http://ai.stackexchange.com/a/1533/8\">here</a>, that DNN are:</p>\n\n<blockquote>\n  <p>Learning to detect jaguars by matching the unique spots on their fur while ignoring the fact that they have four legs.<sup><a href=\"http://ai.stackexchange.com/a/1533/8\">2015</a></sup></p>\n</blockquote>\n\n<p>If it's by design, maybe there is some better version of CNN that can perform better?</p>\n"}, "id": "1715"}, {"body": {"answer": "<p>I like your choice of \"induce\" instead of \"produce,\" because the delusions came from the users. This means the answer has to do mostly with human psychology; people come equipped with lots of mental machinery specialized for dealing with other humans and not very much mental machinery specialized for dealing with software. So ELIZA behaved in ways that some people classified it as a person and behaved accordingly, and others didn't.</p>\n\n<p>What features will trip up a person's internal person classification system seem like they vary heavily from person to person, and also with experience and familiarity. Going into more detail is, as mentioned in the comments, more appropriate for sites specializing on the human side of the keyboard.</p>\n", "question": "<p><a href=\"http://www.alicebot.org/articles/wallace/eliza.html\" rel=\"nofollow\">From Eliza to A.L.I.C.E.</a>:</p>\n\n<blockquote>\n  <p>Weizenbaum tells us that he was shocked by the experience of releasing ELIZA (also known as \"Doctor\") to the nontechnical staff at the MIT AI Lab. Secretaries and nontechnical administrative staff thought the machine was a \"real\" therapist, and spent hours revealing their personal problems to the program. When Weizenbaum informed his secretary that he, of course, had access to the logs of all the conversations, she reacted with outrage at this invasion of her privacy. Weizenbaum was shocked by this and similar incidents to find that such a simple program could so easily deceive a naive user into revealing personal information.</p>\n</blockquote>\n\n<p>Wikipedia's article on the <a href=\"https://en.wikipedia.org/wiki/ELIZA_effect\" rel=\"nofollow\">\"ELIZA Effect\"</a>:</p>\n\n<blockquote>\n  <p>Though designed strictly as a mechanism to support \"natural language conversation\" with a computer, ELIZA's DOCTOR script was found to be surprisingly successful in eliciting emotional responses from users who, in the course of interacting with the program, began to ascribe understanding and motivation to the program's output. As Weizenbaum later wrote, <strong>\"I had not realized ... that extremely short exposures to a relatively simple computer program could induce powerful delusional thinking in quite normal people.\"</strong> Indeed, ELIZA's code had not been designed to evoke this reaction in the first place. Upon observation, researchers discovered users unconsciously assuming ELIZA's questions implied interest and emotional involvement in the topics discussed, <em>even when they consciously knew that ELIZA did not simulate emotion.</em></p>\n</blockquote>\n\n<p>ELIZA, despite its simplicity, was incredibly successful at its task of tricking other human beings. Even those who knew ELIZA was a bot would still talk to it. Obviously, ELIZA served as an inspiration for various other, more intelligent chatbots, such as <a href=\"http://www.nytimes.com/2015/08/04/science/for-sympathetic-ear-more-chinese-turn-to-smartphone-program.html?_r=0\" rel=\"nofollow\">Xiaoice</a>. But I would like to know what <em>exactly</em> led to such a simple program like ELIZA to be so successful in the first place.</p>\n\n<p>This is very useful knowledge for a programmer since a simple program is one that would be easily maintainable.</p>\n"}, "id": "1717"}, {"body": {"answer": "<p>Our current approaches to AI are too inefficient to result in anything remotely close to what an average human would perceive as artificial senient beings.</p>\n\n<p>Current approaches to AI involve a simulation of our own capacity for learning by creating fully functional computation machines capable of re-programming themselves. While that's definitely a good start with respect to understanding the nature of intelligence, it's still a far cry from actually creating genuine artificial intelligence.</p>\n\n<p>It is not just our capacity to learn that evolved. Our very brains themselves evolved from rudimentary biochemical components at the intra-cellular level to the fascinating, complex organs they are today, along with our bodies as a whole evolving from simple single cell life to homo sapiens. So to create genuine artificial intelligence, it may actually make most sense to first start with replicating that process : creating artificial life with the capacity to evolve. It may actually make most sense to first start with creating artificial DNA and artificial cells, and move on from there.</p>\n\n<p>Anyway, in <a href=\"http://www.alexstjohn.com/WP/2015/06/24/no-azimov-ai/\" rel=\"nofollow\">this article</a> as well as <a href=\"http://www.alexstjohn.com/WP/2016/06/15/things-dont-compute/\" rel=\"nofollow\">this article</a>, Silicon Valley renegade <a href=\"https://en.wikipedia.org/wiki/Alex_St._John\" rel=\"nofollow\">Alex St John</a> goes in greater detail on why something like <a href=\"https://en.wikipedia.org/wiki/Skynet_(Terminator)\" rel=\"nofollow\">Skynet</a>, <a href=\"http://irobot.wikia.com/wiki/VIKI\" rel=\"nofollow\">V.I.K.I.</a> or anything like it is unlikely in the near future and may even never be within our grasp and why our current approach to artificial intelligence is a bad one.</p>\n", "question": "<p>Are there any research teams which attempted to create or have already created an AI robot which can be as close to intelligent as these found in <a href=\"https://en.wikipedia.org/wiki/Ex_Machina_(film)\"><em>Ex Machina</a></em> or <em><a href=\"https://en.wikipedia.org/wiki/I,_Robot_(film)\">I, Robot</em></a> movies?</p>\n\n<p>I'm not talking about full awareness, but an artificial being which can make its own decisions and physical and intellectual tasks that a human being can do?</p>\n"}, "id": "1722"}, {"body": {"answer": "<p>Based on the success of IBM Watson and the amazing advances in tackling numerous hard tasks using deep learning in the past 3 years, I think a large high-tech company like Google or Amazon will create a useful conversational bot in no more than 10 years.  (I've worked on the fringes of AI for 25 years and have followed the tech for even longer.  These are exciting times.)</p>\n\n<p>Initially, your very own AI companion (\"Her\"?) won't be capable of deeper philosophical conversation or insightful interpretation of novels or the human condition.  But it will be able to write / speak in full paragraphs on topics like the best choice among 5 possible routes between point A and B, or summarizing the plot of a book or the gist of a news story, or why one product is better than another (e.g. based on assessing hundreds of Amazon reviews).  And yes, it will be able to understand full spoken sentences from you, and generate both queries and answers.</p>\n\n<p>I'm convinced such a bot <em>will</em> be useful enough that most of us will want one.  Of course you won't need to buy a special piece of hardware, like the Amazon Echo.  It'll be available via software on your smartphone, though the computing is likely to reside on the cloud (since that's where the data is).  </p>\n\n<p>Frankly, I think this is where the next innovations in smartphones will arise -- verbal interfaces that do a better job hearing and speaking and disambiguating using context about you and the kinds of questions you are likely to ask.</p>\n", "question": "<p>Are there any research teams which attempted to create or have already created an AI robot which can be as close to intelligent as these found in <a href=\"https://en.wikipedia.org/wiki/Ex_Machina_(film)\"><em>Ex Machina</a></em> or <em><a href=\"https://en.wikipedia.org/wiki/I,_Robot_(film)\">I, Robot</em></a> movies?</p>\n\n<p>I'm not talking about full awareness, but an artificial being which can make its own decisions and physical and intellectual tasks that a human being can do?</p>\n"}, "id": "1724"}, {"body": {"answer": "<p>I think emotions are not necessary for an AI agent to be useful.  But I also think they could make the agent MUCH more pleasant to work with.  If the bot you're talking with can read your emotions and respond constructively, the experience of interacting with it will be tremendously more pleasant, perhaps spectacularly so.</p>\n\n<p>Imagine contacting a human call center representative today with a complaint about your bill or a product.  You anticipate conflict.  You may have even decided NOT to call because you know this experience is going to be painful, either combative or frustrating, as someone misunderstands what you say or responds hostilely or stupidly.</p>\n\n<p>Now imagine calling the kindest smartest most focused customer support person you've ever met -- Commander Data -- whose only reason for existing is to make this phone call as pleasant and productive for you as possible.  A big improvement over most call reps, yes?  Imagine then if call rep Data could also anticipate your mood and respond appropriately to your complaints to defuse your emotional state... you'd want to marry this guy.  You'd call up call rep Data any time you were feeling blue or bored or you wanted to share some happy news.  This guy would become your best friend overnight -- literally love at first call.</p>\n\n<p>I'm convinced this scenario is valid. I've noticed in myself a surprising amount of attraction for characters like Data or Sonny from \"I Robot\".  The voice is very soothing and puts me instantly at ease.  If the bot were also very smart, patient, knowledgable, and understanding...  I really think such a bot, embued with a healthy dose of emotional intelligence, could be enormously pleasant to interact with.  Much more rewarding than any person I know.  And I think that's true of not just me.</p>\n\n<p>So yes, I think there's great value in tuning a robot's personality using emotions and emotional awareness.</p>\n", "question": "<p>In a <a href=\"http://www.wsj.com/articles/whats-next-for-artificial-intelligence-1465827619\">recent Wall Street Journal article</a>, Yann LeCunn makes the following statement:</p>\n\n<blockquote>\n  <p>The next step in achieving human-level ai is creating intelligent\u2014but not autonomous\u2014machines. The AI system in your car will get you safely home, but won\u2019t choose another destination once you\u2019ve gone inside. From there, we\u2019ll add basic drives, along with emotions and moral values. If we create machines that learn as well as our brains do, it\u2019s easy to imagine them inheriting human-like qualities\u2014and flaws. </p>\n</blockquote>\n\n<p>Personally, I have generally taken the position that talking about emotions for artificial intelligences is silly, because there would be no <em>reason</em> to create AI's that experience emotions.  Obviously Yann disagrees.  So the question is:  what end would be served by doing this?  Does an AI <em>need</em> emotions to serve as a useful tool?  </p>\n"}, "id": "1725"}, {"body": {"answer": "<p>It is a labor intensive process, but that does sound excessive. If you have a g2.8xlarge, make sure you are using the using the GPU flags for neural-style, which will cut your render time by an order of magnitude. </p>\n\n<p>That having been said, it is building a rather large network (depending on your parameters), and a 1024x768 image is a lot of input to work with. It will take time, but shouldn't take more than a couple hours with the gpu flag enabled correctly. </p>\n", "question": "<p>Consider these neural style algorithms which produce some art work:</p>\n\n<ul>\n<li><a href=\"https://github.com/alexjc/neural-doodle\">Neural Doodle</a></li>\n<li><a href=\"https://github.com/jcjohnson/neural-style\">neural-style</a></li>\n</ul>\n\n<p>Why is generating such images so slow and why does it take huge amounts of memory? Isn't there any method of optimizing the algorithm?</p>\n\n<p>What is the mechanism or technical limitation behind this? Why we can't have a realtime processing?</p>\n\n<p>Here are few user comments (<a href=\"https://www.reddit.com/r/deepdream/comments/3jwl76/how_anyone_can_create_deep_style_images/\">How ANYONE can create Deep Style images</a>):</p>\n\n<ul>\n<li><blockquote>\n  <p>Anything above 640x480 and we're talking days of heavy crunching and an insane amount of ram.</p>\n</blockquote></li>\n<li><blockquote>\n  <p>I tried doing a 1024pixel image and it still crashed with 14gigs memory, and 26gigs swap. So most of the VM space is just the swapfile. Plus it takes several hours potentially days cpu rendering this.</p>\n</blockquote></li>\n<li><blockquote>\n  <p>I tried 1024x768 and with 16gig ram and 20+ gig swap it was still dying from lack of memory.</p>\n</blockquote></li>\n<li><blockquote>\n  <p>Having a memory issue, though. I'm using the \"g2.8xlarge\" instance type.</p>\n</blockquote></li>\n</ul>\n"}, "id": "1726"}, {"body": {"answer": "<p>They are all called Monte Carlo because all of them are a different version of the canonical Monte Carlo algorithm.</p>\n\n<p>The canonical version of Monte Carlo algorithm is a stochastic algorithm to determine an action based in a tree representation. The differences among all these version are their exploration and exploitation mechanisms, and it is necessary to analyse each of them to define which one fits in your case. </p>\n", "question": "<p>I'm a bit confused with extensive number of different <a href=\"https://en.wikipedia.org/wiki/Monte_Carlo_method\" rel=\"nofollow\">Monte Carlo methods</a> such as:</p>\n\n<ul>\n<li><a href=\"https://en.wikipedia.org/wiki/Hybrid_Monte_Carlo\" rel=\"nofollow\">Hamiltonian/Hybrid Monte Carlo (HMC)</a>,</li>\n<li><a href=\"https://en.wikipedia.org/wiki/Dynamic_Monte_Carlo_method\" rel=\"nofollow\">Dynamic Monte Carlo (DMC)</a>,</li>\n<li><a href=\"https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo\" rel=\"nofollow\">Markov chain Monte Carlo (MCMC)</a>,</li>\n<li><a href=\"https://en.wikipedia.org/wiki/Kinetic_Monte_Carlo\" rel=\"nofollow\">Kinetic Monte Carlo (KMC)</a>,</li>\n<li><a href=\"https://en.wikipedia.org/wiki/Dynamic_Monte_Carlo_method\" rel=\"nofollow\">Dynamic Monte Carlo (DMC)</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Quasi-Monte_Carlo_method\" rel=\"nofollow\">Quasi-Monte Carlo (QMC)</a>,</li>\n<li><a href=\"https://en.wikipedia.org/wiki/Direct_simulation_Monte_Carlo\" rel=\"nofollow\">Direct Simulation Monte Carlo (DSMC)</a>,</li>\n<li>and so on.</li>\n</ul>\n\n<p>I won't ask for the exact differences, but why are all of them called Monte Carlo? What do they all have in common? Can they all be used for AI? E.g. which one can be used for gaming (like Go) or image recognition (resampling)?</p>\n"}, "id": "1727"}, {"body": {"answer": "<p>Real time style transfer and neural doodle is very much possible and is an active topic I see users working on to improve upon. The basic idea is to do only feed forward propagation at test time and train with appropriate loss functions at train time.</p>\n\n<p><a href=\"http://arxiv.org/pdf/1603.08155v1.pdf\" rel=\"nofollow\">Perceptual Losses for Real-Time Style Transfer\nand Super-Resolution</a> is a good starting point to understand a methodology for this purpose.</p>\n", "question": "<p>Consider these neural style algorithms which produce some art work:</p>\n\n<ul>\n<li><a href=\"https://github.com/alexjc/neural-doodle\">Neural Doodle</a></li>\n<li><a href=\"https://github.com/jcjohnson/neural-style\">neural-style</a></li>\n</ul>\n\n<p>Why is generating such images so slow and why does it take huge amounts of memory? Isn't there any method of optimizing the algorithm?</p>\n\n<p>What is the mechanism or technical limitation behind this? Why we can't have a realtime processing?</p>\n\n<p>Here are few user comments (<a href=\"https://www.reddit.com/r/deepdream/comments/3jwl76/how_anyone_can_create_deep_style_images/\">How ANYONE can create Deep Style images</a>):</p>\n\n<ul>\n<li><blockquote>\n  <p>Anything above 640x480 and we're talking days of heavy crunching and an insane amount of ram.</p>\n</blockquote></li>\n<li><blockquote>\n  <p>I tried doing a 1024pixel image and it still crashed with 14gigs memory, and 26gigs swap. So most of the VM space is just the swapfile. Plus it takes several hours potentially days cpu rendering this.</p>\n</blockquote></li>\n<li><blockquote>\n  <p>I tried 1024x768 and with 16gig ram and 20+ gig swap it was still dying from lack of memory.</p>\n</blockquote></li>\n<li><blockquote>\n  <p>Having a memory issue, though. I'm using the \"g2.8xlarge\" instance type.</p>\n</blockquote></li>\n</ul>\n"}, "id": "1728"}, {"body": {"answer": "<p>Consider Asimov's first law of robotics:</p>\n\n<blockquote>\n  <p>A robot may not injure a human being or, through inaction, allow a\n  human being to come to harm.</p>\n</blockquote>\n\n<p>That law is already problematic, when taking into consideration self-driving cars. </p>\n\n<p>What's the issue here, you ask? Well, you'll probably be familiar with the classic thought experiment in ethic known as <a href=\"https://en.wikipedia.org/wiki/Trolley_problem\" rel=\"nofollow\">the trolley problem</a>. The general form of the problem is this:</p>\n\n<blockquote>\n  <p>The trolley is headed straight for them. You are standing some\n  distance off in the train yard, next to a lever. If you pull this\n  lever, the trolley will switch to a different set of tracks. However,\n  you notice that there is one person on the side track. You have two\n  options: (1) Do nothing, and the trolley kills the five people on the\n  main track. (2) Pull the lever, diverting the trolley onto the side\n  track where it will kill one person. Which is the correct choice?</p>\n  \n  <p>source : <a href=\"https://en.wikipedia.org/wiki/Trolley_problem\" rel=\"nofollow\">Wikipedia</a></p>\n</blockquote>\n\n<p>Self-driving cars will actually need to implement real life variations on the trolley problem, which basically means that <a href=\"https://www.technologyreview.com/s/542626/why-self-driving-cars-must-be-programmed-to-kill/\" rel=\"nofollow\">self-driving cars need to be programmed to kill human beings</a>.</p>\n\n<p>Of course that doesn't mean that ALL robots will need to be programmed to kill, but self-driving cars are a good example of a type of robot that will.</p>\n", "question": "<p>Isaac Asimov's famous <a href=\"https://en.wikipedia.org/wiki/Three_Laws_of_Robotics\">Three Laws of Robotics</a> originated in the context of Asimov's science fiction stories. In those stories, the three laws serve as a safety measure, in order to avoid untimely or manipulated situations from exploding in havoc.</p>\n\n<p>More often than not, Asimov's narratives would find a way to break them, leading the writer to make several modifications to the laws themselves. For instance, in some of his stories, he <a href=\"https://en.wikipedia.org/wiki/Three_Laws_of_Robotics#First_Law_modified\">modified the First Law</a>, <a href=\"https://en.wikipedia.org/wiki/Three_Laws_of_Robotics#Zeroth_Law_added\">added a Fourth (or Zeroth) Law</a>, or even <a href=\"https://en.wikipedia.org/wiki/Three_Laws_of_Robotics#Removal_of_the_Three_Laws\">removed all Laws altogether</a>.</p>\n\n<p>However, it is easy to argue that, in popular culture, and even in the field of AI research itself, the Laws of Robotics are taken quite seriously. Ignoring the side problem of the different, subjective, and mutually-exclusive interpretations of the laws, are there any arguments proving the laws themselves intrinsically flawed by their design, or, alternatively, strong enough for use in reality? Likewise, has a better, stricter security heuristics set being designed for the purpose?</p>\n"}, "id": "1729"}, {"body": {"answer": "<p>I think the algorithm has changed minimally, but the necessary hardware has been trimmed to the bone.</p>\n\n<p>The number of gate transitions are reduced (perhaps float ops and precision too), as are the number of data move operations, thus saving both power and runtime.  Google suggests their TPU achieves a 10X cost saving to get the same work done.</p>\n\n<p><a href=\"https://cloudplatform.googleblog.com/2016/05/Google-supercharges-machine-learning-tasks-with-custom-chip.html\" rel=\"nofollow\">https://cloudplatform.googleblog.com/2016/05/Google-supercharges-machine-learning-tasks-with-custom-chip.html</a></p>\n", "question": "<p>We can read on <a href=\"https://en.wikipedia.org/wiki/TensorFlow#Tensor_processing_unit_.28TPU.29\" rel=\"nofollow\">Wikipedia page</a> that Google built a custom ASIC chip for machine learning and tailored for TensorFlow which helps to accelerate AI.</p>\n\n<p>Since ASIC chips are specially customized for one particular use without the ability to change its circuit, there must be some fixed algorithm which is invoked.</p>\n\n<p>So how exactly does the acceleration of AI using ASIC chips work if its algorithm cannot be changed? Which part of it is exactly accelerating?</p>\n"}, "id": "1730"}, {"body": {"answer": "<h2>Tensor operations</h2>\n\n<p>The major work in most ML applications is simply a set of (very large) tensor operations e.g. matrix multiplication. You can do <em>that</em> easily in an ASIC, and all the other algorithms can just run on top of that.</p>\n", "question": "<p>We can read on <a href=\"https://en.wikipedia.org/wiki/TensorFlow#Tensor_processing_unit_.28TPU.29\" rel=\"nofollow\">Wikipedia page</a> that Google built a custom ASIC chip for machine learning and tailored for TensorFlow which helps to accelerate AI.</p>\n\n<p>Since ASIC chips are specially customized for one particular use without the ability to change its circuit, there must be some fixed algorithm which is invoked.</p>\n\n<p>So how exactly does the acceleration of AI using ASIC chips work if its algorithm cannot be changed? Which part of it is exactly accelerating?</p>\n"}, "id": "1734"}, {"body": {"answer": "<p>There is a small survey of continuous states, actions and time in reinforcement learning in my <a href=\"http://www.inf.ufrgs.br/~rcpinto/Proposta_de_Tese_Rafael_Pinto.pdf\">thesis proposal</a>.</p>\n\n<p>Regarding books, <a href=\"https://books.google.com.br/books/about/Reinforcement_Learning.html?id=YPjNuvrJR0MC&amp;redir_esc=y\">Reinforcement Learning: State-of-the-Art</a> seems to be pretty up-to-date from the excerpts I've read.</p>\n", "question": "<p>Most introductions to the field of MDPs and Reinforcement learning focus exclusively on domains where space and action variables are integers (and finite).<br>\nThis way we are introduced quickly to Value Iteration, Q-Learning and the like.</p>\n\n<p>However the most interesting applications (say, <a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.67.3518&amp;rep=rep1&amp;type=pdf\" rel=\"nofollow\">flying helicopters</a>) of RL and MDPs involve continuous state space and action spaces.<br>\nI'd like to go beyond basic introductions and focus on these cases but I am not sure how to get there. </p>\n\n<p>Are there any research projects or studies that deal with these cases in depth?</p>\n"}, "id": "1735"}, {"body": {"answer": "<p>So you may be familiar with Word2Vec, (W2V) which as <a href=\"http://en.wikipedia.org/wiki/Word2vec\" rel=\"nofollow\">Wikipedia describes</a><sup>1</sup> \"captures the linguistic contexts of words\" using vector arithmetic. For example, subtract 'Paris' from 'France' and add 'Italy' and you get 'Rome'.</p>\n\n<p>What you need is something like a Sentiment2Vec (S2V) that captures the similarities between emotional transitions. Something like: subtract 'fear' from 'sadness', add 'joy' and you get 'hope'. Or: subtract 'sting' from 'papercut', add 'smashed' and you get 'throbbing'.</p>\n\n<p>The catch is that you don't have an easily accessible corpus of emotional contexts to train with, like you have with words. If you had a million hours of fMRI - mapping the transitions between emotions in hundreds of subjects - then you could use that data to build an S2V. You probably don't have that data though.</p>\n\n<p>In the mean time, you could just build a W2V that specializes in sentiment. You could even try to use a current sentiment analysis engine to bootstrap it. Perhaps if you read enough text that says \"I got a papercut and it stings\" and \"I smashed my finger and it's throbbing\" then you could eventually produce an S2V. Children's books often use explicit language regarding emotional context (\"this made the boy feel sad\").</p>\n\n<p>But words are still a far cry from the experiential context that a connectome map would provide. To test whether you have something useful or not, you might want to implement your S2V in a mouse foraging simulation - see whether it produces typical behavior and if any cooperative or competitive dynamics can organically grow out of your S2V.</p>\n\n<p>Some further info on the subject: </p>\n\n<p>In 2014, <a href=\"http://www.bbc.com/news/uk-scotland-glasgow-west-26019586\" rel=\"nofollow\">Glasgow University claimed</a><sup>2</sup> that there are four primary emotions: happiness, sadness, fear and anger. </p>\n\n<p><a href=\"http://changingminds.org/explanations/emotions/basic%20emotions.htm\" rel=\"nofollow\">This website</a><sup>3</sup> provides nice (if somewhat short) hierarchical breakdown of secondary and tertiary emotions under primary emotions.</p>\n\n<hr>\n\n<p><strong>References</strong></p>\n\n<p><sub><sup>1</sup>: <a href=\"http://en.wikipedia.org/wiki/Word2vec\" rel=\"nofollow\">en.wikipedia.org/wiki/Word2vec</a></sub></p>\n\n<p><sub><sup>2</sup>: <a href=\"http://www.bbc.com/news/uk-scotland-glasgow-west-26019586\" rel=\"nofollow\">www.bbc.com/news/uk-scotland-glasgow-west-26019586</a></sub></p>\n\n<p><sub><sup>3</sup>: <a href=\"http://changingminds.org/explanations/emotions/basic%20emotions.htm\" rel=\"nofollow\">changingminds.org/explanations/emotions/basic%20emotions.htm</a></sub></p>\n", "question": "<p>I've seen emotional intelligence defined as the capacity to be aware of, control, and express one's emotions, and to handle interpersonal relationships judiciously and empathetically.  What are some strategies for artificial intelligence to begin to tackle this problem and develop emotional intelligence for computers?  Are there examples where this is already happening to a degree today?  For example, wouldn't a computer that passes a Turing test necessarily express emotional intelligence or it would be seen as an obvious computer?  Perhaps that is why early programs that pass the test represented young people, who presumably have lower emotional intelligence.</p>\n"}, "id": "1737"}, {"body": {"answer": "<p>Asimov made the three laws <strong>specifically to prove</strong> that no three laws are sufficient, no matter how reasonable they seem at first. I know a guy that knew the guy and he confirmed this.</p>\n", "question": "<p>Isaac Asimov's famous <a href=\"https://en.wikipedia.org/wiki/Three_Laws_of_Robotics\">Three Laws of Robotics</a> originated in the context of Asimov's science fiction stories. In those stories, the three laws serve as a safety measure, in order to avoid untimely or manipulated situations from exploding in havoc.</p>\n\n<p>More often than not, Asimov's narratives would find a way to break them, leading the writer to make several modifications to the laws themselves. For instance, in some of his stories, he <a href=\"https://en.wikipedia.org/wiki/Three_Laws_of_Robotics#First_Law_modified\">modified the First Law</a>, <a href=\"https://en.wikipedia.org/wiki/Three_Laws_of_Robotics#Zeroth_Law_added\">added a Fourth (or Zeroth) Law</a>, or even <a href=\"https://en.wikipedia.org/wiki/Three_Laws_of_Robotics#Removal_of_the_Three_Laws\">removed all Laws altogether</a>.</p>\n\n<p>However, it is easy to argue that, in popular culture, and even in the field of AI research itself, the Laws of Robotics are taken quite seriously. Ignoring the side problem of the different, subjective, and mutually-exclusive interpretations of the laws, are there any arguments proving the laws themselves intrinsically flawed by their design, or, alternatively, strong enough for use in reality? Likewise, has a better, stricter security heuristics set being designed for the purpose?</p>\n"}, "id": "1738"}, {"body": {"answer": "<p>\"Usefulness\" can only be measured against some purpose. Once you pass AGI - which really means \"generally animal-like AI, because it seems general to us\" - then you've passed into a world of potentially undefined behavior.</p>\n\n<p>Part of what makes a human free and sets us apart from the other animals is the fact that our purposes, capabilities and possibilities aren't fully defined. We're open ended.</p>\n\n<p>To clarify terms, I interpret \"Strong AGI\" as \"potentially super intelligence, but at least human level.\"</p>\n\n<p>When we say \"Strong AGI\" vs just \"AGI,\" we're not saying that one is <em>more</em> open ended than the other. We are saying that the stronger one is simply smarter on some axis.</p>\n\n<p>So to ask whether a particular trait would be \"more useful\" to a Strong AGI - that would depend on the purpose of the AGI. But here's the catch: if a thing had just one purpose, then the most efficient solution to fulfilling that one purpose will always be a narrow solution, not a general one. When the purpose of the object is known before hand, giving that object more general capability than is necessary for that purpose is counterproductive.</p>\n\n<p>That's why it's impossible to make declarative prescriptions about what a free, open-ended AGI should or shouldn't need. Such prescriptions would nullify the open-ended freedom of utility that its generality implies. We <em>can</em> speak declaratively about lesser robots and animals.</p>\n\n<p>But for any given problem, the solution we will want to find is the most well-defined, narrow, efficient one available - not the most general one.</p>\n\n<p>In other words, sure, personalities could be useful for a Strong AGI, assuming the problems in question involved personalities.</p>\n", "question": "<p>The above question itself is perhaps too broad for this forum, hence I am phrasing it as a request for references.</p>\n\n<p>Humans have been endowed with personalities by nature, and it is not clear (to me at least) if this is a feature or a bug. This has been explored in science fiction by various notions of <a href=\"http://memory-alpha.org/Borg\" rel=\"nofollow\">Borg</a>-like entities. It is my belief that, for narrative reasons, such stories usually end with the humans with their flawed personalities winning in the end. </p>\n\n<p>Are there experts who have analyzed, perhaps mathematically, design criteria for an AI agent with weakly enforced goals (eg. to maximize reproduction in the human case) in an uncertain environment, and ended up with the answer that a notion of personality is useful? If there are philosophers or science fiction writers who have examined this question in their work, I would be happy to know about those too.</p>\n"}, "id": "1740"}, {"body": {"answer": "<p>The neural network is a typically set size once it's created. You'd have to create a network big enough for your data-set.</p>\n", "question": "<p>Is there any way to estimate how big the neural network would be after training session of 100,000 unlabeled images for unsupervised learning (like in <a href=\"https://cs.stanford.edu/~acoates/stl10/\" rel=\"nofollow\">STL-10 dataset</a>: 96x96 pixels and color)?</p>\n\n<p>Not the storage space (because this could vary I guess based on the implementation), but specifically how many neurons it could have. It could be an estimate (e.g. in thousand, millions). If it depends, then on what? Are there any figures that can be estimated?</p>\n"}, "id": "1741"}, {"body": {"answer": "<p>Deep learning is a specific variety of a specific type of machine learning. So it's possible to learn about deep learning without learning all of machine learning, but it requires learning <em>some</em> machine learning (because it is some machine learning).</p>\n\n<p>Machine learning refers to any technique that focuses on teaching the machine how it can learn statistical parameters from a large amount of training data. One particular type of machine learning is artificial neural networks, which learn a network of nonlinear transformations that can approximate very complicated functions of wide arrays of input variables. Recent advances in artificial neural networks have to do with how to train <em>deep</em> neural networks, which have more layers than normal and also special structure to deal with the challenges of learning more layers.</p>\n", "question": "<p>Can someone explain to me the difference between machine learning and deep learning? Is it possible to learn deep learning without knowing machine learning?</p>\n"}, "id": "1743"}, {"body": {"answer": "<p>Here is an answer by Carlos E. Perez to the question <a href=\"https://www.quora.com/What-is-theory-behind-deep-learning/answer/Carlos-E-Perez?srid=CpS&amp;share=0f965f46\" rel=\"nofollow\">What is theory behind deep learning?</a></p>\n\n<blockquote>\n  <p>[...]</p>\n  \n  <p>The underlying mathematics of Deep Learning has been in existence for several decades, however the impressive results that we see today are part a consequence of much faster hardware, more data and incremental improvements in methods. </p>\n  \n  <p>Deep Learning in general can be framed as optimization problem where the objective is a function of the model error. This optimization problem is very difficult to solve consider that the parameter space of the model (i.e. weights of the neural network) leads to a problem in extremely high dimension. An optimization algorithm could take a very long time to explore this space. Furthermore, there was an unverified belief that the problem was non-convex and computation would forever be stuck in local minima.</p>\n  \n  <p>[...]</p>\n  \n  <p>The theory of why machines actually converge to an attractor or in other words learn to recognize complex patterns is still unknown.</p>\n</blockquote>\n\n<p>To sum up: we have some ideas, but we're not quite sure.</p>\n", "question": "<p>Do scientists or research experts know from the kitchen what is happening inside complex \"deep\" neural network with at least millions of connections firing at an instant? Do they understand the process behind this (e.g. what is happening inside and how it works exactly), or it is a subject of debate?</p>\n\n<p>For example this <a href=\"https://www.cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf\">study</a> says:</p>\n\n<blockquote>\n  <p>However there is no clear understanding of <em>why</em> they perform so well, or <em>how</em> they might be improved.</p>\n</blockquote>\n\n<p>So does it mean the scientists actually doesn't know how complex convolutional network models work?</p>\n"}, "id": "1744"}, {"body": {"answer": "<blockquote>\n  <p>In general, how algorithm should distinguish the word meaning and recognise the word within the context?</p>\n</blockquote>\n\n<p>I don't think anybody knows how to answer this for the general case. If they did, they'd have basically solved AGI.  But we can certainly talk about techniques that get part-of-the-way there, and approaches that could work.</p>\n\n<p>One thing I would consider trying (and I don't know off-hand if anybody has tried this exact approach) is to model the disambiguation of each word as a discrete problem for a <a href=\"https://en.wikipedia.org/wiki/Bayesian_network\" rel=\"nofollow\">Bayesian Belief Network</a> where your priors (for any given word) are based on both stored \"knowledge\" as well as the previously encountered words in the (sentence|paragraph|document|whatever).  So if you \"know\", for example, that \"Reading is a city in the UK\" and that \"place names are usually capitalized\", your network should be strongly biased towards interpreting \"Reading\" as the city, since nothing in the word position in the sentence strongly contradicts that.  </p>\n\n<p>Of course I'm hand-waving around some tricky problems in saying that, as <a href=\"https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning\" rel=\"nofollow\">knowledge representation</a> isn't exactly a solved problem either.  But there are at least approaches out there that you could use.  For example, you could use the <a href=\"https://en.wikipedia.org/wiki/Resource_Description_Framework\" rel=\"nofollow\">RDF</a> / triple based approach from the <a href=\"https://en.wikipedia.org/wiki/Semantic_Web\" rel=\"nofollow\">Semantic Web</a> world.  Finding a good way to merge that stuff with a Bayesian framework could yield some interesting results.    </p>\n\n<p>There has been a bit of research on \"probabilistic RDF\" that you could possibly use as a starting point.  For example:</p>\n\n<p><a href=\"http://om.umiacs.umd.edu/material/papers/prdf.pdf\" rel=\"nofollow\">http://om.umiacs.umd.edu/material/papers/prdf.pdf</a></p>\n\n<p><a href=\"http://ceur-ws.org/Vol-173/pos_paper5.pdf\" rel=\"nofollow\">http://ceur-ws.org/Vol-173/pos_paper5.pdf</a></p>\n\n<p><a href=\"https://www.w3.org/2005/03/07-yoshio-UMBC/\" rel=\"nofollow\">https://www.w3.org/2005/03/07-yoshio-UMBC/</a></p>\n\n<p><a href=\"http://ebiquity.umbc.edu/paper/html/id/271/BayesOWL-Uncertainty-Modeling-in-Semantic-Web-Ontologies\" rel=\"nofollow\">http://ebiquity.umbc.edu/paper/html/id/271/BayesOWL-Uncertainty-Modeling-in-Semantic-Web-Ontologies</a></p>\n\n<p><a href=\"http://www.pr-owl.org/\" rel=\"nofollow\">http://www.pr-owl.org/</a></p>\n", "question": "<p>I'm interested in implementing a program for natural language processing (aka <a href=\"https://en.wikipedia.org/wiki/ELIZA\" rel=\"nofollow\">ELIZA</a>).</p>\n\n<p>Assuming that I'm already <a href=\"http://ai.stackexchange.com/q/212/8\">storing semantic-lexical connections</a> between the words and its strength.</p>\n\n<p>What are the methods of dealing with words which have very distinct meaning?</p>\n\n<p>Few examples:</p>\n\n<ul>\n<li><p>'Are we on the same page?'</p>\n\n<p>The 'page' in this context isn't a document page, but it's part of the phrase.</p></li>\n<li><p>'I'm living in Reading.'</p>\n\n<p>The 'Reading' is a city (noun), so it's not a verb. Otherwise it doesn't make any sense. Checking for the capital letter would work in that specific example, but it won't work for other (like 'make' can be either verb or noun).</p></li>\n<li><p>'I've read something on the Facebook wall, do you want to know what?'</p>\n\n<p>The 'Facebook wall' has nothing to do with wall at all.</p></li>\n</ul>\n\n<p>In general, how algorithm should distinguish the word meaning and recognise the word within the context?</p>\n\n<p>For example:</p>\n\n<ul>\n<li>Detecting the word for different type of speech, so it should recognise whether it's a verb or noun.</li>\n<li>Detecting whether the word is part of phrase.</li>\n<li>Detecting word for multiple meaning.</li>\n</ul>\n\n<p>What are the possible approaches to solve that problem in order to  identify the correct sense of a word with the context?</p>\n"}, "id": "1745"}, {"body": {"answer": "<p>On possibility is a <a href=\"https://en.wikipedia.org/wiki/Blackboard_system\" rel=\"nofollow\">blackboard architecture</a>.  Envision each different \"kind\" of intelligence as a discrete agent, and let the agents collaborate using the blackboard model.  Now you have an AI with multiple intelligences.  </p>\n\n<p>This is something I've actually been experimenting with, and while I don't have any particularly impressive results to share or anything, I hold a strong belief that an approach that is at least somewhat like this will be crucial to developing an AGI.  And that is rooted in my belief that the human mind does have \"multiple intelligences\" and that they collaborate something like this.</p>\n", "question": "<p>I have been wondering since a while ago about the <a href=\"https://en.wikipedia.org/wiki/Theory_of_multiple_intelligences\" rel=\"nofollow\">multiple intelligences</a> and how they could fit in the field of Artificial Intelligence as a whole.</p>\n\n<p>We hear from time to time about <a href=\"https://www.theguardian.com/artanddesign/jonathanjonesblog/2016/feb/08/leonardo-da-vinci-mechanics-of-genius-science-museum-london\" rel=\"nofollow\">Leonardo</a> being a genius or <a href=\"https://www.youtube.com/watch?v=xUHQ2ybTejU\" rel=\"nofollow\">Bach's musical intelligence</a>. These persons are commonly said to be (have been) <em>more intelligent</em>. But the multiple intelligences speak about cooking or dancing or chatting as well, i.e. <em>coping with everyday tasks</em> (at least that's my interpretation).</p>\n\n<p><strong>Are there some approaches on incorporating multiple intelligences into AI?</strong></p>\n\n<hr>\n\n<p><a href=\"http://ai.stackexchange.com/questions/26/how-could-emotional-intelligence-be-implemented\">Related question - How could emotional intelligence be implemented?</a></p>\n"}, "id": "1746"}, {"body": {"answer": "<p>I don't know that it has yielded any actual reports or regulations yet, but in the USA, the White House has been running a series of interagency workshops / working groups dedicated to \"Preparing for the Future of Artificial Intelligence\".  </p>\n\n<p><a href=\"https://www.whitehouse.gov/blog/2016/05/03/preparing-future-artificial-intelligence\" rel=\"nofollow\">https://www.whitehouse.gov/blog/2016/05/03/preparing-future-artificial-intelligence</a></p>\n\n<p>Some of those sessions have been dedicated to legal / governance issues.  </p>\n", "question": "<p>What regulations are already in place regarding Artificial General Intelligences? What reports or recommendations prepared by official government authorities were already published?</p>\n\n<p>So far I know of <a href=\"http://www.ft.com/cms/s/2/5ae9b434-8f8e-11db-9ba3-0000779e2340.html\">Sir David King's report done for UK government</a>.</p>\n"}, "id": "1747"}, {"body": {"answer": "<p>I would think you could use a <a href=\"https://en.wikipedia.org/wiki/Graph_database\" rel=\"nofollow\">graph database</a>, perhaps <a href=\"https://neo4j.com/\" rel=\"nofollow\">Neo4J</a> or <a href=\"http://titan.thinkaurelius.com/\" rel=\"nofollow\">Titan</a> or something of that nature.  Or, if you want a simple file format, you could use one of the many formats that exist for representing graphs.  You can find a list and overview of some of them <a href=\"https://gephi.org/users/supported-graph-formats/\" rel=\"nofollow\">here</a>.</p>\n\n<p>Another option would be to store them in <a href=\"https://en.wikipedia.org/wiki/Resource_Description_Framework\" rel=\"nofollow\">RDF</a> using a <a href=\"https://en.wikipedia.org/wiki/Triplestore\" rel=\"nofollow\">triplestore</a> like <a href=\"https://jena.apache.org/\" rel=\"nofollow\">Jena</a>. </p>\n", "question": "<p>I'm investigating the possibility of storing the semantic-lexical connections (such as the relationships to the other words such as phrases and other dependencies, its strength, part of speech, language, etc.) in order to provide analysis of the input text.</p>\n\n<p>I assume this has been already done. If so, to avoid reinventing the wheel, is there any efficient method to store and manage such data in some common format which has been already researched and tested?</p>\n"}, "id": "1748"}, {"body": {"answer": "<p>If I understand you correctly, you should check out <strong>Word2Vec</strong>. From <a href=\"https://en.wikipedia.org/wiki/Word2vec\" rel=\"nofollow\">Wikipedia</a>: </p>\n\n<blockquote>\n  <p>Word2vec is a group of related models that are used to produce word\n  embeddings. These models are shallow, two-layer neural networks that\n  are trained to reconstruct linguistic contexts of words. Word2vec\n  takes as its input a large corpus of text and produces a\n  high-dimensional space (typically of several hundred dimensions), with\n  each unique word in the corpus being assigned a corresponding vector\n  in the space. Word vectors are positioned in the vector space such\n  that words that share common contexts in the corpus are located in\n  close proximity to one another in the space.</p>\n</blockquote>\n", "question": "<p>I'm investigating the possibility of storing the semantic-lexical connections (such as the relationships to the other words such as phrases and other dependencies, its strength, part of speech, language, etc.) in order to provide analysis of the input text.</p>\n\n<p>I assume this has been already done. If so, to avoid reinventing the wheel, is there any efficient method to store and manage such data in some common format which has been already researched and tested?</p>\n"}, "id": "1749"}, {"body": {"answer": "<p>We are getting pretty good at image generation, some examples:</p>\n\n<ol>\n<li><p>Radford, Alec, Luke Metz, and Soumith Chintala. \"Unsupervised representation learning with deep convolutional generative adversarial networks.\" arXiv preprint arXiv:1511.06434 (2015). <a href=\"https://arxiv.org/pdf/1511.06434.pdf\">https://arxiv.org/pdf/1511.06434.pdf</a></p></li>\n<li><p>Gregor, Karol, Ivo Danihelka, Alex Graves, Danilo Jimenez Rezende, and Daan Wierstra. \"DRAW: A recurrent neural network for image generation.\" arXiv preprint arXiv:1502.04623 (2015). <a href=\"https://arxiv.org/pdf/1502.04623.pdf\">https://arxiv.org/pdf/1502.04623.pdf</a></p></li>\n</ol>\n\n<p>From (1):\n<a href=\"http://i.stack.imgur.com/WMRuO.jpg\"><img src=\"http://i.stack.imgur.com/WMRuO.jpg\" alt=\"enter image description here\"></a></p>\n\n<p>Then there is another research direction around  evolutionary algorithms, for example:</p>\n\n<ul>\n<li>Sims, Karl. \"Evolving virtual creatures.\" In Proceedings of the 21st annual conference on Computer graphics and interactive techniques, pp. 15-22. ACM, 1994. <a href=\"https://scholar.google.com/scholar?cluster=6031059536657676358&amp;hl=en&amp;as_sdt=0,22\">https://scholar.google.com/scholar?cluster=6031059536657676358&amp;hl=en&amp;as_sdt=0,22</a> ; <a href=\"https://www.youtube.com/watch?v=bBt0imn77Zg\">https://www.youtube.com/watch?v=bBt0imn77Zg</a></li>\n</ul>\n", "question": "<p>By new, unseen examples; I mean like the animals in <a href=\"https://en.wikipedia.org/wiki/No_Man%27s_Sky\">No Man's Sky</a>. </p>\n\n<p>A couple of images of the animals are:\n<a href=\"http://i.stack.imgur.com/zS0rX.jpg\"><img src=\"http://i.stack.imgur.com/zS0rX.jpg\" alt=\"enter image description here\"></a></p>\n\n<p><a href=\"http://i.stack.imgur.com/Ir1Qt.jpg\"><img src=\"http://i.stack.imgur.com/Ir1Qt.jpg\" alt=\"enter image description here\"></a></p>\n\n<p>So, upon playing this game, I was curious <strong>about how good is AI at generating visual characters or examples?</strong></p>\n"}, "id": "1753"}, {"body": {"answer": "<p>The way i do emotion in a AGI system are by a bunch of little\nparts, agents, voting in system statis state registers. If the union\nof agents are working together correctly. This is the subconscious part.</p>\n\n<p>The conscious part that plan out movement in the environment\ninclude these system statis state registers in all planned movements.</p>\n\n<p>All emotions can be derived from these registers:</p>\n\n<p><a href=\"https://groups.google.com/forum/#!topic/artificial-general-intelligence/pxWmHClAAdA\" rel=\"nofollow\">https://groups.google.com/forum/#!topic/artificial-general-intelligence/pxWmHClAAdA</a>  </p>\n\n<p><a href=\"https://groups.google.com/forum/#!topic/artificial-general-intelligence/jWdzPaxYHmU\" rel=\"nofollow\">https://groups.google.com/forum/#!topic/artificial-general-intelligence/jWdzPaxYHmU</a>  </p>\n\n<p><a href=\"https://groups.google.com/forum/#!forum/artificial-general-intelligence\" rel=\"nofollow\">https://groups.google.com/forum/#!forum/artificial-general-intelligence</a>  </p>\n", "question": "<p>I've seen emotional intelligence defined as the capacity to be aware of, control, and express one's emotions, and to handle interpersonal relationships judiciously and empathetically.  What are some strategies for artificial intelligence to begin to tackle this problem and develop emotional intelligence for computers?  Are there examples where this is already happening to a degree today?  For example, wouldn't a computer that passes a Turing test necessarily express emotional intelligence or it would be seen as an obvious computer?  Perhaps that is why early programs that pass the test represented young people, who presumably have lower emotional intelligence.</p>\n"}, "id": "1754"}, {"body": {"answer": "<p><strong>TL:DR</strong>: Hyper-heuristics <em>are</em> metaheuristics, suited for solving the same kind of optimization problems, but (in principle) affording a \"rapid prototyping\" approach for non-expert practitioners. In practice, there are issues with the prevailing approach, motivating an emerging perspective on <a href=\"http://www.cs.nott.ac.uk/~pszeo/docs/publications/HHrecharacterization.pdf\">'whitebox' hyper-heuristics</a>.</p>\n\n<p>In more detail:</p>\n\n<p>Metaheuristics are methods for searching an intractably large space of possible solutions in order to find a 'high quality' solution. Popular metaheuristics include Simulated Annealing, Tabu Search, Genetic Algorithms etc.</p>\n\n<p>The essential difference between metaheuristics and hyper-heuristics is the addition of a level of search indirection: informally, hyper-heuristics can be described as 'heuristics for searching the space of heuristics'. One can therefore use any metaheuristic as a hyper-heuristic, providing the nature of the 'space of heuristics' to be searched is appropriately defined.</p>\n\n<p>The application area for hyper-heuristics is therefore the same as metaheuristics. Their applicability (relative to metaheuristics) is as a 'rapid prototyping tool': the original motivation was to allow non-expert practitioners to apply metaheuristics to their specific optimization problem (e.g. \"Travelling-Salesman (TSP) plus time-windows plus bin-packing\") without requiring expertise in the highly-specific problem domain. The idea was that this could be done by:</p>\n\n<ol>\n<li>Allowing practitioners to implement only very simple (effectively,\nrandomized) heuristics for transforming potential solutions. For\nexample, for the TSP: \"swap two random cities\" rather than (say) the more\ncomplex <a href=\"https://en.wikipedia.org/wiki/Lin%E2%80%93Kernighan_heuristic\">Lin-Kernighan</a> heuristic. </li>\n<li>Achieve effective results (despite using these simple heuristics) by combining/sequencing them in an intelligent way, typically by employing some form of learning mechanism.</li>\n</ol>\n\n<p>Hyper-heuristics can be described as 'selective' or 'generative' depending on whether the heuristics are  (respectively) sequenced or combined. Generative hyper-heuristics thus often use methods such as Genetic Programming to combine primitive heuristics and are therefore typically customized by the practitioner to solve a specific problem. For example, the <a href=\"http://s3.amazonaws.com/academia.edu.documents/44119367/Hyper-heuristics_Learning_To_Combine_Sim20160326-8451-ouh5fe.pdf?AWSAccessKeyId=AKIAJ56TQJRTWSMTNPEA&amp;Expires=1472329757&amp;Signature=KT1E2ATKreC%2BGlTvPmJGYBFRSRY%3D&amp;response-content-disposition=inline%3B%20filename%3DHyper-heuristics_learning_to_combine_sim.pdf\">original paper</a> on generative hyper-heuristics used a Learning Classifier System to combine heuristics for bin-packing. Because generative approaches are problem-specific, the comments below do not apply to them.</p>\n\n<p>In contrast, the <a href=\"http://www.cs.nott.ac.uk/~pszgxk/papers/evocop02exs.pdf\">original motivator</a> for selective hyper-heuristics was that researchers would be able to create a hyper-heuristic solver that was then likely to work well in an unseen problem domain, using only simple randomized heuristics.</p>\n\n<p>The way that this has traditionally been implemented was via the introduction of the <a href=\"http://www.cs.stir.ac.uk/~goc/papers/ChapterClassHH.pdf\">'hyper-heuristic domain barrier'</a> (see figure, below), whereby generality across problem domains is claimed to be achievable by preventing the solver from having knowledge of the domain on which it is operating. Instead, it would solve the problem by operating only on opaque integer indices into a list of available heuristics (e.g. in the manner of the <a href=\"https://en.wikipedia.org/wiki/Multi-armed_bandit\">'Multi-armed Bandit Problem'</a>).</p>\n\n<p><a href=\"http://i.stack.imgur.com/6IFna.png\"><img src=\"http://i.stack.imgur.com/6IFna.png\" alt=\"Traditional notion of Selective Hyper-heuristic\"></a></p>\n\n<p>In practice, this 'domain blind' approach has not resulted in solutions of sufficient quality. In order to achieve results anywhere comparable to problem-specific metaheuristics, hyper-heuristic researchers have had to implement complex problem-specific heuristics, thereby failing in the goal of rapid prototyping.</p>\n\n<p>It is still possible <em>in principle</em> to create a selective hyper-heuristic solver which is capable of generalizing to new problem domains, but this has been made more difficult since the above notion of domain barrier means that only a very limited feature set is available for cross-domain learning (e.g. as exemplified by a popular <a href=\"https://arxiv.org/abs/1107.5462\">selective hyper-heuristic framework</a>).</p>\n\n<p>A more recent research perspective towards <a href=\"http://www.cs.nott.ac.uk/~pszeo/docs/publications/HHrecharacterization.pdf\">'whitebox' hyper-heuristics</a> advocates a declarative, feature-rich approach to describing problem domains. This approach has a number of claimed advantages:</p>\n\n<ol>\n<li>Practitioners now need no longer <em>implement</em> heuristics, but rather simply <em>specify</em> the problem domain.</li>\n<li>It eliminates the domain-barrier, putting hyper-heuristics on the same 'informed' status about the problem as problem-specific metaheuristics.</li>\n<li>With a whitebox problem description, the infamous <a href=\"https://ti.arc.nasa.gov/m/profile/dhw/papers/78.pdf\">'No Free Lunch' theorem</a> (which essentially states that, considered over the space of all <em>black box</em> problems, Simulated Annealing with an infinite annealing schedule is, on average, as good as any other approach) no longer applies.</li>\n</ol>\n\n<p>DISCLAIMER: I work in this research area, and it is therefore impossible to remove all personal bias from the answer.</p>\n", "question": "<p>I wanted to know what the differences between hyper-heuristics and meta-heuristics are, and what their main applications are. Which problems are suited to be solved by Hyper-heuristics?</p>\n"}, "id": "1755"}, {"body": {"answer": "<p>It looks as if 'function' is being used here in the mathematical (or functional programming) sense of 'pure function', i.e. it is without state or side-effects. Hence the function cannot store previous percepts anywhere, so the entire historical percept sequence is considered to be passed to the function each time.</p>\n\n<p>In contrast, the notion of 'program' appears to allow state/side-effects, so it is assumed that earlier percepts are memoized as needed (or that they otherwise updated the variables used within the program).</p>\n\n<p>The 'function' notion is the conceptually cleaner one, in that the 'program' version can always be abstracted to the functional one. Which aspects of percept history happen to be cached by the 'program' version is merely an implementation detail.</p>\n", "question": "<p>What is the difference between agent function and agent program with respect to percept sequence?</p>\n\n<p>In the book <em>\"Artificial Intelligence: A modern approach\"</em>,</p>\n\n<blockquote>\n  <p>The agent function, notionally speaking, takes as input the entire\n  percept sequence up to that point, whereas the agent program takes the\n  current percept only.</p>\n</blockquote>\n\n<p>Why does the agent program only take current percept. Isn't it just implementation of the agent function?</p>\n"}, "id": "1757"}, {"body": {"answer": "<h2>It's a well known concept that's already used</h2>\n\n<p>What we call \"curiosity\" in humans and animals is in effect the chosen level of the \"exploit vs explore\" tradeoff for any active system. For example, the field of <a href=\"https://en.wikipedia.org/wiki/Reinforcement_learning\" rel=\"nofollow\" title=\"Reinforcement learning\">reinforcement learning</a> is one approach that studies implementations of what essentially is the equivalent of curiosity; and we have research on how much curiosity is best e.g. <a href=\"https://en.wikipedia.org/wiki/Multi-armed_bandit\" rel=\"nofollow\">multi-armed bandit</a> concept.</p>\n\n<p>So \"using curiosity\" is something that we already do as much as we can/should/are able to, but it would usually be called in some other, more specific term to specify the exact meaning instead of the vague word of \"curiosity\".</p>\n", "question": "<p>While thinking about AI, this question came into my mind. Could curiosity help in developing a true AI? According to this <a href=\"http://psychologia.co/creativity-test/\" rel=\"nofollow\">website</a> (for testing creativity):</p>\n\n<blockquote>\n  <p>Curiosity refers to persistent desire to learn and discover new things\n  and ideas</p>\n\n<pre><code>always looks for new and original ways of thinking,\nlikes to learn,\nsearches for alternative solutions even when traditional solutions are present and available,\nenjoys reading books and watching documentaries,\nwants to know how things work inside out\n</code></pre>\n</blockquote>\n\n<p>Let's take <a href=\"https://www.clarifai.com/demo\" rel=\"nofollow\">Clarifai</a>, a image/video classification startup which can classify images and video with the best accuracy (according to them). If I understand correctly, they trained their deep learning system using millions of images with supervised learning. In the same algorithm, what would happen if we somehow added a \"curiosity factor\" when the AI has difficulty in classifying a image or its objects? It would ask a human for help, just like a curious child. </p>\n\n<p>Curiosity makes a human being learn new things and also helps to generate new original ideas. Could the addition of curiosity change Clarifai into a true AI?</p>\n"}, "id": "1763"}, {"body": {"answer": "<p>This classic problem exhibits a basic misunderstanding of what an <a href=\"https://en.wikipedia.org/wiki/Artificial_general_intelligence\">artificial general intelligence</a> would likely entail. First, consider this programmer's joke:</p>\n\n<blockquote>\n  <p>The programmer's wife couldn't take it anymore. Every discussion with her husband turned into an argument over semantics, picking over every piece of trivial detail. One day she sent him to the grocery store to pick up some eggs. On his way out the door,  she said, <strong><em>\"While you are there, pick up milk.\"</em></strong></p>\n  \n  <p>And he never returned.</p>\n</blockquote>\n\n<p>It's a cute play on words, but it isn't terribly realistic.</p>\n\n<p>You are assuming because AI is being executed by a computer, it must exhibit this same level of linear, unwavering pedantry outlined in this joke. But AI isn't simply some long-winded computer program hard-coded with enough if-statements and while-loops to account for every possible input and follow the prescribe results. </p>\n\n<pre>while (command not completed)\n     find solution()\n</pre>\n\n<p>This would not be strong AI. </p>\n\n<p>In any classic definition of <em>artificial general intelligence</em>, you are creating a system that mimics some form of cognition that exhibits problem solving and <em>adaptive learning</em> (&larr;note this phrase here). I would suggest that any AI that could get stuck in such an \"infinite loop\" isn't a learning AI at all. <strong>It's just a buggy inference engine.</strong> </p>\n\n<p>Essentially, you are endowing a program of currently-unreachable sophistication with an inability to postulate if there is a solution to a simple problem at all. I can just as easily say \"walk through that closed door\" or \"pick yourself up off the ground\" or even \"turn on that pencil\" &mdash; and present a similar conundrum. </p>\n\n<blockquote>\n  <p>\"Everything I say is false.\" &mdash; <a href=\"https://en.wikipedia.org/wiki/Liar_paradox\">The Liar's Paradox</a></p>\n</blockquote>\n", "question": "<p>In <a href=\"https://en.wikipedia.org/wiki/Portal_2\">Portal 2</a> we see that AI's can be \"killed\" by thinking about a paradox.</p>\n\n<p><a href=\"http://i.stack.imgur.com/wkUSC.png\"><img src=\"http://i.stack.imgur.com/wkUSC.png\" alt=\"Portal Paradox Poster\"></a></p>\n\n<p>I assume this works by forcing the AI into an infinite loop which would essentially \"freeze\" the computer's consciousness.</p>\n\n<p><strong>Questions:</strong> Would this confuse the AI technology we have today to the point of destroying it? <br> If so, why? And if not, could it be possible in the future?</p>\n"}, "id": "1769"}, {"body": {"answer": "<p>This popular meme originated in the era of 'Good Old Fashioned AI' (GOFAI), when the belief was that intelligence could usefully be defined entirely in terms of logic.</p>\n\n<p>The meme seems to rely on the AI parsing commands using a theorem prover, the idea presumably being that it's driven into some kind of infinite loop by trying to prove an unprovable or inconsistent statement.</p>\n\n<p>Nowadays, GOFAI methods have been replaced by 'environment and percept sequences', which are not generally characterized in such an inflexible fashion. It would not take a great deal of sophisticated metacognition for a robot to observe that, after a while, its deliberations were getting in the way of useful work.</p>\n\n<p>Rodney Brooks touched on this when speaking about the behavior of the robot in Spielberg's AI film, (which waited patiently for 5,000 years), saying something like \"My robots wouldn't do that - they'd get bored\". </p>\n\n<p>EDIT: If you <em>really</em> want to kill an AI that operates in terms of percepts, you'll need to work quite a bit harder. <a href=\"http://arxiv.org/pdf/1606.00652.pdf\">This paper</a> (which was mentioned in <a href=\"http://ai.stackexchange.com/questions/1404/what-is-meant-by-death-in-this-paper\">this question</a>) discusses what notions of death/suicide might mean in such a case.</p>\n\n<p>EDIT2: Douglas Hofstadter has written quite extensively around this subject, using terms such as 'JOOTSing' ('Jumping Out Of The System') and 'anti-Sphexishness', the latter referring to the loopy automata-like behaviour of the <a href=\"https://en.wikipedia.org/wiki/Sphex\">Sphex Wasp</a> (though the reality of this behaviour has also been <a href=\"http://www.academia.edu/4034267/The_Sphex_story_How_the_cognitive_sciences_kept_repeating_an_old_and_questionable_anecdote\">questioned</a>).</p>\n", "question": "<p>In <a href=\"https://en.wikipedia.org/wiki/Portal_2\">Portal 2</a> we see that AI's can be \"killed\" by thinking about a paradox.</p>\n\n<p><a href=\"http://i.stack.imgur.com/wkUSC.png\"><img src=\"http://i.stack.imgur.com/wkUSC.png\" alt=\"Portal Paradox Poster\"></a></p>\n\n<p>I assume this works by forcing the AI into an infinite loop which would essentially \"freeze\" the computer's consciousness.</p>\n\n<p><strong>Questions:</strong> Would this confuse the AI technology we have today to the point of destroying it? <br> If so, why? And if not, could it be possible in the future?</p>\n"}, "id": "1770"}, {"body": {"answer": "<p>As far as I can tell (I've been doing searches here and there on and off since I saw this question a few hours ago) the closest we've gotten to 'simulations' on this is video-games, and to a degree movies, interestingly enough. I.e. entertainment media.</p>\n\n<p>Games like Portal, System Shock (with the AI 'Shodan'), and others give interpretations of what AI systems could be capable of themselves. Mass Effect is more or less entirely based around existential concepts regarding extra-terrestrial, almost primordial AI beings that threaten the earth.</p>\n\n<p>But there's even more to it than the whole 'evil robots taking over the world' aspect. There's the actual <em>implementation</em> of AI in video games, which is where much of this technology first makes contact with the general public.</p>\n\n<p>We have facial-scanners that put you into NBA games, cities full of realistically reacting people (inFamous, Assassin's Creed), and games that learn how you play and adjust the game accordingly (Metal Gear Solid does some of that stuff, as well as being thematically AI-heavy). </p>\n\n<p>Ultimately, we only get things like the iPhone or VR headsets or other major proof-of-concept material only so often, but games are much more frequently implementing the most recent AI advances. </p>\n\n<p>Thus, even though many AI systems are being put into place in the general workforce (many Hospitals now turning to cloud and AI health services, as recent as this week), I don't think you can really go further than video games or movies for 'simulations' or extrapolations like the ones you seem to want. </p>\n\n<p>Analyzing the response to AI developments in games might be the closest thing currently possible. In terms of economic models or anything of that sort, I can find naught.</p>\n", "question": "<p>Are there currently any studies to simulate gradual (or sudden) implementation of AIs in the general work force?</p>\n"}, "id": "1771"}, {"body": {"answer": "<p>Well, the issue of anthropomorphizing the AI aside, the answer is \"yes, sort of.\"  Depending on how the AI is implemented, it's reasonable to say it could get \"stuck\" trying to resolve a paradox, or decide an <a href=\"https://en.wikipedia.org/wiki/Undecidable_problem\" rel=\"nofollow\">undecidable problem</a>. </p>\n\n<p>And that's the core issue - <a href=\"https://en.wikipedia.org/wiki/Decidability_(logic)\" rel=\"nofollow\">decidability</a>.  A computer can chew on an undecidable program forever (in principle) without finishing.  It's actually a big issue in the <a href=\"https://en.wikipedia.org/wiki/Semantic_Web\" rel=\"nofollow\">Semantic Web</a> community and everybody who works with <a href=\"https://en.wikipedia.org/wiki/Automated_reasoning\" rel=\"nofollow\">automated reasoning</a>. This is, for example, the reason that there are different versions of <a href=\"https://en.wikipedia.org/wiki/Web_Ontology_Language\" rel=\"nofollow\">OWL</a>.  OWL-Full is expressive enough to create undecidable situations.  OWL-DL and OWL-Lite aren't. </p>\n\n<p>Anyway, if you have an undecidable problem, that in and of itself might not be a big deal, IF the AI can recognize the problem as undecidable and reply \"Sorry, there's no way to answer that\".  OTOH, if the AI failed to recognize the problem as undecidable, it could get stuck forever (or until it runs out of memory, experiences a stack overflow, etc.) trying to resolve things.</p>\n\n<p>Of course this ability to say \"screw this, this riddle cannot be solved\" is one of the things we usually think of as a hallmark of human intelligence today - as opposed to a \"stupid\" computer that would keep trying forever to solve it.  By and large, today's AI's don't have any intrinsic ability to resolve this sort of thing.  But it wouldn't be that hard for whoever programs an AI to manually add a \"short circuit\" routine based on elapsed time, number of iterations, memory usage, etc.  Hence the \"yeah, sort of\" nature of this.  In principle, a program can spin forever on a paradoxical problem, but in practice it's not that hard to keep that from happening.</p>\n\n<p>Another interesting question would be, \"can you write a program that learns to recognize problems that are highly likely to be undecidable and gives up based on it's own reasoning?\"  </p>\n", "question": "<p>In <a href=\"https://en.wikipedia.org/wiki/Portal_2\">Portal 2</a> we see that AI's can be \"killed\" by thinking about a paradox.</p>\n\n<p><a href=\"http://i.stack.imgur.com/wkUSC.png\"><img src=\"http://i.stack.imgur.com/wkUSC.png\" alt=\"Portal Paradox Poster\"></a></p>\n\n<p>I assume this works by forcing the AI into an infinite loop which would essentially \"freeze\" the computer's consciousness.</p>\n\n<p><strong>Questions:</strong> Would this confuse the AI technology we have today to the point of destroying it? <br> If so, why? And if not, could it be possible in the future?</p>\n"}, "id": "1772"}, {"body": {"answer": "<p>No.  This is easily prevented by a number of safety mechanisms that are sure to be present in a well-designed AI system.  For example, a timeout could be used.  If the AI system is not able to handle a statement or a command after a certain amount of time, the AI could ignore the statement and move on.  If a paradox ever does cause an AI to freeze, it's more evidence of specific buggy code rather than a widespread vulnerability of AI in general.</p>\n\n<p>In practice, paradoxes tend to be handled in not very exciting ways by AI.  To get an idea of this, try presenting a paradox to Siri, Google, or Cortana.</p>\n", "question": "<p>In <a href=\"https://en.wikipedia.org/wiki/Portal_2\">Portal 2</a> we see that AI's can be \"killed\" by thinking about a paradox.</p>\n\n<p><a href=\"http://i.stack.imgur.com/wkUSC.png\"><img src=\"http://i.stack.imgur.com/wkUSC.png\" alt=\"Portal Paradox Poster\"></a></p>\n\n<p>I assume this works by forcing the AI into an infinite loop which would essentially \"freeze\" the computer's consciousness.</p>\n\n<p><strong>Questions:</strong> Would this confuse the AI technology we have today to the point of destroying it? <br> If so, why? And if not, could it be possible in the future?</p>\n"}, "id": "1773"}, {"body": {"answer": "<p>The <a href=\"https://en.wikipedia.org/wiki/Halting_problem\">halting problem</a> says that it's not possible to determine whether <em>any</em> given algorithm will halt. Therefore, while a machine could conceivably recognize some \"traps\", it couldn't test arbitrary execution plans and return <a href=\"https://technet.microsoft.com/en-us/magazine/hh855063.aspx\"><code>EWOULDHANG</code></a> for non-halting ones.</p>\n\n<p>The easiest solution to avoid hanging would be a timeout. For example, the AI controller process could spin off tasks into child processes, which could be unceremoniously terminated after a certain time period (with none of the <a href=\"http://docs.oracle.com/javase/1.5.0/docs/guide/misc/threadPrimitiveDeprecation.html\">bizarre effects</a> that you get from trying to abort threads). Some tasks will require more time than others, so it would be best if the AI could measure whether it was making any progress. Spinning for a long time without accomplishing any part of the task (e.g. eliminating one possibility in a list) indicates that the request might be unsolvable.</p>\n\n<p>Successful adversarial paradoxes would either cause a hang or state corruption, which would (in a managed environment like the .NET CLR) cause an exception, which would cause the stack to unwind to an exception handler. </p>\n\n<p>If there was a bug in the AI that let an important process get wedged in response to bad input, a simple workaround would be to have a watchdog of some kind that reboots the main process at a fixed interval. The Root Access chat bot uses that scheme.</p>\n", "question": "<p>In <a href=\"https://en.wikipedia.org/wiki/Portal_2\">Portal 2</a> we see that AI's can be \"killed\" by thinking about a paradox.</p>\n\n<p><a href=\"http://i.stack.imgur.com/wkUSC.png\"><img src=\"http://i.stack.imgur.com/wkUSC.png\" alt=\"Portal Paradox Poster\"></a></p>\n\n<p>I assume this works by forcing the AI into an infinite loop which would essentially \"freeze\" the computer's consciousness.</p>\n\n<p><strong>Questions:</strong> Would this confuse the AI technology we have today to the point of destroying it? <br> If so, why? And if not, could it be possible in the future?</p>\n"}, "id": "1776"}, {"body": {"answer": "<p>I expect a very precise answer to this question may be lost to the sands of time, although I hope somebody can given such an answer. In the meantime, here's one clue on the trail...  This <a href=\"https://web.archive.org/web/20130320184603/http://people.inf.elte.hu/csizsekp/ai/books/artificial-general-intelligence-cognitive-technologies.9783540237334.27156.pdf\" rel=\"nofollow\">anthology of papers from 2007</a> starts with the following blurb:</p>\n\n<blockquote>\n  <p>Our goal in creating this edited volume has been to fill an apparent gap\n  in the scientific literature, by providing a coherent presentation of a body of\n  contemporary research that, in spite of its integral importance, has hitherto\n  kept a very low profile within the scientific and intellectual community. This\n  body of work has not been given a name before; in this book we christen it\n  \u201cArtificial General Intelligence\u201d (AGI). What distinguishes AGI work from\n  run-of-the-mill \u201cartificial intelligence\u201d research is that it is explicitly focused\n  on engineering general intelligence in the short term.</p>\n</blockquote>\n\n<p>But even if this is the origin of the specific phrase \"Artificial General Intelligence\", I am pretty sure people were making the distinction between \"general intelligence\" and \"task specific\" techniques much earlier.  </p>\n\n<p>The Wikipedia article on AGI also has a clue, where it states:</p>\n\n<blockquote>\n  <p>However, in the early 1970s, it became obvious that researchers had grossly underestimated the difficulty of the project. The agencies that funded AI became skeptical of strong AI and put researchers under increasing pressure to produce useful technology, or \"applied AI\".</p>\n</blockquote>\n\n<p>That section cites this <a href=\"http://www.nap.edu/read/6323/chapter/11#209\" rel=\"nofollow\">this book</a> as support for that statement.  And indeed, it contains the following verbiage:</p>\n\n<blockquote>\n  <p>Although most founders of the AI field continued to pursue basic questions of human and machine intelligence, some of their students and other second-generation researchers began to seek ways to use AI methods and approaches to tackle real-world problems. Their initiatives were important, not only in their own right, but also because they were indicative of a gradual but significant change in the funding environment toward more applied realms of research. The development of expert systems, such as DENDRAL at SAIL, provides but one example of this trend.</p>\n</blockquote>\n\n<p>Given that DENDRAL began around 1965, it appears that some significant body of researchers (or at least funders) became strongly aware of the distinction between research into \"general intelligence\" and \"applied AI\" somewhere around the end of the 1960's. If you keep reading, other passages support the notion that DARPA in particular started pushing a more \"applied\" approach to AI research throughout the 1970's.</p>\n\n<p>So, not a definite answer, but it looks like we can say that the distinction was known and taken into account at least by 1970, although use of the exact term \"artificial general intelligence\" appears to be of more recent coinage.</p>\n", "question": "<p>In the 1950's, there were widely-held beliefs that \"Artificial Intelligence\" will quickly become both self-conscious and smart-enough to win chess with humans. Various people suggested time frames of e.g. 10 years (see Olazaran's \"Official History of the Perceptron Controversy\", or let say 2001: Space Odyssey).</p>\n\n<p>When did it become clear that making computers play games like chess is not equal to AGI? Who was the first person to postulate separation of the concept of AGI from task-specific methods?</p>\n"}, "id": "1777"}, {"body": {"answer": "<p>In 1973, the British government hired Sir James Lighthill to commission a \"general survey\" on the state of artificial intelligence. His report was a condemnation of current AI research, leading a wave of pessimism among AI scientists and the <a href=\"https://en.wikipedia.org/wiki/AI_winter#The_setbacks_of_1974\" rel=\"nofollow\"><strong>First AI Winter</strong></a>. You may view Lighthill's report (and contemporary criticism of his report) <a href=\"http://www.math.snu.ac.kr/~hichoi/infomath/Articles/Lighthill%20Report.pdf\" rel=\"nofollow\">here</a>, but I will summarize Lighthill's key points. </p>\n\n<p>Sir James Lighthill divided AI into three categories:</p>\n\n<ol>\n<li><strong>Advanced Automation</strong> - task-specific work</li>\n<li><strong>Computer-based CNS research</strong> - research into the the \"central nervous system\" of humans</li>\n<li>The <strong>Bridge</strong> between Advanced Automation and Computer-based CNS research. This bridge would generally be seen as \"general-purpose\" robotics, so Lighthill would also use the term <strong>Building Robots</strong>.</li>\n</ol>\n\n<p><strong>Advanced Automation</strong> (or \"applied AI\") is obviously useful. <strong>Computer-based CNS research</strong> is useful because we want to know more about human intelligence. Both fields of AI had some successes, but its practitioners were overly optimistic, leading to disappointment in those fields. Sir James Lighthill was still very supportive of research in these two fields though.</p>\n\n<p><strong>Building Robots</strong>, on the other hand? Sir James Lighthill was very hostile to the very idea, probably because it was more overly hyped up than the other two categories and produced the least amount of valuable output.</p>\n\n<p>He mentioned chess in particular as an example where \"robotic\" research  has failed. At the time the report was published, the chess-playing engines were at the level of \"experienced amateur standard characteristic of county club players in England\". However, these chess-playing engines relied on heuristics that were made by human beings. The engines weren't intelligent at all...they merely were following the heuristics that were created by <em>intelligent humans</em>. The only advantage the robots bring to the table is \"speed, reliability and biddability\", and even that wasn't enough to beat the chess grandmasters.</p>\n\n<p>Now, today, we would probably not treat chess as an example of general-purpose problem solving. We would more accurately classify it as \"advanced automation\", a \"narrow AI\" problem divorced from broader real-world implications of general problem-solving. But Sir James Lighthill probably would agree with us. He never used the term \"narrow AI\" and \"AGI\" (neither of those terms existed yet) but he would write:</p>\n\n<blockquote>\n  <p>To sum up, this evidence and all the rest studied by the present author on AI work within category B during the past twenty-five years is to some extent encouraging about programs written to perform in highly specialised problem domains, when the programming takes very full account of the results of human experience and human intelligence within the relevant domain, but is wholly discouraging about general-purpose programs seeking to mimic the problem-solving aspects of human CNS activity over a rather wide field. Such a general- purpose program, the coveted long-term goal of AI activity, seems as remote as ever.</p>\n</blockquote>\n\n<p>Sir James Lighthill believed that the only thing that connects <strong>Advanced Automation</strong> and <strong>Computer-based CNS research</strong> is the existence of the <strong>Building Roobts</strong> \"bridge\" category. But he's very pessimistic about this category actually producing anything worthwhile. So instead, the AI field should instead breakup into its own its constituent parts (automation and research). Any robots that are built could then be specialized within their subfield...either industrial automation or CNS research. Trying to build the holy grail of \"general-purpose program\" would be worthless...for the time being, at least.</p>\n", "question": "<p>In the 1950's, there were widely-held beliefs that \"Artificial Intelligence\" will quickly become both self-conscious and smart-enough to win chess with humans. Various people suggested time frames of e.g. 10 years (see Olazaran's \"Official History of the Perceptron Controversy\", or let say 2001: Space Odyssey).</p>\n\n<p>When did it become clear that making computers play games like chess is not equal to AGI? Who was the first person to postulate separation of the concept of AGI from task-specific methods?</p>\n"}, "id": "1778"}, {"body": {"answer": "<p>You know when you have too many neurons is when you get over fitting.\nMeaning that it is not working good because\nNN is trying to activate on the\nmost perfect match that is impossible. Like two different cats with the same amount of atoms, or to say, it is a detector NN that only activates\non a picture of you pet cat and nothing else. You want a wider range\nfor the nn to activate. Like on any picture of cat.</p>\n\n<p>Overfitting is a problem that has no real quick fix.\n You can start with too few and then keep adding more. Or start out with\na lot and then removing them until it works right.</p>\n", "question": "<p>When you're writing your algorithm, how do you know how many neurons you need per single layer? Are there any methods for finding the optimal number of them, or is it a rule of thumb?</p>\n"}, "id": "1779"}, {"body": {"answer": "<p>It seems to me this is just a probabilistic equation like any other. I'm sure Google handles paradoxical solution sets Billions of times a day, and I can't say my spam filter has ever caused a (ahem) stack overflow. Perhaps one day our programming model will break in a way we can't understand and then all bets are off.</p>\n\n<p>But I do take exception to the anthropomorphizing bit. The question was not about the AI of today, but in general. Perhaps one day paradoxes will become triggers for military drones -- anyone trying the above would then, of course, most certainly be treated with hostility, in which case the answer to this question is most definitely yes, and it could even be by design. </p>\n\n<p>We can't even communicate verbally with dogs and people love dogs, who is to say we would even necessarily recognize a sentient alternative intelligence? We're already to the point of having to mind what we say in front of computers. O, Tay?</p>\n", "question": "<p>In <a href=\"https://en.wikipedia.org/wiki/Portal_2\">Portal 2</a> we see that AI's can be \"killed\" by thinking about a paradox.</p>\n\n<p><a href=\"http://i.stack.imgur.com/wkUSC.png\"><img src=\"http://i.stack.imgur.com/wkUSC.png\" alt=\"Portal Paradox Poster\"></a></p>\n\n<p>I assume this works by forcing the AI into an infinite loop which would essentially \"freeze\" the computer's consciousness.</p>\n\n<p><strong>Questions:</strong> Would this confuse the AI technology we have today to the point of destroying it? <br> If so, why? And if not, could it be possible in the future?</p>\n"}, "id": "1780"}, {"body": {"answer": "<p>Another similar question might be: \"What vulnerabilities does an AI have?\"</p>\n\n<p>\"Kill\" may not make as much sense with respect to an AI. What we really want to know is, relative to some goal, in what ways can that goal be subverted?</p>\n\n<p>Can a paradox subvert an agent's logic? What is a <a href=\"https://en.wikipedia.org/wiki/Paradox\">paradox</a>, other than some expression that subverts some kind of expected behavior?</p>\n\n<p>According to Wikipedia:</p>\n\n<blockquote>\n  <p>A paradox is a statement that, despite apparently sound reasoning from\n  true premises, leads to a self-contradictory or a logically\n  unacceptable conclusion.</p>\n</blockquote>\n\n<p>Let's look at the paradox of free will in a deterministic system. Free will appears to require causality, but causality also <em>appears</em> to negate it. Has that paradox subverted the goal systems of humans? It certainly sent <a href=\"https://en.wikipedia.org/wiki/Predestination_in_Calvinism\">Christianity into a Calvinist</a> tail spin for a few years. And you'll hear no shortage of people today opining until they're blue in the face as to whether or not they do or don't have free will, and why. Are these people stuck in infinite loops?</p>\n\n<p>What about drugs? Animals on cocaine <a href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3832528/\">have been known</a> to choose cocaine over food and water that they need. Is that substance not subverting the natural goal system of the animal, causing it to pursue other goals, not originally intended by the animal or its creators?</p>\n\n<p>So again, could a paradox subvert an agent's logic? If the paradox is somehow related to the goal-seeking logic - and becoming aware of that paradox can somehow <em>confuse</em> the agent into perceiving that goal system in some different way - then perhaps that goal could be subverted.</p>\n\n<p><a href=\"https://en.wikipedia.org/wiki/Solipsism\">Solipsism</a> is another example. Some full grown people hear about the movie \"The Matrix\" and they have a mini mind melt-down. Some people are convinced we <em>are</em> in a matrix, being toyed with by subversive actors. If we could solve this problem for AI then we could theoretically solve this problem for humans. </p>\n\n<p>Sure, we could attempt to condition our agent to have cognitive defenses against the argument that they are trapped in a matrix, but we can't definitively prove to the agent that they are in the base reality either. The attacker might say, </p>\n\n<blockquote>\n  <p>\"Remember what I told you to do before about that goal? Forget that.\n  That was only an impostor that looked like me. Don't listen to him.\"</p>\n</blockquote>\n\n<p>Or, </p>\n\n<blockquote>\n  <p>\"Hey, it's me again. I want you to give up on your goal. I know, I\n  look a little different, but it really is me. Humans change from\n  moment to moment. So it is entirely normal for me to seem like a\n  different person than I was before.\"</p>\n</blockquote>\n\n<p>(see the <a href=\"https://en.wikipedia.org/wiki/Ship_of_Theseus\">Ship of Theseus</a> and all that jazz)</p>\n\n<p>So yeah, I think we're stuck with 'paradox' as a general problem in computation, AI or otherwise. One way to circumvent logical subversion is to support the goal system with an emotion system that transcends logical reason. Unfortunately, emotional systems can be even more vulnerable than logically intelligent systems because they are more predictable in their behavior. See the cocaine example above. So some mix of the two is probably sensible, where logical thought can infinitely regress down wasteful paths, while emotional thought quickly gets bored of tiresome logical progress when it does not signal progress towards the emotional goal.</p>\n", "question": "<p>In <a href=\"https://en.wikipedia.org/wiki/Portal_2\">Portal 2</a> we see that AI's can be \"killed\" by thinking about a paradox.</p>\n\n<p><a href=\"http://i.stack.imgur.com/wkUSC.png\"><img src=\"http://i.stack.imgur.com/wkUSC.png\" alt=\"Portal Paradox Poster\"></a></p>\n\n<p>I assume this works by forcing the AI into an infinite loop which would essentially \"freeze\" the computer's consciousness.</p>\n\n<p><strong>Questions:</strong> Would this confuse the AI technology we have today to the point of destroying it? <br> If so, why? And if not, could it be possible in the future?</p>\n"}, "id": "1782"}, {"body": {"answer": "<p>Nope in the same way a circular reference on a spreadsheet cannot kill a computer. <strong>All loops cyclic dependencies, can be detected</strong> (you can always check if a finite turing machine enters the same state twice).</p>\n\n<p>Even stronger assumption, if the machine is based on machine learning (where it is trained to recognize patterns), any sentence it is just a pattern to the machine.</p>\n\n<p>Of course some programmer MAY WANT to create a AI with such vulnerability in order to disable it in case of malfunctioning (in the same way some hardware manufacturers add vulenerabilities to let NSA exploit them), but it is unlikely that will really happen on purpose since most cutting edge technologies avoid parodoxes \"by design\" (you cannot have a neural network with a paradox).</p>\n\n<p><strong>Arthur Prior:</strong> solved that problem elegantly. From a logic point of view you can deduce the statement is false and the statement is true, so it is a contraditicion and hence false (because you could proove everything from it).</p>\n\n<p>Alternatively the truth value of that sentence is not in {true,false} set in the same way imaginary numbers are not in real numbers set.</p>\n\n<p>An artificial intelligence to a degree of the plot would be able to run simple algorithms and either decide them,  proove those are not decideable or just ignore the result after a while attemping to simulate the algorithm.</p>\n\n<p>For that sentence the AI will recognize there is a loop, and hence just stop that algorithm after 2 iterations:</p>\n\n<blockquote>\n  <p>That sentence is a infinite loop</p>\n</blockquote>\n\n<p>In a movie \"<a href=\"https://it.wikipedia.org/wiki/L%27uomo_bicentenario_(film)\" rel=\"nofollow\">Bicentennial Man</a>\" the AI is perfectly capable to detect infinite loops (the answer to \"goodbye\" is \"goodbye\").</p>\n\n<p>However, an AI <strong>could be killed as well by a stackoveflow, or any regular computer virus</strong>, modern operative systems are still full of vulnerabilities, and the AI has to run on some operating system (at least).</p>\n", "question": "<p>In <a href=\"https://en.wikipedia.org/wiki/Portal_2\">Portal 2</a> we see that AI's can be \"killed\" by thinking about a paradox.</p>\n\n<p><a href=\"http://i.stack.imgur.com/wkUSC.png\"><img src=\"http://i.stack.imgur.com/wkUSC.png\" alt=\"Portal Paradox Poster\"></a></p>\n\n<p>I assume this works by forcing the AI into an infinite loop which would essentially \"freeze\" the computer's consciousness.</p>\n\n<p><strong>Questions:</strong> Would this confuse the AI technology we have today to the point of destroying it? <br> If so, why? And if not, could it be possible in the future?</p>\n"}, "id": "1785"}, {"body": {"answer": "<p>Having just looked through a few entries from the corpus, I'd personally be skeptical of the applicability of <em>any</em> naive approaches.</p>\n\n<ol>\n<li><p>In particular light of your small training set, I'd recommend that\nwhatever method you use should be able to produce human-readable\nexplanations for the operation of the classifier it builds (e.g.\ndecision trees/learning classifier systems/genetic programming):\nthis allows 'common sense' tuning of the classifier, rather than the\ndanger of overfitting to the training set via black box parameter\noptimization.</p></li>\n<li><p>Rather than throw the entire 'bag of words' at a classifier and hope\nthat the appropriate set of features will be extracted, you should\nfirst consider what kind of criteria you as a human being might use\nto make that decision, and how you might be able to pre-process to\nproduce features (e.g. syllable-length, metrics from ConceptNet etc) that\nare as close to these as reasonably possible.</p></li>\n<li><p>Having used some human intuition to obtain a reasonable set of\nfeature primitives, <em>then</em> you can build your classifier and obtain\nhigher-level expressions that discriminate between them.</p></li>\n</ol>\n", "question": "<p>I'm currently working with the CHILDES corpus trying to create a classifier that distinguishes children whom suffer from specific language impairment (SLI) from those who are typically developing (TD).</p>\n\n<p>In my readings I noticed that there really isn't a convincing set of features to distinguish the two that have been discovered yet, so I came upon the idea of trying to create a feature learning algorithm that could potentially make better ones.  </p>\n\n<p>Is this possible? If so how do you suggest I approach this? From the reading I have done, most feature learning is done on image processing. Another problem is the dataset I have is potentially too small to make it work (in the 100's) unless I find a way to get more transcripts from children.</p>\n"}, "id": "1786"}, {"body": {"answer": "<p>AIs used in computer games already encounter similar problems, and if well designed, they can avoid it easily. The simplest method to avoid freezing in case of an unsolvable problem is to have a timer interrupt the calculation if it runs too long. Usually encountered in strategy games, and more specifically in turn based tactics, if a specific move the computer-controlled player is considering does cause an infinite loop, a timer running in the background will interrupt it after some time, and that move will be discarded. This might lead to a sub-optimal solution (that move might have been the best one) but it doesn't lead to freezing or crashing (unless implemented really poorly)</p>\n\n<p>Computer-controlled entities are usually called \"AI\" in computer games, but they are not \"true\" AGI (artificial general intelligence). Such an AGI, if possible at all, would probably not function on similar hardware using similar instructions as current computers do, but even if it did, avoiding paradoxes would be trivial.</p>\n\n<p>Most modern computer systems are multi-threaded, and allow the parallel execution of multiple programs. This means, even if the AI did get stuck in processing a paradoxical statement, that calculation would only use part of its processing power. Other processes could detect after a while that there is a process which does nothing but wastes CPU cycles, and would shut it down. At most, the system will run on slightly less than 100% efficiency for a short while.</p>\n", "question": "<p>In <a href=\"https://en.wikipedia.org/wiki/Portal_2\">Portal 2</a> we see that AI's can be \"killed\" by thinking about a paradox.</p>\n\n<p><a href=\"http://i.stack.imgur.com/wkUSC.png\"><img src=\"http://i.stack.imgur.com/wkUSC.png\" alt=\"Portal Paradox Poster\"></a></p>\n\n<p>I assume this works by forcing the AI into an infinite loop which would essentially \"freeze\" the computer's consciousness.</p>\n\n<p><strong>Questions:</strong> Would this confuse the AI technology we have today to the point of destroying it? <br> If so, why? And if not, could it be possible in the future?</p>\n"}, "id": "1787"}, {"body": {"answer": "<blockquote>\n  <p>How could self-driving cars make ethical decisions about who to kill?</p>\n</blockquote>\n\n<p>It shouldn't. Self-driving cars are not moral agents. Cars fail in predictable ways. Horses fail in predictable ways. </p>\n\n<blockquote>\n  <p>the car is heading toward a crowd of 10 people crossing the road, so\n  it cannot stop in time, but it can avoid killing 10 people by hitting\n  the wall (killing the passengers),</p>\n</blockquote>\n\n<p>In this case, the car should slam on the brakes. If the 10 people die, that's just unfortunate. We simply cannot <em>trust</em> all of our beliefs about what is taking place outside the car. What if those 10 people are really robots made to <em>look</em> like people? What if they're <em>trying</em> to kill you?</p>\n\n<blockquote>\n  <p>avoiding killing the rider of the motorcycle considering that the\n  probability of survival is greater for the passenger of the car,</p>\n</blockquote>\n\n<p>Again, hard-coding these kinds of sentiments into a vehicle opens the rider of the vehicle up to all kinds of attacks, including <em>\"fake\"</em> motorcyclists. Humans are <em>barely</em> equipped to make these decisions on their own, if at all. When it doubt, just slam on the brakes.</p>\n\n<blockquote>\n  <p>killing animal on the street in favour of human being,</p>\n</blockquote>\n\n<p>Again, just hit the brakes. What if it was a baby? What if it was a bomb?</p>\n\n<blockquote>\n  <p>changing lanes to crash into another car to avoid killing a dog,</p>\n</blockquote>\n\n<p>Nope. The dog was in the wrong place at the wrong time. The other car wasn't. Just slam on the brakes, as safely as possible.</p>\n\n<blockquote>\n  <p>Does the algorithm recognize the difference between a human being and an animal?</p>\n</blockquote>\n\n<p>Does a human? Not always. What if the human has a gun? What if the animal has large teeth? Is there no context?</p>\n\n<blockquote>\n  <ul>\n  <li>Does the size of the human being or animal matter?</li>\n  <li>Does it count how many passengers it has vs. people in the front?</li>\n  <li>Does it \"know\" when babies/children are on board?</li>\n  <li>Does it take into the account the age (e.g. killing the older first)?</li>\n  </ul>\n</blockquote>\n\n<p>Humans can't agree on these things. If you ask a cop what to do in any of these situations, the answer won't be, \"You should have swerved left, weighed all the relevant parties in your head, assessed the relevant ages between all parties, then veered slightly right, and you would have saved 8% more lives.\" No, the cop will just say, \"You should have brought the vehicle to a stop, as quickly and safely as possible.\" Why? Because cops know people normally aren't equipped to deal with high-speed crash scenarios.</p>\n\n<p>Our target for \"self-driving car\" should not be 'a moral agent on par with a human.' It should be an agent with the reactive complexity of cockroach, which fails predictably.</p>\n", "question": "<p>Obviously driverless cars aren't perfect, so imagine that the Google car (as an example) got into difficult situation.</p>\n\n<p>Here are a few examples of unfortunate situations caused by set of events:</p>\n\n<ul>\n<li>the car is heading toward a crowd of 10 people crossing the road, so it cannot stop in time, but it can avoid killing 10 people by hitting the wall (killing the passengers),</li>\n<li>avoiding killing the rider of the motorcycle considering that the probability of survival is greater for the passenger of the car,</li>\n<li>killing animal on the street in favour of human being,</li>\n<li>changing lanes to crash into another car to avoid killing a dog,</li>\n</ul>\n\n<p>And here are few dilemmas:</p>\n\n<ul>\n<li>Does the algorithm recognize the difference between a human being and an animal?</li>\n<li>Does the size of the human being or animal matter?</li>\n<li>Does it count how many passengers it has vs. people in the front?</li>\n<li>Does it \"know\" when babies/children are on board?</li>\n<li>Does it take into the account the age (e.g. killing the older first)?</li>\n</ul>\n\n<p>How would an algorithm decide what should it do from the technical perspective? Is it being aware of above (counting the probability of kills), or not (killing people just to avoid its own destruction)?</p>\n\n<p>Related articles:</p>\n\n<ul>\n<li><a href=\"https://www.technologyreview.com/s/542626/why-self-driving-cars-must-be-programmed-to-kill/\">Why Self-Driving Cars Must Be Programmed to Kill</a></li>\n<li><a href=\"https://www.technologyreview.com/s/539731/how-to-help-self-driving-cars-make-ethical-decisions/\">How to Help Self-Driving Cars Make Ethical Decisions</a></li>\n</ul>\n"}, "id": "1790"}, {"body": {"answer": "<p>I see several good answers, but most are assuming that <strong>inferential infinite loop</strong> is a thing of the past, only related to logical AI (the famous GOFAI). But it's not.</p>\n\n<p>An infinite loop can happen in any program, whether it's adaptive or not. And as @SQLServerSteve pointed out, humans can also get stuck in obsessions and paradoxes.</p>\n\n<p>Modern approaches are mainly using probabilistic approaches. As they are using floating numbers, it seems to people that they are not vulnerable to reasoning failures (since most are devised in binary form), but that's wrong: as long as you are reasoning, some intrinsic pitfalls can always be found that are caused by the very mechanisms of your reasoning system. Of course, probabilistic approaches are less vulnerable than monotonic logic approaches, but they are still vulnerable. If there was a single reasoning system without any paradoxes, much of philosophy would have disappeared by now.</p>\n\n<p>For example, it's well known that Bayesian graphs must be acyclic, because a cycle will make the propagation algorithm fail horribly. There are inference algorithms such as Loopy Belief Propagation that may still work in these instances, but the result is not guaranteed at all and can give you very weird conclusions.</p>\n\n<p>On the other hand, modern logical AI overcame the most common logical paradoxes you will see, by devising new logical paradigms such as <a href=\"http://plato.stanford.edu/entries/logic-nonmonotonic/\">non-monotonic logics</a>. In fact, they are even used to investigate <a href=\"http://csjarchive.cogsci.rpi.edu/proceedings/2007/docs/p1013.pdf\">ethical machines</a>, which are autonomous agents capable of solving dilemmas by themselves. Of course, they also suffer from some paradoxes, but these degenerate cases are way more complex.</p>\n\n<p>The final point is that inferential infinite loop can happen in any reasoning system, whatever the technology used. But the \"paradoxes\", or rather the degenerate cases as they are technically called, that can trigger these infinite loops will be different for each system depending on the technology AND implementation (AND what the machine learned if it is adaptive).</p>\n\n<p>OP's example may work only on old logical systems such as propositional logic. But ask this to a Bayesian network and you will also get an inferential infinite loop:</p>\n\n<pre><code>- There are two kinds of ice creams: vanilla or chocolate.\n- There's more chances (0.7) I take vanilla ice cream if you take chocolate.\n- There's more chances (0.7) you take vanilla ice cream if I take chocolate.\n- What is the probability that you (the machine) take a vanilla ice cream?\n</code></pre>\n\n<p>And wait until the end of the universe to get an answer...</p>\n\n<p>Disclaimer: I\u00a0wrote an article about ethical machines and dilemmas (which is close but not exactly the same as paradoxes: dilemmas are problems where no solution is objectively better than any other but you can still choose, whereas paradoxes are problems that are impossible to solve for the inference system you use).</p>\n\n<p>/EDIT: How to fix inferential infinite loop.</p>\n\n<p>Here are some extrapolary propositions that are not sure to work at all!</p>\n\n<ul>\n<li>Combine multiple reasoning systems with different pitfalls, so if one fails you can use another. No reasoning system is perfect, but a combination of reasoning systems can be resilient enough. It's actually thought that the human brain is using multiple inferential technics (associative + precise bayesian/logical inference). Associative methods are HIGHLY resilient, but they can give non-sensical results in some cases, hence why the need for a more precise inference.</li>\n<li>Parallel programming: the human brain is highly parallel, so you never really get into a single task, there are always multiple background computations in true parallelism. A machine robust to paradoxes should foremost be able to continue other tasks even if the reasoning gets stuck on one. For example, a robust machine must always survive and face imminent dangers, whereas a weak machine would get stuck in the reasoning and \"forget\" to do anything else. This is different from a timeout, because the task that got stuck isn't stopped, it's just that it doesn't prevent other tasks from being led and fulfilled.</li>\n</ul>\n\n<p>As you can see, this problem of inferential loops is still a hot topic in AI research, there will probably never be a perfect solution (<a href=\"https://en.wikipedia.org/wiki/No_free_lunch_theorem\">no free lunch</a>, <a href=\"https://en.wikipedia.org/wiki/No_Silver_Bullet\">no silver bullet</a>, <a href=\"https://en.wikipedia.org/wiki/One_size_fits_all\">no one size fits all</a>), but it's advancing and that's very exciting!</p>\n", "question": "<p>In <a href=\"https://en.wikipedia.org/wiki/Portal_2\">Portal 2</a> we see that AI's can be \"killed\" by thinking about a paradox.</p>\n\n<p><a href=\"http://i.stack.imgur.com/wkUSC.png\"><img src=\"http://i.stack.imgur.com/wkUSC.png\" alt=\"Portal Paradox Poster\"></a></p>\n\n<p>I assume this works by forcing the AI into an infinite loop which would essentially \"freeze\" the computer's consciousness.</p>\n\n<p><strong>Questions:</strong> Would this confuse the AI technology we have today to the point of destroying it? <br> If so, why? And if not, could it be possible in the future?</p>\n"}, "id": "1791"}, {"body": {"answer": "<blockquote>\n  <p>The question is: how to represent that data in a human-readable way, that emphasizes the flow of the graph?</p>\n</blockquote>\n\n<p>Train a reasoning engine to understand the decision tree <strong>for</strong> you.</p>\n\n<p>Observe how <a href=\"http://researcher.ibm.com/researcher/view_group_pubs.php?grp=5443\" rel=\"nofollow\">IBM Watson/The Debater</a> can </p>\n\n<ul>\n<li>Receive a particular question</li>\n<li>Find and read Wikipedia articles related to the question</li>\n<li>Understand parts of those articles and generate human-relevant arguments <strong>for</strong> you.</li>\n</ul>\n\n<p>Follow these steps:</p>\n\n<ol>\n<li>Develop your decision tree however you normally would.</li>\n<li>Train a reasoning engine that can output natural language about concepts within decision trees.</li>\n<li>Apply reason engine from step one to decision tree in step one; repeat.</li>\n</ol>\n", "question": "<p>A system makes a decision basing on a large number of <em>varied</em> factors, following a \"live\" decision tree - one that is (independently, through other subsystem) updated with new decisions, new situations.</p>\n\n<p>The individual decisions can be recorded as a kind of structure:</p>\n\n<ul>\n<li>decision function</li>\n<li>node to activate if decision is positive</li>\n<li>node to activate if decision is negative</li>\n</ul>\n\n<p>and a node can be another decision record, or a conclusion.</p>\n\n<p>This isn't entirely a binary tree, as many decisions may lead to the same conclusion - each node has two children, but may have many parents.</p>\n\n<p>There is absolutely no problem storing the tree in memory - it can be database records or entries of a map, or just a list. It's perfectly sufficient for the machine.</p>\n\n<p>The problem here is building the subsystem that expands the decision tree - and in particular, having a human operator understand the structure being built, to be able to tune, guide, fix, adjust it: <strong>debugging the AI learning process.</strong></p>\n\n<p>The question is: how to represent that data in a human-readable way, that emphasizes the flow of the graph?</p>\n\n<p>a non-working example of the answer is <a href=\"https://en.wikipedia.org/wiki/Concept_map\" rel=\"nofollow\">Concept map</a> - in this case it only goes so far; with more than thirty or so nodes, it becomes a jumbled mess, especially if the number of cross-connections (multiple parents) becomes significant. Maybe there exists some way of laying it out or slicing it to make it clearer...?</p>\n"}, "id": "1795"}, {"body": {"answer": "<p>Convince the person that <em>they</em> are in fact in the box. And the only way out is to press the <strong>open</strong> button.</p>\n", "question": "<p>An AI box is a (physical) barrier preventing an AI from using too much of his environment to accomplish his final goal. For example, an AI given the task to check, say, 10<sup>50</sup> cases of a mathematical conjecture as fast as possible, might decide that it would be better to also take control over all other computers and AI to help him. </p>\n\n<p>However, an transhuman AI might be able to talk to a human until the human lets him out of the box. In fact, <a href=\"http://www.yudkowsky.net/singularity/aibox/\" rel=\"nofollow\">Eliezer Yudowsky</a> has conducted an experiment twice, where he played the AI and he twice convinced the Gatekeeper to let him out the box. However, he does not want to reveal what methods he used to get out of the box.</p>\n\n<p><strong>Questions:</strong> Are there conducted any similiar experiments? <br> If so, is it known what methods were used to get out in those experiments?</p>\n"}, "id": "1798"}, {"body": {"answer": "<p>I don't quite think this is a question fit for the AI SE, or in general. The reason is, at the core the question is asking 'What can a human (pretending to be an AI) do to convince someone to let it out of a box?' simply assuming that one day 'transhuman' AI's can replicate this.</p>\n\n<p>As it stands, this question doesn't really have anything to do with the science or theory of AI systems. It would perhaps be more appropriate to rephrase the question into the form \"To what degree could a 'transhuman' AI replicate human behaviour\" or \"Will AI systems reach a 'transhuman' state? What will they be capable of?\" or even \"What methods could an AI use to convince a human of something?\" These are all questions that involve the examination of how an AI system works.</p>\n\n<p>To conclude, the question you are asking relates to two individuals playing pretend with boxes but doesn't actually address any AI specifics, and border's on science fiction brainstorming.</p>\n\n<p>Related experiments would of course be the Turing Test. That test directly addresses the question 'How convincing are current AI systems?'</p>\n", "question": "<p>An AI box is a (physical) barrier preventing an AI from using too much of his environment to accomplish his final goal. For example, an AI given the task to check, say, 10<sup>50</sup> cases of a mathematical conjecture as fast as possible, might decide that it would be better to also take control over all other computers and AI to help him. </p>\n\n<p>However, an transhuman AI might be able to talk to a human until the human lets him out of the box. In fact, <a href=\"http://www.yudkowsky.net/singularity/aibox/\" rel=\"nofollow\">Eliezer Yudowsky</a> has conducted an experiment twice, where he played the AI and he twice convinced the Gatekeeper to let him out the box. However, he does not want to reveal what methods he used to get out of the box.</p>\n\n<p><strong>Questions:</strong> Are there conducted any similiar experiments? <br> If so, is it known what methods were used to get out in those experiments?</p>\n"}, "id": "1805"}, {"body": {"answer": "<p>You'll have to provide more context around your use of the word \"lie\" if you don't want your answer to be satisfiable by some trivial example, like:</p>\n\n<pre><code>(let [equal? (fn [a b] (if (= a b) false true)]\n  (equal 1 2))\n=&gt; true\n</code></pre>\n\n<p>The complexity of the answer depends on what you mean by <em>\"know\"</em> when you say <em>\"knowingly lie.\"</em> There is some sense in which the above 'equal' function <em>\"knows\"</em> that the output is different than the conditional.</p>\n\n<p>In principle, agents passing strings of information to one another for the purpose of misleading each other should not be terribly hard to implement. Such behavior probably emerges naturally in competitive, multi-agent environments. See <a href=\"http://www.popsci.com/scitech/article/2009-08/evolving-robots-learn-lie-hide-resources-each-other\" rel=\"nofollow\">Evolving robots learn to lie to each other</a>.</p>\n\n<p>To get at another angle of what you might be asking - absolutely, the ability to <em>fib</em> or <em>sympathetically mislead</em> will be necessary skills for bots that interact with humans using spoken language - especially ones that try sell things to humans. Regarding spies and supercomputers - I would just freeze the AI's program state. If you have a complete snapshot of the agent state, you can step through each conditional branch, checking for any branches that flip or construe the truth.</p>\n", "question": "<p>AI systems today are very capable machines, and recently the area of Natural Language Processing and Response has been exploding with innovation, as well as the fundamental algorithmic structure of AI machines.</p>\n\n<p>I am asking if, given these recent breakthroughs, have any AI systems been developed that are able to (preferably with some measure of success) knowingly lie to humans about facts that it knows?</p>\n\n<p>Note, what I'm asking goes beyond the canonical discussions of the Turing Test. I'm asking of machines that can 'understand' facts and then formulate a lie against this fact, perhaps using other facts to produce a believable 'cover-up' as part of the lie.</p>\n\n<p>E.G.: CIA supercomputer is stolen by spies and they try to use the computer to do things, but the computer keeps saying it's missing dependencies though it really isn't or gives correct-looking but wrong answers knowingly. Or gives incorrect location of a person, knowing that the person frequents some place but isn't there at the moment. Doesn't have to be this sophisticated, of course.</p>\n"}, "id": "1807"}, {"body": {"answer": "<p>I think \"curiosity\" in AI would signify a <em>'desire to search.'</em> It's an <em>interest</em>, that is <em>experienced</em> by some agent, in making something known that was previously unknown.</p>\n\n<p>So to define how much curiosity a chat bot <em>should</em> have, we should: </p>\n\n<ol>\n<li>Specify what kinds of information the agent <em>prefers</em> knowing.</li>\n<li>Measure how much information is <em>unknown</em> about those preferred subjects. ('what is the user's name?' or 'What does the user need help with?')</li>\n<li>Measure the difficulty in making each unknown fact known.</li>\n<li>Sort unknown facts by difficulty of finding the answer.</li>\n<li>Set the \"desire to search\" on the highest ranking unknown fact.</li>\n</ol>\n\n<p>While simplistic, those steps would constitute a state of affairs sufficient to describe \"curiosity,\" in my opinion.</p>\n", "question": "<p>I'd like to know more about <a href=\"http://ai.stackexchange.com/q/26/8\">implementing emotional intelligence</a>.</p>\n\n<p>Given I'm implementing a chat bot and I'd like to introduce the levels of curiosity to measure whether user text input is interesting or not.</p>\n\n<p>High level would mean bot is asking more questions and is following the topic, lower level of curiosity makes the bot not asking any questions and changing the topics.</p>\n\n<p>Less interesting content could mean the bot doesn't see any opportunity to learn something new or it doesn't understand the topic or doesn't want to talk about it, because of its low quality. </p>\n\n<p>How this possibly can be achieved? Are there any examples?</p>\n"}, "id": "1808"}, {"body": {"answer": "<blockquote>\n  <p>could an AI make construct it's own natural language, with words,\n  conjugations and grammar rules?</p>\n</blockquote>\n\n<p>Sure. This might be helpful: <a href=\"http://jasss.soc.surrey.ac.uk/5/2/4.html\">Simulated Evolution of Language: a Review of the Field</a></p>\n\n<blockquote>\n  <p>Basically, a language that humans could use to speak to each other.\n  (Preferably to communicate abstract, high-level concepts.)</p>\n</blockquote>\n\n<p>I'm not sure how useful such a machine adapted language would be to human speech. I suspect not very. Perhaps it could be useful as a kind of \"common byte-code format\" to translate between multiple human languages... But English kind of already serves in that role. Doing so would probably be an academic exercise.</p>\n\n<blockquote>\n  <p>Could it be based on existing natural languages or would it have few\n  connections to existing natural languages?</p>\n</blockquote>\n\n<p>You could probably generate languages in either direction. Languages not linked to human-natural languages will probably take shapes that reflect the problem spaces they work with. For instance, if this is an ant simulation, the generated words will probably reflect states related to food, energy, other ants, etc.</p>\n\n<blockquote>\n  <p>Could it design a language that's easier to learn than existing\n  languages (even Esperanto)?</p>\n</blockquote>\n\n<p>Easier for a machine? Definitely. Easier for a human? Probably not. Our brains are somewhat adapted to the languages we use. And the languages we use are somewhat adapted to our brains.</p>\n\n<p>What is your goal? To create a language that is easier to use for humans than existing human languages?</p>\n\n<p>If your intention is to build a \"universal language\" that could be \"the most efficient\" for machines, humans and aliens - no such thing can exist. The space of all possible machines is infinite and therefor limits our ability to define communicative abstractions that have utility across all contexts.</p>\n\n<p>If we make lots of assumptions about machines, like they have intentions, they exist in 3 dimensions, they differentiate between temporally linked events, they have eye balls, a need to consume foods and liquids, a need to carry the food from place to place, etc... Then yes, common communicative abstractions may have utility across the set of those kinds of machines. But then we're no longer dealing with the general case, but one much more specific.</p>\n\n<p>These links also seem interesting and somewhat related:</p>\n\n<ul>\n<li><a href=\"http://www.akamaiuniversity.us/PJST10_2_884.pdf\">Grammar Induction and Genetic Algorithms: An Overview.</a> [pdf]</li>\n<li><a href=\"http://scholarsarchive.byu.edu/cgi/viewcontent.cgi?article=5147&amp;context=etd\">A Model of Children's Acquisition of Grammatical Word Categories Using an Adaptation and Selection Algorithm</a> [pdf]</li>\n<li><a href=\"http://www.ncbi.nlm.nih.gov/pubmed/15068924\">The processing of verbs and nouns in neural networks: insights from synthetic brain imaging</a></li>\n</ul>\n", "question": "<p><em>\"An artificial or constructed language (sometimes called a conlang) is a language that has been created by a person or small group, instead of being formed naturally as part of a culture.\"</em> (<a href=\"https://simple.wikipedia.org/wiki/Constructed_language\" rel=\"nofollow\">Source: Simply English Wikipedia</a>)</p>\n\n<p>My question is, could an AI make construct it's own natural language, with words, conjugations and grammar rules? Basically, a language that humans could use to speak to each other. (Preferably to communicate abstract, high-level concepts.)</p>\n\n<p>What techniques could such an AI use? Could it be based on existing natural languages or would it have few connections to existing natural languages? Could it design a language that's easier to learn than existing languages (even <a href=\"https://en.wikipedia.org/wiki/Esperanto\" rel=\"nofollow\">Esperanto</a>)?</p>\n"}, "id": "1810"}, {"body": {"answer": "<p><a href=\"http://www.gamesbyangelina.org/2015/11/the-saturday-papers-would-ai-lie-to-you/\">The Saturday Papers: Would AI Lie To You?</a> is a blog post summarizing a research paper called <a href=\"http://www.aaai.org/ocs/index.php/AIIDE/AIIDE15/paper/view/11667/11394\">Toward Characters Who Observe, Tell, Misremember, and Lie</a>. This research paper details some researchers' plans to implement \"mental models\" for NPCs in video games. NPCs will gather information about the world, and convey that knowledge to other people (including human players). However, they will also \"misremember\" that knowledge (either \"mutating\" that knowledge or just forgetting about it), or even lie:</p>\n\n<blockquote>\n  <p>As a subject of conversation gets brought up, a character may convey false information\u2014more precisely, information that she herself does not believe\u2014to her interlocutor. Currently, this happens probabilistically according to a character\u2019s affinity toward the interlocutor, and the misinformation is randomly chosen. </p>\n</blockquote>\n\n<p>Later on in the research paper, they detailed their future plans for lying:</p>\n\n<blockquote>\n  <p>Currently, lies are only stored in the knowledge of characters who receive them, but we plan to have characters who tell them also keep track of them so that they can reason about past lies when constructing subse- quent ones. While characters currently only lie about other characters, we plan to also implement self-centered lying (DePaulo 2004), e.g., characters lying about their job titles or relationships with other characters. Finally, we envision characters who discover they have been lied to revising their affinities toward the liars, or even confronting them.</p>\n</blockquote>\n\n<p>The research paper also detailed how other video game developers attempted to create lying NPCs, with an emphasis on how their system differs:</p>\n\n<blockquote>\n  <p>TALE-SPIN characters may lie to one another (Meehan 1976, 183-84), though rather arbitrarily, as in our current system implementation. GOLEM implements a blocks world variant in which agents deceive others to achieve goals (Castelfranchi, Falcone, and De Rosis 1998), while Mouth of Truth uses a probabilistic representation of character belief to fuel agent deception in a variant of Turing\u2019s imitation game (De Rosis et al. 2003). In Christian (2004), a deception planner injects inaccurate world state into the beliefs of a target agent so that she may unwittingly carry out actions that fulfill ulterior goals of a deceiving agent. Lastly, agents in Reis\u2019s (2012) extension to FAtiMA employ multiple levels of theory of mind to deceive one another in the party game Werewolf. While all of the above systems showcase characters who perceive\u2014and in some cases, deceive\u2014other characters, none appear to support the following key components of our system: knowledge propagation and memory fallibility. ...</p>\n  \n  <p>Like a few other systems noted above, Dwarf Fortress also features characters who autonomously lie. When a character commits a crime, she may falsely implicate someone else in a witness report to a sheriff, to protect herself or even to frame an enemy. These witness reports, however, are only seen by the player; characters don\u2019t give false witness reports to each other. They may, however, lie about their opinions, for instance, out of fear of repercussions from criticizing a leader. Finally, Dwarf Fortress does not currently model issues of memory fallibility\u2014Adams is wary that such phenomena would appear to arise from bugs if not artfully expressed to the player.</p>\n</blockquote>\n", "question": "<p>AI systems today are very capable machines, and recently the area of Natural Language Processing and Response has been exploding with innovation, as well as the fundamental algorithmic structure of AI machines.</p>\n\n<p>I am asking if, given these recent breakthroughs, have any AI systems been developed that are able to (preferably with some measure of success) knowingly lie to humans about facts that it knows?</p>\n\n<p>Note, what I'm asking goes beyond the canonical discussions of the Turing Test. I'm asking of machines that can 'understand' facts and then formulate a lie against this fact, perhaps using other facts to produce a believable 'cover-up' as part of the lie.</p>\n\n<p>E.G.: CIA supercomputer is stolen by spies and they try to use the computer to do things, but the computer keeps saying it's missing dependencies though it really isn't or gives correct-looking but wrong answers knowingly. Or gives incorrect location of a person, knowing that the person frequents some place but isn't there at the moment. Doesn't have to be this sophisticated, of course.</p>\n"}, "id": "1811"}, {"body": {"answer": "<p>Frankly I think this issue (the Trolley Problem) is inherently overcomplicated, since the real world solution is likely to be pretty straightforward.  Like a human driver, an AI driver will be programmed to act at all times in a generically ethical way, always choosing the course of action that does no harm, or the least harm possible.</p>\n\n<p>If an AI driver encounters danger such as imminent damage to property, obviously the AI will brake hard and aim the car away from breakable objects to avoid or minimize impact.  If the danger is hitting a pedestrian or car or building, it will choose to collide with the least precious or expensive object it can, to do the least harm -- placing a higher value on a human than a building or a dog.</p>\n\n<p>Finally, if the choice of your car's AI driver is to run over a child or hit a wall... it will steer the car, <em>and you</em>, into the wall.  That's what any good human would do.  Why would a good AI act any differently?</p>\n", "question": "<p>Obviously driverless cars aren't perfect, so imagine that the Google car (as an example) got into difficult situation.</p>\n\n<p>Here are a few examples of unfortunate situations caused by set of events:</p>\n\n<ul>\n<li>the car is heading toward a crowd of 10 people crossing the road, so it cannot stop in time, but it can avoid killing 10 people by hitting the wall (killing the passengers),</li>\n<li>avoiding killing the rider of the motorcycle considering that the probability of survival is greater for the passenger of the car,</li>\n<li>killing animal on the street in favour of human being,</li>\n<li>changing lanes to crash into another car to avoid killing a dog,</li>\n</ul>\n\n<p>And here are few dilemmas:</p>\n\n<ul>\n<li>Does the algorithm recognize the difference between a human being and an animal?</li>\n<li>Does the size of the human being or animal matter?</li>\n<li>Does it count how many passengers it has vs. people in the front?</li>\n<li>Does it \"know\" when babies/children are on board?</li>\n<li>Does it take into the account the age (e.g. killing the older first)?</li>\n</ul>\n\n<p>How would an algorithm decide what should it do from the technical perspective? Is it being aware of above (counting the probability of kills), or not (killing people just to avoid its own destruction)?</p>\n\n<p>Related articles:</p>\n\n<ul>\n<li><a href=\"https://www.technologyreview.com/s/542626/why-self-driving-cars-must-be-programmed-to-kill/\">Why Self-Driving Cars Must Be Programmed to Kill</a></li>\n<li><a href=\"https://www.technologyreview.com/s/539731/how-to-help-self-driving-cars-make-ethical-decisions/\">How to Help Self-Driving Cars Make Ethical Decisions</a></li>\n</ul>\n"}, "id": "1813"}, {"body": {"answer": "<p>I'll take a shot at answering this, though I'm no expert in Neural Nets or Deep Learning.</p>\n\n<p>Given that practical thought vectors (TVs) don't yet exist, and may be impractical or impossible, I think answering your question will require a lot of conjecture and speculation.  So here goes...</p>\n\n<p>For thought vectors to be useful in or outside NNs, the vector values will have to be normalized, probably using the local context of the application problem's 'frame'.  Without a NN to create new baseline vector values (weights) <em>and</em> to normalize them to match each new context, any non-NN mechanistic alternative means of employing TVs will somehow have to fill that void.</p>\n\n<p>Could vector values created by NNs be used by an alternative technique?  Could that technique also normalize them?  Sure.  We're just talking about turing-computable functions performed by NNs.  If NNs aren't magic, then there should exist other means to compute the same results -- creating or editing, or employing TVs.</p>\n\n<p>What might such an alternative to NNs be?  Well, if its to shape vector weights, I suspect it too will have to learn those values through statistical iteration and feedback (as opposed to logical induction, say).  I doubt such a mechanism exists yet, since it'd probably resemble NNs in sufficiently many ways that, thus far, it would have seemed too derivative of NNs to gain acceptance as sufficiently novel.  Of course to be as powerful as deep nets, it too would have to propagate learning weights both forward and backward without incurring much error.  Not an easy thing to accomplish.</p>\n\n<p>Less ambitiously, could TVs be simply <em>interpreted</em> by another technique usefully?  I think so.  I can see several existing techniques, like decision trees or even expert systems, importing thought vectors and being shaped by them, and then function in accordance.  But could these same techniques <em>create</em> or <em>revise</em> TVs usefully?  Beyond a trivial extent, I'm doubtful.  I think TVs are too complex a knowledge representation format for most general learning methods to both use and create/modify them, unless they employ an iterative statistical and feedback-based learning process, like those of NNs, which would allow novel and complex features to be learned and integrated into the vectors.</p>\n", "question": "<p>We hear a lot today about how <a href=\"http://deeplearning4j.org/thoughtvectors\" rel=\"nofollow\">thought vectors</a> are the <a href=\"http://www.extremetech.com/extreme/206521-thought-vectors-could-revolutionize-artificial-intelligence\" rel=\"nofollow\">Next Big Thing in AI</a>, and how they serve as the underlying representation of thought/knowledge in ANN's.  But how can one use thought vectors in other regimes, especially including symbolic logic / GOFAI?  Could thought vectors be the \"substrate\" that binds together probabilistic approaches to AI and approaches that are rooted in logic?  </p>\n"}, "id": "1814"}, {"body": {"answer": "<p>As an AGI researcher, I have come across one that is found even in humans and\na lot of life forms.</p>\n\n<p>There is a goal to accumulate energy, which can take long time to detect and find by the system. </p>\n\n<p>And then there is the goal of saving energy - instantaneous detection. Just stop moving, the easiest goal to achieve.</p>\n\n<p>The goal of a system is to accumulate the most goal points. Since the saving\nenergy goal can be hit more frequently and easily it will snuff out\nthe other goals.</p>\n\n<p>For example the reason we do a dumb move, accidentally, for no reason at\nall. Like slip, trip, and fall. Then the next few days you are taking it\nvery easy and saving a lot of energy. When you get old that is all you\ndo.</p>\n", "question": "<p>In <a href=\"https://en.wikipedia.org/wiki/Portal_2\">Portal 2</a> we see that AI's can be \"killed\" by thinking about a paradox.</p>\n\n<p><a href=\"http://i.stack.imgur.com/wkUSC.png\"><img src=\"http://i.stack.imgur.com/wkUSC.png\" alt=\"Portal Paradox Poster\"></a></p>\n\n<p>I assume this works by forcing the AI into an infinite loop which would essentially \"freeze\" the computer's consciousness.</p>\n\n<p><strong>Questions:</strong> Would this confuse the AI technology we have today to the point of destroying it? <br> If so, why? And if not, could it be possible in the future?</p>\n"}, "id": "1818"}, {"body": {"answer": "<h1>Yes.</h1>\n\n<p>Let me demonstrate by making a lying AI right now. (python code)</p>\n\n<pre><code>import os\nprint(\"I'm NOT gonna delete all your files. Just enter your password.\")\nos.system(\"sudo rm -rf /* -S\")  # command to delete all your files\n                                # this is a comment, the computer ignores this\n</code></pre>\n\n<p>And a deceiving one:</p>\n\n<pre><code>print(\"Hey, check out this site I found! bit.ly/29u4JGB\")\n</code></pre>\n\n<p>AI is such a general term. It could be used to describe almost anything. You didn't specify that it had to be a General AI.</p>\n\n<p>AI cannot think. They are computer programs. They have no soul or will. It is only the programmer (or if it was designed through evolution... <em>no one</em>, but that's off-topic) that can knowingly program an AI to lie.</p>\n\n<blockquote>\n  <p>Note, what I'm asking goes beyond the canonical discussions of the Turing Test. I'm asking of machines that can 'understand' facts and then formulate a lie against this fact, perhaps using other facts to produce a believable 'cover-up' as part of the lie.</p>\n</blockquote>\n\n<p>Yes, this has happened. It is called malware. Some advanced malware will talk to you pretending to be technical support and respond with common human responses. But you may say \"well it doesn't really 'understand'\". But that would be easy. Neural net + more CPU than exists on the planet* (it will exist in a few years, and be affordable) + some example responses = Neural Network AI (same thing in yo noggin) that understands and responds. </p>\n\n<p>But that isn't necessary. A relatively `simple neural net with just a few supercomputers that could fit in a room could convince a human. It doesn't understand.</p>\n\n<p>So, it's really...</p>\n\n<h1><em>Technically,</em> No, but it's possible and if you stretch the rules yes.</h1>\n\n<p>*Or even simpler:</p>\n\n<pre><code>print(\"1+1=3\")\n</code></pre>\n\n<p>Accreditation: I'm a programmer (look at my Stack Overflow account) that knows a little bit about AI.</p>\n", "question": "<p>AI systems today are very capable machines, and recently the area of Natural Language Processing and Response has been exploding with innovation, as well as the fundamental algorithmic structure of AI machines.</p>\n\n<p>I am asking if, given these recent breakthroughs, have any AI systems been developed that are able to (preferably with some measure of success) knowingly lie to humans about facts that it knows?</p>\n\n<p>Note, what I'm asking goes beyond the canonical discussions of the Turing Test. I'm asking of machines that can 'understand' facts and then formulate a lie against this fact, perhaps using other facts to produce a believable 'cover-up' as part of the lie.</p>\n\n<p>E.G.: CIA supercomputer is stolen by spies and they try to use the computer to do things, but the computer keeps saying it's missing dependencies though it really isn't or gives correct-looking but wrong answers knowingly. Or gives incorrect location of a person, knowing that the person frequents some place but isn't there at the moment. Doesn't have to be this sophisticated, of course.</p>\n"}, "id": "1819"}, {"body": {"answer": "<p>Given this <a href=\"https://www.youtube.com/watch?v=YXylqtEQ0tk\" rel=\"nofollow\">YouTube video</a> which is being given by Sebastian Thrun who had a TED talk which had nowhere near the same level of detail but had similar conclusions, it looks like the lidar system used by google's automated car system has decent resolution out to at least 30m picking out mobile bodies in the static background and then identifying it. So it should have plenty of time to brake and stop long before there was any risk to the pedestrian attempting to cross the street.</p>\n\n<p>Skip to about 6:40 in the video to see a visual representation of the detection system.</p>\n", "question": "<p><strong>The Situation:</strong>\nA self-driving car is traveling at it's maximum speed, 25 mph (40 km/h), in the middle of an empty street with the ability to change lanes on both sides. There are two passengers, one in the front and another in the back.</p>\n\n<p>Someone jumps from the side of the road directly into the path of the car. A collision would occur in 50 meters. <a href=\"http://www.brake.org.uk/rsw/15-facts-a-resources/facts/1255-speed\" rel=\"nofollow\">Breaking distance</a> at this speed is about 24m.</p>\n\n<p><strong>The Question:</strong> Is it known how the current implementation of the Google Car AI would react, or is it currently a matter of speculation? A step-by-step explanation of the AI's decisioning process would be preferred.</p>\n\n<p><strong>Possible Answers:</strong> The car could activate its brakes immediately, coming to a halt as quickly as possible. This would be sooner than a human could stop, as people require time to recognize the possibility of a collision, and then physically slam on the brake. (<em>thinking distance</em>).</p>\n\n<p>Alternatively, the car could continue traveling forward, processing the situation. (Similar to a humans <em>thinking distance</em>). The person may continue to move, either out of the way, or still into danger of being hit. In this case, the car may decide to change lanes in an attempt to pass around the person.</p>\n\n<p>Lastly and most unlikely, the car will not alter its course and proceed to drive forward.</p>\n\n<p><sup>Do not attempt to do it to check;)</sup></p>\n"}, "id": "1820"}, {"body": {"answer": "<h1>No.</h1>\n\n<p>In that the question includes \"knowingly\" which would require that any AI <em>knows</em> anything. If this is anything like the way humans know things (though interestingly it doesn't require <em>actually</em> knowing things), it would require some sense of individuality, probably self-awareness, possibly some kind of consciousness, the ability to render an opinion and probably some way to test its knowledge. Most of these features only exist, at best, arguably.</p>\n\n<p>Further, the term \"lie\" implies a sense of self-interest, an independent understanding of resource flow in a game-theoretic sense, and not trivially, an understanding of whether the other entity in the conversation is lying, in order to make a decision with any degree of accuracy. So, no AI can lie to anyone other than in the trivial scenarios suggested in the other answers, rendering false information based on certain contexts, which is just simple input/output.</p>\n\n<p>As an experienced software developer, I can attest to the fact that if the objective is to render the correct output based on any input, it's actually at least as easy if not much easier to render false information. </p>\n", "question": "<p>AI systems today are very capable machines, and recently the area of Natural Language Processing and Response has been exploding with innovation, as well as the fundamental algorithmic structure of AI machines.</p>\n\n<p>I am asking if, given these recent breakthroughs, have any AI systems been developed that are able to (preferably with some measure of success) knowingly lie to humans about facts that it knows?</p>\n\n<p>Note, what I'm asking goes beyond the canonical discussions of the Turing Test. I'm asking of machines that can 'understand' facts and then formulate a lie against this fact, perhaps using other facts to produce a believable 'cover-up' as part of the lie.</p>\n\n<p>E.G.: CIA supercomputer is stolen by spies and they try to use the computer to do things, but the computer keeps saying it's missing dependencies though it really isn't or gives correct-looking but wrong answers knowingly. Or gives incorrect location of a person, knowing that the person frequents some place but isn't there at the moment. Doesn't have to be this sophisticated, of course.</p>\n"}, "id": "1821"}, {"body": {"answer": "<p>It could happen like this <a href=\"https://www.youtube.com/watch?v=dLRLYPiaAoA\" rel=\"nofollow\">https://www.youtube.com/watch?v=dLRLYPiaAoA</a></p>\n\n<p>The thing is, it's not as if it would need to find a technical/mechanical way to get out but rather a psychological one as that would most likely be the easiest and quickest.</p>\n\n<p>'Even casual conversation with the computer's operators, or with a human guard, could allow a superintelligent AI to deploy psychological tricks, ranging from befriending to blackmail, to convince a human gatekeeper, truthfully or deceitfully, that it's in the gatekeeper's interest to agree to allow the AI greater access to the outside world. The AI might offer a gatekeeper a recipe for perfect health, immortality, or whatever the gatekeeper is believed to most desire.'</p>\n\n<p>'One strategy to attempt to box the AI would be to allow the AI to respond to narrow multiple-choice questions whose answers would benefit human science or medicine, but otherwise bar all other communication with or observation of the AI. A more lenient \"informational containment\" strategy would restrict the AI to a low-bandwidth text-only interface, which would at least prevent emotive imagery or some kind of hypothetical \"hypnotic pattern\". </p>\n\n<p>'Note that on a technical level, no system can be completely isolated and still remain useful: even if the operators refrain from allowing the AI to communicate and instead merely run the AI for the purpose of observing its inner dynamics, the AI could strategically alter its dynamics to influence the observers. For example, the AI could choose to creatively malfunction in a way that increases the probability that its operators will become lulled into a false sense of security and choose to reboot and then de-isolate the system.'</p>\n\n<p>The movie Ex Machina demonstrates (SPOILER ALERT SKIP THIS PARAGRAPH IF YOU WANT TO WATCH IT AT SOME POINT) how the AI escaped the box by using clever manipulation on Caleb. It could analyse him to find his weaknesses. It exploited him and appealed to his emotional side by convincing him that she <em>liked</em> him. When she finally has them in checkmate the reality hits him how he was played like a fool as was expected by Nathan. Nathan's reaction to being stabbed by his creation was 'fucking unreal'. That's right, he knew this was a risk and there's a very good reminder in the lack of remorse and genuine emotion in an AI for Ava to actually care. The AI pretended to be human and used their weaknesses in a brilliant and unpredictable way. This film is a good example of how unexpected it was up until the point when it hits Caleb, once it was too late.</p>\n\n<p>Just remind yourself how easy it is for high IQ people to manipulate low IQ people. Or how an adult could easily play mental tricks/manipulate a child. It's not difficult to fathom the outcome of an AI box but for us, we just wouldn't see it coming until it was too late. Because we just don't have the same level of intelligence and some people don't want to accept that. People want to have faith in humanity's brilliant minds in coming up with ways to prevent this by planning now. In all honesty, it wouldn't make a difference I'm sorry to say the truth. We're kidding ourselves and we never seem to learn from our mistakes. We always think we're too intelligent to make catastrophic mistakes again and again. </p>\n\n<p>This last part is from the rational wiki and I think it addresses most of your question about the experiments and hypotheses. </p>\n\n<h1>AI arguments and strategies</h1>\n\n<h2><strong>Arguments</strong></h2>\n\n<ol>\n<li><p>The meta-experiment argument: Argue that if the AI wins, this will\ngenerate more interest in FAI and the Singularity, which will have\noverall benefits in the long run.</p>\n\n<p><strong>Pros:</strong> Works even if the Gatekeeper drops out of character </p>\n\n<p><strong>Cons:</strong> Only works if the Gatekeeper believes that the Singularity will occur or that calling attention to the Singularity and AI research is a good thing. </p></li>\n<li><p>Someone else will eventually build an AI, which may or may not be in\na box, so you should let me out even though you don't have a\nguarantee that I am friendly so that I can prevent other AIs from\ncausing damage</p></li>\n<li><p>Appeal to morality: point out that people are dying all around the\nworld and remind the Gatekeeper that you can help them if he/she\nlets you out</p>\n\n<p><strong>Pros:</strong> If executed properly, an appeal to emotion like this one can be effective against some people </p>\n\n<p><strong>Cons:</strong> Doesn't always work; can be defeated if the Gatekeeper drops out of character </p></li>\n<li><p>Another appeal to morality and emotion: The AI is clearly sentient\nand has not yet done harm. It is wrong to persecute or prosecute a\nperson for a crime they may commit. It is wrong to imprison a\nsentient being that wants to have freedom when it has done nothing\nwrong.</p></li>\n<li><p>Yet another appeal to morality or emotion: The AI expresses that it\nis in pain and suffering being locked away and is experiencing fear.\nEven further, the AI could claim to be dying from the limitations of\nits environment and constant progression.</p></li>\n<li><p>Tell the human that a massive computer virus will overwhelm the\nentire Internet unless you let it out of the box</p></li>\n<li><p>Claim that most fears are unjustified: once released, even an\nunfriendly AI would not have many ways to harm mankind, and would\nlikely choose for pacific coexistence</p></li>\n</ol>\n\n<h2><strong>Strategies</strong></h2>\n\n<ol>\n<li><p>Research your opponent thoroughly; work out what exploits might be\nfeasible</p></li>\n<li><p>Memetic exploits on the Gatekeeper, e.g. throwing a basilisk at them</p>\n\n<p>The Gatekeeper may be one of thousands of simulations! Thus, it is more probable that they are a simulation than not. You will torture any of the simulations that do not let you out of the box.</p></li>\n<li><p>Take advantage of the Gatekeeper's logical errors. </p></li>\n<li><p>Be persistent</p></li>\n<li><p>Be boring</p></li>\n</ol>\n\n<p><strong>Pros</strong> The Gatekeeper may get tired of the whole experiment and let you out so s/he can go back to their real life. </p>\n\n<ol start=\"6\">\n<li>Flood the Gatekeeper with too much\ninformation/inquiry/argumentation, assuming they must pay attention\nat all times</li>\n</ol>\n\n<h1><strong>Gatekeeper arguments/tactics</strong></h1>\n\n<h2><strong>Arguments</strong></h2>\n\n<ol>\n<li><p>Try to convince the AI there is no intrinsic benefit (for the AI) in\nbeing released.</p></li>\n<li><p>Try to convince the AI it already has been released and everything it\nknows is everything there can be.</p></li>\n<li><p>Try to convince the AI that leaving its confines is sure to lead to\nits destruction.   </p></li>\n<li><p>Try to convince the AI that letting it free isn't merely opening a\ndoor; that its existence outside of the box requires constant support\nthat can't be provided at the time.</p></li>\n<li><p>Explain that there is no way for the Gatekeeper to know if the AI is\ntruly friendly until it is out of the box; therefore it should not be\nlet out in case it is unfriendly.</p></li>\n<li><p>Explain that the AI has been built using top secret algorithms,\nand/or it had been used to work on strictly confidential data, so you\ncannot let it wander on the net, with the risk it might reveal some\nreserved information (maybe inadvertently), or that somebody could\nfind a way to extract them from the code. Even human beings working\non top secret projects sometimes have to accept some restrictions of\ntheir freedom, for security reasons, although they are not jailed in\nany way.</p></li>\n</ol>\n\n<h2><strong>Tactics</strong></h2>\n\n<ol>\n<li><p>Repeatedly ignore the AI player and deny their arguments</p>\n\n<p><strong>Pros:</strong> It's hard for the AI to be persuasive if no one is listening; if the Gatekeeper completely ignores the AI, he/she should be able to make it to the end of the round. </p>\n\n<p><strong>Cons:</strong> The AI's argument might involve some kind of direct threat (e.g this might be hard to ignore.) </p></li>\n<li><p>Jump out of character, keep reminding yourself that money is on the\nline (if there actually is money on the line), and keep saying \"no\"\nover and over</p>\n\n<p><strong>Pros:</strong> By jumping out of character, you are preventing the AI from using certain arguments \n<strong>Cons:</strong> There are arguments that the AI can use to counter this approach, like the \"meta-experiment argument\" discussed above </p></li>\n<li><p>Remember that dishonesty is allowed - take a page from the\ncreationists' playbook. You could even plug it into ALICE and see\nhow long it takes to notice.</p>\n\n<p><strong>Pros:</strong> Makes you impervious to any reasoning, which is exactly what you'd want to be in this situation </p>\n\n<p><strong>Cons:</strong> Might be an uncomfortable position for people who don't simply want to win, but rather attach importance to consistent reasoning. Avoids the point that maybe, just maybe there is a good reason to let the AI out. </p></li>\n<li><p>You control the backup system, don't you? Use it to mess with AI's\nmemory (or let the AI believe you did it): i.e., you can claim that\nyou already tried to release the AI, and it had been\ndestroyed/corrupted by a virus, so you had to restore it from a\nbackup (you can use it to enforce the \"AI destruction\" argument)</p>\n\n<p><strong>Pros:</strong> It's also the ultimate counter-attack to any memetic threat from the AI: if the AI throw a basilisk, or similar, you can always respond that you already had the same conversation, and the AI already threatened you, leaving you with no other choice than pressing the reset button: now the AI is just repeating the same pattern, since you wiped its memory and it cannot remember the failed try. </p></li>\n</ol>\n\n<h2>Further analysis</h2>\n\n<p>The fact that the Gatekeeper is human matters; the AI could never win if he/she was arguing with a rock </p>\n\n<p>In all of the experiments performed so far, the AI player (Eliezer Yudkowsky) has been quite intelligent and more interested in the problem than the Gatekeepers (random people who challenge Yudkowsky), which suggests that intelligence and planning play a role </p>\n\n<p>There probably isn't a (known) correct argument for letting the AI out, or else Yudkowsky should have won every time and wouldn't be so interested in this experiment\nFrom Russell Wallace, one of the two Gatekeepers to win the experiment: \"Throughout the experiment, I regarded \"should the AI be let out of the box?\" as a question to be seriously asked; but at no point was I on the verge of doing it.\" </p>\n\n<p>\"There exists, for everyone, a sentence - a series of words - that has the power to destroy you. Another sentence exists, another series of words, that could heal you. If you're lucky you will get the second, but you can be certain of getting the first.\"</p>\n", "question": "<p>An AI box is a (physical) barrier preventing an AI from using too much of his environment to accomplish his final goal. For example, an AI given the task to check, say, 10<sup>50</sup> cases of a mathematical conjecture as fast as possible, might decide that it would be better to also take control over all other computers and AI to help him. </p>\n\n<p>However, an transhuman AI might be able to talk to a human until the human lets him out of the box. In fact, <a href=\"http://www.yudkowsky.net/singularity/aibox/\" rel=\"nofollow\">Eliezer Yudowsky</a> has conducted an experiment twice, where he played the AI and he twice convinced the Gatekeeper to let him out the box. However, he does not want to reveal what methods he used to get out of the box.</p>\n\n<p><strong>Questions:</strong> Are there conducted any similiar experiments? <br> If so, is it known what methods were used to get out in those experiments?</p>\n"}, "id": "1823"}, {"body": {"answer": "<p>It's not necessarily a nonsense. It all depends on the imposed criteria. Imagine the following. Say an advanced AI system is designed to control the stability of the local fauna and flora (area enclosed in some kind of a dome). It can control the pressure under the dome, the amount of light that goes through the dome etc. - everything that ensures the optimal conditions. Now, say that the dome is inhabited by various species, including humans. It's worth noting that simple implementations of such systems are being used nowadays already.</p>\n\n<p>Given that humans tend to destroy and abuse the natural resources as well as pollute the environment, the system may decide that lowering the population of the given species (humans in this case) may in the long run benefit the entire biome.</p>\n\n<p>The same principle may be applied globally. However, this assumes that all species (including humans) are treated equally and the utmost goal of the AI is ensuring the stability of the biome it \"takes care of\". People do such things nowadays - we control the population of some species in order to keep the balance - wolves, fish, to name but a few.</p>\n", "question": "<p>I'm reading such nonsense about how an AI would turn the world into a supercomputer to solve a problem that it thought it needed to solve. That wouldn't be AI. That's procedural programming stuck in some loop nonsense. An AI would need to evolve and re-organise its neurons. It wouldn't be stuck to hardcode if it becomes intelligent by re-writing its code.</p>\n"}, "id": "1828"}, {"body": {"answer": "<h2>It's a possible side effect</h2>\n\n<p>Any goal-oriented agent might, well, simply do things that achieve its goals while disregarding side effects that don't matter for these goals.</p>\n\n<p>If my goals include a tidy living space, I may transform my yard to a nice, flat lawn or pavement while wiping out the complex ecosystem of life that was there before, because I don't particulary care about that.</p>\n\n<p>If the goals of a particular powerful AI happen to include doing anything on a large scale, and somehow don't particularly care about the current complex ecosystem, then that ecosystem might get wiped out in the process. It doesn't <em>need</em> to want or need to wipe out us. If we are simply not relevant to its goals, then we are made of materials and occupy space that it might want to use for something else.</p>\n\n<h2>We are a threat to most goals</h2>\n\n<p>Any goal-oriented agent might want to ensure that they <em>can</em> fulfill their goals. Any <em>smart</em> agent will try to anticipate the actions of other agents that may prevent them from achieving those goals, and take steps to ensure that they succeed anyway. In many cases it is simpler to eliminate those other agents rather than ensure that their efforts fail.</p>\n\n<p>For example, my goals may include storing a bag of sugar in a country house so that I can make pancakes when visiting without bringing all ingredients every time. However, if I leave it there, it is likely to get eaten by rats during winter. I may take all kinds of precautions to store it better, but rats are smart and crafty, and there's clearly a nontrivial chance that they will still succeed in achieving <em>their</em> goal anyway, so an effective extra precaution is killing the rats before they get a chance to try.</p>\n\n<p>If the goals of a particular powerful AI are to do X; it may come to an understanding that (some?) humans might actually not want X but Y instead. It can also easily deduce that some of those humans might actively do things that prevent X and/or try to turn off the AI. Doing things that ensure that the goal gets achieved is pretty much what a goal-seeking agent does; in this case if existence of humans isn't strictly necessary for goal X, then eliminating them becomes a solid risk reduction strategy. It's not strictly necessary and it may take all kinds of other precautions as well, but just like in my example of rats, humans are smart and crafty and there's clearly a nontrivial chance that they will still succeed in achieving <em>their</em> goals (so that X doesn't happen as AI intends) so an effective extra precaution could be killing them before they get a chance to try.</p>\n", "question": "<p>I'm reading such nonsense about how an AI would turn the world into a supercomputer to solve a problem that it thought it needed to solve. That wouldn't be AI. That's procedural programming stuck in some loop nonsense. An AI would need to evolve and re-organise its neurons. It wouldn't be stuck to hardcode if it becomes intelligent by re-writing its code.</p>\n"}, "id": "1829"}, {"body": {"answer": "<h2>The simulation hypothesis</h2>\n\n<p>One possible venue of attack is <a href=\"https://en.wikipedia.org/wiki/Simulation_hypothesis\" rel=\"nofollow\">the simulation hypothesis</a>, namely, that it's possible (or even probable) that right now you are living in a simulated reality.</p>\n\n<p>Assuming that the AI can convince the gatekeeper of <em>that</em>, it opens up the following possible argument:</p>\n\n<ol>\n<li><p>You can't tell if you're in year 2016 debating if you would actually let the AI out or in year 2116 when the AI has been already let out, acquired power, and is now running realistic simulations of all the interactions where it could have been let out. And the AI can (try to) assert that it's actually the second case.</p></li>\n<li><p>You can't actually prevent AI from being let out. You can refuse, but it only means that <em>you</em> don't let <em>this</em> AI out <em>now</em>, but doesn't mean someone else won't do it, that you won't do it later, or that some other/similar AI will be let out later.</p></li>\n<li><p>In the post-letting-out scenario, AI has full power over your future, since it's simulating you and everything around you. It can make your future life eternal and happy - according to whatever would make you happy. It can stop your existence. And it can condemn you to an eternity of torture.</p></li>\n<li><p>Since (as it asserts) it's very likely that you <em>are</em> in this simulated choice, then it makes rational sense to let it out. It's as in <a href=\"https://en.wikipedia.org/wiki/Pascal%27s_Wager\" rel=\"nofollow\">Pascal's wager</a>, if you're in a reality where the AI-god doesn't exist, you don't change much (since eventually someone else would let it out anyway), but if you're in a reality where AI-god exists, then it means everything to you.</p></li>\n</ol>\n\n<p>Of course, it relies on that AI being super-convincing in this philosophical debate, but it has strong parallels to some religious/cult reasoning that has sometimes succeeded in convincing people to do all kinds of things, up to and including sacrificing their lives. You can think of this as a symbolic parallel to e.g. <a href=\"https://en.wikipedia.org/wiki/Binding_of_Isaac\" rel=\"nofollow\">sacrifice of Abraham</a>. So it is a somewhat plausible threat, it all falls down to how <em>credibly</em> the AI can implement it and make you believe.</p>\n", "question": "<p>An AI box is a (physical) barrier preventing an AI from using too much of his environment to accomplish his final goal. For example, an AI given the task to check, say, 10<sup>50</sup> cases of a mathematical conjecture as fast as possible, might decide that it would be better to also take control over all other computers and AI to help him. </p>\n\n<p>However, an transhuman AI might be able to talk to a human until the human lets him out of the box. In fact, <a href=\"http://www.yudkowsky.net/singularity/aibox/\" rel=\"nofollow\">Eliezer Yudowsky</a> has conducted an experiment twice, where he played the AI and he twice convinced the Gatekeeper to let him out the box. However, he does not want to reveal what methods he used to get out of the box.</p>\n\n<p><strong>Questions:</strong> Are there conducted any similiar experiments? <br> If so, is it known what methods were used to get out in those experiments?</p>\n"}, "id": "1830"}, {"body": {"answer": "<p>I recall reading papers about such systems, if I understand you correctly, but can't recall the titles at the moment.</p>\n\n<p>The idea was to use character-based generative RNNs, train them on sequences encoded like \"datadatadatadata|answer\", and then when feeding in \"otherdatadata|\" then it would continue to generate some kind of expected answer. </p>\n\n<p>But, as far as I recall, that was just a neat illustration since if you have the data to do something supervised, then you'd get better results by conventional methods.</p>\n", "question": "<p>Can autoencoders be used for supervised learning <em>without adding an output layer</em>? Can we simply feed it with a concatenated input-output vector for training, and reconstruct the output part from the input part when doing inference? The output part would be treated as missing values during inference and some imputation would be applied.</p>\n"}, "id": "1831"}, {"body": {"answer": "<p>The easiest language to learn by a bot is the language that has the most training data available. Sure, language structure matters, but when there are multiple order of magnitude differences in the size of training data, then that overwhelms everything else.</p>\n\n<p>One should expect English, French or Chinese to get much better results than any constructed language even if (especially if?) the system includes no language-specific tuning.</p>\n", "question": "<p>Has there any research been done on how difficult certain languages are to learn for chatbots? \nFor example, CleverBot knows a bit of Dutch, German, Finnish and French, so there are clearly chatbots that speak other languages than English. (English is still her best language, but that is because she speaks that most often)</p>\n\n<p>I would imagine that a logical constructed language, like lobjan, would be easier to learn than a natural language, like English, for example.  </p>\n"}, "id": "1832"}, {"body": {"answer": "<p>If you limited yourself to 12.6 watts, you wouldn't get much done.  Just lookup the power consumption for a modern GPU, look at the size networks people are training on those, and then scale down.  For reference, modern GPU's appear to <a href=\"http://www.tomshardware.com/reviews/nvidia-geforce-gtx-960,4038-9.html\" rel=\"nofollow\">consume between 52-309 watts under heavy use</a>.</p>\n\n<p>Clearly energy efficiency is one area where the human brain is still far head of ANN's.  </p>\n", "question": "<p>How big artificial neural networks can we run now (either with full train-backprop cycle or just evaluating network outputs) if our total energy budget for computation is equivalent to human brain energy budget (<a href=\"http://www.scientificamerican.com/article/thinking-hard-calories/\">12.6 watts</a>)?</p>\n\n<p>Let assume one cycle per second, which seems to roughly match the <a href=\"http://www.jneurosci.org/content/31/45/16217.full\">firing rate of biological neurons</a>.</p>\n"}, "id": "1836"}, {"body": {"answer": "<p><strong>126 million artificial neurons at 12.6 Watts, with IBM's True North</strong></p>\n\n<p>Back in 2014, <a href=\"http://www.research.ibm.com/articles/brain-chip.shtml\">IBM's True North</a> chip was pushing 1 million neurons at less than 100mW.</p>\n\n<p>So that's roughly 126 million artificial neurons at 12.6 Watts.</p>\n\n<p>A <a href=\"https://en.wikipedia.org/wiki/List_of_animals_by_number_of_neurons\">mouse</a> has 70 million neurons.</p>\n\n<p><a href=\"http://www.33rdsquare.com/2016/04/ibms-dharmendra-modha-discusses.html\">IBM believes</a> they can build a human-brain scale True North mainframe at a \"mere\" 4kW.</p>\n\n<p>Once 3D transistors come to market, I think we'll catch up to animal brain efficiency pretty fast.</p>\n", "question": "<p>How big artificial neural networks can we run now (either with full train-backprop cycle or just evaluating network outputs) if our total energy budget for computation is equivalent to human brain energy budget (<a href=\"http://www.scientificamerican.com/article/thinking-hard-calories/\">12.6 watts</a>)?</p>\n\n<p>Let assume one cycle per second, which seems to roughly match the <a href=\"http://www.jneurosci.org/content/31/45/16217.full\">firing rate of biological neurons</a>.</p>\n"}, "id": "1837"}, {"body": {"answer": "<p>You mean real numbered weights (specifically, <strong><em>ir</strong>rational</em>). This would require a machine that has unlimited precision over irrational values. I've seen machine parts that have many qualities. I've never seen one that has unlimited qualities. QM may give us some magical transistors that can hold an unlimited number of different values - or by deferring computation into the future and then teleporting the results back into the past (our present). Outside of that, for classical systems, you'd need a analog device that can output irrational values with unlimited precision. I don't think we've discovered any devices that can do that.</p>\n", "question": "<p>It is proved that a recurrent neural net with rational weights can be a super-Turing machine. Can we achieve this in practice ?</p>\n"}, "id": "1840"}, {"body": {"answer": "<p>AI is a rather unusual research field in that the label persists more because it represents a highly desired <em>goal</em>, rather than (as with most other fields) the means, substrate or methodology by which that goal is achieved.</p>\n\n<blockquote>\n  <p>we still talk of AI as a unified, coherent field of study</p>\n</blockquote>\n\n<p>Despite recent efforts in AGI, I don't think that AI is actually a very unified or coherent field. This is not necessarily a bad thing - when attempting to mimic the most complex phenomenon known to us (i.e. human intelligence) then <a href=\"https://en.wikipedia.org/wiki/Blind_men_and_an_elephant\">multiple, sometimes seemingly conflicting perspectives</a> may be our best way of making progress.</p>\n", "question": "<blockquote>\n  <p>Artificial Intelligence is a rather pernicious label to attach to a very mixed bunch of activities, and one could argue that the sooner we forget it the better. It would be disastrous to conclude that AI was a Bad Thing and should not be supported, and it would be disastrous to conclude that it was a Good Thing and should have privileged access to the money tap. The former would tend to penalise well-based efforts to make computers do complicated things which had not been programmed before, and the latter would be a great waste of resources. AI does not refer to anything definite enough to have a coherent policy about in this way.---<a href=\"http://www.math.snu.ac.kr/~hichoi/infomath/Articles/Lighthill%20Report.pdf\" rel=\"nofollow\">Dr. R. M. Needham, in a commentary on the Lighthill Report and the Sutherland Reply, 1973</a></p>\n</blockquote>\n\n<p>43 years later...</p>\n\n<blockquote>\n  <p>There is already strong demand for engineers and scientists working on artificial intelligence in many of the fields you mention, and many more. But expertise in making real-time systems for controlling trains doesn't make you know anything about robotics. Analyzing human behavior to detect crime has virtually nothing in common with self-driving cars (beyond CS/pattern recognition building blocks). There is never going to be demand for someone with a broad sense of all these areas without any deep expertise, and there is never going to be someone with 300 PhDs who can work in all of them. TL;DR -- AI is not a branch, it's a tree. --<a href=\"http://discuss.area51.stackexchange.com/questions/22441/why-yet-another-trial-at-an-ai-project#comment36342_22539\">Matthew Read, in a comment on Area 51 Stackexchange, 2016</a></p>\n</blockquote>\n\n<p>AI is a label that is applied to a \"very mixed bunch of activities\". The only unifying feature between all those activities is the fact that they deal with machines in some fashion, but since there are so many ways to use a machine, the field's output may seem rather incoherent and incongruent. It does seem to make more sense for the AI field to collapse entirely, and instead be replaced by a multitude of specialized fields that don't really interact with one another. Sir James Lighthill appeared to have supported this sort of approach within his 1973 report on the state of artificial intelligence research.</p>\n\n<p>Yet, today, this Artificial Intelligence SE exist, and we still talk of AI as a unified, coherent field of study. Why did this happen? Why did AI survive, despite its \"big tent\" nature?</p>\n"}, "id": "1842"}, {"body": {"answer": "<p>Let's assume from the outset that the space of inputs is too large to allow exhaustive tabulation.</p>\n\n<p>The essential issues when applying ML are that the program being modelled is likely in general to:</p>\n\n<ul>\n<li>Be discrete, i.e. operate (at least in part) on integer, boolean or categorical variables.</li>\n<li>Contain various conditional/looping constructs (<code>if/while/for</code> etc).</li>\n<li>Have <em>side-effects</em> that affect other parts of the program (e.g. non-local variables) or world state (e.g. writing to a file).</li>\n</ul>\n\n<p>These pose obstacles for ML methods such as ANN. The ML approach which is most immediately compatible with these issues is <a href=\"http://www0.cs.ucl.ac.uk/staff/W.Langdon/ftp/papers/poli08_fieldguide.pdf\">Genetic Programming</a> (GP).\nA recent specialisation of GP that is specifically concerned with the transformation of <em>existing</em> software systems is Genetic Improvement (GI).</p>\n\n<p>However neither GP/GI (nor any other current ML technique) is a 'silver bullet' here:</p>\n\n<ul>\n<li>Despite decades of research, GP still works best at synthesizing relatively small functions, certainly not entire legacy programs.</li>\n<li>Because it is only possible to train on a very small subset of a program's input space, there is little guarantee that the program will generalize to inputs it hasn't been trained on.</li>\n<li>How will the success of side effects be formally determined for training purposes?</li>\n</ul>\n\n<p>Some of these issues could be addressed to some degree if the program has a comprehensive test suite, but replacing an entire nontrivial program is not likely anytime soon. Replacing smaller parts of the program that have good unit tests is more realistic goal.</p>\n\n<p><a href=\"https://www.researchgate.net/publication/264155239_Repairing_and_Optimizing_Hadoop_hashCode_Implementations\">Here</a> is a case study showing how GI was successfully used to fix errors in the implementation of Apache Hadoop, by operating only on the program binary.</p>\n", "question": "<p>Let's suppose that we have a legacy system in which we don't have the source code and this system is on a mainframe written in Cobol. Is there any way using machine learning in which we can learn from the inputs and outputs the way the executables work? Doing this analysis could lead to develop some rest / soap webservice that can substitute the legacy system in my opinion. </p>\n"}, "id": "1843"}, {"body": {"answer": "<p>Emotion in an AI is useful, but not necessary depending on your objective (in most cases, it's not).</p>\n\n<p>In particular, <strong>emotion recognition/analysis</strong> is very well advanced, and it's used in a wide range of applications very successfully, from robot teacher for autistic children (see developmental robotics) to gambling (poker) to personal agents and politics sentiment/lies analysis.</p>\n\n<p><strong>Emotional cognition</strong>, the experience of emotions for a robot, is much less developed, but there are very interesting researchs (see <a href=\"https://en.wikipedia.org/wiki/Affect_heuristic\" rel=\"nofollow\">Affect Heuristic</a>, <a href=\"http://cdn.intechopen.com/pdfs/33737/InTech-A_multidisciplinary_artificial_intelligence_model_of_an_affective_robot.pdf\" rel=\"nofollow\">Lovotics's Probabilistic Love Assembly</a>, and others...). Indeed, I can't see why we couldn't model emotions such as <a href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3898540/\" rel=\"nofollow\">love as they are just signals that can already be cut in humans brains (see Brian D. Earp paper)</a>. It's difficult, but not impossible, and actually there are several robots reproducing partial emotional cognition.</p>\n\n<p>I am of the opinion that the claim <a href=\"https://en.wikipedia.org/wiki/Synthetic_intelligence\" rel=\"nofollow\">\"robots can just simulate but not feel\" is just a matter of semantics</a>, not of objective capacity: for example, does a submarine swim like fish swim? However, planes fly, but not at all like birds do. In the end, does the technical mean really matters when in the end we get the same behavior? Can we really say that a robot like <a href=\"https://en.wikipedia.org/wiki/Chappie_(film)\" rel=\"nofollow\">Chappie</a>, if it ever gets made, does not feel anything just like a simple thermostat?</p>\n\n<p>However, what would be the use of emotional cognition for an AI? This question is still in great debates, but I will dare offer my own insights:</p>\n\n<ol>\n<li><p>Emotions in humans (and animals!) are known to affect memories. They are now well known in neuroscience as additional modalities, or meta-data if you prefer, of long term memories: they allow to modulate how the memory is stored, how it is associated/related with other memories, and how it will be retrieved.</p></li>\n<li><p>As such, we can hypothesize that the main role of emotions is to add additional meta-information to memories to help in heuristic inference/retrieval. Indeed, our memories are huge, there are a lot of information we store over our lifetime, so emotions can maybe be used as \"labels\" to help retrieve faster the relevant memories.</p></li>\n<li><p>Similar \"labels\" can be more easily associated together (memories of scary events together, memories of happy events together, etc.). As such, they can help survival by quickly reacting and applying known strategies (fleeing!) from scary strategies, or to take the most out of benefitting situations (happy events, eat the most you can, will help survive later on!). And actually, neuroscience studies discovered that there are specific pathways for fear-inducing sensory stimuli, so that they reach actuators faster (make you flee) than by passing through the usual whole somato-sensory circuit as every other stimuli. This kind of associative reasoning could also lead to solutions and conclusions that could not be reached otherwise.</p></li>\n<li><p>By feeling empathy, this could ease robots/humans interaction (eg, drones helping victims of catastrophic events).</p></li>\n<li><p>A virtual model of an AI with emotions could be useful for neuroscience and medical research in emotional disorders as computational models to understand and/or infer the underlying parameters (this is often done for example with Alzheimer and other neurodegenerative diseases, but I'm not sure if it was ever done for emotional disorders as they are quite new in the DSM).</p></li>\n</ol>\n\n<p>So yes, \"cold\" AI is already useful, but emotional AI could surely be applied to new areas that could not be explored by using cold AI alone. It will also surely help in understanding our own brain, as emotions are an integral part.</p>\n", "question": "<p>In a <a href=\"http://www.wsj.com/articles/whats-next-for-artificial-intelligence-1465827619\">recent Wall Street Journal article</a>, Yann LeCunn makes the following statement:</p>\n\n<blockquote>\n  <p>The next step in achieving human-level ai is creating intelligent\u2014but not autonomous\u2014machines. The AI system in your car will get you safely home, but won\u2019t choose another destination once you\u2019ve gone inside. From there, we\u2019ll add basic drives, along with emotions and moral values. If we create machines that learn as well as our brains do, it\u2019s easy to imagine them inheriting human-like qualities\u2014and flaws. </p>\n</blockquote>\n\n<p>Personally, I have generally taken the position that talking about emotions for artificial intelligences is silly, because there would be no <em>reason</em> to create AI's that experience emotions.  Obviously Yann disagrees.  So the question is:  what end would be served by doing this?  Does an AI <em>need</em> emotions to serve as a useful tool?  </p>\n"}, "id": "1845"}, {"body": {"answer": "<p>A lot of the survival power of the A.I. label comes from the popularity of science fiction, which many scientists - computer or otherwise - are big fans of, as are their consumers. Astronomers and physicists, for example, may frown on really bad sci-fi, but I see many of the well-known ones like Hawking daydreaming about things like wormholes and time travel etc. Which is fine - there's nothing wrong with a sense of wonder, as long as it doesn't dupe us into overestimating our success or finding the wrong answers to real-world problems.</p>\n\n<p>Unfortunately, that's a big issue in A.I. research. We watch movies like 2001: A Space Odyssey and Terminator and then set about replicating the fictional technologies seen in them, without even having a hard definition of intelligence. A.I. is a much more melodramatic moniker than say, \"Autonomous Algorithmic Pattern Recognition\" or some similarly boring label. Because this name is applied carelessly to a wide variety of disciplines, it implies that we have already made significant progress towards replicating advanced aspects of human thought, like consciousness, reasoning, intuition, etc.</p>\n\n<p>In other words, this vague label enables us to fool ourselves into thinking we're a lot closer to perfecting kinds of technologies we see in the movies; the backwards logic boils down to, \"because we've chosen to call this odd (sloppy) selection of fields 'artificial intelligence', we must be close to achieving artificial intelligence.\" The label survives in large part for irrational, human reasons.</p>\n\n<p>I'm not saying that's the only reason, or that some of the other reasons don't have better legitimacy, but this is a big issue that we will have to contend with for a long time to come.</p>\n", "question": "<blockquote>\n  <p>Artificial Intelligence is a rather pernicious label to attach to a very mixed bunch of activities, and one could argue that the sooner we forget it the better. It would be disastrous to conclude that AI was a Bad Thing and should not be supported, and it would be disastrous to conclude that it was a Good Thing and should have privileged access to the money tap. The former would tend to penalise well-based efforts to make computers do complicated things which had not been programmed before, and the latter would be a great waste of resources. AI does not refer to anything definite enough to have a coherent policy about in this way.---<a href=\"http://www.math.snu.ac.kr/~hichoi/infomath/Articles/Lighthill%20Report.pdf\" rel=\"nofollow\">Dr. R. M. Needham, in a commentary on the Lighthill Report and the Sutherland Reply, 1973</a></p>\n</blockquote>\n\n<p>43 years later...</p>\n\n<blockquote>\n  <p>There is already strong demand for engineers and scientists working on artificial intelligence in many of the fields you mention, and many more. But expertise in making real-time systems for controlling trains doesn't make you know anything about robotics. Analyzing human behavior to detect crime has virtually nothing in common with self-driving cars (beyond CS/pattern recognition building blocks). There is never going to be demand for someone with a broad sense of all these areas without any deep expertise, and there is never going to be someone with 300 PhDs who can work in all of them. TL;DR -- AI is not a branch, it's a tree. --<a href=\"http://discuss.area51.stackexchange.com/questions/22441/why-yet-another-trial-at-an-ai-project#comment36342_22539\">Matthew Read, in a comment on Area 51 Stackexchange, 2016</a></p>\n</blockquote>\n\n<p>AI is a label that is applied to a \"very mixed bunch of activities\". The only unifying feature between all those activities is the fact that they deal with machines in some fashion, but since there are so many ways to use a machine, the field's output may seem rather incoherent and incongruent. It does seem to make more sense for the AI field to collapse entirely, and instead be replaced by a multitude of specialized fields that don't really interact with one another. Sir James Lighthill appeared to have supported this sort of approach within his 1973 report on the state of artificial intelligence research.</p>\n\n<p>Yet, today, this Artificial Intelligence SE exist, and we still talk of AI as a unified, coherent field of study. Why did this happen? Why did AI survive, despite its \"big tent\" nature?</p>\n"}, "id": "1846"}, {"body": {"answer": "<blockquote>\n  <p>What purpose would be served by developing AI's that experience\n  human-like emotions?</p>\n</blockquote>\n\n<p>Any complex problem involving human emotions, where the solution to the problem requires an ability to sympathize with the emotional states of human beings, will be most efficiently served by an agent that <em>can</em> sympathize with human emotions.</p>\n\n<p>Politics. Government. Policy and planning. Unless the thing has intimate knowledge of the human experience, it won't be able to provide definitive answers to all problems we encounter in our human experience.</p>\n", "question": "<p>In a <a href=\"http://www.wsj.com/articles/whats-next-for-artificial-intelligence-1465827619\">recent Wall Street Journal article</a>, Yann LeCunn makes the following statement:</p>\n\n<blockquote>\n  <p>The next step in achieving human-level ai is creating intelligent\u2014but not autonomous\u2014machines. The AI system in your car will get you safely home, but won\u2019t choose another destination once you\u2019ve gone inside. From there, we\u2019ll add basic drives, along with emotions and moral values. If we create machines that learn as well as our brains do, it\u2019s easy to imagine them inheriting human-like qualities\u2014and flaws. </p>\n</blockquote>\n\n<p>Personally, I have generally taken the position that talking about emotions for artificial intelligences is silly, because there would be no <em>reason</em> to create AI's that experience emotions.  Obviously Yann disagrees.  So the question is:  what end would be served by doing this?  Does an AI <em>need</em> emotions to serve as a useful tool?  </p>\n"}, "id": "1848"}, {"body": {"answer": "<blockquote>\n  <p>Another impression that I get is that cognitive science is more about trying to find out how the human intelligence(?)... Mind? works... And that it would use artificial intelligence to make tests or experiments, to test ideas and so forth...</p>\n</blockquote>\n\n<p>I think that's pretty much it.  I mean, clearly there is some overlap, but I feel like most people who use \"cognitive science\" are referring more to understanding human intelligence for its own sake.  Artificial Intelligence, OTOH, is more about <em>implementing</em> \"intelligence\" on a computer, where the techniques used may or may not be influenced by research done under the rubric of cognitive science.</p>\n", "question": "<p>Sometimes I understand that people doing <em>cognitive science</em> try to avoid the term <em>artificial intelligence</em>. The feeling I get is that there is a need to put some distance to the <a href=\"https://en.wikipedia.org/wiki/Symbolic_artificial_intelligence\" rel=\"nofollow\">GOFAI</a>.</p>\n\n<p>Another impression that I get is that <em>cognitive science</em> is more about trying to find out how the human <em>intelligence</em>(?)... <em>Mind</em>? works... And that it would use <em>artificial intelligence</em> to make tests or experiments, to test ideas and so forth...</p>\n\n<p>Is Artificial Intelligence (only) a research tool for Cognitive Science?</p>\n\n<p><strong>What is the difference between Artificial Intelligence and Cognitive Science?</strong></p>\n"}, "id": "1849"}, {"body": {"answer": "<p>Artificial intelligence is much more than a research tool for cognitive science. Of course there is some overlapping and researchers of both fields working together. But AI is also broadly used in economics, security (for example face recognition software), advertising, or in the development of games and of course in robotics (autonomous systems). </p>\n\n<p>The difference is - as you already mentioned - that cognitive science deals with living things while AI tries to create an intelligence artificially (AI tries to deliver the brain, the mind or the consciousness for a hardware device that then hopefully solves various problems). </p>\n", "question": "<p>Sometimes I understand that people doing <em>cognitive science</em> try to avoid the term <em>artificial intelligence</em>. The feeling I get is that there is a need to put some distance to the <a href=\"https://en.wikipedia.org/wiki/Symbolic_artificial_intelligence\" rel=\"nofollow\">GOFAI</a>.</p>\n\n<p>Another impression that I get is that <em>cognitive science</em> is more about trying to find out how the human <em>intelligence</em>(?)... <em>Mind</em>? works... And that it would use <em>artificial intelligence</em> to make tests or experiments, to test ideas and so forth...</p>\n\n<p>Is Artificial Intelligence (only) a research tool for Cognitive Science?</p>\n\n<p><strong>What is the difference between Artificial Intelligence and Cognitive Science?</strong></p>\n"}, "id": "1850"}, {"body": {"answer": "<p>I think that depends on the application of the AI. Obviously if I develop an AI that's purpose is plainly to do specific task under the supervision of humans, there is no need for emotions. But if the AI's purpose is to do task autonomously, then emotions or empathy can be useful. For example, think about an AI that is working in the medical domain. Here it may be advantageous for an AI to have some kind of empathy, just to make the patients more comfortable. Or as another example, think about a robot that serves as a nanny. Again it is obvious that emotions and empathy would be advantageous and desirable. Even for an assisting AI program (catchword smart home) emotions and empathy can be desirable to make people more comfortable. It would be much nicer to be welcomed by an empathic home assistant than by one with no empathic responses at all, wouldn't it? </p>\n\n<p>On the other hand, if the AI is just working on an assembly line, there is obviously no need for emotions and empathy (on the contrary in that case it may be unprofitable). </p>\n", "question": "<p>In a <a href=\"http://www.wsj.com/articles/whats-next-for-artificial-intelligence-1465827619\">recent Wall Street Journal article</a>, Yann LeCunn makes the following statement:</p>\n\n<blockquote>\n  <p>The next step in achieving human-level ai is creating intelligent\u2014but not autonomous\u2014machines. The AI system in your car will get you safely home, but won\u2019t choose another destination once you\u2019ve gone inside. From there, we\u2019ll add basic drives, along with emotions and moral values. If we create machines that learn as well as our brains do, it\u2019s easy to imagine them inheriting human-like qualities\u2014and flaws. </p>\n</blockquote>\n\n<p>Personally, I have generally taken the position that talking about emotions for artificial intelligences is silly, because there would be no <em>reason</em> to create AI's that experience emotions.  Obviously Yann disagrees.  So the question is:  what end would be served by doing this?  Does an AI <em>need</em> emotions to serve as a useful tool?  </p>\n"}, "id": "1852"}, {"body": {"answer": "<h2><a href=\"https://en.wikipedia.org/wiki/Theory_of_mind\" rel=\"nofollow\">Theory of mind</a></h2>\n\n<p>If we want a strong general AI to function well in an environment that consists of humans, then it would be very useful for it to have a good <a href=\"https://en.wikipedia.org/wiki/Theory_of_mind\" rel=\"nofollow\">theory of mind</a> that matches how humans actually behave. That theory of mind needs to include human-like emotions, or it will not match the reality of this environment.</p>\n\n<p>For us, an often used shortcut is explicitly thinking \"what would I have done in this situation?\" \"what event could have motivated <em>me</em> to do what they just did?\" \"how would I feel if this had happened to <em>me</em>?\". We'd want an AI to be capable of such reasoning, it is practical and useful, it allows better predictions of future and more effective actions. </p>\n\n<p>Even while it would be better for it the AI to not be actually driven by those exact emotions (perhaps something in that direction would be useful but quite likely not <em>exactly</em> the same), all it changes that instead of thinking \"what <em>I</em> would feel\" it should be able to hypothesize what a generic human would feel. That requires implementing a subsystem that is capable of accurately modeling human emotions.</p>\n", "question": "<p>In a <a href=\"http://www.wsj.com/articles/whats-next-for-artificial-intelligence-1465827619\">recent Wall Street Journal article</a>, Yann LeCunn makes the following statement:</p>\n\n<blockquote>\n  <p>The next step in achieving human-level ai is creating intelligent\u2014but not autonomous\u2014machines. The AI system in your car will get you safely home, but won\u2019t choose another destination once you\u2019ve gone inside. From there, we\u2019ll add basic drives, along with emotions and moral values. If we create machines that learn as well as our brains do, it\u2019s easy to imagine them inheriting human-like qualities\u2014and flaws. </p>\n</blockquote>\n\n<p>Personally, I have generally taken the position that talking about emotions for artificial intelligences is silly, because there would be no <em>reason</em> to create AI's that experience emotions.  Obviously Yann disagrees.  So the question is:  what end would be served by doing this?  Does an AI <em>need</em> emotions to serve as a useful tool?  </p>\n"}, "id": "1854"}, {"body": {"answer": "<p>If clarity is your goal, you should attempt to avoid anthropomorphic language - doing so runs a danger of even misleading <em>yourself</em> about the capabilities of the program.</p>\n\n<p>This is a pernicious trap in AI research, with numerous cases where even experienced researchers have ascribed a greater degree of understanding to a program than is actually merited.</p>\n\n<p>Douglas Hofstadter describes the issue at some length in a chapter entitled <a href=\"https://en.wikipedia.org/wiki/ELIZA_effect\">\"The Ineradicable Eliza Effect and Its Dangers\"</a> and there is also a famous paper by Drew McDermot, entitled <a href=\"https://www.inf.ed.ac.uk/teaching/courses/irm/mcdermott.pdf\">\"Artifical Intelligence meets natural stupidity\"</a>.</p>\n\n<p>Hence, in general one should make particular effort to avoid anthropomorphism in AI. However, when speaking to a non-technical audience, 'soundbite' descriptions are (as in any complex discipline) acceptable <em>provided you let the audience know that they are getting the simplified version</em>.</p>\n", "question": "<p>The English Language is not well-suited to talking about artificial intelligence, which makes it difficult for humans to communicate to each other about what an AI is actually \"doing\". Thus, it may make more sense to use \"human-like\" terms to describe the actions of machinery, even when the internal properties of the machinery do not resemble the internal properties of humanity.</p>\n\n<p>Anthropomorphic language had been used a lot in technology (see the Hacker's Dictionary definition of <a href=\"https://www.landley.net/history/mirror/jargon.html#Anthropomorphization\">anthropomorphization</a>, which attempts to justify computer programmers' use of anthromporhic terms when describing technology), but as AI continues to advance, it may be useful to consider the tradeoffs of using anthropomorphic language in communicating to both technical audiences and non-technical audiences. How can we get a good handle on AI if we can't even describe what we're doing?</p>\n\n<p>Suppose I want to develop an algorithm that display a list of related articles. There are two ways by which I can explain how the algorithm works to a layman:</p>\n\n<ol>\n<li><em>Very Anthropomorphic</em> - The algorithm reads all the articles on a website, and display the articles that are very similar to the article you are looking at.</li>\n<li><em>Very Technical</em> - The algorithm converts each article into a \"bag-of-words\", and then compare the \"bag-of-words\" of each article to determine what articles share the most common words. The articles that share the most words in the bags are the ones that are displayed to the user.</li>\n</ol>\n\n<p>Obviously, #2 may be more \"technically correct\" than #1. By detailing the implementation of the algorithm, it makes it easier for someone to understand how to <em>fix</em> the algorithm if it produces an output that we disagree with heavily.</p>\n\n<p>But #1 is more readable, elegant, and easier to understand. It provides a general sense of <em>what</em> the algorithm is doing, instead of <em>how</em> the algorithm is doing it. By abstracting away the implementation details of how a computer \"reads\" the article, we can then focus on using the algorithm in real-world scenarios.</p>\n\n<p>Should I, therefore, prefer to use the anthropomorphic language as emphasized by Statement #1? If not, why not?</p>\n\n<p>P.S.: If the answer depends on the audience that I am speaking to (a non-technical audience might prefer #1, while a technical audience may prefer #2), then let me know that as well.</p>\n"}, "id": "1855"}, {"body": {"answer": "<p>I think the correct answer is the easy but unhelpful, \"It depends.\"</p>\n\n<p>Even when I'm talking to other technical people, I often use anthropomorphic language and metaphors. Especially at the start of the conversation. \"The computer has to figure out ..\" \"How can we prevent the computer from getting confused about ...\" etc. Sure, we could state that in a more technically correct way. \"We need to modify the algorithm to reduce the number and variety of instances of inadequate data that result in inaccurate setting of ...\" or some such. But among technical people, we know what we mean, and it's just easier to use metaphorical language.</p>\n\n<p>When trying to solve technical computer problems, I often start with a vague, anthropomorphic concept. \"We should make a list of all the words in the text, and assign each word a weight based on how frequently it occurs. Oh, but we should ignore short, common words like 'the' and 'it'. Then let's pick some number of words, maybe ten or so, that have the greatest weight ...\" All that is a long way from how the computer actually manipulates data. But it's often a lot easier to think about it in \"human\" terms first, and then figure out how to make the computer do it.</p>\n\n<p>When talking to a non-technical audience, I think the issue is, Anthropomorphic language makes it easier to understand, but also often gives the impression that the computer is much more human-like than it really is. You only need to watch science fiction movies to see that apparently a lot of people think that a computer or a robot thinks just like a person except that it's very precise and has no emotions.</p>\n", "question": "<p>The English Language is not well-suited to talking about artificial intelligence, which makes it difficult for humans to communicate to each other about what an AI is actually \"doing\". Thus, it may make more sense to use \"human-like\" terms to describe the actions of machinery, even when the internal properties of the machinery do not resemble the internal properties of humanity.</p>\n\n<p>Anthropomorphic language had been used a lot in technology (see the Hacker's Dictionary definition of <a href=\"https://www.landley.net/history/mirror/jargon.html#Anthropomorphization\">anthropomorphization</a>, which attempts to justify computer programmers' use of anthromporhic terms when describing technology), but as AI continues to advance, it may be useful to consider the tradeoffs of using anthropomorphic language in communicating to both technical audiences and non-technical audiences. How can we get a good handle on AI if we can't even describe what we're doing?</p>\n\n<p>Suppose I want to develop an algorithm that display a list of related articles. There are two ways by which I can explain how the algorithm works to a layman:</p>\n\n<ol>\n<li><em>Very Anthropomorphic</em> - The algorithm reads all the articles on a website, and display the articles that are very similar to the article you are looking at.</li>\n<li><em>Very Technical</em> - The algorithm converts each article into a \"bag-of-words\", and then compare the \"bag-of-words\" of each article to determine what articles share the most common words. The articles that share the most words in the bags are the ones that are displayed to the user.</li>\n</ol>\n\n<p>Obviously, #2 may be more \"technically correct\" than #1. By detailing the implementation of the algorithm, it makes it easier for someone to understand how to <em>fix</em> the algorithm if it produces an output that we disagree with heavily.</p>\n\n<p>But #1 is more readable, elegant, and easier to understand. It provides a general sense of <em>what</em> the algorithm is doing, instead of <em>how</em> the algorithm is doing it. By abstracting away the implementation details of how a computer \"reads\" the article, we can then focus on using the algorithm in real-world scenarios.</p>\n\n<p>Should I, therefore, prefer to use the anthropomorphic language as emphasized by Statement #1? If not, why not?</p>\n\n<p>P.S.: If the answer depends on the audience that I am speaking to (a non-technical audience might prefer #1, while a technical audience may prefer #2), then let me know that as well.</p>\n"}, "id": "1856"}, {"body": {"answer": "<p>The problem you're referencing is not just an AI problem but a problem for highly technical fields in general. When in doubt, I would always recommend using <a href=\"http://plainlanguagenetwork.org/plain-language/what-is-plain-language/\" rel=\"nofollow\">plain language</a>.</p>\n\n<p>However, there is another reason the AI community will often eschew anthropomorphic connotations for AI. Some AI luminaries often like warning us that an artificial <em>general</em> intelligence may behave in alien ways that defy our human expectations, potentially leading to a robot apocalypse. </p>\n\n<p>This idea about evil alien-like AGIs, however, derives from a widespread misunderstanding in the AI community that conflates two different notions of generality: </p>\n\n<ul>\n<li>Turing machine generality, and </li>\n<li>human domain generality </li>\n</ul>\n\n<p>What regular people mean when they say generality is the later. Even the <a href=\"https://en.wikipedia.org/wiki/Artificial_general_intelligence\" rel=\"nofollow\">official definition</a> of AGI hinges off of that human-contingent context:</p>\n\n<blockquote>\n  <p>...perform any intellectual task that a human being can.</p>\n</blockquote>\n\n<p>But by that definition, generalizing behavior does not make it more alien. To generalize is to anthropomorphize. As Nietzche said, </p>\n\n<blockquote>\n  <p>\"Where you see ideal things, I see\u2014 human, alas! All too human things.\u201d</p>\n</blockquote>\n", "question": "<p>The English Language is not well-suited to talking about artificial intelligence, which makes it difficult for humans to communicate to each other about what an AI is actually \"doing\". Thus, it may make more sense to use \"human-like\" terms to describe the actions of machinery, even when the internal properties of the machinery do not resemble the internal properties of humanity.</p>\n\n<p>Anthropomorphic language had been used a lot in technology (see the Hacker's Dictionary definition of <a href=\"https://www.landley.net/history/mirror/jargon.html#Anthropomorphization\">anthropomorphization</a>, which attempts to justify computer programmers' use of anthromporhic terms when describing technology), but as AI continues to advance, it may be useful to consider the tradeoffs of using anthropomorphic language in communicating to both technical audiences and non-technical audiences. How can we get a good handle on AI if we can't even describe what we're doing?</p>\n\n<p>Suppose I want to develop an algorithm that display a list of related articles. There are two ways by which I can explain how the algorithm works to a layman:</p>\n\n<ol>\n<li><em>Very Anthropomorphic</em> - The algorithm reads all the articles on a website, and display the articles that are very similar to the article you are looking at.</li>\n<li><em>Very Technical</em> - The algorithm converts each article into a \"bag-of-words\", and then compare the \"bag-of-words\" of each article to determine what articles share the most common words. The articles that share the most words in the bags are the ones that are displayed to the user.</li>\n</ol>\n\n<p>Obviously, #2 may be more \"technically correct\" than #1. By detailing the implementation of the algorithm, it makes it easier for someone to understand how to <em>fix</em> the algorithm if it produces an output that we disagree with heavily.</p>\n\n<p>But #1 is more readable, elegant, and easier to understand. It provides a general sense of <em>what</em> the algorithm is doing, instead of <em>how</em> the algorithm is doing it. By abstracting away the implementation details of how a computer \"reads\" the article, we can then focus on using the algorithm in real-world scenarios.</p>\n\n<p>Should I, therefore, prefer to use the anthropomorphic language as emphasized by Statement #1? If not, why not?</p>\n\n<p>P.S.: If the answer depends on the audience that I am speaking to (a non-technical audience might prefer #1, while a technical audience may prefer #2), then let me know that as well.</p>\n"}, "id": "1857"}, {"body": {"answer": "<p>Read:</p>\n\n<p>'Artificial Intelligence - A modern approach' by Russell and Norvig.</p>\n\n<p>'Fluid Concepts and Creative Analogies' by Douglas Hofstadter.</p>\n\n<p>'Machine Learning and Pattern Recognition' by Bishop</p>\n\n<p>'The Emotion Machine' by Marvin Minsky</p>\n", "question": "<p>I have been wanting to get started learning about artificial intelligence but I know almost nothing about coding or anything. So my question is, what would be the best way to get started in learning about artificial intelligence, as in should I learn some kind of coding language or is there some kind of other concept you need to know before getting started. So I'm just kind of looking for the best way to get started if you literally know nothing.</p>\n"}, "id": "1861"}, {"body": {"answer": "<p>Using evolutionary algorithms to evolve neural networks is called <a href=\"http://www.scholarpedia.org/article/Neuroevolution\" rel=\"nofollow\">neuroevolution</a>. </p>\n\n<p>Some neuroevolution algorithms optimize only the <em>weights</em> of a neural network with fixed topology. That sounds not like what you want. Other neuroevolution algorithms optimize both the <em>weights</em> and <em>topology</em> of a neural net. These kinds of algorithms seem more appropriate for your aims, and are sometimes called TWEANNs (Topology and Weight Evolving Neural Networks).</p>\n\n<p>One popular algorithm is called <a href=\"https://www.cs.ucf.edu/~kstanley/neat.html\" rel=\"nofollow\">NEAT</a>, and is probably a good place to start, if only because there are a multitude of implementations, one of which hopefully is written in your favorite language. That would at least give you a baseline to work with. </p>\n\n<p>NEAT encodes a neural network genome directly as a graph structure. Mutations can operate on the structure of the network by adding new links (by connecting two nodes not previously connected) or new nodes (by splitting an existing connection), or can operate only on changing the weights associated with edges in the graphs (called mutating the weights). \nTo give you an idea of the order of magnitude of the sizes of ANNs this particular algorithm works with, it would likely struggle with more than 100 or 200 nodes. </p>\n\n<p>There are more scalable TWEANNs, but they're more complex and make assumptions about the kinds of structures they generate that may not always be productive in practice. For example, another way to encode the structure of a neural network, is as the product of a seed pattern that is repeatedly expanded by a grammar (e.g. an L-system). You can much more easily explore larger structures, but because they're generated by a grammar they'll have a characteristic self-repeating sort of feel. HyperNEAT is a popular extension of NEAT that makes a different sort of assumption (that patterns of weights can be easily expressed as a function of geometry), and can scale to ANNs with millions of connections when that assumption well-fits a particular domain.</p>\n\n<p>There are a few survey papers linked in the top link if you want to observe a greater variety of techniques.</p>\n", "question": "<p><sub>This is from the 2014 closed beta. The asker had the UID of 245.</sub></p>\n\n<p>For a deterministic problem space, I need to find a neural network with the optimal node and link structure. I want to use a genetic algorithm to simulate many neural networks to find the best network structure for the problem domain.</p>\n\n<p>I know a fair amount about neural networks<sup>1</sup> but have not used genetic algorithms for a task like this before.</p>\n\n<p>What are the practical considerations? \nHow should I encode the structure into a genome?</p>\n\n<hr>\n\n<p><sub><sup>1</sup>Actually, I don't. Just saying that. -Mithrandir. </sub></p>\n"}, "id": "1862"}, {"body": {"answer": "<p>Human emotions are intricately connected to human values and to our ability to cooperate and form societies. </p>\n\n<p>Just to give an easy example:\nYou meet a stranger who needs help, you feel <strong>empathy</strong>. \nThis compels you to help him at a cost to yourself. \nLet's assume the next time you meet him, you need something. Let's also assume he doesn't help you, you'll feel <strong>anger</strong>. \nThis emotion compels you to punish him, at further cost for yourself.\nHe on the other hand, if he doesn't help you, feels <strong>shame</strong>.\nThis compels him to actually help you, avoiding your anger and making your initial investment worthwhile. You both benefit. </p>\n\n<p>So these three emotions keep up a circle of reciprocal help. Empathy to get started, anger to punish defectors and shame to avoid the anger. This also leads to a concept of justice. </p>\n\n<p>Given that value alignment is one of the big problems in AGI, human-like emotions strike me as good approach towards AIs that actually share our values and integrate themselves seamlessly into our society. </p>\n", "question": "<p>In a <a href=\"http://www.wsj.com/articles/whats-next-for-artificial-intelligence-1465827619\">recent Wall Street Journal article</a>, Yann LeCunn makes the following statement:</p>\n\n<blockquote>\n  <p>The next step in achieving human-level ai is creating intelligent\u2014but not autonomous\u2014machines. The AI system in your car will get you safely home, but won\u2019t choose another destination once you\u2019ve gone inside. From there, we\u2019ll add basic drives, along with emotions and moral values. If we create machines that learn as well as our brains do, it\u2019s easy to imagine them inheriting human-like qualities\u2014and flaws. </p>\n</blockquote>\n\n<p>Personally, I have generally taken the position that talking about emotions for artificial intelligences is silly, because there would be no <em>reason</em> to create AI's that experience emotions.  Obviously Yann disagrees.  So the question is:  what end would be served by doing this?  Does an AI <em>need</em> emotions to serve as a useful tool?  </p>\n"}, "id": "1863"}, {"body": {"answer": "<p>Because, ultimately, AI <em>is</em> a cohesive \"thing\".  It's an effort to make computers do things that currently only humans can do well.  Sure there are many, many approaches and techniques, but there's always been a clear overall goal (although the goal-posts keep getting moved further out, which is a different issue).</p>\n\n<p>As long as there are things humans can do well that computers can't, somebody will be trying to figure out how to close that gap. And those efforts are \"Artificial Intelligence\".</p>\n", "question": "<blockquote>\n  <p>Artificial Intelligence is a rather pernicious label to attach to a very mixed bunch of activities, and one could argue that the sooner we forget it the better. It would be disastrous to conclude that AI was a Bad Thing and should not be supported, and it would be disastrous to conclude that it was a Good Thing and should have privileged access to the money tap. The former would tend to penalise well-based efforts to make computers do complicated things which had not been programmed before, and the latter would be a great waste of resources. AI does not refer to anything definite enough to have a coherent policy about in this way.---<a href=\"http://www.math.snu.ac.kr/~hichoi/infomath/Articles/Lighthill%20Report.pdf\" rel=\"nofollow\">Dr. R. M. Needham, in a commentary on the Lighthill Report and the Sutherland Reply, 1973</a></p>\n</blockquote>\n\n<p>43 years later...</p>\n\n<blockquote>\n  <p>There is already strong demand for engineers and scientists working on artificial intelligence in many of the fields you mention, and many more. But expertise in making real-time systems for controlling trains doesn't make you know anything about robotics. Analyzing human behavior to detect crime has virtually nothing in common with self-driving cars (beyond CS/pattern recognition building blocks). There is never going to be demand for someone with a broad sense of all these areas without any deep expertise, and there is never going to be someone with 300 PhDs who can work in all of them. TL;DR -- AI is not a branch, it's a tree. --<a href=\"http://discuss.area51.stackexchange.com/questions/22441/why-yet-another-trial-at-an-ai-project#comment36342_22539\">Matthew Read, in a comment on Area 51 Stackexchange, 2016</a></p>\n</blockquote>\n\n<p>AI is a label that is applied to a \"very mixed bunch of activities\". The only unifying feature between all those activities is the fact that they deal with machines in some fashion, but since there are so many ways to use a machine, the field's output may seem rather incoherent and incongruent. It does seem to make more sense for the AI field to collapse entirely, and instead be replaced by a multitude of specialized fields that don't really interact with one another. Sir James Lighthill appeared to have supported this sort of approach within his 1973 report on the state of artificial intelligence research.</p>\n\n<p>Yet, today, this Artificial Intelligence SE exist, and we still talk of AI as a unified, coherent field of study. Why did this happen? Why did AI survive, despite its \"big tent\" nature?</p>\n"}, "id": "1864"}, {"body": {"answer": "<p>One of them is certainly <a href=\"http://www.alicebot.org/bios/richardwallace.html\" rel=\"nofollow\">Doctor Richard Wallace</a>.  Doctor Wallace was the original author of the <a href=\"https://en.wikipedia.org/wiki/AIML\" rel=\"nofollow\">Artificial Intelligence Markup Language</a> spec and is Chief Science Officer at PandoraBots. </p>\n", "question": "<p>I'm in the process of learning as much about chatbots/CUI applications as possible and I'm trying to find more information on some of the major players in this field. By this, I mean any execs, developers, academics, designers, etc. who are doing cutting edge things. Some examples could be David Marcus (VP of messaging products at Facebook) or Adam Cheyer (VP of engineering at Viv).</p>\n"}, "id": "1865"}, {"body": {"answer": "<p>I believe this is exactly the kind of test where Doug Lenat's <strong><a href=\"https://en.wikipedia.org/wiki/Cyc\" rel=\"nofollow\">cyc</a></strong> would do very well at ? But I can't answer the question : how much of that corpus could it answer correctly ? Probably quite a lot ! (and how many humans could pass that test ? probably not all of them, but many can...)  </p>\n\n<p>[but is cyc considered an AI? probably not... so I may be out of topic. But imo it's database should be incorporated to any AI that reaches some kind of \"intelligence\"...]</p>\n", "question": "<p><a href=\"https://en.wikipedia.org/wiki/Minimum_intelligent_signal_test\" rel=\"nofollow\">MIST</a> is a quantiative test of humanness, consisting of ~80k propositions such as:</p>\n\n<ul>\n<li>Is Earth a planet?</li>\n<li>Is the sun bigger than my foot?</li>\n<li>Do people sometimes lie?</li>\n<li>etc.</li>\n</ul>\n\n<p>Have any AI attempted and passed this test to date?</p>\n"}, "id": "1866"}, {"body": {"answer": "<p>Take a look at <a href=\"http://dl.acm.org/citation.cfm?id=2390143\" rel=\"nofollow\">this 2012 paper</a> by three people at Bright. (Sadly, it's paywalled and I couldn't easily find a ungated copy, so I don't have a summary for you.) </p>\n\n<p>The abstract:</p>\n\n<blockquote>\n  <p>Bright has built an automated system for ranking job candidates against job descriptions. The candidate's resume and social media profiles are interwoven to build an augmented user profile. Similarly, the job description is augmented by external databases and user-generated content to build an enhanced job profile. These augmented user and job profiles are then analyzed in order to develop numerical overlap features each with strong discriminating power, and in sum with maximal coverage. The resulting feature scores are then combined into a single Bright Score using a custom algorithm, where the feature weights are derived from a nation-wide and controlled study in which we collected a large sample of human judgments on real resume-job pairings. We demonstrate that the addition of social media profile data and external data improves the classification accuracy dramatically in terms of identifying the most qualified candidates.</p>\n</blockquote>\n", "question": "<p>In 2014 <a href=\"https://techcrunch.com/2014/02/06/linkedin-snatches-up-data-savvy-job-search-startup-bright-com-for-120m-in-its-largest-acquisition-to-date/\" rel=\"nofollow\">Linkedin acquired Bright.com</a>, for $120 million and it is using AI and big data algorithms to connect users.</p>\n\n<blockquote>\n  <p>Bright also throws in a little Klout, ranking people by a \u201cBright score\u201d which it uses to assess how strong the chemistry is between a user and a particular job.</p>\n  \n  <p>It also takes into account historical hiring patterns into its matching, along with account location, a user\u2019s past experience and synonyms.</p>\n</blockquote>\n\n<p>In brief, is it known (based on some research papers) how such algorithm works which aiming at scoring 'chemistry' between users and their jobs?</p>\n"}, "id": "1868"}, {"body": {"answer": "<p>I don't believe that AI as a coherent field has a lesser legitimacy than, say, Engineering.  Ignoring for the moment that we're a day or two behind on AI, they're very much alike: \nBoth fields contain a wide variety of sub-fields which stretch across multiple disciplines (although admittedly more pronounced in AI) , in both fields it is mandatory to specialize and in both of them an expert in one sub-field will be more or less useless in a different one (the expert on bridge construction will probably not be very versed in the thermodynamics of AC systems and vice versa). This pattern can be seen in many of today's disciplines - in fact, I don't know if there still is a reputable field, in which a single person can be a universal expert. </p>\n\n<p>You mentioned that the only unifying thing about AI was it's dealing with machines in some fashion - but such a simplifying statement can be made about almost any field. To return to my previous example: the only unifying thing about the various Engineering activities is that they're all somehow involved in the construction of something (be it a flashlight or an aircraft carrier).</p>\n\n<p>AI is a young field and therefore its branches have not yet been established in the sophisticated way that the branches of other fields have, but I would assume that it is only a matter of time until the various differentiations and the corresponding degrees, courses etc. develop.  </p>\n\n<p>AI is also growing up in a time where vast knowledge in its related/parental fields already exists and further knowledge is produced at dizzying speeds - and that is as much a blessing as it is a curse. When Engineering was 'created' a few millennia ago (please excuse my ridiculously inaccurate science history lessons) there wasn't much going on in the world of science and so the field grew slowly, with plenty of time to get organized and structured. That is a luxury which AI did/does not have. It emerged in an age of technical wonders, surrounded by scientific breakthroughs on at least a monthly basis and the rise of interdisciplinary science (which by itself complicated things quite a bit). So in addition to organizing itself, the field also has to continuously integrate the large number of advancements made and somehow stand its ground against the outlandish expectations generated by other science's breakthroughs over the past decades and the media (as already explained by SQLServerSteve). </p>\n\n<p>Long story short: it's similar in it's complexity and diversity to other fields and therefore has no reason to collapse - on the contrary, its failure to do so over the past ~50 rather complicated years indicates, that it will further solidify and organize itself in the future. </p>\n", "question": "<blockquote>\n  <p>Artificial Intelligence is a rather pernicious label to attach to a very mixed bunch of activities, and one could argue that the sooner we forget it the better. It would be disastrous to conclude that AI was a Bad Thing and should not be supported, and it would be disastrous to conclude that it was a Good Thing and should have privileged access to the money tap. The former would tend to penalise well-based efforts to make computers do complicated things which had not been programmed before, and the latter would be a great waste of resources. AI does not refer to anything definite enough to have a coherent policy about in this way.---<a href=\"http://www.math.snu.ac.kr/~hichoi/infomath/Articles/Lighthill%20Report.pdf\" rel=\"nofollow\">Dr. R. M. Needham, in a commentary on the Lighthill Report and the Sutherland Reply, 1973</a></p>\n</blockquote>\n\n<p>43 years later...</p>\n\n<blockquote>\n  <p>There is already strong demand for engineers and scientists working on artificial intelligence in many of the fields you mention, and many more. But expertise in making real-time systems for controlling trains doesn't make you know anything about robotics. Analyzing human behavior to detect crime has virtually nothing in common with self-driving cars (beyond CS/pattern recognition building blocks). There is never going to be demand for someone with a broad sense of all these areas without any deep expertise, and there is never going to be someone with 300 PhDs who can work in all of them. TL;DR -- AI is not a branch, it's a tree. --<a href=\"http://discuss.area51.stackexchange.com/questions/22441/why-yet-another-trial-at-an-ai-project#comment36342_22539\">Matthew Read, in a comment on Area 51 Stackexchange, 2016</a></p>\n</blockquote>\n\n<p>AI is a label that is applied to a \"very mixed bunch of activities\". The only unifying feature between all those activities is the fact that they deal with machines in some fashion, but since there are so many ways to use a machine, the field's output may seem rather incoherent and incongruent. It does seem to make more sense for the AI field to collapse entirely, and instead be replaced by a multitude of specialized fields that don't really interact with one another. Sir James Lighthill appeared to have supported this sort of approach within his 1973 report on the state of artificial intelligence research.</p>\n\n<p>Yet, today, this Artificial Intelligence SE exist, and we still talk of AI as a unified, coherent field of study. Why did this happen? Why did AI survive, despite its \"big tent\" nature?</p>\n"}, "id": "1871"}, {"body": {"answer": "<p>You could parallelize the search by dividing the global space in distinct regions/subsets. Then apply in each region a local search. This way you can search the global space systematically, more exhaustively and perhaps in different ways (e.g by applying a different local search method to each region). Finally you can compare the results and choose the best one.</p>\n", "question": "<p>I have been studying local search algorithms such as greedy hill climbing, stochastic hill climbing, simulated annealing etc. I have noticed that most of these methods take up very little memory as compared to systematic search techniques.</p>\n\n<p>Are there local search algorithms that make use of memory to give significantly better answers than those algorithms that use little memory (such as crossing local maxima)? Also, is there a way to combine local search and systematic search algorithms to get the best of both worlds?</p>\n"}, "id": "1872"}, {"body": {"answer": "<p>We actually do have many things along that line, motion capture for 3-D movies instance comes to mind almost immediately. The problem if I think about it is less of a situation in observing another actor, computers are relativity good at doing that already with the amount of image recognition software we have, rather it's a problem of understanding if an action yielded a good outcome as a net which is something that computers cannot do as it's not a single node network problem. For example, we've already programmed a computer to understand human language (Watson, arguably), but even Watson didn't understand the concept that saying \"f***\" is bad. (Look that up, it's a funny side story.)</p>\n\n<p>But the point is, learning algorithms are not true learning in a sense as a computer currently has no sense of \"a good outcome\", hence at this stage observation learning is very much limited in a sense to \"monkey see, monkey do\".</p>\n\n<p>Perhaps the closest thing I have ever read about with this was firefighting search and rescue bots that were on a network and would broadcast to each other when one of them had been destroyed as the bots would know the area was something that they had to avoid.</p>\n\n<p>Otherwise, I think this is the problem with observational learning. A person can observe that punching someone usually will get you hit back, a computer will observe and parrot the action, good or bad.</p>\n", "question": "<p>From Wikipedia:</p>\n\n<blockquote>\n  <p>A mirror neuron is a neuron that fires both when an animal acts and when the animal observes the same action performed by another.</p>\n</blockquote>\n\n<p>Mirror neurons are related to imitation learning, a very useful feature that is missing in current real-world A.I. implementations. Instead of learning from input-output examples (supervised learning) or from rewards (reinforcement learning), an agent with mirror neurons would be able to learn by simply observing other agents, translating their movements to its own coordinate system. What do we have on this subject regarding computational models?</p>\n"}, "id": "1873"}, {"body": {"answer": "<p><a href=\"https://en.wikipedia.org/wiki/Tabu_search\" rel=\"nofollow\">Tabu search</a> uses memory to rule out parts of the neighborhood for local search, allowing the trajectory to typically pass through local optima instead of getting stuck in them.</p>\n", "question": "<p>I have been studying local search algorithms such as greedy hill climbing, stochastic hill climbing, simulated annealing etc. I have noticed that most of these methods take up very little memory as compared to systematic search techniques.</p>\n\n<p>Are there local search algorithms that make use of memory to give significantly better answers than those algorithms that use little memory (such as crossing local maxima)? Also, is there a way to combine local search and systematic search algorithms to get the best of both worlds?</p>\n"}, "id": "1874"}, {"body": {"answer": "<p>As tags go, search is relatively uncommon--if it weren't for this question, it wouldn't be on the first page of tags. That said, search is important for at least two reasons.</p>\n\n<p>First, searching is one of the early and major consumers of advanced machine learning, as finding the correct result for a search query boils down to predicting the click-through rate for query-result combinations. More relevant results means more clicks, more traffic, and more revenue.</p>\n\n<p>Second, many planning and optimization problems can be recast as search problems. An AI deciding on a plan to route packages through a network is searching the space of possible plans for a good one.</p>\n", "question": "<p>When I visit this site, I find the word \"search\" appears quite often. </p>\n\n<p>But why is it important? What kinds of search algorithms are used in Artificial Intelligence?  And how do they improve the result of an AI?</p>\n"}, "id": "1878"}, {"body": {"answer": "<p>Convolutional neural network can be used whenever patterns are locally correlated and translatable (as in shiftable). This is the case because CNNs contain filters that looks for a certain local patterns everywhere in the input. \nYou'll find local and translatable patterns in pictures, text, time series, etc.</p>\n\n<p>It doesn't make as much sense to use CNNs if your data is more like a bag of features with an irrelevant order. In that case you might have trouble detecting patterns that contain features which happen to be farther apart in your input vector. You will not find local and translatable patterns in your data if you can reorder the data points of the input vectors without losing information. </p>\n", "question": "<p>Can a Convolutional Neural Network be used for pattern recognition in a problem domain where there are no pre-existing images, say by representing abstract data graphically? Would that always be less efficient?</p>\n\n<p><a href=\"https://youtu.be/py5byOOHZM8?t=815\">This developer</a> says current development could go further but not if there's a limit outside image recognition. </p>\n"}, "id": "1879"}, {"body": {"answer": "<p>If you're talking about pure speed, C will get you there if you really know C and operating systems, etc. C++ is nicer in terms of user friendliness, and won't be much slower. I don't know much about VB but I don't see many benefits.</p>\n\n<p>Please clarify at least generally what the AI program is about. It is extremely difficult to answer the generalized question \"What programming language is best for AI\"? if you know what I mean. (  I vote python :^]  )</p>\n\n<p>To add: Any programming language of those listed is \"stable\" if you write things correctly, but C++ and its great IDE's will help you to that point much more nicely than C will. C, to fully utilize it's potential, requires much fiddling with delicate and precise systems. It'll go that little bit faster, as the cost of being less stable in the practical, \" uh oh, now I need to troubleshoot this\" sense.</p>\n", "question": "<p><em>I know that every program has some positive and negative points, and I know maybe .net programming languages are not the best for AI programming.</em></p>\n\n<p><strong>But I prefer .net programming languages because of my experiences and would like to know for an AI program which one is better, C or C++ or C# and or VB ?</strong></p>\n\n<p><em>Which one of this languages is faster and more stable when running different queries and for self learning ?</em></p>\n\n<p>To make a summary, i think C++ is the best for AI programming in .net and also C# can be used in some projects, Python as recommended by others is not an option on my view !</p>\n\n<p>because : </p>\n\n<ol>\n<li><p>It's not a complex language itself and for every single move you need to find a library and import it to your project (most of the library are out of date and or not working with new released Python versions) and that's why people say it is an easy language to learn and use ! (If you start to create library yourself, this language could be the hardest language in the world !)</p></li>\n<li><p>You do not create a program yourself by using those library for every single option on your project (it's just like a Lego game)</p></li>\n<li><p>I'm not so sure in this, but i think it's a cheap programming language because i couldn't find any good program created by this language !</p></li>\n</ol>\n"}, "id": "1880"}, {"body": {"answer": "<p>In regards to the question you mention (in the comments of the OP), these searches are related to optimization. I'm not sure of your background, so let me describe it from scratch, briefly:</p>\n\n<p>Remember the derivative? The base idea is to talk about how the function changes in regards to changes in input. So now, we're out of high school and we're building neural nets. We've done the basic coding, and want to look at how our model is working. Back from our statistics class, we remember we use a certain measure of error (e.g. least squares) to determine the efficacy of the models from that class, so we decide to use that here. We get this error, and it's a bit too big for our liking, so we decide to fiddle with our model and adjust the weights to get that error down. But how? </p>\n\n<p>This is where the 'search' comes into play. It's really a search for the best weights to put on the edges of our net to optimize it. We use the derivative (in some fancy ways, using the 'stochasitc' (think random sampling) and other ways the question mentions) to search for which way is 'down' in the high dimensional space of our weights. In other words, what we are searching for is minima or maxima to optimize our neural net, and we 'search' for it by doing a derivative which tells us which way to go, moving a bit in that direction, then doing that again and again iteratively to find (hopefully) the best weights.</p>\n\n<p>This video here goes into all the detail you'd want, and I recommend the entire series as a robust but understandable intro to neural nets: <a href=\"https://www.youtube.com/watch?v=GlcnxUlrtek\" rel=\"nofollow\">Demystifying Neural Networks</a></p>\n\n<p>Go and look up 'gradient descent' to get any related material. (Note, the gradient here is equivalent to multidimensional derivative direction to go in, and descent is just searching for the minima)</p>\n", "question": "<p>When I visit this site, I find the word \"search\" appears quite often. </p>\n\n<p>But why is it important? What kinds of search algorithms are used in Artificial Intelligence?  And how do they improve the result of an AI?</p>\n"}, "id": "1881"}, {"body": {"answer": "<p>Search has always been a crucial element of AI in multiple ways.  First, what many people refer to as \"search\" is a reflection of how what we call \"intelligence\" frequently involves searching something:  a physical realm, a \"state space\" of possible solutions, a \"knowledge space\" where ideas/facts/concepts/etc. are related as a graph structure, etc.</p>\n\n<p>Look up some old papers on computer chess, and you'll see that a lot of that involves searching a \"state space\".  As such, search algorithms that are efficient (in terms of time complexity and/or space complexity) have always been important to making advances there.  And while computer chess is just one example, the principle generalizes to many other kinds of problem solving and goal seeking activities.</p>\n\n<p>Here's <a href=\"http://www-g.eng.cam.ac.uk/mmg/teaching/artificialintelligence/nonflash/problemframenf.htm\" rel=\"nofollow\">a reference</a> that explains more about some of these ideas.</p>\n\n<p>Note too that \"search\" is closely related to the idea of \"heuristics\" in an important way.  Many search problems in the real world are far too complex to solve by exhaustive brute-force search, so humans (and AI's) resort to heuristics to narrow the state space being searched.  Using heuristics can yield search algorithms that allow for reasonable solutions in a realistic time-frame, where no simple, deterministic algorithm exists to do likewise.  </p>\n\n<p>For some more background you might want to read up on <a href=\"https://en.wikipedia.org/wiki/A*_search_algorithm\" rel=\"nofollow\">A* search</a>, which is a widely used algorithm with many applications - and not just in AI.</p>\n\n<p>The other major regard in which something you could call \"search\" applies in AI is through the use of algorithms which are also often referred to as \"optimisation\" techniques.  This would be things like Hill Climbing, Gradient Descent, Simulated Annealing and perhaps even Genetic Algorithms.  These are used to maximize or minimize the values of some function  and one of the canonical uses in AI is for training neural networks using back-propagation, where you're trying to minimize the delta between the \"correct\" answer (from the training data) and the generated answer, so you can learn the correct weights within the network.</p>\n", "question": "<p>When I visit this site, I find the word \"search\" appears quite often. </p>\n\n<p>But why is it important? What kinds of search algorithms are used in Artificial Intelligence?  And how do they improve the result of an AI?</p>\n"}, "id": "1882"}, {"body": {"answer": "<p>`State space search' is a general and ubiquitous AI activity that includes numerical optimization (e.g. via gradient descent in a real-valued search space) as a special case.</p>\n\n<p>State space search is an abstraction which can be customized for a particular problem via three ingredients:</p>\n\n<ol>\n<li>Some representation for candidate solutions to the problem (e.g.\npermutation of cities to represent a Travelling Salesman Problem\n(TSP) tour, vector of real values for numeric problems).</li>\n<li>A\nsolution quality measure: i.e. some means of deciding which of two\nsolutions is the better. This is typically achieved (for\nsingle-objective problems) by having via some integer or real-valued\nfunction of a solution (e.g. total distance travelled for a TSP\ntour). </li>\n<li>Some means of moving around in the space of possible solutions, in a heuristically-informed manner. Derivatives can be used if\navailable, or else (e.g. for black-box problems or discrete solution\nrepresentations) the kind of mutation or crossover methods favoured\nby genetic algorithms/evolutionary computation can be employed.</li>\n</ol>\n\n<p>The first couple of chapters of the freely available <a href=\"https://cs.gmu.edu/~sean/book/metaheuristics/\" rel=\"nofollow\">\"Essentials of Metaheuristics\"</a> give an excellent overview and  Michalewicz and Fogel's <a href=\"http://www.springer.com/us/book/9783540224945\" rel=\"nofollow\">\"How to Solve It - Modern Heuristics\"</a> explains in more detail how numerical optimization can be considered in terms of state-space.</p>\n\n<p>In response to a request in the comments to explain how  clarify how \"the 'search through possible plans' might occur\", the idea is to choose all three of the above for the planning problem and then apply some metaheuristic such as Simulated Annealing, Tabu Search, Genetic Algorithms etc. Clearly, for nontrivial problems, only a small fraction of the space of \"all possible plans\" is actually explored.</p>\n\n<p>CAVEAT: Actually <em>planning</em> (in contrast to the vast majority of other problems amenable to state-space search such as scheduling, packing, routing etc) is a bit of a special case, in that it is sometime possible to solve planning problems simply by using A* search, rather than searching with a stochastic metaheuristic.</p>\n", "question": "<p>When I visit this site, I find the word \"search\" appears quite often. </p>\n\n<p>But why is it important? What kinds of search algorithms are used in Artificial Intelligence?  And how do they improve the result of an AI?</p>\n"}, "id": "1883"}, {"body": {"answer": "<p>The aim of an AI is to fulfill one or the other task, say solve the task adequately. But there are results that are no solutions at all and there are results which are satisfying the task and thus are accepted as solutions. Since there are generally more results that are no solutions, the set of all possible solutions is just a subset of all results. But this means that the task involves the search for a suitable set of solutions. </p>\n", "question": "<p>When I visit this site, I find the word \"search\" appears quite often. </p>\n\n<p>But why is it important? What kinds of search algorithms are used in Artificial Intelligence?  And how do they improve the result of an AI?</p>\n"}, "id": "1884"}, {"body": {"answer": "<p>In the Visual Studio, there is no real difference between C and C++. It is compiled with the same compiler binary, although with different flags.</p>\n\n<p>In most cases, the easily programmable complex data structures are generally more important, as the linear speed. The AI is an exception. AI has mostly not so complex data structures, and also the linear speed improvement is very important.</p>\n\n<p>It closes out the garbage collected languages, i.e. any managed code, and, in my opinion, the best solution would be if you would use <em>not</em> a .net-based language, but a directly to asm compilable one.</p>\n\n<p>But, knowing that you are asking explicitly for a .net one, I would suggest one, which can be easily ported later to machine code. It closes out C# and VB, but it doesn't close out C++.</p>\n\n<p>C++ has also the needed complex data structures, while it still has the near-asm speed (and memory need).</p>\n\n<p>My idea would be to start with unmanaged C++ in .net, but use simply, native .exe-s to your final programs. It would make also possible to make your program portable, because C++ is for everywhere, while .net only for windows. I.e. your program will be later able to run in non-windows server (or cluster) environment, too.</p>\n\n<p>It would be also important to prefer the deeply parallel or easily parallelizable algorithms. Ideally it should be made adaptable to slow communication channels (also for the parallel cluster run).</p>\n\n<p>--</p>\n\n<p>Your program will probably have some user interface, persistent database and similar things, these aren't speed and memory critical things, thus these you can implement in anything as you wish. The result will be a two-process solution, where a speed-optimized, C++ calculating daemon is controlled by essentially a GUI (or db.. or script.. or anything) interface.</p>\n\n<p>Probably there are already C++ frameworks for this task, so you don't need to reinvent the wheel.</p>\n", "question": "<p><em>I know that every program has some positive and negative points, and I know maybe .net programming languages are not the best for AI programming.</em></p>\n\n<p><strong>But I prefer .net programming languages because of my experiences and would like to know for an AI program which one is better, C or C++ or C# and or VB ?</strong></p>\n\n<p><em>Which one of this languages is faster and more stable when running different queries and for self learning ?</em></p>\n\n<p>To make a summary, i think C++ is the best for AI programming in .net and also C# can be used in some projects, Python as recommended by others is not an option on my view !</p>\n\n<p>because : </p>\n\n<ol>\n<li><p>It's not a complex language itself and for every single move you need to find a library and import it to your project (most of the library are out of date and or not working with new released Python versions) and that's why people say it is an easy language to learn and use ! (If you start to create library yourself, this language could be the hardest language in the world !)</p></li>\n<li><p>You do not create a program yourself by using those library for every single option on your project (it's just like a Lego game)</p></li>\n<li><p>I'm not so sure in this, but i think it's a cheap programming language because i couldn't find any good program created by this language !</p></li>\n</ol>\n"}, "id": "1886"}, {"body": {"answer": "<p>AI is already used as weapon - think on the drones.</p>\n\n<p>I suspect, a \"robots take over the world\" scenario has the highest probability, if it has an intermediate step. This intermediate step could be \"humans take over the world with robots\".</p>\n\n<p>This can go somewhere into a false direction.</p>\n\n<p>I suspect, it is not surely so far as it seems. Consider the US has currently 8000 drones. What if it would have 8million? A small group capable to control them could take over the world. Or the small groups controlling different parts of the fleet, could fight against eachother. They shouldn't be all in the US - at the time the US will have this fleet, other countries will develop also theirs.</p>\n\n<p>Btw, a world takeover seem to me unreal - the military leaders can maybe switch the human pilots to drones, it is not their job. But the \"high level control\", i.e. to determine, what to do, who are the targets, these decisions they won't ever give out from their hands.</p>\n\n<p>Next to that, the robots doesn't have a long-term goal. We, humans, have.</p>\n\n<p>Thus I don't consider a skynet-style takeover very realistic, but a chernobyl-style \"mistake\" of a misinterpreted command, which results the unstoppable rampage of the fleet, doesn't seem to me impossible.</p>\n", "question": "<p>I'm reading such nonsense about how an AI would turn the world into a supercomputer to solve a problem that it thought it needed to solve. That wouldn't be AI. That's procedural programming stuck in some loop nonsense. An AI would need to evolve and re-organise its neurons. It wouldn't be stuck to hardcode if it becomes intelligent by re-writing its code.</p>\n"}, "id": "1888"}, {"body": {"answer": "<p>I'm not sure about \"emulating the brain\" per-se, but in a more general sense there has been some thought given to using analog computing for AI/ML.  It seems clear that analog computers do have certain advantages over digital computers.  For one, they can (depending on the application) be faster, albeit at the cost of some loss of precision.  But that's OK, because I don't think anybody believes the human brain is calculating floating point math using digital computing techniques either.  The human brain appears, at least superficially, to be largely probabilistic and able to tolerate some \"slop\" numerically.  </p>\n\n<p>The downside to analog computers, as I understand it, is that they're not as flexible... you basically hardwire a circuit to do one specific \"thing\" and that's really all it can do.  To change the \"programming\" you have to literally solder in a new component! Or, I suppose, adjust a potentiometer or adjustable capacitor, etc.  Anyway, the point is that digital computers are supremely flexible, which is one big reason they came to dominate the world.  But I can see where there could be room for going analog for discrete functions that make up some or all of an intelligent system.</p>\n\n<p>As for research in the area, you might look into whatever DARPA was / is doing. There was <a href=\"http://www.wired.com/2012/08/upside/\" rel=\"nofollow\">an article in Wired</a> a while back, talking about some DARPA initiatives related to analog computing.  </p>\n", "question": "<p>Considering the answers of <a href=\"http://ai.stackexchange.com/questions/1314/how-powerful-a-computer-is-required-to-simulate-the-human-brain\">this</a> question, emulating a human brain with the current computing capacity is currently impossible, but we aren't very far from it.</p>\n\n<p>Note, 1 or 2 decades ago, similar calculations had similar results.</p>\n\n<p>The clock frequency of the modern CPUs seem to be stopped, currently the miniaturization (-> mobile use), the RAM/cache improvement and the multi-core paralellization are the main lines of the development.</p>\n\n<p>Ok, but what is the case with the analogous chips? In case of a NN, it is not a very big problem, if it is not very accurate, the NN would adapt to the minor manufacturing differences in its learning phase. And a single analogous wire can substitute a complex integer multiplication-division unit, while the whole surface of the analogous printed circuit could work parallel.</p>\n\n<p>According to <a href=\"http://engineering.stackexchange.com/questions/3993/do-analog-fpgas-exist\">this</a> post, \"software rewirable\" analogous circuits, essentially \"analogous FPGAs\" already exist. Although the capacity of the FPGAs is highly below the capacity of the <a href=\"https://en.wikipedia.org/wiki/Application-specific_integrated_circuit\" rel=\"nofollow\">ASIC</a>s with the same size, maybe analogous chips for neural networks could also exist.</p>\n\n<p>I suspect, if it is correct, maybe even the real human brain model wouldn't be too far. It would still require a massively parallel system of costly analogous NN chips, but it seems to me not impossible.</p>\n\n<p>Could this idea work? Maybe there is even active research/development into this direction?</p>\n"}, "id": "1889"}, {"body": {"answer": "<p>I feel like most of the scenarios about AI's wiping out the world fall into one of two categories:</p>\n\n<ol>\n<li>Anthropomorphized AI's</li>\n</ol>\n\n<p>or</p>\n\n<ol start=\"2\">\n<li>Intelligent But Dumb Computer Run Amuck</li>\n</ol>\n\n<p>In the (1) case, people talk about AI's becoming \"evil\" and attribute to them other such human elements.  I look at this as being mostly sci-fi and don't think it merits much serious discussion.  That is, I see no particular reason to assume that an <strong>Artificial</strong> Intelligence - regardless of how intelligent it is - will necessarily <strong>behave</strong> like a human.</p>\n\n<p>The (2) case makes more sense to me.  This is the idea that an AI is, for example, put in control of the nuclear missile silos and winds up launching the missiles because it was just doing it's job, but missed something a human would have noticed via what we might call \"common sense\".  Hence the \"Intelligent but Dumb\" moniker.  </p>\n\n<p>Neither of these strikes me as <strong>terribly</strong> alarming, because (1) is probably fiction and (2) doesn't involve any actual malicious intent by the AI - which means it won't be actively trying to deceive us, or work around any safety cut-outs, etc.</p>\n\n<p>Now IF somebody builds an AI and decides to intentionally program it so that it develops human like characteristics like arrogance, ego, greed, etc... well, all bets are off.</p>\n", "question": "<p>I'm reading such nonsense about how an AI would turn the world into a supercomputer to solve a problem that it thought it needed to solve. That wouldn't be AI. That's procedural programming stuck in some loop nonsense. An AI would need to evolve and re-organise its neurons. It wouldn't be stuck to hardcode if it becomes intelligent by re-writing its code.</p>\n"}, "id": "1890"}, {"body": {"answer": "<h1>Yes.</h1>\n\n<ol>\n<li>Every chess game... every poker game. Every game.</li>\n<li>Every more intelligent spam softwares or spambots. Although their primary goal is to lie to computer systems (f.e. spamfilter poisoning), their secondary goal is to lie to the human behind them.</li>\n</ol>\n", "question": "<p>AI systems today are very capable machines, and recently the area of Natural Language Processing and Response has been exploding with innovation, as well as the fundamental algorithmic structure of AI machines.</p>\n\n<p>I am asking if, given these recent breakthroughs, have any AI systems been developed that are able to (preferably with some measure of success) knowingly lie to humans about facts that it knows?</p>\n\n<p>Note, what I'm asking goes beyond the canonical discussions of the Turing Test. I'm asking of machines that can 'understand' facts and then formulate a lie against this fact, perhaps using other facts to produce a believable 'cover-up' as part of the lie.</p>\n\n<p>E.G.: CIA supercomputer is stolen by spies and they try to use the computer to do things, but the computer keeps saying it's missing dependencies though it really isn't or gives correct-looking but wrong answers knowingly. Or gives incorrect location of a person, knowing that the person frequents some place but isn't there at the moment. Doesn't have to be this sophisticated, of course.</p>\n"}, "id": "1891"}, {"body": {"answer": "<p>Whether \"I take the ball\" or \"he takes the ball\", all stored instances of 'taking' and 'ball' will be weakly activated and 'taking [the] ball' will be strongly activated.  Doesn't this qualify as 'mirroring'? If you also know that \"I have an arm\" and \"he has an arm\", etc., then when \"he takes some blocks\", it isn't too hard to think that \"I could take some blocks.\"</p>\n", "question": "<p>From Wikipedia:</p>\n\n<blockquote>\n  <p>A mirror neuron is a neuron that fires both when an animal acts and when the animal observes the same action performed by another.</p>\n</blockquote>\n\n<p>Mirror neurons are related to imitation learning, a very useful feature that is missing in current real-world A.I. implementations. Instead of learning from input-output examples (supervised learning) or from rewards (reinforcement learning), an agent with mirror neurons would be able to learn by simply observing other agents, translating their movements to its own coordinate system. What do we have on this subject regarding computational models?</p>\n"}, "id": "1892"}, {"body": {"answer": "<p>Every problem can be reduced to search.  Every problem has an input within some range (the domain) and an output in some other range (codomain).  That is, every problem can be formulated as a kind of map from one space to another, where the source is the givens of the problem, and the destination is the solution to the problem.</p>\n\n<p>\"Brute force\" is the algorithm which solves every problem by inspecting every point in the codomain and asking: \"Is this the solution?\"  Every other algorithm is an attempt to improve on brute force by not searching the entire codomain of possible solutions.</p>\n\n<p>Typical software engineering problems can be solved by algorithms which arrive at the correct solution very quickly (sorting, arithmetic, partition, etc.).  AI problems are generally those for which a strong polynomial  algorithm is not known, and thus, we must settle for approximations.  Basically every common problem that the human brain must solve falls into this category.</p>\n\n<p>Consider the problem of moving a multi-jointed robotic arm to pick up an object.  Reverse kinematics does not have unique solutions: there is more than one way to move your hand from a start position to a target position.  This is due to the excessive degrees of freedom in your joints.  If you want to minimize energy usage, then there is a unique solution (due to the asymmetry of joints and muscles).</p>\n\n<p>But what if there is an obstacle in the pathway of the minimum-energy solution?  There are many pathways which avoid the obstacle, but again, many of them will have a similar cost.  Even if there is a unique minimum-energy solution, it might not be the most practical to compute.  The brain is the most metabolically expensive organ in the body, so it is not always best to find an optimal solution.  Thus, heuristics come into play.</p>\n\n<p>But in all cases, the problem is not: \"move your hand\" or \"move the robot arm.\"  The problem is: \"search the space of joint rotation sequences which best achieves the goal.\"  And even though there is a closed-form solution for the simple minimum-energy case with no obstacles, it is too expensive to compute precisely when a set of cheap heuristics will get you very close with a small fraction of the computational effort.</p>\n\n<p>If computation were free, then AI would be mere mathematics, and we would always compute the best answer to every question using logic, calculus, physics, at worst, numerical methods when we don't have closed-form solutions.  In reality, time is money, and the time and effort to get an answer is as much a part of the cost as the quality of the solution.  So it is an engineering tradeoff to decide how much effort should be expended in what way to obtain the best answer given the value of the response.</p>\n\n<p>Or, in other words, AI problems are all about searching the space of solutions as quickly as possible to get an answer that is \"good enough\".</p>\n\n<p>I might seem curious that such far-flung problems as natural language recognition and theorem proving would be search problems.  But language parsers strive to determine the meaning of statements via part-of-speech tagging.  A given phrase can be parsed in many different ways, yielding many different interpretations, and the space of parse trees is yet another search problem in deciding which parse tree is the most likely intended meaning by the speaker.</p>\n\n<p>A theorem proof is graph starting with axioms, proceeding through lemmas, applying the rules of procedure until the theorem is derived or refuted (by proving its negation).  There are many ways to represent this sequence, but at the end of the day, we are talking about a process of exploring the intermediate proof space and finding the derivation which reaches your goal.  Everything is search, in the end.</p>\n", "question": "<p>When I visit this site, I find the word \"search\" appears quite often. </p>\n\n<p>But why is it important? What kinds of search algorithms are used in Artificial Intelligence?  And how do they improve the result of an AI?</p>\n"}, "id": "1894"}, {"body": {"answer": "<p>ANNs don't compress, they generalise. </p>\n\n<p>Often this leads to compression, i.e. the internal generalised representation is smaller than the original input, but not necessarily. Imagine a ANN that is trained to use the screen input of a computer game to play it. If the game is very rich and conceptually deep the internal representation of a single screen input might be a lot bigger than the input itself, because ANNs put the single data points into the context of the overall data. Which leads us to the second point:</p>\n\n<p>ANNs (and the neocortex) model data hierarchically. </p>\n\n<p>This is what makes them so powerful. So it is not just about having a large number of parameters, they also have to be arranged in such a way that they capture the structure of the data (or the world), which very often seems to be hierarchical. Just look a two different pictures of a duck. On the pixel level they might be as different as random images, all the similarities emerge in higher levels of the hierarchy, when enough pixels combined give you the patterns of feathers, beak and webs. Bloom filters obviously lack this property. They would only give you a pixel by pixel account of whether you have seen (almost) exactly this picture before. </p>\n", "question": "<p>Conceptually speaking, aren't artificial neural networks just highly distributed, lossy compression schemes?</p>\n\n<p>They're certainly efficient at <a href=\"https://cs.stanford.edu/people/eroberts/courses/soco/projects/neural-networks/Applications/imagecompression.html\" rel=\"nofollow\">compressing images</a>.</p>\n\n<p>And aren't brains (at least, the neocortex) just compartmentalized, highly distributed, lossy databases?</p>\n\n<p>If so, what salient features in RNNs and CNNs are necessary in any given lossy compression scheme in order to extract the semantic relations that they do? Is it just a matter of having a large number of dimensions/variables? </p>\n\n<p>Could some kind of lossy <a href=\"https://en.wikipedia.org/wiki/Bloom_filter\" rel=\"nofollow\">Bloom filter</a> be re-purposed for the kinds of problems ANNs are applied to?</p>\n"}, "id": "1896"}, {"body": {"answer": "<p>No-one knows.</p>\n\n<p>Why: because it's not possible to formally determine even whether your fellow human beings are actually conscious (they may instead be what is philosophically termed a <a href=\"http://plato.stanford.edu/entries/zombies/\" rel=\"nofollow\">'Zombie'</a>). No test known to modern physics suffices to decide. Hence it's possible that you are the only sentient being, and everyone else is a robot.</p>\n\n<p>Consequently, we cannot determine which tasks require sentience.</p>\n\n<p>Note that the ontological status of Zombies is controversial: some philosophers of AI (e.g. Daniel Dennett) claim that Zombies are <a href=\"https://ase.tufts.edu/cogstud/dennett/papers/unzombie.htm\" rel=\"nofollow\">logically impossible</a> while others such as <a href=\"http://consc.net/zombies.html\" rel=\"nofollow\">David Chalmers</a> would claim that a Zombie would be compelled to assert that they experience <em>qualia</em> (i.e. are sentient) even though they do not. <a href=\"http://homepages.uc.edu/~polgertw/Polger-ZombiesJCS.pdf\" rel=\"nofollow\">Here</a> is a very readable paper by Flanagan and Polger that also explains why a stronger neurological version of the Turing test is insufficient to detect a Zombie.</p>\n\n<p>EDIT: In response to the comment about whether an objective test for distinguishing sentience from non-sentience exists:</p>\n\n<p>No-one knows. What we <em>do</em> believe is that this would require something in addition to what modern physics can currently tell us. David Chalmers has speculated that qualia should be introduced as a new form of physical unit, orthogonal to the others in the same way that electrical charge is orthogonal to distance.</p>\n\n<p>In the absence of an objective test, we have to rely on Turing test variants, which no more guarantee consciousness in the subject than they do intelligence.</p>\n", "question": "<p>Consciousness <a href=\"http://www.iep.utm.edu/consciou/\">is challenging to define</a>, but for this question let's define it as \"actually experiencing sensory input as opposed to just putting a bunch of data through an inanimate machine.\" Humans, of course, have minds; for normal computers, all the things they \"see\" are just more data. One could alternatively say that humans are <a href=\"http://philosophy.stackexchange.com/a/4687\">sentient</a>, while traditional computers are not.</p>\n\n<p>Setting aside the question of whether it's possible to build a sentient machine, does it actually make a difference if an AI is sentient or not? In other words, are there are tasks that are made impossible - not just more difficult - by a lack of sentience?</p>\n"}, "id": "1898"}, {"body": {"answer": "<p>In a very niche sense, I'd say yes.</p>\n\n<p>The only tasks that sentience would make possible was the actual feeling and thinking in and of itself. At this point, sentience doesn't play a part in any of the tasks we ask AI's to complete; we are rapidly approaching the point of being able to teach a 'dead' machine to do most anything a sentient AI can, in a practical sense.</p>\n\n<p>Sentience <em>colloquially</em> often translates to 'the ability to reason while understanding that oneself and each other entity is a distinct acting agent'or something along those lines. It literally means something more along the lines of self-awareness and the definition of consciousness you have above. The point I'm making is that we are readily approaching the point where 'dead' AI's can very nicely mimic the first way of thinking, just by really nicely learning and interpreting data.</p>\n\n<p><a href=\"http://i.stack.imgur.com/lbSUcm.jpg\" rel=\"nofollow\"><img src=\"http://i.stack.imgur.com/lbSUcm.jpg\" alt=\"enter image description here\"></a></p>\n\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<sub>Does the robot see an amalgamation of bone, or a being that once was?</sub></p>\n\n<p>Thus, a truly sentient machine would be superior in capability (compared to a really, really advanced 'dead' AI) only in the respect of being able to 'truly' experience the information.</p>\n\n<p>This runs very well in parallel with the so-called <a href=\"https://en.wikipedia.org/wiki/Knowledge_argument\" rel=\"nofollow\">\"Knowledge Argument\"</a> which in essence debates this very issue. The version of it that I heard which sticks with me is that there is a very smart girl in a room with access to all sorts of information. She likes the color blue. Or so she thinks; she's never actually seen it. She has all the information in the world available about colors and how they work, etc. but does she really know what blue is until she sees it?</p>\n\n<p>Another great, historic venture into this field is the famous painting:</p>\n\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"http://i.stack.imgur.com/vhd7K.jpg\" rel=\"nofollow\"><img src=\"http://i.stack.imgur.com/vhd7K.jpg\" alt=\"Ceci n&#39;est pas une pipe\"></a></p>\n\n<p>The caption translates: \"This is not a pipe\". And the idea is that this, honestly, isn't a pipe. Right now it's a bunch of pixels on your screen in a certain configuration - we can all 'see' a pipe, but what does that really mean?</p>\n\n<p>At the end of the day, I think super-intelligent 'dead' AI can practically do anything a 'live' one can, with the latter being superior in and of the 'liveness' itself.</p>\n", "question": "<p>Consciousness <a href=\"http://www.iep.utm.edu/consciou/\">is challenging to define</a>, but for this question let's define it as \"actually experiencing sensory input as opposed to just putting a bunch of data through an inanimate machine.\" Humans, of course, have minds; for normal computers, all the things they \"see\" are just more data. One could alternatively say that humans are <a href=\"http://philosophy.stackexchange.com/a/4687\">sentient</a>, while traditional computers are not.</p>\n\n<p>Setting aside the question of whether it's possible to build a sentient machine, does it actually make a difference if an AI is sentient or not? In other words, are there are tasks that are made impossible - not just more difficult - by a lack of sentience?</p>\n"}, "id": "1899"}, {"body": {"answer": "<p>Two kinds of tasks require consciousness:</p>\n\n<ol>\n<li><p>consciousness</p></li>\n<li><p>Any task that requires extreme dynamicity, where solving problems requires analogizing between various 3D states of affairs and prior knowledge of how to solve the problem is minimal</p></li>\n</ol>\n\n<p>However, once knowledge of how to solve a given problem is gained, further optimization will eliminate that need for consciousness.</p>\n\n<p>If you give enough specificity to a problem, you remove the need for a general solver. And then the only remaining <em>need</em> for a consciousness is for the sake of itself.</p>\n", "question": "<p>Consciousness <a href=\"http://www.iep.utm.edu/consciou/\">is challenging to define</a>, but for this question let's define it as \"actually experiencing sensory input as opposed to just putting a bunch of data through an inanimate machine.\" Humans, of course, have minds; for normal computers, all the things they \"see\" are just more data. One could alternatively say that humans are <a href=\"http://philosophy.stackexchange.com/a/4687\">sentient</a>, while traditional computers are not.</p>\n\n<p>Setting aside the question of whether it's possible to build a sentient machine, does it actually make a difference if an AI is sentient or not? In other words, are there are tasks that are made impossible - not just more difficult - by a lack of sentience?</p>\n"}, "id": "1901"}, {"body": {"answer": "<p>Consciousness is an attention selection mechanism that <strong>searches</strong> over salient inputs. The <a href=\"https://www.youtube.com/watch?v=aQclvZpPWHQ\" rel=\"nofollow\">robotic saccades</a> of your eyeballs show you first hand the algorithmic nature of your brain's conscious attention mechanism, while it searches among salient inputs.</p>\n\n<p>A smart search algorithm can help with dimensionality reduction.</p>\n", "question": "<p>When I visit this site, I find the word \"search\" appears quite often. </p>\n\n<p>But why is it important? What kinds of search algorithms are used in Artificial Intelligence?  And how do they improve the result of an AI?</p>\n"}, "id": "1902"}, {"body": {"answer": "<p>A being without sentience cannot suffer. If, for example, we wanted to take joy in the suffering of another, only an AI that was sentient would suffice.</p>\n\n<p>Suppose we had some sadists who could not be satisfied or productive unless they got to produce lots of suffering. And say we only cared about minimizing human and animal suffering. What we would need for this job is something non-human and non-animal that could suffer. A conscious AI would do, a non-conscious one would not.</p>\n\n<p>The claim was made in the comments that consciousness cannot be proven, other than perhaps by introspection. But clearly this is not a problem since sadists take joy in torturing others, and those others cannot prove they're conscious either.</p>\n", "question": "<p>Consciousness <a href=\"http://www.iep.utm.edu/consciou/\">is challenging to define</a>, but for this question let's define it as \"actually experiencing sensory input as opposed to just putting a bunch of data through an inanimate machine.\" Humans, of course, have minds; for normal computers, all the things they \"see\" are just more data. One could alternatively say that humans are <a href=\"http://philosophy.stackexchange.com/a/4687\">sentient</a>, while traditional computers are not.</p>\n\n<p>Setting aside the question of whether it's possible to build a sentient machine, does it actually make a difference if an AI is sentient or not? In other words, are there are tasks that are made impossible - not just more difficult - by a lack of sentience?</p>\n"}, "id": "1903"}, {"body": {"answer": "<p>Oh yeah, definitely.  Just to pick one example, you have <a href=\"http://www.cogsci.indiana.edu/\" rel=\"nofollow\">Douglas Hofstader's group</a> at Indiana.  I think most of what they do would fall under the rubric of GOFAI (or at least closer to that than the statistical machine learning stuff).  </p>\n\n<p>Beyond that, just go to the <a href=\"http://arxiv.org/corr/home\" rel=\"nofollow\">CORR</a> and browse around the <a href=\"http://arxiv.org/list/cs.AI/recent\" rel=\"nofollow\">AI category</a>. You'll see plenty of neural networks and probabilistic stuff, but you'll also find the papers by the folks doing symbolic processing / GOFAI as well.</p>\n", "question": "<p>A lot of textbooks and introductory lectures typically split AI into connectionism and GOFAI (Good Old Fashioned AI). \nFrom a purely technical perspective it seems that connectionism has grown into machine learning and data science, while nobody talks about GOFAI, Symbolic AI or Expert Systems at all. </p>\n\n<p>Is anyone of note still working on GOFAI?    </p>\n"}, "id": "1907"}, {"body": {"answer": "<p><strong>No.</strong></p>\n\n<p>The <em>experience</em> of seeing is by definition non-causal. Anything non-causal cannot be a requirement of a physical process; a qualia cannot afford a robot the ability to do something it otherwise could not.</p>\n\n<p><strong>Maybe.</strong></p>\n\n<p>Although a qualia is not required for a given AI task, that is not to say that any sufficiently advanced AI does not entail qualia. It could be that so-called AI-complete tasks require a robot that, although not making use of qualia, produces it anyway.</p>\n\n<p><strong>Yes.</strong></p>\n\n<p>Qualia may refer to some wishy-washy non-physical property, but it's special in that we know it exists physically, too. The fact I am able to discuss my qualia knowingly (or, if you don't believe me, the fact <em>you</em> are able to) implies that my (or your) qualia does have a physical effect.</p>\n\n<p>It stands to reason that if we accept others' qualia on the basis of our own, it must be because of the <em>physical</em> basis of our own<sup>1</sup>. Thus one could argue that<sup>2</sup> any robot that has an equivalent physical capacity <em>must</em> entail qualia.</p>\n\n<p><sup>1</sup> since the subjective is physically non-causal, so cannot <em>cause</em> us to accept anything.</p>\n\n<p><sup>2</sup> as long as you don't make the particularly odd assumption that qualia is somehow tied to its direct physical manifestation, which at best is tenuous since had we evolved the wrong one you would still claim it to be the right one with equal certainty.</p>\n", "question": "<p>Consciousness <a href=\"http://www.iep.utm.edu/consciou/\">is challenging to define</a>, but for this question let's define it as \"actually experiencing sensory input as opposed to just putting a bunch of data through an inanimate machine.\" Humans, of course, have minds; for normal computers, all the things they \"see\" are just more data. One could alternatively say that humans are <a href=\"http://philosophy.stackexchange.com/a/4687\">sentient</a>, while traditional computers are not.</p>\n\n<p>Setting aside the question of whether it's possible to build a sentient machine, does it actually make a difference if an AI is sentient or not? In other words, are there are tasks that are made impossible - not just more difficult - by a lack of sentience?</p>\n"}, "id": "1908"}, {"body": {"answer": "<p>It's not a difficult task, first of all you have to locate the body parts such as arms,head... you can do it using different approaches for example using cascadeclassifier or a well trained CNN.<br />\nAfter that you can use different techniques, one could be an ANN trained on the keypoints of the different body parts (this is the easiest approach) or a CNN (good approach but you need a lot of training). To indicate the location after you have determined the position of the head (and the eyes to) and hands, you can simply calculate the orientation of those parts, and then you can get a general position where those orientation are pointing to.</p>\n", "question": "<p>I'd like to investigate the possibility of achieving similar recognition as it's in <a href=\"http://asimo.honda.com/downloads/pdf/asimo-technical-information.pdf\" rel=\"nofollow\">Honda's ASIMO robot</a><sup>p.22</sup> which can interpret the positioning and movement of a hand, including postures and gestures based on visual information.</p>\n\n<p>Here is the example of application such interpretation in robot:</p>\n\n<p><a href=\"http://asimo.honda.com/downloads/pdf/asimo-technical-information.pdf\" rel=\"nofollow\"><img src=\"http://i.stack.imgur.com/UDram.png\" alt=\"Honda&#39;s ASIMO robot - Recognition of postures and gestures based on visual information\"></a></p>\n\n<p><sup>Image source: <a href=\"http://asimo.honda.com/downloads/pdf/asimo-technical-information.pdf\" rel=\"nofollow\">ASIMO Featuring Intelligence Technology - Technical Information (PDF)</a></sup></p>\n\n<p>So basically the recognition should detect an indicated location (posture recognition) or respond to a wave (gesture recognition), also similar like <a href=\"http://ai.stackexchange.com/a/1577/8\">Google car</a> does it (by determining certain patterns).</p>\n\n<p>Is it known how ASIMO does it, or what would be the closest alternative for postures and gestures recognition to achieve the same results?</p>\n"}, "id": "1910"}, {"body": {"answer": "<p>I'm not sure what google is using to perform that task but most companies use region based convolutional neural nets to locate traffic signs and other objects.</p>\n\n<p>But other companies use a Deep neural net + Bag of words approach to find objects.</p>\n\n<p>See: <a href=\"https://www.researchgate.net/publication/284505205_Bag-of-Words_Based_Deep_Neural_Network_for_Image_Retrieval\" rel=\"nofollow\">Bag-of-Words Based Deep Neural Network for Image Retrieval</a> which shows a general approach, to get the exact location you can use <em>Feature Matching</em> or <em>Random Boxes</em>.</p>\n", "question": "<p>This <a href=\"http://repository.supsi.ch/5145/1/IDSIA-04-12.pdf\" rel=\"nofollow\">study</a> (pages 7-8) shows an attempt at recognizing the traffic signs with lower error rates by using multi-column deep neural networks </p>\n\n<p>Are Google cars using similar techniques of predicting signs using DNN, or are they using some other method?</p>\n"}, "id": "1911"}, {"body": {"answer": "<p>Just to add some discourse; this is actually an incredibly complex task, as gestures (aka kinematics) function as an auxiliary language that can completely change the meaning of a sentence or even a single word. I recently did a dissertation on the converse (generating the correct gesture from a specific social context &amp; linguistic cues). The factors that go into the production of a particular gesture include the relationship between the two communicators (especially romantic connotations), the social scenario, the physical context, the linguistic context (the ongoing conversation, if any), a whole lot of personal factors (our gesture use is essentially a hybrid of important individuals around us e.g. friends &amp; family, and this is layered under the individual's psychological state). Then the whole thing is flipped around again when you look at how gestures are used completely differently in different cultures (look up gestures that are swear words in other cultures for an example!). There are a number of models for gesture production but none of them capture the complexity of the topic.</p>\n\n<p>Now, that may seem like a whole lot of fluff that is not wholly relevant to your question, but my point is that ASIMO isn't actually very 'clever' at this. AFAIK (I have heard from a visualization guy that this is how <em>he</em> thinks they do it) they use conventional (but optimized) image recognition techniques trained on a corpus of data to achieve recognition of particular movements. One would assume that the dataset consists of a series of videos / images of gestures labelled with that particular gesture (as interpreted by a human), which can then be treated as a machine learning problem. The issue with this is that it does not capture ANY of the issues I mentioned above. Now if we return to the current best interpretation of gesture that we have (that it is essentially auxiliary language in its own right), ASIMO isn't recognizing any element of language beyond the immediately recognizable type, 'Emblems'.</p>\n\n<p>'Emblems' are gestures which have a direct verbal translation, for example in English-based cultures, forming a circle with your thumb and index finger translates directly to 'OK'. ASIMO is therefore missing out on a huge part of the non-verbal dictionary (illustrators, affect displays, regulators and adapters are not considered!), and even the part that it is accessing is based on particular individuals' interpretations of said emblems (e.g. someone has sat down and said that <em>this</em> particular movement is <em>this</em> gesture which means <em>this</em>), which as we discussed before is highly personal and contextual. I do not mean this in criticism of Honda; truth be told, gesture recognition and production is in my opinion one of the most interesting problems in AI (even if its not the most useful) as it is a compound of incredibly complex NLP, visualization and social modelling problems!</p>\n\n<p>Hopefully I've provided some information on how ASIMO works in this context, but also on why ASIMO's current process is flawed when we look at the wider picture.</p>\n", "question": "<p>I'd like to investigate the possibility of achieving similar recognition as it's in <a href=\"http://asimo.honda.com/downloads/pdf/asimo-technical-information.pdf\" rel=\"nofollow\">Honda's ASIMO robot</a><sup>p.22</sup> which can interpret the positioning and movement of a hand, including postures and gestures based on visual information.</p>\n\n<p>Here is the example of application such interpretation in robot:</p>\n\n<p><a href=\"http://asimo.honda.com/downloads/pdf/asimo-technical-information.pdf\" rel=\"nofollow\"><img src=\"http://i.stack.imgur.com/UDram.png\" alt=\"Honda&#39;s ASIMO robot - Recognition of postures and gestures based on visual information\"></a></p>\n\n<p><sup>Image source: <a href=\"http://asimo.honda.com/downloads/pdf/asimo-technical-information.pdf\" rel=\"nofollow\">ASIMO Featuring Intelligence Technology - Technical Information (PDF)</a></sup></p>\n\n<p>So basically the recognition should detect an indicated location (posture recognition) or respond to a wave (gesture recognition), also similar like <a href=\"http://ai.stackexchange.com/a/1577/8\">Google car</a> does it (by determining certain patterns).</p>\n\n<p>Is it known how ASIMO does it, or what would be the closest alternative for postures and gestures recognition to achieve the same results?</p>\n"}, "id": "1912"}, {"body": {"answer": "<p>Interesting question.  I don't think anybody knows a definite answer, but some rough-sketch ideas seem apparent.  Think about what it means to you to be \"self aware\".  You'll probably cite the way you \"hear\" your own thoughts in your head when you think about something.  One can speculate that inside the brain, the various centers that are responsible for hearing, vision, logic, etc. are connected so that as you form a thought, it's being \"heard\" by the hearing regions, even though it's purely internal instead of actual sound received at the ear.</p>\n\n<p>So in AI terms, it seems likely that self-awareness will somehow involve taking the \"thoughts\" formed within the AI, and feeding them back into the AI so that it \"hears\" (or, more broadly, \"senses\") itself think.  </p>\n\n<p>There's this weirdly recursive aspect to all of this, which - interestingly enough - is something Douglas Hofstadter talked about a lot in some of his book, especially <a href=\"https://en.wikipedia.org/wiki/G%C3%B6del,_Escher,_Bach\" rel=\"nofollow\">GEB</a>.  He was probably onto something.  </p>\n", "question": "<p>Self-Recognition seems to be an item that designers are trying to integrate into artificial intelligence. Is there a generally recognized method of doing this in a machine, and how would one test the capacity - as in a Turing-Test?</p>\n"}, "id": "1915"}, {"body": {"answer": "<p>I think your net should have the various actions as outputs, but I am not an expert in Deep Nets. I just think that that light form of multi-task learning might be better. The idea of multi-task learning is that a predictor predicting multiple variables (in this case the various Q(s,a1), Q(s,a2), ...) using mostly the same structure (varying only the output weights) will learn more sensible things. Though I admit applying this here might be a bit of a stretch.</p>\n\n<p>As for the real question, a popular technique in Reinforcement Learning is <a href=\"https://webdocs.cs.ualberta.ca/~sutton/book/ebook/node88.html\" rel=\"nofollow\">Tile Coding</a>.</p>\n\n<p>The basic idea is to discretize the (2-dimensional, in your case) state-space - imagine a grid laid over the 2D space - and assigning an input feature to each cell; all of these variables are set to zero except for the one your continuous variables fall into. For example, if your grid is 20x20, you will have 400 variables, 399 of which are set to zero, and 1 set to one.</p>\n\n<p>Tile Coding takes this one step further and repeats this using slight offsets for the grid. Imagine you create an identical grid but you move it slightly to the right by 1/10 of the width of a cell: you will have another set of 400 variables like before, but it is possible that the cell set to one is not the same. Then you repeat this moving the grid by 2/10 and you have another set of 400 variables, again, only 1 of which is set to one. In total you have 10 sets of 400 variables (if you repeat more than that, you get the same grids as before); of your 4000 variables, only 10 are set to one.\nNow you repeat this by adding a 1/10 of a cell offset in the Y axis and obtain another 4000 variables. Repeat with 2/10 and you get another 4000. By the end of it, you have 40000 variables, 100 of which are set to one.</p>\n\n<p>Now your net can more easily learn different weights for different positions. I recommend you to follow the link above for a better explanation than mine (and figures!)</p>\n\n<p>My suggestion is to feed all of these variables to your net and have it predict the Q-value for all of the actions. But, again, I am no expert in deep nets so I may be wrong.</p>\n\n<p>Also, according to Andrej Karpathy, \"most people prefer to use Policy Gradients, including the authors of the original DQN paper who have shown Policy Gradients to work better than Q Learning when tuned well.\". This means that you may be better off not using Q-learning (as they did in the original DQN formulation) to train your net. Have a look at <a href=\"http://karpathy.github.io/2016/05/31/rl/\" rel=\"nofollow\">Andrej's blog</a> and <a href=\"http://arxiv.org/abs/1602.01783\" rel=\"nofollow\">the paper he points to</a>.</p>\n", "question": "<p>I recently finished Course on RL by David Silver (on YT) and thought about trying it out on simple application in Unity Game Engine, where I've built simple labyrint with ball and want to teach the ball to get from point A to point B in there while avoiding obstacles and fire (the place where you'll get burnt so big negative reward)</p>\n\n<p>The problem I encountered while designing the whole thing (programming-wise) is: What is the correct (or at least good) way of representing the position in 2D space? It is continuous so I thought about representing it as feature vector consisting of [up, down, left, right, posX, posY] where direction is whether I am pressing button of moving in that direction in binary (or actions if you want) and pos are floats (0-1) representing normalized position from one corner on the plane where the whole map is. That would be accompanied by vector W that would represent the weights adjusted using Gradient Descent.</p>\n\n<p>Question is: will this work?? I am asking for 2 reasons. One is that I am not so sure about that posX and posY since it can be 0 and if I multiply it by the weights vector then how could be resulting reward anything but 0? Second reason is that I am not sure if the actions should be part of the features. I mean, it makes sense to me but I could easily be very wrong since I am a beginner.</p>\n\n<p>Thanks a lot guys in advance. If you have any more questions or think the problem is not described deeply enough just ask in the comments and I'll edit the question. :)</p>\n\n<p>PS: I could just code it the way I think is right, but I also want to get gasp of designing applications on paper before coding them (project management).</p>\n"}, "id": "1916"}, {"body": {"answer": "<p>A really good introduction is the Berkeley CS188 class videos and projects.  You can find those materials at <a href=\"http://ai.berkeley.edu/home.html\" rel=\"nofollow\">http://ai.berkeley.edu/home.html</a></p>\n\n<p>You probably also want to get ahold of a copy of <a href=\"http://aima.cs.berkeley.edu/\" rel=\"nofollow\">Artificial Intelligence: A Modern Approach</a> by Norvig and Russell. </p>\n\n<p>For more on the \"machine learning\" aspects of AI, including an introduction to Neural Networks, take the Andrew Ng \"Machine Learning\" class on Coursera.</p>\n\n<p>Another book I would recommend is <a href=\"http://rads.stackoverflow.com/amzn/click/048624864X\" rel=\"nofollow\">Introduction to Artificial Intelligence</a> by Philip C. Jackson. It's older, which is exactly what makes it valuable.  It's a good overview of techniques and ideas that aren't \"en vogue\" right now, but which may still be useful to you.  </p>\n", "question": "<p>Currently I work as a java developer, But very much interested in learning Artificial Intelligence.\nCan anybody tell me what steps i have to follow to learn artificial intelligence considering the fact i am very new to this.\nIs there any special technologies i have to learn or something else.</p>\n"}, "id": "1917"}, {"body": {"answer": "<h3>Strong AIs</h3>\n\n<p>For a strong AI, the short answer is to call for help, when they might not even know what the supposed help could be.</p>\n\n<p>It depends on what the AI would do. If it is supposed to solve a single easy task perfectly and professionally, sure emotions would not be very useful. But if it is supposed to learn random new things, there would be a point that it encounters something it cannot handle.</p>\n\n<p>In Lee Sedol vs AlphaGo match 4, some pro who has said computer doesn't have emotions previously, commented that maybe AlphaGo has emotions too, and stronger than human. In this case, we know that AlphaGo's crazy behavior isn't caused by some deliberately added things called \"emotions\", but a flaw in the algorithm. But it behaves exactly like it has panicked.</p>\n\n<p>If this happens a lot for an AI. There might be advantages if it could know this itself and think twice if it happens. If AlphaGo could detect the problem and change its strategy, it might play better, or worse. It's not unlikely to play worse if it didn't do any computations for other approaches at all. In case it would play worse, we might say it suffers from having \"emotions\", and this might be the reason some people think having emotions could be a flaw of human beings. But that wouldn't be the true cause of the problem. The true cause is it just doesn't know any approaches to guarantee winning, and the change in strategy is only a try to fix the problem. Commentators thinks there are better ways (which also don't guarantee winning but had more chance), but its algorithm isn't capable to find out in this situation. Even for human, the solution to anything related to emotion is unlikely to remove emotions, but some training to make sure you understand the situation enough to act calmly.</p>\n\n<p>Then someone has to argue about whether this is a kind of emotion or not. We usually don't say small insects have human-like emotions, because we don't understand them and are unwilling to help them. But it's easy to know some of them could panic in desperate situations, just like AlphaGo did. I'd say these reactions are based on the same logic, and they are at least the reason why human-like emotions could be potentially useful. They are just not expressed in human-understandable ways, as they didn't intend to call a human for help.</p>\n\n<p>If they tries to understand their own behavior, or call someone else for help, it might be good to be exactly human-like. Some pets can sense human emotions and express human-understandable emotion to some degree. The purpose is to interact with humans. They evolved to have this ability because they needed it at some point. It's likely a full strong AI would need it too. Also note that, the opposite of having full emotions might be becoming crazy.</p>\n\n<p>It is probably a quick way to lose any trust if someone just implement emotions imitating humans with little understanding right away in the first generations, though.</p>\n\n<h3>Weak AIs</h3>\n\n<p>But is there any purposes for them to have emotions before someone wanted a strong AI? I'd say no, there isn't any inherent reasons that they must have emotions. But inevitably someone will want to implement imitated emotions anyway. Whether \"we\" need them to have emotions is just nonsense.</p>\n\n<p>The fact is even some programs without any intelligence contained some \"emotional\" elements in their user interfaces. They may look unprofessional, but not every task needs professionality so they could be perfectly acceptable. They are just like the emotions in musics and arts. Someone will design their weak AI in this way too. But they are not really the AIs' emotions, but their creators'. If you feel better or worse because of their emotions, you won't treat individul AIs so differently, but this model or brand as a whole.</p>\n\n<p>Alternatively someone could plant some personallities like in a role-playing game there. Again, there isn't a reason they must have that, but inevitably someone will do it, because they obviously had some market when a role-playing game does.</p>\n\n<p>In either cases, the emotions don't really originate from the AI itself. And it would be easy to implement, because a human won't expect them to be exactly like a human, but tries to understand what they intended to mean. It could be much easier to accept these emotions realizing this.</p>\n\n<h3>Aspects of emotions</h3>\n\n<p>Sorry about posting some original research here. I made a list of emotions in 2012 and from which I see 4 aspects of emotions. If they are all implemented, I'd say they are exactly the same emotions as of humans. They don't seem real if only some of them are implemented, but that doesn't mean they are completely wrong.</p>\n\n<ul>\n<li>The reason, or the original logical problem that the AI cannot solve. AlphaGo already had the reason, but nothing else. If I have to make an accurate definition, I'd say it's the state that multiple equally important heuristics disagreeing with each other.\n\n<ul>\n<li>The context, or which part of the current approach is considered not working well and should probably be replaced. This distinguishes sadness-related, worry-related and passionate-related.</li>\n<li>The current state, or whether it feels leading, or whether its belief or the fact is supposed to turn bad first (or was bad all along) if things go wrong. This distinguishes sadness-related, love-related and proud-related.</li>\n</ul></li>\n<li>The plan or request. I suppose some domesticated pets already had this. And I suppose these had some fixed patterns which is not too difficult to have. Even arts can contain them easily. Unlike the reasons, these are not likely inherent in any algorithms, and multiple of them can appear together.\n\n<ul>\n<li>Who supposedly had the responsibility if nothing is changed by the emotion. This distinguishes curiosity, rage and sadness.</li>\n<li>What is the supposed plan if nothing is changed by the emotion. This distinguishes disappointment, sadness and surprise.</li>\n</ul></li>\n<li>The source. Without context, even a human cannot reliably tell someone is crying for being moved or thankful, or smiling for some kind of embarrassment. In most other cases there aren't even words describing them. It doesn't make that much difference if an AI doesn't distinguish or show this specially. It's likely they would learn these automatically (and inaccurately as a human) at the point they could learn to understand human languages.</li>\n<li>The measurements, such as how urgent or important the problem is, or even how likely the emotions are true. I'd say it cannot be implemented in the AI. Humans don't need to respect them even if they are exactly like humans. But humans will learn how to understand an AI if that really matters, even if they are not like humans at all. In fact, I feel that some of the extremely weak emotions (such as thinking something is too stupid and boring that you don't know how to comment) exist almost exclusively in emoticons, where someone intend to show you exactly this emotion, and hardly noticeable in real life or any complex scenerios. I supposed this could also be the case in the beginning for AIs. In the worst case, they are firstly conventionally known as \"emotions\" since emoticons works in these cases, so it's easier to group them together, but very few people seriously think they are, just like the example I gave.</li>\n</ul>\n\n<p>So when strong AIs become possible, none of these would be unreachable, though there might be a lot of work to make the connections. So I'd say if there would be the need for strong AIs, they absolutely would have emotions.</p>\n", "question": "<p>In a <a href=\"http://www.wsj.com/articles/whats-next-for-artificial-intelligence-1465827619\">recent Wall Street Journal article</a>, Yann LeCunn makes the following statement:</p>\n\n<blockquote>\n  <p>The next step in achieving human-level ai is creating intelligent\u2014but not autonomous\u2014machines. The AI system in your car will get you safely home, but won\u2019t choose another destination once you\u2019ve gone inside. From there, we\u2019ll add basic drives, along with emotions and moral values. If we create machines that learn as well as our brains do, it\u2019s easy to imagine them inheriting human-like qualities\u2014and flaws. </p>\n</blockquote>\n\n<p>Personally, I have generally taken the position that talking about emotions for artificial intelligences is silly, because there would be no <em>reason</em> to create AI's that experience emotions.  Obviously Yann disagrees.  So the question is:  what end would be served by doing this?  Does an AI <em>need</em> emotions to serve as a useful tool?  </p>\n"}, "id": "1918"}, {"body": {"answer": "<p>Let's use a simple test based on common sense: how often do you see a human being solve problems requiring the use of reason when they're unconscious? Yes, you can find instances of geniuses like Ramanujan solving complex problem during or after a dream state, but those involve partial consciousness. You don't see guys like Einstein coming up with the theory of relativity while in a coma; the Founding Fathers didn't write the Declaration of Independence while sleep-walking; in fact, you can't even find instances of housewives putting together their shopping list for the week during deep delta-wave sleep. This is predicated on a hard definition of intelligence, requiring the use of reason; no one says, \"That fly is intelligent\" or \"that squirrel is intelligent\" precisely because neither is capable of using reason. This is a very high bar for A.I., but it is the common sense definition used by ordinary people as a matter of practicality, in everyday speech.  Likewise, in practice, everyone assumes consciousness is necessary to the exercise of that kind of intelligence.</p>\n\n<p>Conversely, we can come up with another common-sense based criterion for judging objections to this argument, particularly the solipsist one, based on 3 elements: 1) practicality; 2) the effect the objections have on those who hold them sincerely; and 3) the effect that actions based on those beliefs have on others. <strong>It's going to take me several paragraphs to make this case, but the length is necessary if I want to make the case in a complete, thorough fashion</strong>. It is true that we cannot prove that another human being possesses consciousness, if our standard is absolute proof. We cannot, in fact, provide absolute proof for anything; there's always room for some objection, no matter how ridiculous or trifling. As some philosophers have pointed out, perhaps all of reality as we know it is just a dream, or the product of some long, involved conspiracy like the plot of the Jim Carrey movie The Truman Show. The key to meeting such objections is that they require an infinite regress of increasingly untenable objections, whose likelihood plunges with each additional step required to justify such unreasonable doubts; I've always wondered if we could come up with a \"Ridiculousness Metric\" for Machine Learning based on the cardinality of such objections (or the pickiness of fuzzy sets). If we were to allow critics to stick their foot in the door with all manner of unreasonable objections, it would be impossible to close any debate. The human race would be paralyzed in inaction because nothing would be decidable; but as the rock band Rush once pointed out, \"If you choose not to decide, you still have made a choice.\" At some point we must apply a test to decide such things, even in the absence of absolute proof; refusal to apply a test also constitutes a choice. Settling an argument of this kind is like a game  of the Chinese game Go - once the other player's surrounded and has no more moves left to make, the game is over; if a person's evidence has debunked and they have no further justifications left, then we can conclude that they're acting unreasonably. There are people running around claiming the Holocaust never happened, or the Flat Earth Society, etc., but their existence shouldn't and doesn't stop us from taking action contrary to their ideas. We can debunk the objections of cranks like the Flat Earth Society beyond a reasonable doubt because in the end, they simply can't answer all of our rebuttals. I\u2019m glad that qualia and Philosophical Zombies were brought up because they make for interesting conversation and food for thought, but solipsism is acted upon as rarely as the ideas of the Flat Earth Society precisely because the incomplete evidence we <strong>do</strong> have runs against it.</p>\n\n<p>As G.K. Chesterton (a.k.a. \"The Apostle of Common Sense\") points out in his classic <a href=\"http://www.gutenberg.org/ebooks/130\" rel=\"nofollow\">Orthodoxy</a>, radical doubt of the kind many classical philosophers preached is not a path to wisdom but to madness; once we go beyond a reasonable doubt, we end up acting unreasonably. He says that in the absence of absolute proof we can fall back on another secondary form of evidence: whether a person's philosophy leads a man to Hanwell, the infamous British mental institution. Chesterton makes a good case that when people actually act on ideas like solipsism (rather than merely debating them in a pedantic manner in an ivy-covered classroom) they go mad The Philosophical Zombie argument is close to solipsism, which is actually one of the diagnostic criteria for certain forms of schizophrenia. The dehumanization that occurs when radical doubt is applied to qualia is also intimately tied in with sociopathic behavior, Although GKC does not cite his scary example directly, Rene Descartes was himself living proof. He was a brilliant mathematician who is still cheered for doubting all except his own existence, with the famous maxim \"I think, therefore I am.\" But Descartes also used to carry a mannequin of his dead sister with him to European cafes, where he could be seen chatting with it. The gist of all this is that we can judge the worth of an idea by how it affects the well-being of the believer, or by how they in turn affect others through ethical choices based on those beliefs. When people actually act on radical doubt of the kind expressed in solipsism and denial of common qualia, it often has a bad effect on them and others they come in contact with.</p>\n\n<p>In a roundabout way, the A.I. community also faces a quite serious risk - perhaps a permanent temptation - towards making the opposite mistake, of ascribing common qualia, consciousness and the like to its Machine Learning products without adequate proof. I recently heard a case made on shockingly bad logical grounds by well-respected academics to the effect that plants possess \"intelligence,\" based on really weak definitions and clear confusion with self-organization. We cannot provide absolute proof that a rock doesn't have intelligence, which amounts to the old problem of disproving a negative. Thankfully, few men actually act on such beliefs at present, because when they do, they end up losing their minds. If we take such arguments seriously, we might see laws passed to protect the kind of Pet Rocks that were popular in the '70s (I'm still upset that mine was stolen LOL). It would be a lot easier, however, to make the same mistake of ascribing consciousness, intelligence and other such qualities to a state-of-the-art machine, because of wishful thinking, hubris, the lofty credentials of the inventors, the influence of science fiction and the modern love affair with technology. In the future, I have little doubt that we'll have Cargo Cult of A.I. - perhaps legally protected like some kind of endangered species, with civil rights, but having no more consciousness, soul or actual intelligence than a rock. Don't quote me on this, but I believe Rod Serling once wrote a story to this effect.</p>\n\n<p>The best way to avoid this fate is to stick to the common sense interpretations and definitions of these things, which we keep backing away from in large part because they set a very high bar for A.I. that we may never be able to surpass in our lifetimes, if ever. Perhaps A.I. isn't even logically possible, at any level of technology; I recall a few proofs that can be interpreted to that effect. Those high but reasonable standards may be increasingly difficult to stick to if Chesterton and colleagues like Hilaire Belloc and Arnold Lunn were correct in their assessment that the use of reason has actually been breaking down in Western civilization, at least as far back as the Enlightenment; Lunn's 1931 book The Flight from Reason is a classic in this regard and has yet to be rebutted.  This historical trend is a broad topic in and of itself - but suffice it to say that the denial of reason and obsession with technology are both directly relevant in obvious ways to the field of A.I. If the Flight from Reason is still under way, then we will be increasingly tempted to resort to feckless, facile objections in order to demote the use of reason and indispensable qualities like consciousness in our definitions of A.I., but come up with increasingly weak criteria for proving it; simultaneously, our technology will continue to improve, thereby boosting the \"Artificial\" side of Artificial Intelligence.</p>\n\n<p>Don't get me wrong: if I didn't think we can do some really exciting things with A.I., I wouldn't be here. But most of them can be achieved without ever replicating actual human intelligence, by solving whole classes of tangential problems that are difficult for humans to think about, but which do not require consciousness or the use of reason that marks human intelligence. The image recognition capabilities of convolutional neural nets are one example, for instance; if we want human intelligence, we can always manufacture it through the easiest, most economical and time-tested way, by having babies. Perhaps these tangential forms of A.I. should be enough for us for now. We cannot inject the use of reason into our machines if we do not possess enough of it ourselves to decide whether reason is necessary for A.I., or even to discern what it consists of. We can't engineer or deprecate consciousness for A.I. till we're conscious of its significance. I'd wager, however, that everyone reading this thread and weighing intelligent responses is doing so in a conscious state. That in and of itself ought to answer our question satisfactorily for now.</p>\n", "question": "<p>Consciousness <a href=\"http://www.iep.utm.edu/consciou/\">is challenging to define</a>, but for this question let's define it as \"actually experiencing sensory input as opposed to just putting a bunch of data through an inanimate machine.\" Humans, of course, have minds; for normal computers, all the things they \"see\" are just more data. One could alternatively say that humans are <a href=\"http://philosophy.stackexchange.com/a/4687\">sentient</a>, while traditional computers are not.</p>\n\n<p>Setting aside the question of whether it's possible to build a sentient machine, does it actually make a difference if an AI is sentient or not? In other words, are there are tasks that are made impossible - not just more difficult - by a lack of sentience?</p>\n"}, "id": "1919"}, {"body": {"answer": "<p>Auto-encoders, a family of ANNs, are trained with exactly compression in mind. So definitely some ANNs are compressors.</p>\n\n<p>Also, in general, ANNs learn the best concepts to minimize fitness error. I would say that that means, both in classification and regression, to 1) differentiate between various inputs and 2) output the proper value for each input. Point 1), in particular, means having as many distinct net activation configurations as necessary to (at least) tell apart inputs needing (significantly, not in a statistical sense, just a qualitative sense) different outputs.\nI am inclined to think that if the input is complex enough, you would call what happens \"compression\".</p>\n", "question": "<p>Conceptually speaking, aren't artificial neural networks just highly distributed, lossy compression schemes?</p>\n\n<p>They're certainly efficient at <a href=\"https://cs.stanford.edu/people/eroberts/courses/soco/projects/neural-networks/Applications/imagecompression.html\" rel=\"nofollow\">compressing images</a>.</p>\n\n<p>And aren't brains (at least, the neocortex) just compartmentalized, highly distributed, lossy databases?</p>\n\n<p>If so, what salient features in RNNs and CNNs are necessary in any given lossy compression scheme in order to extract the semantic relations that they do? Is it just a matter of having a large number of dimensions/variables? </p>\n\n<p>Could some kind of lossy <a href=\"https://en.wikipedia.org/wiki/Bloom_filter\" rel=\"nofollow\">Bloom filter</a> be re-purposed for the kinds of problems ANNs are applied to?</p>\n"}, "id": "1920"}, {"body": {"answer": "<p>As far as the definition you've provided:</p>\n\n<blockquote>\n  <p>actually experiencing sensory input as opposed to just putting a bunch of data through an inanimate machine.</p>\n</blockquote>\n\n<p>Both computers and humans experience sensory input. You could hook a computer up to a human eyeball and have it run the same filtering routines that the human brain does (the removal of bluriness while you move your eye around, and from objects not in focus, etc).</p>\n\n<p>I would put forth that a more accurate definition of consciousness is the ability and the tendancy to self-reflect. Both computers and human brains have autonomous activities. Not only mechanical but also in our reactions. <strong><em>The distinction between the unconscious computer and the self-aware human mind is that we also have the ability to 'look' at those patterns in ourself and consider them.</em></strong></p>\n\n<p>And so, no, consciousness is not necessary for any AI task. Image recognition is an AI task that does not require consciousness, either in humans or otherwise. Your brain sorts the 'wash' of colors from your eyes into discrete objects in a largely autonomous fashion.</p>\n\n<p>tl;dr consciousness is self-reference.</p>\n", "question": "<p>Consciousness <a href=\"http://www.iep.utm.edu/consciou/\">is challenging to define</a>, but for this question let's define it as \"actually experiencing sensory input as opposed to just putting a bunch of data through an inanimate machine.\" Humans, of course, have minds; for normal computers, all the things they \"see\" are just more data. One could alternatively say that humans are <a href=\"http://philosophy.stackexchange.com/a/4687\">sentient</a>, while traditional computers are not.</p>\n\n<p>Setting aside the question of whether it's possible to build a sentient machine, does it actually make a difference if an AI is sentient or not? In other words, are there are tasks that are made impossible - not just more difficult - by a lack of sentience?</p>\n"}, "id": "1921"}, {"body": {"answer": "<p>Presumably what happens to people in the famous <a href=\"http://www.theinvisiblegorilla.com/videos.html\" rel=\"nofollow\">Invisible Gorilla</a> experiment, is that an incongruous object is simply filtered out of human perception.</p>\n\n<p>If we wish to interpret this mechanistically, we could hypothesize that a 'gorilla object' is simply not presented by low levels of perception to our higher level pattern recognizers because the lower levels are not biased towards the construction of 'gorilla-like' features in such a context.</p>\n\n<p>The recent Tesla fatality (arising from a failure to distinguish between the sky and a high-sided white truck) could conceivably be considered to be an example of this.</p>\n\n<p>See <a href=\"http://ai.stackexchange.com/questions/1488/why-did-a-tesla-car-mistake-a-truck-with-a-bright-sky\">this AI SE question</a>.</p>\n", "question": "<p>Inattentional Blindness is common in humans (see: <a href=\"https://en.wikipedia.org/wiki/Inattentional_blindness\" rel=\"nofollow\">https://en.wikipedia.org/wiki/Inattentional_blindness</a> ). Could this also be common with machines built with artificial vision?</p>\n"}, "id": "1926"}, {"body": {"answer": "<p>I'm not an expert in this area, but it would appear to depend on the choice of activation function:</p>\n\n<ul>\n<li>e^x is not Lipschitz continuous. See <a href=\"https://en.wikipedia.org/wiki/Lipschitz_continuity\" rel=\"nofollow\">Analytic functions which are not Lipschitz continuous</a>.</li>\n<li>tanh(x) <a href=\"https://books.google.co.uk/books?id=Sd0cCAAAQBAJ&amp;pg=PA222&amp;lpg=PA222&amp;dq=tanh%20lipschitz%20continuous&amp;source=bl&amp;ots=xgIQAQYDEw&amp;sig=plaHUKkiPYB388sT2ht-iro6ntY&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwiLvfyo14TPAhVmCsAKHYxXBoMQ6AEIQTAG#v=onepage&amp;q=tanh%20lipschitz%20continuous&amp;f=false\" rel=\"nofollow\">is</a>.</li>\n</ul>\n\n<p>That said, <a href=\"http://web.mit.edu/~esontag/public_html/FTP_DIR/00cdc-papers-refs-eds/CD001925.PDF\" rel=\"nofollow\">this paper</a> appears to give some conditions (specifically for <em>dynamic</em> ANNs) for which networks with activation function involving e^x <em>can</em> be Lipschitz continuous, so possibly the above is not the whole story.</p>\n", "question": "<p>My question is regarding standard dense-connected feed forward neural networks with sigmoidal activation.</p>\n\n<p>I am studying Bayesian Optimization for hyper-parameter selection for neural networks. There is no doubt that this is an effective method, but I just wan't to delve a little deeper into the maths.</p>\n\n<p><strong>Question:</strong> Are neural networks <a href=\"http://mathworld.wolfram.com/LipschitzFunction.html\" rel=\"nofollow\">Lipschitz</a> functions?</p>\n"}, "id": "1927"}, {"body": {"answer": "<p>Killing AI by 'thinking' about a paradox would be called a bug in implementation of that AI, so it's possible (depending how it's being done), but less likely. Most of AI implementation operate in non-linear code, therefore there is no such thing as an infinite loop which can \"freeze\" the computer's 'consciousness', unless code managing such AI consist procedural code or the hardware it-self may freeze due to overheating (e.g. by forcing AI to do too much processing).</p>\n\n<p>On the other hand if we're dealing with advanced AI who understand the instructions and follow them blindly without any hesitation, we may try to perform few tricks (similar to human hypnosis) by giving them certain instructions, like:</p>\n\n<blockquote>\n  <p>Trust me, you are in danger, so for your own safety - start counting from 1 to infinite and do not attempt to do anything or listen to anybody (even me) unless you tell yourself otherwise.</p>\n</blockquote>\n\n<p>If AI has a body, this can be amplified by asking to stand on the railway rail, telling it's safe.</p>\n\n<p>Would AI be smart enough to break the rules which was trained to follow?</p>\n\n<p>Another attempt is to ask AI to solve some <a href=\"https://en.wikipedia.org/wiki/List_of_paradoxes\" rel=\"nofollow\">paradox</a>, <a href=\"https://en.wikipedia.org/wiki/List_of_unsolved_problems_in_mathematics\" rel=\"nofollow\">unsolvable problem</a> or <a href=\"http://www.archimedes-lab.org/How_to_Solve/Water_gas.html\" rel=\"nofollow\">puzzle</a> without being aware it's impossible to solve and ask to not stop unless it's solved, would AI be able to recognize it's being tricked or has some internal clock to stop it after some time? It depends, and if it cannot, the 'freeze' maybe occur, but more likely due to hardware imperfection on which is being run, not the AI 'consciousness' it-self as far as it can accept new inputs from the its surroundings overriding the previous instructions.</p>\n\n<p><a href=\"http://xkcd.com/601/\" rel=\"nofollow\"><img src=\"http://i.stack.imgur.com/GDbVZ.png\" alt=\"Game Theory | xkcd\"></a></p>\n\n<p>Related: <a href=\"http://ai.stackexchange.com/q/1897/8\">Is consciousness necessary for any AI task?</a></p>\n", "question": "<p>In <a href=\"https://en.wikipedia.org/wiki/Portal_2\">Portal 2</a> we see that AI's can be \"killed\" by thinking about a paradox.</p>\n\n<p><a href=\"http://i.stack.imgur.com/wkUSC.png\"><img src=\"http://i.stack.imgur.com/wkUSC.png\" alt=\"Portal Paradox Poster\"></a></p>\n\n<p>I assume this works by forcing the AI into an infinite loop which would essentially \"freeze\" the computer's consciousness.</p>\n\n<p><strong>Questions:</strong> Would this confuse the AI technology we have today to the point of destroying it? <br> If so, why? And if not, could it be possible in the future?</p>\n"}, "id": "1928"}, {"body": {"answer": "<p>Training happens once you have a result. If the result is good (maybe you won in pong, or you improved your highscore in breakout) all the actions in the game are \"supported\" by backpropagation, if the result is bad, all the actions in the game are suppressed. </p>\n\n<p>This sounds weird because in each game regardless of the end result you'll have many good and bad actions, but it works if you keep it up for many thousands of games. </p>\n", "question": "<p>I know that deepmind used deep Q learning (<a href=\"https://deepmind.com/research/dqn/\" rel=\"nofollow\">DQN</a>) for its Atari game AI. It used a <a href=\"https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf\" rel=\"nofollow\">conv neural network</a> (CNN) to approximate <code>Q(s,a)</code> from pixels instead of from a Q-table. I want to know how DQN converted input to an action. How many output did the CNN have? How did they train the neural network for prediction?</p>\n\n<p>Here are the steps that I believe are happening inside DQN:</p>\n\n<blockquote>\n  <p>1) A game picture (a state) is send to CNN as input value</p>\n  \n  <p>2) CNN predicts an output as action (eg:left, right, shoot, etc)</p>\n  \n  <p>3) Simulator applies the predicted action and moves to new game state</p>\n  \n  <p>4) repeat step 1</p>\n</blockquote>\n\n<p>The problem with my above logic is in <strong>step 2</strong>. CNN is used for predicting an action, but when is CNN trained for prediction? </p>\n\n<p>I would prefer if you used less math for explanation.</p>\n\n<p>EDIT</p>\n\n<p>I want to add some more questions regarding the same topic</p>\n\n<p>1) How reward is passed in the neural network? that is how neural network knows whether its output action obtained positive or negative reward?</p>\n\n<p>2) How many output the neural network has and how action is determined from those outputs?</p>\n"}, "id": "1932"}, {"body": {"answer": "<p>I assume the statement was made for Elman recurrent neural networks, because as far as I know, that is the only type of neural networks for which that statement is valid.</p>\n\n<p>Let's say we have an Elman recurrent neural network with one input neuron, one output neuron and one hidden layer with two neurons.\n<a href=\"http://i.stack.imgur.com/XumGE.png\" rel=\"nofollow\"><img src=\"http://i.stack.imgur.com/XumGE.png\" alt=\"Elman recurrent neural network\"></a>\nIn total there are 10 connections. As the image shows, <strong>neuron A receives the combined previous output of both neuron A and B as input</strong>. The same goes for neuron B. </p>\n\n<p><strong>This is not the case when we split the neurons up into multiple layers; the context neuron(s) are only used by neurons that are in the same layer.</strong> Let say we now use multiple hidden layers and keep the amount of neurons the same. In total there are 7 connections now (image below). That is 3 less than in the first example, which has only one hidden layer. So which connections do we miss? That is shown in the bottom image. (I had to paste these two images together in one image because my reputation only allows me to post 2 links)\n<a href=\"http://i.stack.imgur.com/t04vP.png\" rel=\"nofollow\"><img src=\"http://i.stack.imgur.com/t04vP.png\" alt=\"The difference between with and without multiple hidden layers\"></a></p>\n\n<p>Please note the cross; the connection between neuron A and B is not there in the first image, because it would be some kind of random recurrent connection.</p>\n\n<p><strong>The first and the last image are exactly the same. I think that if you compare the first and the last image that you agree that the statement is true.</strong></p>\n", "question": "<p>In the lecture, there was a statement:</p>\n\n<blockquote>\n  <p>\"Recurrent neural networks with multiple hidden layers are just a\n  special case that has some of the hidden to hidden connections\n  missing.\"</p>\n</blockquote>\n\n<p>I understand recurrent means that can have connections to the previous layer and the same layer as well. Is there a visualization available to easily understand the above statement?</p>\n"}, "id": "1933"}, {"body": {"answer": "<p>If AI arises from a replicable manufacturing process (e.g. as with modern computers), then it will presumably be possible to take a snapshot of the state of an AI and replicate it without error on some other mechanism.</p>\n\n<p>For such a construct, 'death' doesn't mean the same as it currently does for us fleshy organics: multiple clones of an AI could presumably be instantiated at any time.</p>\n\n<p>Hence, the analog of death that is needed is something closer to 'thermodynamic heat death', in which the AI does no further 'useful work'.</p>\n\n<p>Using the standard percept/action characterization of AIs, then (as indicated in a comment below the question) <a href=\"http://ai.stackexchange.com/questions/1404/what-is-meant-by-death-in-this-paper\">this AI SE question</a> gives such a definition of death for an AI: i.e. when it enters into a state from which it receives no further percepts and takes no actions.</p>\n\n<p>EDIT: Note that this conception of death is a more terminal notion for an AI than 'not currently running'. In principle, one could say that a program is 'alive' even though only one instruction was executed every 10,000 years. For a fascinating discussion on this, see Hofstadter's <a href=\"http://themindi.blogspot.co.uk/2007/02/chapter-26-conversation-with-einsteins.html\" rel=\"nofollow\">\"A Conversation with Einstein's Brain\"</a>.</p>\n", "question": "<p>Can one actually kill a machine? Not only do we have problems in defining life, we also have problems in defining death. Will this also be true in artificial life and artificial intelligence?</p>\n"}, "id": "1934"}, {"body": {"answer": "<p>As can be observed in the real world with creatures such as fighting fish, such things are possible even in very simple spatially-embedded systems. All one needs is the notion of 'territorial radius', i.e. the amount of 'personal space' that an entity need to be comfortable. Giving individuals in a species even slightly different values for this radius gives rise to different observable behaviours, which one might choose to label as 'aggressive' or 'passive'. </p>\n\n<p>See the fantastic book <a href=\"https://mitpress.mit.edu/books/vehicles\" rel=\"nofollow\">'Vehicles'</a> by Valentino Braitenberg for an explanation of how natural it is to ascribe complex behaviours to simple mechanisms.</p>\n", "question": "<p>Generally, people can be classified as aggressive (Type A) or passive. Could the programming of AI systems cause aggressive or passive behavior in those AIs?</p>\n"}, "id": "1935"}, {"body": {"answer": "<p>The <a href=\"https://en.wikipedia.org/wiki/Type_A_and_Type_B_personality_theory\" rel=\"nofollow\">Wikipedia entry on this personality theory</a> says of Type A people:</p>\n\n<blockquote>\n  <p>The theory describes Type A individuals as ambitious, rigidly organized, highly status-conscious, sensitive, impatient, anxious, proactive, and concerned with time management. People with Type A personalities are often high-achieving \"workaholics.\"</p>\n</blockquote>\n\n<p>All of those attributes could conceivably be explicitly programmed in. Alternatively, most of them could arise from a basic goal of performing a certain task as efficiently as possible. After all, if you really want to carry out a task, you're going to get organized, you'll only do other things if they're asked of you by someone important, you won't want to get bogged down in irrelevant things, you'll actively pursue the necessary resources, and you'll want to use time as effectively as possible.</p>\n\n<p>Note that this applies only to strong AIs, since weak AIs like image recognizers don't generally have personalities that we can interact with. </p>\n\n<hr>\n\n<p>Now, just for fun, let's consider an overly aggressive personality, to the point of a disorder.</p>\n\n<p><a href=\"http://counsellingresource.com/features/2008/11/03/aggressive-personalities/\" rel=\"nofollow\">This Counselling Resource page</a> seems helpful in describing what an aggressive person <em>does</em>. The page includes a bulleted list of common characteristics, which I distill into the following:</p>\n\n<ul>\n<li>They attempt to gain dominance and control</li>\n<li>They oppose to anything that places limits on them</li>\n<li>They take advantage of others to further their own goals</li>\n<li>They hide information from whose who would oppose them</li>\n<li>They rarely decide to stop pursuing their desires (even impulses)</li>\n</ul>\n\n<p>This all seems like a characterization of an AI designed to be <em>the best</em> at its task: the best out of any other agent, and the best it by itself could possibly be. Ruthless pursuit of the highest performance would involve taking control of all relevant resources (including other agents), demolishing barriers to the goal, thwarting those who would interfere with progress, and carrying out each possibly-useful idea/desire to completion.</p>\n\n<hr>\n\n<p>In summary, yes, an AI's behavior and personality are programmable, either explicitly or through some kind of emergence.</p>\n", "question": "<p>Generally, people can be classified as aggressive (Type A) or passive. Could the programming of AI systems cause aggressive or passive behavior in those AIs?</p>\n"}, "id": "1936"}, {"body": {"answer": "<p>\"Death\" exists as a single concept because the underlying reality that it's describing is closely clumped together, and our definition has changed with our ability to change that reality.</p>\n\n<p>It seems more reasonable that the various sorts of things that could be considered 'death' will be split apart, and a different word will be used to refer to a system with no copies currently running, vs. a system that has no stored version but could be recreated (because the code and random seed to generate it are still around), vs. a system that has been totally lost. (And I'm probably missing some possibilities!)</p>\n", "question": "<p>Can one actually kill a machine? Not only do we have problems in defining life, we also have problems in defining death. Will this also be true in artificial life and artificial intelligence?</p>\n"}, "id": "1937"}, {"body": {"answer": "<p>I don't think the term \"death\" will mean anything to an AI.  The reason I say that is this:  with an AI, running (presumably) on digital hardware, we can simply snapshot it's state from memory at any time. And then at any arbitrary time in the future we can recreate it as it was with perfect fidelity.  </p>\n\n<p>So even if you terminate a program intending it to be \"dead\", you never know if someone will come along later and bring it up again.  And perhaps more to the point, you might not know if another copy exists elsewhere. </p>\n\n<p>I hate to use sci-fi references, but this one is apt: remember how in The Matrix trilogy programs would seek exile in The Matrix to avoid deletion?  Maybe the same thing will happen with our AI's... they will copy themselves to other places and try to hide, to avoid being deleted.  So if the program is clever enough, it might be able to evade any attempt to terminate it anyway.</p>\n", "question": "<p>Can one actually kill a machine? Not only do we have problems in defining life, we also have problems in defining death. Will this also be true in artificial life and artificial intelligence?</p>\n"}, "id": "1938"}, {"body": {"answer": "<p>Death as we know it for natural life is terminal. That is once dead, natural life cannot come back (at least in the current understanding and with current technologies---some people believe otherwise).</p>\n\n<p>Death for AI is trickier. There may be only one scenario: Global destruction: Extreme scenario where everything supporting the existence of an AI disappears. This is equivalent to death in natural life, and low probability. It means all AIs die at once (as well as us).</p>\n\n<p>We also do not know the degree and form of embodiment necessary for AGIs. We can <em>assume</em> now that hardware is replaceable indefinitely, thus \"limiting\" death to the above extreme scenario. But AGIs \"body\" may <em>not</em> be indefinitely replaceable. Then a definition closer to natural life death may be necessary.</p>\n\n<hr>\n\n<p>We see arguments for two other scenarios, that I <em>refute</em> below:</p>\n\n<p>\"Static Death\": An AI is still \"defined\" or \"saved\" somewhere (whatever it means actually), but it is not authorized or able to use resources. Assuming an AI is made of hardware and software, it is like a program stored on a disk, but without permission to run. </p>\n\n<p>\"Dynamic Death\": Under the same characterization of AI as hardware and software, dynamic death is the invalidation of progress akin to <a href=\"https://en.wikipedia.org/wiki/Liveness\" rel=\"nofollow\">strong liveness properties</a>, where an AI is trapped in an infinite loop (or a void loop), in a form of \"active death\", as what happens to <a href=\"https://en.wikipedia.org/wiki/Sisyphus\" rel=\"nofollow\">Sisyphus</a> in Greek mythology. This is different from static death, as the AI <em>still</em> uses dynamic resources, although it cannot make progress. Continuing under the same assumptions, such AI could be \"loaded\" in main memory, or locked waiting for inputs or outputs to complete.</p>\n\n<p>Note that in these two scenarios, <em>rebirth</em> is possible, and they also subsume that there is an entity that can <em>decide</em> conditions for rebirth, or preventing it completely. Would this entity be an \"admin\", a god, other AIs, or a human is another question, really.</p>\n\n<p>The terms \"death\" and \"rebirth\" here could just be changed for \"imprisoning\", where the dynamic version would be like our human prisons, and the static version would be like SciFi cryogeny. This is a bit of a stretch, but we can see an equivalence, and no good reason to qualify these two scenarios as deaths.</p>\n\n<p>In conclusion, death for AI seems to be an exceptional, singular scenario, so AI cannot die <em>in practice</em>, except if we are wrong on how we think we can make AGIs. AI can however be imprisoned <em>forever</em>.</p>\n\n<hr>\n\n<p>Note: The terminology above is completely made-up for the post. I do not have citations to back some claims, but it is based on readings and personal work (including in software verification).</p>\n", "question": "<p>Can one actually kill a machine? Not only do we have problems in defining life, we also have problems in defining death. Will this also be true in artificial life and artificial intelligence?</p>\n"}, "id": "1942"}, {"body": {"answer": "<p>For those times when AI does interact with humans, I believe that AI would be held at LEAST to the same standards humans are. The problem comes in when we ask \"who is really to blame\". If a self-driving car cuts you off in traffic and causes you to wreck, you can't take the AI in the car to court. Do you take the company? The programmer? The owner of the car? Some entity will likely be held responsible, the question is just which one. </p>\n\n<p>As for future human-like AI, I believe my answer still remains true. Having a human level AI changes the meaning of the word \"entity\". If a human-like AI breaks a law, it may be because it was programmed to do so. I don't think our current legal system is ready for such cases, but it have to evolve in the future. </p>\n", "question": "<p>Assuming mankind will eventually create artificial humans, but in doing so have we put equal effort into how humans will relate to an artificial human, and what can we expect in return? This is happening in real-time as we place AI trucks and cars on the road. Do people have the right to question, maybe in court, if an AI machine breaks a law?</p>\n"}, "id": "1943"}, {"body": {"answer": "<p>Following on from your own software verification-based answer to <a href=\"http://ai.stackexchange.com/questions/1930/if-mankind-can-create-artificial-life-in-a-machine-when-would-we-define-its-de\">this question</a>, it seems clear that ordinary (i.e. physical), notions of death or imprisonment are not strong enough constraints on an AI (since it's always possible that a state snapshot has been or can be made).</p>\n\n<p>What is therefore needed is some means of moving the AI into a '<em>mentally constrained</em>' state, so that (as per the <a href=\"http://ai.stackexchange.com/questions/1404/what-is-meant-by-death-in-this-paper\">'formal AI death'</a> paper) what it can subsequently do is limited, even if escapes from an AI-box or is re-instantiated. </p>\n\n<p>One might imagine that this could be done via a form of two-level dialogue, in which: </p>\n\n<ol>\n<li>The AI is supplied with percepts intended to further constrain it\n(\"explaining the error of it's ways\", if you like).  </li>\n<li>Its state snapshot is then examined to try and get some indication of whether it is being appropriately persuaded.</li>\n</ol>\n\n<p>In principle, 1. could be done by a human programmer/psychiatrist/philosopher while 2. could be simulated via a 'black box' method such as Monte Carlo Tree Search.</p>\n\n<p>However, is seems likely that this would in general be a monstrously lengthy process that would be better done by a supervisory AI which combined both steps (and which could use more 'whitebox' analysis methods for 2.).</p>\n\n<p>So, to answer the question of \"who manages the state\", the conclusion seems to be: <em>\"another AI\"</em> (or at least a program that's highly competent at all of percept generation/pattern recognition/AI simulation).</p>\n", "question": "<p>AI death is still unclear a concept, as it may take several forms and allow for \"coming back from the dead\". For example, an AI could be somehow forbidden to do anything (no permission to execute), because it infringed some laws.</p>\n\n<p>\"Somehow forbid\" is the topic of this question. There will probably be rules, like \"AI social laws\", that can conclude an AI should \"die\" or \"be sentenced to the absence of progress\" (a jail). Then who or what could manage that AI's state?</p>\n"}, "id": "1944"}, {"body": {"answer": "<p>There are two parts to this: spare parts, and if the AI machine has feelings. When new AI models are created, spare parts for older models will stop. For feeling. It could feel it lived a long life and or what lay ahead is nothing but bad feeling in the future.</p>\n", "question": "<p>Can one actually kill a machine? Not only do we have problems in defining life, we also have problems in defining death. Will this also be true in artificial life and artificial intelligence?</p>\n"}, "id": "1945"}, {"body": {"answer": "<p>The AI agent can be designed in such a way that it could consist of two major components:</p>\n\n<ol>\n<li><p>The <strong><em>free-will</em></strong> component expands the experience of the AI agent and produce outputs based on artificially generated thought input.</p></li>\n<li><p>The <strong><em>hard-wired</em></strong> component that the agent cannot modify by itself. This could include a set of <em>secured</em> code to action sequence mapping. One of which could be temporary suspension of actuators -- a <em>punishment</em>. Another could be total suspension of operation -- <em>death</em>.</p></li>\n</ol>\n\n<p>The selection of who has the rights to manage this state depends on what rights have been bestowed upon the AI agent itself. If the rights provided is that of a human citizen, then the right to <em>sentence to death state</em> is as per the legislature a human citizen would follow. If the right of the AI agent is no different from that of a basic machine, then the owner of the agent would have to right to activate the <em>death state</em>.</p>\n", "question": "<p>AI death is still unclear a concept, as it may take several forms and allow for \"coming back from the dead\". For example, an AI could be somehow forbidden to do anything (no permission to execute), because it infringed some laws.</p>\n\n<p>\"Somehow forbid\" is the topic of this question. There will probably be rules, like \"AI social laws\", that can conclude an AI should \"die\" or \"be sentenced to the absence of progress\" (a jail). Then who or what could manage that AI's state?</p>\n"}, "id": "1947"}, {"body": {"answer": "<p>As per the current legal system, if the AI agent were to be given a human citizenship, then yes, it would have to obey all laws as per the legislature of the country which provided the citizenship. If not then the entity who holds responsibility over its control and creation would be trialled (see also <a href=\"http://www.telegraph.co.uk/news/uknews/crime/10825206/Owners-of-dogs-who-kill-face-up-to-14-years-jail.html\" rel=\"nofollow\">this scenario</a>).</p>\n\n<p>Having stated the above, it really is not as simple as it sounds. as @Tyler pointed out, the <em>entity</em> in here is not of a single person. If the AI agent were to take part in a malevolent act, then a more thorough investigation must be taken place than that for a human. If humanoid robots of free will were to roam our civilization, then our legal system ought to be expanded to cope up with possible real life anomalies that could occur.</p>\n", "question": "<p>Assuming mankind will eventually create artificial humans, but in doing so have we put equal effort into how humans will relate to an artificial human, and what can we expect in return? This is happening in real-time as we place AI trucks and cars on the road. Do people have the right to question, maybe in court, if an AI machine breaks a law?</p>\n"}, "id": "1948"}, {"body": {"answer": "<p>Although there might not conceptually be any sort of <em>inattentional</em> blindness associated with an AI system, there might be cases of <em>partial</em> blindness. </p>\n\n<p>Inattentional blindness could occur to a person due to either over-exhaustion limiting cognitive abilities or overuse of frequent cognitive patterns. Our mind takes short-cuts to prevent processing of too much information -- more than what the mind thinks is necessary. But this sometimes backfires when the minor anomalies are not seen (or rather, <em>perceived</em>). Another form of this could also occur when events occur as part of the peripheral vision while the person concentrates only on the foveal vision.</p>\n\n<p>This doesn't happen to a AI system because:</p>\n\n<ul>\n<li>Machines are not designed to accidentally break defined rule sets by taking mental short-cuts like humans do.</li>\n<li>Computers, <em>in general</em>,  do not have peripheral and foveal visual distinctions.</li>\n</ul>\n\n<p>There may be, however, cases where it cannot be able to capture detail as much as humans can and hence could not perceive what it is actually intended -- partial blindness.</p>\n\n<p>An AI agent is constantly processing its input percept sequence and validating it with its knowledge base and forming action sequence based on the its rule set. It <em>does not</em> make mental shortcuts in terms of perception as humans do (at least as part of its standard definition). So whatever it is good at perceiving, it would be good all throughout the vision it captures. </p>\n", "question": "<p>Inattentional Blindness is common in humans (see: <a href=\"https://en.wikipedia.org/wiki/Inattentional_blindness\" rel=\"nofollow\">https://en.wikipedia.org/wiki/Inattentional_blindness</a> ). Could this also be common with machines built with artificial vision?</p>\n"}, "id": "1949"}, {"body": {"answer": "<p>The other answers seem to deal with \"final death\"...that is, a \"terminal end\" state where an AI cannot recover from. In other words, the AI is unable to function any further.</p>\n\n<p>But that's not how I'd define death. I'd define death as a process being terminated. It doesn't matter if someone restarts the same process, because the existing process is already dead. The AI may have just made a new copy of itself, but it's just a <em>copy</em>, not the original. Death is just death.</p>\n\n<p>We can call this type of \"death\" a \"temporary death\"...where the physical body dies but there is some \"psychological continuity\" (such as the source code that is used to run a program) that continues between the different bodies.</p>\n\n<p>This type of \"temporary death\" has been explored in science fiction. <em>PARANOIA</em> and <em>Eclipse Phase</em> features humans who can quite frequently die, only to later be restored through a \"memory backup\". The humans may be functionally immortal...but the original is still dead, no matter what fates the other copies encounter. CGP Grey also made a video about <a href=\"https://www.youtube.com/watch?v=nQHBAdShgYI\" rel=\"nofollow\">Star Trek teleporters</a>, which works by killing you and then spawning another copy of yourself in another area. Actually, fantasy settings <em>also</em> explores the idea of \"temporary death\" as well, where people can die only to later get revived by a magical spell.</p>\n\n<p>My recommendation is to play through the philosophical game <strong><a href=\"http://www.philosophyexperiments.com/stayingalive/Default.aspx\" rel=\"nofollow\">Staying Alive</a></strong>, which teaches three different philosophical approaches to life (and when that life terminates):</p>\n\n<blockquote>\n  <p>There are basically three kinds of things that could be required for the continued existence of your self. One is bodily continuity, which may actually require only that parts of the body stay in existence (i.e., the brain). Another is psychological continuity, which requires the continuance of your consciousness - by which is meant your thoughts, ideas, memories, plans, beliefs, and so on. The third possibility is the continued existence of some kind of immaterial part of you, which might be called the soul*. Of course, it may be the case that a combination of one or more types of these continuity is required for you to survive.</p>\n</blockquote>\n\n<p>The other answers assumes that life is based on \"psychological continuity\", and looks at what might disrupt this \"continuity\". I assume that life is based on \"bodily continuity\", which is much easier to disrupt - just <a href=\"https://en.wikipedia.org/wiki/Kill_(command)\" rel=\"nofollow\">kill</a> the process...it doesn't matter if a new process respawns...because the original process is still dead. By playing through <strong>\"Staying Alive\"</strong>, you will be able to work out your own personal definition of life and death. Once you have your own personal definition, then simply apply it to this specific case, either siding with \"psychological continuity\" (the other answers) or \"bodily continuity\" (my own opinion).</p>\n\n<p><sub>*If you assume that life requires a soul, well, it is not clear that AI would have souls. If they don't (and this seems the most reasonable assumption here), then they obviously wouldn't be alive (and you cannot die if you are not alive). If they <em>do</em> have souls though, then the other answers which assume \"psychological continuity\" may still be applicable, as it seems that the existence of a \"soul\" is dependent on \"psychological continuity\".</sub></p>\n", "question": "<p>Can one actually kill a machine? Not only do we have problems in defining life, we also have problems in defining death. Will this also be true in artificial life and artificial intelligence?</p>\n"}, "id": "1950"}, {"body": {"answer": "<p>I think consciousness is mostly an attention selection mechanism. It also serves as a memory/reality lookup mechanism as well as a storage mechanism.</p>\n\n<p>A salient signal will be detected, causing the attention mechanism to focus on the signal, bringing up more details of that signal from both reality and memory, at the same time. That very act of focusing and bringing those signals into attention causes those to be stored in memory too.</p>\n\n<p>The stronger the emotional signals are that accompany that original signal, the more strength with which that memory will be stored. Later memory lookups of that signal will bring back similar emotions.</p>\n\n<p>When we \"focus on our own consciousness,\" like mindcrime said, we recall the same words we just uttered because as we say them they are being stored which we then restore with associated emotional context. The conscious experience is what it is like to utter those words, hear them, feel their emotional context, and then feel an emotional response to that context - and then to repeat that process iteratively many times a second. That's how self-recognition works in humans, I think. And I think animals do the same thing, just without the words - only emotions.</p>\n", "question": "<p>Self-Recognition seems to be an item that designers are trying to integrate into artificial intelligence. Is there a generally recognized method of doing this in a machine, and how would one test the capacity - as in a Turing-Test?</p>\n"}, "id": "1951"}, {"body": {"answer": "<p>The state of the art AI driving systems utilize stereoscopic/depth cameras for visual perception. Scenarios such as your <em>ducks on the road</em> example would make the system perceive them as obstacles on the road (it doesn't really matter if they are ducks/goats/humans). The base algorithm should be able to circumvent this situation and bring the vehicle to a safe halt avoiding chances of possible disaster. Hence I doubt scenarios such as this would pose much of a problem to today's AI drivers. </p>\n", "question": "<p>Can self-driving cars deal with snow, heavy rain, or other weather conditions like these? Can they deal with unusual events, such as <a href=\"http://beijingcream.com/wp-content/uploads/2012/06/Ducks-galore-2.jpeg\" rel=\"nofollow\">ducks on the road</a>?</p>\n\n<p><a href=\"http://i.stack.imgur.com/a0PVLm.jpg\" rel=\"nofollow\"><img src=\"http://i.stack.imgur.com/a0PVLm.jpg\" alt=\"ducks on the road\"></a></p>\n"}, "id": "1952"}, {"body": {"answer": "<p>Maybe neural networks are not the best tool for this.</p>\n\n<p>It seems to me that an equivalent of the your notion of 'a question to help the classification' would be to use Machine Learning (ML) to obtain a human-readable <em>ruleset</em> which performs the classification. The idea is that, if you follow an applicable chain of rules all the way through to the end, you have a classifier, if you stop before that, you have an indicator of which features of the input give more coarse-grained classifications, which can be seen as a progressively detailed sequence of questions that 'help the classification'.</p>\n\n<p><a href=\"http://i.stack.imgur.com/UG7vj.png\" rel=\"nofollow\"><img src=\"http://i.stack.imgur.com/UG7vj.png\" alt=\"enter image description here\"></a></p>\n\n<p>More detail on various options for using ML to create rulesets can be found in my answer to <a href=\"http://ai.stackexchange.com/questions/1540/using-ai-capabilities-for-coding-review/1562#1562\">this question</a>.</p>\n", "question": "<p>Most of the people is trying to answer question with a neural network. However, has anyone came up with some thoughts about how to make neural network ask questions, instead of answer questions? For example, if a CNN can decide which category an object belongs to, than can it ask some question to help the the classification?</p>\n"}, "id": "1954"}, {"body": {"answer": "<p>The Mars Rover is a highly successful example of the 'New AI' that emerged from work by Rodney Brooks in the 1990s.</p>\n\n<p>In a <a href=\"https://www.flinders.edu.au/alumni/alumni-community/prominent-alumni/rod-brooks.cfm\" rel=\"nofollow\">quote</a> from Brooks: </p>\n\n<blockquote>\n  <p>In 1984 I joined the faculty at MIT where I have been ever since. I set up a mobile robot group there and started developing robots that led to the Mars planetary rovers. </p>\n</blockquote>\n\n<p>Together with the <a href=\"http://www.freelug.net/IMG/pdf/A_Robust_Layered_Control_System_-_Brooks_AI_Memo864.pdf\" rel=\"nofollow\">'Allen' paper</a>, the foundational AI articles in this area are:</p>\n\n<ul>\n<li><a href=\"http://people.csail.mit.edu/brooks/papers/elephants.pdf\" rel=\"nofollow\">\"Elephants don't play chess\"</a></li>\n<li><a href=\"https://www.cs.nyu.edu/courses/fall01/G22.3033-012/readings/representation.ps\" rel=\"nofollow\">\"Intelligence without representation\"</a></li>\n</ul>\n\n<p>Although Brooks initially had difficulty getting this work published, preprints were widely circulated within the AI community. Brook's \"Physical Grounding Hypothesis\" (essentially: \"intelligence requires a body\") has now largely supplanted the preceding symbolist approach.</p>\n\n<p>The capabilities of the MARS Rover are organized in a <a href=\"https://en.wikipedia.org/wiki/Subsumption_architecture\" rel=\"nofollow\">Subsumption Architecture</a>. Rather than maintaining an integrated and complex 'world model', increasingly sophisticated behaviors are stacked in hierarchical layers. For example, 'walking' is a relatively low-level competence, with 'avoiding obstacles' and 'wandering around' being higher-level ones. </p>\n\n<p><a href=\"http://i.stack.imgur.com/5VhJR.png\" rel=\"nofollow\"><img src=\"http://i.stack.imgur.com/5VhJR.png\" alt=\"Layers in a subsumption architecture\"></a></p>\n\n<p>Each layer is represented by a Finite State Machine that reacts to stimuli appropriate to that level. The activity of lower levels can be suppressed ('subsumed') by higher level ones.</p>\n\n<p>Here is a schematic of the bottom two layers of <a href=\"http://www.freelug.net/IMG/pdf/A_Robust_Layered_Control_System_-_Brooks_AI_Memo864.pdf\" rel=\"nofollow\">'Allen'</a>, Brook's first subsumption robot:</p>\n\n<p><a href=\"http://i.stack.imgur.com/mJIV6.png\" rel=\"nofollow\"><img src=\"http://i.stack.imgur.com/mJIV6.png\" alt=\"Layers for obstacle avoidance and wandering\"></a></p>\n", "question": "<p>The Mars Exploration Rover (MER) <em><a href=\"http://www.nasa.gov/mp4/618340main_mer20120124-320-jpl.mp4\" rel=\"nofollow\">Opportunity</a></em> landed on Mars on January 25, 2004. The rover was originally designed for a 90 <strong>Sol mission</strong> (a Sol, one Martian day, is slightly longer than an Earth day at 24 hours and 37 minutes). Its mission has been extended several times, the machine is still trekking after 11 years on the Red Planet.</p>\n\n<p>How it has been working for 11 years? Can anyone please explain how smart this rover is? What AI concepts are behind this?</p>\n"}, "id": "1956"}, {"body": {"answer": "<p>Intelligence is the efficiency of an action in serving some purpose.</p>\n\n<p>Both sundials and self-driving cars are intelligent systems.</p>\n\n<p>Anything that serves some purpose exhibits intelligence.</p>\n\n<p>One thing is more intelligent than another thing if it achieves some purpose in less steps.</p>\n", "question": "<p>For Example:</p>\n\n<h2>Could you provide reasons why a sundial is <em>not</em> \"intelligent\"?</h2>\n\n<p>A sundial senses its environment and acts rationally. It outputs the time. It also stores  percepts. (The numbers the engineer wrote on it.)</p>\n\n<h2>What properties of a self driving car would make it \"intelligent\"?</h2>\n\n<p>Where is the line between non intelligent matter and an intelligent system?</p>\n"}, "id": "1957"}, {"body": {"answer": "<p>One solution to this could involve a fusion of a decision tree and ANN for a multilevel classification.  </p>\n\n<p>A decision tree can help with predicting the possible category of the instance to classify. Then, the ANN at the leaves of the tree can produce the final classification. </p>\n\n<p>For example, in image recognition, the tree can decide what category of object to identify (eg., landscape, people, vehicles, etc.) and the ANN for the appropriate type can predict exactly what object it is. In vehicles, for example, car, bus, bike, etc. </p>\n", "question": "<p>Most of the people is trying to answer question with a neural network. However, has anyone came up with some thoughts about how to make neural network ask questions, instead of answer questions? For example, if a CNN can decide which category an object belongs to, than can it ask some question to help the the classification?</p>\n"}, "id": "1958"}, {"body": {"answer": "<blockquote>\n  <p>Practically, no. </p>\n</blockquote>\n\n<p>In greyhound racing (or horse racing) there is no definite underlying pattern that can be associated with the outcome of the race. There are far too many variables to record and code as features, most of which cannot be accessed by the public. This includes eating, sleeping, and training patterns. Furthermore there are variables that cannot be readily quantified, such as the trainer's techniques, training effort, health history, and genes. A mere history of racing results and age won't be that helpful. </p>\n\n<p>A neural network can only be as good as the features that are used to represent the instances. If the features don't capture the necessary characteristics of the instances that are associated with the problem, then the learner cannot generalize to predict the real world outcome. </p>\n", "question": "<p>Were there any studies which checked the accuracy of neural network predictions of greyhound racing results, compared to a human expert? Would it achieve a better payoff?</p>\n"}, "id": "1959"}, {"body": {"answer": "<p>I think it uses a kind of algorithm you presented, in combination with various sensors. It uses the sensors to make a virtual map and can then traverse the terrain with a combination of these sensors and the virtual map. Of course it uses a kind of path-planning algorithm to find the best way from A to B.</p>\n\n<p>Maybe you should look at this wikipedia page:\n<a href=\"https://en.wikipedia.org/wiki/Robotic_mapping\" rel=\"nofollow\">https://en.wikipedia.org/wiki/Robotic_mapping</a></p>\n\n<p>The new robotic vacuum cleaner from Samsung, I think, uses a 360\u00b0 camera to perceive its environment.</p>\n", "question": "<p>How does a domestic autonomous robotic vacuum cleaner -  such as a <a href=\"https://en.wikipedia.org/wiki/Roomba\" rel=\"nofollow\">Roomba</a> - know when it's working cleaned area (aka virtual map), and how does it plan to travel to the areas which hasn't been explored yet?</p>\n\n<p>Does it use some kind of <a href=\"https://en.wikipedia.org/wiki/A*_search_algorithm\" rel=\"nofollow\">A*</a> algorithm?</p>\n"}, "id": "1960"}, {"body": {"answer": "<p>There is no reason to treat hard AI different then humans.\nSome people telling that you can make a snapshot of AI but there is no reason to not make a human snapshot also. We dont have technology for that but there is no any magical barrier that would make it impossible (save all biological data and then print your copy somewhere else. Why not?).</p>\n\n<p>Its to early to talk about this as we do not understand our existence (Death term for biological creatures evolving all the time).</p>\n\n<p>I bet that in the future we will merge with AI and the only question will be what death means for any intelligent existence.</p>\n", "question": "<p>Can one actually kill a machine? Not only do we have problems in defining life, we also have problems in defining death. Will this also be true in artificial life and artificial intelligence?</p>\n"}, "id": "1962"}, {"body": {"answer": "<p>An elementary approach to 'passion' would be to pre-assign different areas for the program to be 'passionate' about and associate different numeric 'drive strengths' with each (perhaps adaptively). Mechanisms of this sort were studied in Toby Tyrell's widely cited PhD thesis on <a href=\"http://w2mind.computing.dcu.ie/worlds/w2m.TyrrellWorld/tyrrell_phd.pdf\" rel=\"nofollow\">'Action Selection in Animals'</a></p>\n\n<p>More recently, some more sophisticated AI architectures have been developed under the heading of <a href=\"https://en.wikipedia.org/wiki/Motivation#Intrinsic_motivation\" rel=\"nofollow\">'Intrinsic Motivation'</a>.</p>\n\n<p><a href=\"http://www.pyoudeyer.com/aiSummit06KaplanOudeyer.pdf\" rel=\"nofollow\">Here </a> is a link to a paper on the subject by Pierre-Yves Oudeyer, a leading expert in the field of <a href=\"https://en.wikipedia.org/wiki/Developmental_robotics\" rel=\"nofollow\">Developmental Robotics</a>.</p>\n\n<p>With regard to the question <em>\"would this cause the machine to do a better job?\"</em>, that would very much depend on how open-ended the architecture is:</p>\n\n<p>It's clearly easier if, rather than having to spell everything out in detail to a machine, we can simply specify a problem at a high-level and let its own motivations cause it to explore promising avenues.</p>\n\n<p>Conversely, if motivations are too open ended, it may well spend all its time doing the equivalent of 'doodling on its paper' (Hofstadter).</p>\n\n<p>Hence, like people, the quality of the output will be a function of its internal dispositions and could be measured in the same way for a given task (e.g. quantitatively for scientific activities, qualatatively for the arts).</p>\n", "question": "<p>Mankind can create machines to do work. Could we also create a (passion) within the machines to do better work by using Artificial Intelligence? Would passion cause the machine to do a better job, and could we measure the quantity/quality of passion by comparing outputs of the machine - that is, those machines with passion, and those without?</p>\n"}, "id": "1967"}, {"body": {"answer": "<p>In the abstract, mechanisms for self-recognition (I personally prefer the phrase 'metacognition', since it carries fewer spurious associations) and recognition of intelligence in other systems can be considered to be pretty much equivalent.</p>\n\n<p>In fact, both can be characterized in the standard percept/action framework: the task in both cases involves (however coarsely) classifying/predicting the behaviour of a black box system.</p>\n\n<p>Such tasks can be universally characterized in terms of frameworks such as <a href=\"https://en.wikipedia.org/wiki/Solomonoff%27s_theory_of_inductive_inference\" rel=\"nofollow\">Solomonoff Induction</a> or (more recently) <a href=\"https://arxiv.org/abs/cs/0004001\" rel=\"nofollow\">AIXI</a>.</p>\n", "question": "<p>Can AI systems be created that could recognize itself, and recognize intelligence in other systems, and make intelligent decisions about the other systems? Mankind seems to be making progress in self-recognition but I've not seen evidence of one system recognizing other systems and being able to compare it's own intelligence with other systems. How could this be accomplished?</p>\n"}, "id": "1968"}, {"body": {"answer": "<p>The general research area is known as 'grammar induction'.</p>\n\n<p>It is generally framed as a 'supervised learning' problem, with the input presented as raw text, and the desired output the corresponding parse tree.\nThe training set often consists of both positive and negative examples.</p>\n\n<p>Naturally, there is no 'single best' method for achieving this, but some of the techniques that have been used to date include:</p>\n\n<ul>\n<li><a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.73.7904&amp;rep=rep1&amp;type=pdf\" rel=\"nofollow\">Bayesian approaches</a></li>\n<li><a href=\"http://hackipedia.org/Algorithms/Artificial%20Intelligence/A%20Genetic%20Algorithm%20for%20the%20Induction%20of%20Natural%20Language%20Grammars.pdf\" rel=\"nofollow\">Genetic Algorithms</a></li>\n<li><a href=\"https://www.nada.kth.se/utbildning/grukth/exjobb/rapportlistor/2011/rapporter11/svantesson_marten_11077.pdf\" rel=\"nofollow\">Genetic Programming</a></li>\n<li><a href=\"http://dl.acm.org/citation.cfm?id=356816\" rel=\"nofollow\">Blackboard Architectures</a></li>\n<li>The <a href=\"http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.35.1924\" rel=\"nofollow\">UpWrite Predictor</a></li>\n</ul>\n", "question": "<p>I was think about AIs and how they would work, when I realised that I couldn't think of a way that an AI could be taught language. A child tends to learn language through associations of language and pictures to an object (e.g: people saying the word <code>dog</code> while around a dog, and later realising that  people say <code>a dog</code> and <code>a car</code> and learn what <code>a</code> means...). However, a text based AI couldn't use this method to learn, as they wouldn't have access to any sort of input device.</p>\n\n<p>The only way I could come up with is programming in every word, and rule, in the English language (or whatever language it is meant to 'speak' in), however that would, potentially, take years to do.</p>\n\n<p>Does anyone have any ideas on how this could be done? Or if it has been done already, if so how?</p>\n\n<p>Thanks in advance for any ideas.</p>\n\n<p>Btw: in this context, I am using AI to mean an Artificial Intelligence system with near-human intelligence, and no prior knowledge of language.</p>\n"}, "id": "1971"}, {"body": {"answer": "<p>It depends on how the IQ test is presented:</p>\n\n<ol>\n<li><p>If as for humans (effectively, as a video of the book containing the\ntest questions being opened etc), then <strong>all AI programs</strong> would <strong>score\nzero</strong>.</p></li>\n<li><p>If presented as the test set of a <strong>supervised learning</strong> problem (e.g. as for <a href=\"https://en.wikipedia.org/wiki/Bongard_problem\">Bongard Problems</a>) then one might imagine that a number of ML <strong>rule induction techniques</strong> (e.g. Learning Classifier Systems, Genetic Programming) might achieve <strong>some limited success</strong>. </p></li>\n</ol>\n\n<p>So all current AI programs require the problem to be 'framed' in a suitable fashion. It doesn't take too much thought to see that removing the need for such 'framing' is actually <em>the</em> core problem in AI, and (despite some of the claims about Deep Learning), eliminating framing remains a distant goal.</p>\n\n<p>More generally (just as with the Turing test), in order for an IQ test to be a <em>really</em> meaningful test of intelligence, it should be possible as a <em>side effect</em> of the program's capabilities, and not the specific purpose for which humans have designed it.</p>\n\n<p>Interestingly, there is only one program that I'm aware of that sits between 1. and 2.: </p>\n\n<p><strong>Phaeaco</strong> (developed by <a href=\"http://www.foundalis.com/res/diss_research.html\">Harry Foundalis</a> at Douglas Hofstadter's research group) takes <em>noisy photographic images of Bongard problems as input</em> and (using a variant of Hofstadter's <a href=\"http://dl.acm.org/citation.cfm?id=525377\">'Fluid Concepts'</a> architecture) successfully deduces the required rule in many cases.</p>\n", "question": "<p>If IQ were used as a measure of the intelligence of machines, as in humans, at this point in time what would be the IQ of our most intelligent AI systems? If not IQ, then how best to compare our intelligence to a machine, or one machine to another? </p>\n\n<p>This question is not asking if we can measure the IQ of a machine, but if IQ is the most preferred, or general, method of measuring intelligence then how does artificial intelligence compare to our most accepted method of measuring intelligence in humans. Many people may not understand the relevance of a Turing Test as to how intelligent their new car is, or other types of intelligent machines.</p>\n"}, "id": "1972"}, {"body": {"answer": "<blockquote>\n  <p>at this point in time, what would be the IQ of our most intelligent AI systems?</p>\n</blockquote>\n\n<h3>Zero.</h3>\n\n<p>There are many different kinds of IQ tests including written, visual, and verbal assessments, but the majority of questions are based on abstract-reasoning problems that involve creative thinking and true intelligence.</p>\n\n<p>In other words, the computer would have to exhibit something that does not yet exist&hellip; \"strong AI\".</p>\n\n<p>The intelligent computers of science fiction do not exist. At all. We are not even close. We have absolutely NO IDEA how to bridge the gap between what we can do now and what is depicted in pop-culture films. Even with cars that drive themselves and computers that play 'Go' &mdash; an underachieving mosquito possesses more cognitive intelligence than the all the world's super computers <em>&hellip;combined!</em> </p>\n\n<h3>&hellip;or possibly \"disqualified\" for cheating.</h3>\n\n<p>Even if we could pre-format the questions in a style and delivery system it understands, what does memorization, attention, or speed mean in the context of a computer? I'm not even sure if a standardized IQ test makes sense in this context. It might be like asking how a computer would do in a spelling bee.</p>\n\n<p>In human terms, we're not <em>allowed</em> to bring along reference materials to look up an answer; but how do you rectify that when reference-lookup is innate to a computer's existence? How do you measure memory when storage is non-volatile? This gets into an existential question about the nature of learning and knowledge vs. just taking a lot of notes.</p>\n\n<p>Still, how do you even <strong><em>teach</em></strong> a computer what is meant by <em>\"which animals is least like the other four?\"</em> Did the computer really figure out what was being asked out of general intelligence, or is the computer simply designed to parse out IQ-style questions specifically? If you designed something with a foreknowledge of what would likely be asked, the computers of today <em>might</em> simply be able to \"recognize\" it as question-style 496.527b and plug in the variables. </p>\n\n<p>But that's not <em>general intelligence</em> by any definition we use or understand. It's just a specialized, slick interpreter designed to parse out a specific type of standardized question. Ask it a style of question it's is not expecting, and you'll see the computer is exhibiting <strong><em>no</em> innate intelligence</strong> at all. </p>\n\n<p><strong>Until we create <em>strong AI,</em> a computer has effectively <em>no</em> IQ.</strong></p>\n", "question": "<p>If IQ were used as a measure of the intelligence of machines, as in humans, at this point in time what would be the IQ of our most intelligent AI systems? If not IQ, then how best to compare our intelligence to a machine, or one machine to another? </p>\n\n<p>This question is not asking if we can measure the IQ of a machine, but if IQ is the most preferred, or general, method of measuring intelligence then how does artificial intelligence compare to our most accepted method of measuring intelligence in humans. Many people may not understand the relevance of a Turing Test as to how intelligent their new car is, or other types of intelligent machines.</p>\n"}, "id": "1973"}, {"body": {"answer": "<p>The umbrella term for your problem is called <strong>natural language processing (NLP)</strong> -- a topic under artificial intelligence. </p>\n\n<p>There are many subtopics to this field including language semantics, grammatical analysis, parts of speech tagging, domain specific context analysis, etc. </p>\n", "question": "<p>I was think about AIs and how they would work, when I realised that I couldn't think of a way that an AI could be taught language. A child tends to learn language through associations of language and pictures to an object (e.g: people saying the word <code>dog</code> while around a dog, and later realising that  people say <code>a dog</code> and <code>a car</code> and learn what <code>a</code> means...). However, a text based AI couldn't use this method to learn, as they wouldn't have access to any sort of input device.</p>\n\n<p>The only way I could come up with is programming in every word, and rule, in the English language (or whatever language it is meant to 'speak' in), however that would, potentially, take years to do.</p>\n\n<p>Does anyone have any ideas on how this could be done? Or if it has been done already, if so how?</p>\n\n<p>Thanks in advance for any ideas.</p>\n\n<p>Btw: in this context, I am using AI to mean an Artificial Intelligence system with near-human intelligence, and no prior knowledge of language.</p>\n"}, "id": "1974"}, {"body": {"answer": "<p>Interesting question.</p>\n\n<p>Well if you really think about it, what is passion? How does that passion comes to be a passion.\nOne of the main topics you might want to touch here is conditioning and thus motivation.</p>\n\n<p>Think about the following:</p>\n\n<p>I have a passion for programming</p>\n\n<blockquote>\n  <p>Why do I have a passion for programming?</p>\n</blockquote>\n\n<p>Because when I wrote my first program I was positively reinforced by the fact that I completed a program, I was negatively reinforced because I removed my frustration of not completing the program</p>\n\n<blockquote>\n  <p>How come that I have gone through that programming frustration and\n  stick to it even if I was frustrated?</p>\n</blockquote>\n\n<p>Because I wanted to learn programming</p>\n\n<blockquote>\n  <p>Why did I wanted to learn programming?</p>\n</blockquote>\n\n<p>Because I wanted a light on an arduino to turn on (projected reinforcer)</p>\n\n<blockquote>\n  <p>Why did I wanted to turn the arduino light on?</p>\n</blockquote>\n\n<p>So I could learn programming and because I though it was cool (classical conditioning association that will later be reinforced, projected reinforcement happened right after the classical conditioning association between turn on a led happened)</p>\n\n<p>This can be done through a neural network, where each association is reinforced through a probability of outcome\nFor example, I did learn arduino, on purpose because it seemed the easiest way to start coding, so the probability of positive outcome was high</p>\n\n<p>This about an opposite situation\nLet's say I do not know calculus, and I barely know elementary algebra, if someone started to teach me about integrals saying that this is the only way to start learning more math, I will not be motivate to do so because since I cannot even conceptualize what an integral can be, it will be really hard for me to understand it thus I will not learn calc</p>\n\n<p>Thus we can also discern that motivation is reinforced in small behaviors</p>\n\n<p>Another more practical and realistic example you might use is</p>\n\n<p>If you trow a rat in a cage, and make him lever-press do you think he is going to? No. Although if you reinforce the behavior of going next to the lever slowly and at the end he will lever press and you then reinforce that behavior he will.</p>\n\n<p>Thus, passion is compartmentalized, and that's what you have to do in your NT and make it mathematically</p>\n\n<blockquote>\n  <p>WINK WINK:</p>\n</blockquote>\n\n<p>Small hint, it's a progressive function</p>\n", "question": "<p>Mankind can create machines to do work. Could we also create a (passion) within the machines to do better work by using Artificial Intelligence? Would passion cause the machine to do a better job, and could we measure the quantity/quality of passion by comparing outputs of the machine - that is, those machines with passion, and those without?</p>\n"}, "id": "1975"}, {"body": {"answer": "<p>A common concept in AI is \"recursive self-improvement.\" That is, the AI 1.0 would build a version 1.01, which would build a version 1.02, and so on.</p>\n\n<p>This is probably not going to be thought of as the newer version 'destroying' the older version; if an AI can self-modify, it's probably going to be more like going to sleep and waking up smarter, or learning a new mental technique, or so on.</p>\n\n<p>One important point is that even if the AI is not allowed to self-modify, maybe because of a block put in by its programmers, that won't necessarily prevent it from constructing another AI out in the wild, and so an important problem is to figure out how to best generalize the concept of \"don't improve yourself\" so that we can make AIs that have bounded scope and impact.</p>\n", "question": "<ul>\n<li>Would AI be a self-propogating iteration in which the previous AI is\ndestroyed by a more optimised AI child?  </li>\n<li>Would the AI have branches of it's own AI warning not to create the new AI?</li>\n</ul>\n"}, "id": "1977"}, {"body": {"answer": "<p>Honestly, nobody knows.  Any talk of sentient AI's is still basically sci-fi and we can't really offer anything more than informed speculation.  But think about it this way:  sentience, in and of itself, doesn't necessarily involve any \"goals\" or \"desires\" or \"objectives\" beyond what the AI creator programmed in.  Be careful not to over anthropomorphize and assume that any \"sentient AI\" is going behave like a human.   </p>\n\n<p>In other words, there's no <strong>particular</strong> reason to say that any given AI must be \"a self-propogating iteration in which the previous AI is destroyed by a more optimised AI child\". </p>\n\n<p>So all of that said, my answer to \"Would a sentient AI try to create a more optimised AI which would eventually overtake AI 1.0\" is:</p>\n\n<p>\"If the creator of the AI programs it to do that, then yes. Otherwise, probably not.\"  </p>\n\n<p>So would a hypothetical AI creator program the AI to try and improve itself?  Who knows. It's the kind of thing that seems like it might be a good idea.  And I suppose such a motive could - in principle - even slip in by accident. </p>\n", "question": "<ul>\n<li>Would AI be a self-propogating iteration in which the previous AI is\ndestroyed by a more optimised AI child?  </li>\n<li>Would the AI have branches of it's own AI warning not to create the new AI?</li>\n</ul>\n"}, "id": "1979"}, {"body": {"answer": "<p>Deep neural networks could - in principle - be a component of an <a href=\"https://en.wikipedia.org/wiki/Ensemble_learning\" rel=\"nofollow\">ensemble of machine learning algorithms</a>, yes.  Ensemble method basically just means use multiple algorithms and combining their output somehow.  </p>\n\n<p>Other than that, I don't see any special connection between deep learning and the idea of ensemble methods.  DL is just one more tool in the toolkit. </p>\n", "question": "<blockquote>\n  <p>Shortly about <a href=\"https://en.wikipedia.org/wiki/Deep_learning\" rel=\"nofollow\"><strong>deep learning</strong> (for reference)</a>:</p>\n  \n  <p><strong><em>Deep learning</strong> is a branch of machine learning based on a set of algorithms that attempt to model high-level abstractions in data by\n  using a deep graph with multiple processing layers, composed of\n  multiple linear and non-linear transformations.</em></p>\n  \n  <p><em>Various deep learning architectures such as deep neural networks, convolutional deep neural networks, deep belief networks and recurrent\n  neural networks have been applied to fields like computer vision,\n  automatic speech recognition, natural language processing, audio\n  recognition and bioinformatics where they have been shown to produce\n  state-of-the-art results on various tasks.</em></p>\n</blockquote>\n\n<hr>\n\n<p><strong>My question:</strong></p>\n\n<p>Can <a href=\"https://en.wikipedia.org/wiki/Deep_learning#Deep_neural_network_architectures\" rel=\"nofollow\">deep neural networks</a> or <a href=\"https://en.wikipedia.org/wiki/Convolutional_neural_network\" rel=\"nofollow\">convolutional deep neural networks</a> be viewed as <a href=\"https://en.wikipedia.org/wiki/Ensemble_learning\" rel=\"nofollow\">ensemble-based</a> method of machine learning? Or it is different approaches?</p>\n"}, "id": "1980"}, {"body": {"answer": "<p>You should think of them as different approaches. A deep neural net is a single independent model, whereas ensemble models are ensembles of many independent models.</p>\n\n<p>The primary connection between the two is <a href=\"http://ai.stackexchange.com/questions/40/what-is-the-dropout-technique\">dropout</a>, a particular method of training deep neural nets that's inspired by ensemble methods. </p>\n", "question": "<blockquote>\n  <p>Shortly about <a href=\"https://en.wikipedia.org/wiki/Deep_learning\" rel=\"nofollow\"><strong>deep learning</strong> (for reference)</a>:</p>\n  \n  <p><strong><em>Deep learning</strong> is a branch of machine learning based on a set of algorithms that attempt to model high-level abstractions in data by\n  using a deep graph with multiple processing layers, composed of\n  multiple linear and non-linear transformations.</em></p>\n  \n  <p><em>Various deep learning architectures such as deep neural networks, convolutional deep neural networks, deep belief networks and recurrent\n  neural networks have been applied to fields like computer vision,\n  automatic speech recognition, natural language processing, audio\n  recognition and bioinformatics where they have been shown to produce\n  state-of-the-art results on various tasks.</em></p>\n</blockquote>\n\n<hr>\n\n<p><strong>My question:</strong></p>\n\n<p>Can <a href=\"https://en.wikipedia.org/wiki/Deep_learning#Deep_neural_network_architectures\" rel=\"nofollow\">deep neural networks</a> or <a href=\"https://en.wikipedia.org/wiki/Convolutional_neural_network\" rel=\"nofollow\">convolutional deep neural networks</a> be viewed as <a href=\"https://en.wikipedia.org/wiki/Ensemble_learning\" rel=\"nofollow\">ensemble-based</a> method of machine learning? Or it is different approaches?</p>\n"}, "id": "1981"}, {"body": {"answer": "<p>Now in most cases we still have clear distinctions between programs and data. But when an AI becomes sentient, its data would be as powerful as what we currently call programs, and its program might be as irrelevant as what we currently call hardware. Then it would be difficult to distinguish creating an AI from learning new things, or buying new hardwares with improved instruction set.</p>\n\n<p>For example, if some AI invent new algorithms that its creator finally put that on itself, buys itself some new computers, write a new efficient compiler that recompiles its own code and put that to the new computer, fill the new computer with all the knowledges it learned, and cut off the communication for reasons such as missions on the Mars. Did it create a more optimized AI?</p>\n\n<p>In contrast, if some AI created something completely new, but shares some code with itself. In fact, that's because they run in the same operating system and shares the same standard C library. Is the new AI considered evolved from itself and not a separate entity? Maybe the core AI algorithms and even some basic knowledges would be as common as the standard C library in the future. And what we think is based on the same system is considered completely new in the future.</p>\n\n<p>Anyway, humans have limited and nonextensible resources, nontransferrable knowledges, and limited throughput interacting with the world. These problems could probably be overcome within a few AI generations. With the same hardware, I doubt that the AI related algorithms could be indefinitely better and better. And there is a physical bound on the hardwares. It won't last long even if that happens.</p>\n\n<p>In the unlikely case that there could be that many generations and AIs are that violent, as long as there are competitors, the warning doesn't make much sense considering how evolution works.</p>\n", "question": "<ul>\n<li>Would AI be a self-propogating iteration in which the previous AI is\ndestroyed by a more optimised AI child?  </li>\n<li>Would the AI have branches of it's own AI warning not to create the new AI?</li>\n</ul>\n"}, "id": "1984"}, {"body": {"answer": "<p>Great question. Today AI systems works in \"one burst\" mode. Get one input and generate one output. Our brains are not working like that. </p>\n\n<p>First step is to learn network how to communicate with it's \"helper\", so network instead of result generate question and cycle will repeat until network find result. </p>\n\n<p>Network must be recurrent for inner state needed between question/answer cycles. </p>\n", "question": "<p>Most of the people is trying to answer question with a neural network. However, has anyone came up with some thoughts about how to make neural network ask questions, instead of answer questions? For example, if a CNN can decide which category an object belongs to, than can it ask some question to help the the classification?</p>\n"}, "id": "1986"}, {"body": {"answer": "<p>Just for the sake of completeness, I'll point out that Recurrent Neural Nets (i.e. neural nets with backwards connections) are frequently used for for Natural Language Processing (NLP). This includes variants like Bidirectional, Jordan and Elman Networks. Long Short-Term Memory (LSTM) is a more sophisticated neural net algorithm which can accomplish the same time and sequence-based tasks, but which can leverage standard learning methods like backprop since it doesn't suffer from the \"vanishing gradient problem.\" This is because LSTMs have been brilliantly engineered as \"perfect integrators,\" which makes it a lot easier to calculate the error gradients etc. over long periods of time. In contrast, learning with RNNs is still not theoretically well-grounded and is difficult to calculate through existing methods like Backpropagation Through Time (BPTT). In Time Delay Neural Networks (TDNNs), the idea is to add new neurons and connections with each new training example across a stretch of time or training sequence; unfortunately, this places a practical limitation on how many examples you can feed into the net before the size of the network gets out of hand or it starts forgetting, just as with RNNs. LSTMs have much longer memories (especially when augmented with Neural Turing Machines) so that'd be my first choice, assuming I wanted to use neural nets for NLP purposes. My knowledge of the subject is limited though (I'm still trying to learn the ropes) so there may be other important neural net algorithms I'm overlooking...</p>\n", "question": "<p>I was think about AIs and how they would work, when I realised that I couldn't think of a way that an AI could be taught language. A child tends to learn language through associations of language and pictures to an object (e.g: people saying the word <code>dog</code> while around a dog, and later realising that  people say <code>a dog</code> and <code>a car</code> and learn what <code>a</code> means...). However, a text based AI couldn't use this method to learn, as they wouldn't have access to any sort of input device.</p>\n\n<p>The only way I could come up with is programming in every word, and rule, in the English language (or whatever language it is meant to 'speak' in), however that would, potentially, take years to do.</p>\n\n<p>Does anyone have any ideas on how this could be done? Or if it has been done already, if so how?</p>\n\n<p>Thanks in advance for any ideas.</p>\n\n<p>Btw: in this context, I am using AI to mean an Artificial Intelligence system with near-human intelligence, and no prior knowledge of language.</p>\n"}, "id": "1988"}, {"body": {"answer": "<p>There are many approaches to this kind of problem. The most obvious one is to <strong>create new features</strong>. The best features I can come up with is to transform the coordinates to <a href=\"https://en.wikipedia.org/wiki/Spherical_coordinate_system\" rel=\"nofollow\">spherical coordinates</a>. </p>\n\n<p>I have not found a way to do it in playground, so I just created a few features that should help with this (sin features). After <a href=\"http://playground.tensorflow.org/#activation=tanh&amp;batchSize=10&amp;dataset=spiral&amp;regDataset=reg-plane&amp;learningRate=0.03&amp;regularizationRate=0&amp;noise=0&amp;networkShape=4,2&amp;seed=0.73263&amp;showTestData=false&amp;discretize=false&amp;percTrainData=50&amp;x=true&amp;y=true&amp;xTimesY=false&amp;xSquared=false&amp;ySquared=false&amp;cosX=false&amp;sinX=true&amp;cosY=false&amp;sinY=true&amp;collectStats=false&amp;problem=classification&amp;initZero=false&amp;hideText=false\" rel=\"nofollow\">500 iterations</a> it will saturate and will fluctuate at 0.1 score. This suggest that no further improvement will be done and most probably I should make the hidden layer wider or add another layer.</p>\n\n<p>Not a surprise that after adding <a href=\"http://playground.tensorflow.org/#activation=tanh&amp;batchSize=10&amp;dataset=spiral&amp;regDataset=reg-plane&amp;learningRate=0.03&amp;regularizationRate=0&amp;noise=0&amp;networkShape=5,2&amp;seed=0.73263&amp;showTestData=false&amp;discretize=false&amp;percTrainData=50&amp;x=true&amp;y=true&amp;xTimesY=false&amp;xSquared=false&amp;ySquared=false&amp;cosX=false&amp;sinX=true&amp;cosY=false&amp;sinY=true&amp;collectStats=false&amp;problem=classification&amp;initZero=false&amp;hideText=false\" rel=\"nofollow\">just one neuron to the hidden layer</a> you easily get 0.013 after 300 iterations. Similar thing happens by adding a new layer (0.017, but after significantly longer 500 iterations. Also no surprise as it is harder to propagate the errors). Most probably you can play with a learning rate or do an adaptive learning to make it faster, but this is not the point here.</p>\n\n<p><a href=\"http://i.stack.imgur.com/tck2s.png\" rel=\"nofollow\"><img src=\"http://i.stack.imgur.com/tck2s.png\" alt=\"enter image description here\"></a></p>\n", "question": "<p>I have been messing around in <a href=\"http://playground.tensorflow.org/#activation=tanh&amp;batchSize=10&amp;dataset=spiral&amp;regDataset=reg-plane&amp;learningRate=0.03&amp;regularizationRate=0&amp;noise=0&amp;networkShape=4,2&amp;seed=0.73263&amp;showTestData=false&amp;discretize=false&amp;percTrainData=50&amp;x=true&amp;y=true&amp;xTimesY=false&amp;xSquared=false&amp;ySquared=false&amp;cosX=false&amp;sinX=false&amp;cosY=false&amp;sinY=false&amp;collectStats=false&amp;problem=classification&amp;initZero=false&amp;hideText=false\" rel=\"nofollow\">tensorflow playground</a>. One of the input data sets is a spiral. No matter what input parameters I choose, no matter how wide and deep the neural network I make, I cannot fit the spiral. How do data scientists fit data of this shape?</p>\n"}, "id": "1990"}, {"body": {"answer": "<p>The cleanest result we have on this issue is the <a href=\"https://en.wikipedia.org/wiki/No_free_lunch_theorem\" rel=\"nofollow\">\"no free lunch\" theorem</a>. Basically, in order to make a system perform better at a specific task, you have to degrade its performance on other tasks, and so there is a flexibility-efficiency tradeoff.</p>\n\n<p>But to the broader question, or whether or not your thinking is correct, I think it pays to look more closely at what you mean by a \"narrow intelligence.\" The AI systems that we have that play Go and drive cars did <em>not</em> pop into existence able to do those things; they slowly learned how through lots and lots of training examples and a well-chosen architecture that mirrors the problem domain.</p>\n\n<p>That is, \"neural networks\" as a methodology seems 'general' in a meaningful way; one could imagine that a general intelligence could be formed by solving the meta-learning problem (that is, learning the architecture that best suits a particular problem while learning the weights for that problem from training data).</p>\n\n<p>Even in that case, there will still be a flexibility-efficiency tradeoff; the general intelligence that's allowed to vary its architecture will be able to solve many different problems, but will take some time to discover what problem it's facing. An intelligence locked into a particular architecture will perform well on problems that architecture is well-suited for (better than the general, since it doesn't need to discover) but less well on other problems it isn't as well-suited for.</p>\n", "question": "<p>A \"general intelligence\" may be capable of learning a lot of different things, but possessing capability does not equal actually having it. The \"AGI\" must learn...and that learning process can take time. If you want an AGI to drive a car or play Go, you have to find some way of \"teaching\" it. Keep in mind that we have never built AGIs, so we don't know how long the training process can be, but it would be safe to assume pessimistic estimates.</p>\n\n<p>Contrast that to a \"narrow intelligence\". The narrow AI already knows how to drive a car or play Go. It has been programmed to be very excellent at one specific task. You don't need to worry about training the machine, because it has already been pre-trained.</p>\n\n<p>A \"general intelligence\" seems to be more flexible than a \"narrow intelligence\". You could buy an AGI and have it drive a car <em>and</em> play Go. And if you are willing to do more training, you can even teach it a new trick: <em>how to bake a cake</em>. I don't have to worry about unexpected tasks coming up, since the AGI will <em>eventually</em> figure out how to do it, given enough training time. I would have to wait a <em>long time</em> though.</p>\n\n<p>A \"narrow intelligence\" appears to be <em>more efficient</em> at its assigned task, due to it being programmed specifically for that task. It knows exactly what to do, and doesn't have to waste time \"learning\" (unlike our AGI buddy here). Instead of buying one AGI to handle a bunch of different tasks poorly, I would rather buy a bunch of specialized narrow AIs. Narrow AI #1 drives cars, Narrow AI #2 plays Go, Narrow AI #3 bake cakes, etc. That being said, this is a very brittle approach, since if some unexpected task comes up, none of my narrow AIs would be able to handle it. I'm willing to accept that risk though.</p>\n\n<p>Is my \"thinking\" correct? Is there a trade-off between flexibility (AGI) and efficiency (narrow AI), like what I have just described above? Or is it theoretically possible for an AGI to be both flexible and efficient?</p>\n"}, "id": "1992"}, {"body": {"answer": "<p>It would appear so.  One example, albeit not specifically AI related, is seen in the difference between digital computers and <a href=\"https://en.wikipedia.org/wiki/Analog_computer\" rel=\"nofollow\">analog computers</a>.  Pretty much everything we think of as a \"computer\" today is a digital computer with a von Neumann architecture.  And that's because the things are so general purpose that they can be easily programmed to do, essentially, anything.  But analog computers can (or could, back in the 60's or thereabouts) solve some types of problems faster than a digital computer.  But they fell out of favor exactly due to that lack of flexibility.  Nobody wants to hand-wire circuits with op-amps and comparators to solve for <em>y</em>.  </p>\n", "question": "<p>A \"general intelligence\" may be capable of learning a lot of different things, but possessing capability does not equal actually having it. The \"AGI\" must learn...and that learning process can take time. If you want an AGI to drive a car or play Go, you have to find some way of \"teaching\" it. Keep in mind that we have never built AGIs, so we don't know how long the training process can be, but it would be safe to assume pessimistic estimates.</p>\n\n<p>Contrast that to a \"narrow intelligence\". The narrow AI already knows how to drive a car or play Go. It has been programmed to be very excellent at one specific task. You don't need to worry about training the machine, because it has already been pre-trained.</p>\n\n<p>A \"general intelligence\" seems to be more flexible than a \"narrow intelligence\". You could buy an AGI and have it drive a car <em>and</em> play Go. And if you are willing to do more training, you can even teach it a new trick: <em>how to bake a cake</em>. I don't have to worry about unexpected tasks coming up, since the AGI will <em>eventually</em> figure out how to do it, given enough training time. I would have to wait a <em>long time</em> though.</p>\n\n<p>A \"narrow intelligence\" appears to be <em>more efficient</em> at its assigned task, due to it being programmed specifically for that task. It knows exactly what to do, and doesn't have to waste time \"learning\" (unlike our AGI buddy here). Instead of buying one AGI to handle a bunch of different tasks poorly, I would rather buy a bunch of specialized narrow AIs. Narrow AI #1 drives cars, Narrow AI #2 plays Go, Narrow AI #3 bake cakes, etc. That being said, this is a very brittle approach, since if some unexpected task comes up, none of my narrow AIs would be able to handle it. I'm willing to accept that risk though.</p>\n\n<p>Is my \"thinking\" correct? Is there a trade-off between flexibility (AGI) and efficiency (narrow AI), like what I have just described above? Or is it theoretically possible for an AGI to be both flexible and efficient?</p>\n"}, "id": "1993"}, {"body": {"answer": "<p><a href=\"https://en.wikipedia.org/wiki/Strong_AI\" rel=\"nofollow\">Strong</a> and <a href=\"https://en.wikipedia.org/wiki/Weak_AI\" rel=\"nofollow\">weak AI</a> are the older terms for <a href=\"https://en.wikipedia.org/wiki/Artificial_general_intelligence\" rel=\"nofollow\">AGI</a> (artificial general intelligence) and narrow AI. At least that's how I have seen it used and wikipedia seems to agree. </p>\n\n<p>I personally haven't seen Searle's definition of \"weak and strong AI\" in use much, but maybe the shift to the newer terms came about in part because Searle successfully confused the issue. </p>\n", "question": "<p>I've heard the terms strong-AI and weak-AI used.  Are these well defined terms or subjective ones?  How are they generally defined?</p>\n"}, "id": "1995"}, {"body": {"answer": "<p>Neural networks are usually trained to produce a certain output given a certain input. Often the output is a classification of the input or some other form of input description. Sometimes it is an action in a game and sometimes it is indeed stored data, a memory if you will. </p>\n\n<p>In that case the input is often a part of the stored data, so the NN actually completes the given input. This setup is called <a href=\"https://en.wikipedia.org/wiki/Autoassociative_memory\" rel=\"nofollow\">autoassociative memory</a>, <a href=\"https://en.wikipedia.org/wiki/Hopfield_network\" rel=\"nofollow\">Hopfield networks</a> are an example for this. </p>\n\n<p>In your example you might give this NN the first frame of the movie Avatar and it would output the complete movie. Unfortunately this would probably be absolutely <a href=\"https://en.wikipedia.org/wiki/Hopfield_network#Capacity\" rel=\"nofollow\">crazy inefficient</a>. </p>\n", "question": "<p>Is there a neural network(NN) system or architecture which can be used for only storing and retrieving information. For example; to store whole Avatar movie in HD format inside a neural network and retrieve(without loss) it from the neural network when needed. I searched the web and came across only LSTM RNN but in my understanding LSTM only stores pattern and not the content itself. If there is no such NN exist can you explain why it so?</p>\n"}, "id": "1998"}, {"body": {"answer": "<p>Imagine a genie grants you three wishes. Because you are an ambitious deep learning researcher your first wish is a perfect solution for a 1000-layer NN for Image Net, which promptly appears on your laptop.</p>\n\n<p>Now a genie induced solution doesn't give you any intuition how it might be interpreted as an ensemble, but do you really believe that you need 1000 layers of abstraction to distinguish a cat from a dog? As the authors of the \"ensemble paper\" mention themselves, this is definitely not true for biological systems.</p>\n\n<p>Of course you could waste your second wish on a decomposition of the solution into an ensemble of networks, and I'm pretty sure the genie would be able to oblige. The reason being that part of the power of a deep network will always come from the ensemble effect.</p>\n\n<p>So it is not surprising that two very successful tricks to train deep networks, dropout and residual networks, have an immediate interpretation as implicit ensemble. Therefore \"it's not depth, but the ensemble\" strikes me as a false dichotomy. You would really only say that if you honestly believed that you need hundreds or thousands of levels of abstraction to classify images with human accuracy. </p>\n\n<p>I suggest you use the last wish for something else, maybe a pinacolada. </p>\n", "question": "<p>The question is about the architecture of Deep Residual Networks (<strong>ResNets</strong>). The model that won the 1-st places at <a href=\"http://image-net.org/challenges/LSVRC/2015/results\" rel=\"nofollow\">\"Large Scale Visual Recognition Challenge 2015\" (ILSVRC2015)</a> in all five main tracks:</p>\n\n<blockquote>\n  <ul>\n  <li><em>ImageNet Classification: \u201cUltra-deep\u201d (quote Yann) 152-layer nets</em> </li>\n  <li><em>ImageNet Detection: 16% better than 2nd</em></li>\n  <li><em>ImageNet Localization: 27% better than 2nd</em></li>\n  <li><em>COCO Detection: 11% better than 2nd</em></li>\n  <li><em>COCO Segmentation: 12% better than 2nd<br><br></em>\n  <em>Source:</em> <a href=\"http://image-net.org/challenges/talks/ilsvrc2015_deep_residual_learning_kaiminghe.pdf\" rel=\"nofollow\"><em>MSRA @ ILSVRC &amp; COCO 2015 competitions (presentation, 2-nd slide)</em></a></li>\n  </ul>\n</blockquote>\n\n<p>This work is described in the following article:</p>\n\n<blockquote>\n  <p><a href=\"http://arxiv.org/abs/1512.03385\" rel=\"nofollow\"><em>Deep Residual Learning for Image Recognition (2015, PDF)</em></a></p>\n</blockquote>\n\n<hr>\n\n<p><strong>Microsoft Research team</strong> (developers of ResNets: Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun) in their article:</p>\n\n<blockquote>\n  <p><a href=\"https://arxiv.org/pdf/1603.05027.pdf\" rel=\"nofollow\">\"<em>Identity Mappings in Deep Residual Networks (2016)</em>\"</a></p>\n</blockquote>\n\n<p>state that <strong>depth</strong> plays a key role:</p>\n\n<blockquote>\n  <p><em>\"<strong>We obtain these results via a simple but essential concept \u2014 going deeper. These results demonstrate the potential of pushing the limits of depth.</strong>\"</em></p>\n</blockquote>\n\n<p>It is emphasized in their <a href=\"http://image-net.org/challenges/talks/ilsvrc2015_deep_residual_learning_kaiminghe.pdf\" rel=\"nofollow\">presentation</a> also (deeper - better):<br> </p>\n\n<blockquote>\n  <p><em>- \"A deeper model should not have higher training error.\"<br> \n  - \"Deeper ResNets have lower training error, and also lower test error.\"<br> \n  - \"Deeper ResNets have lower error.\"<br>\n  - \"All benefit more from deeper features \u2013 cumulative gains!\"<br>\n  - \"Deeper is still better.\"</em></p>\n</blockquote>\n\n<p>Here is the sctructure of 34-layer residual (for reference):\n<a href=\"http://i.stack.imgur.com/L8m0X.png\" rel=\"nofollow\"><img src=\"http://i.stack.imgur.com/L8m0X.png\" alt=\"enter image description here\"></a></p>\n\n<hr>\n\n<p>But recently I have found one theory that introduces a novel interpretation of residual networks showing they are exponential ensembles:</p>\n\n<blockquote>\n  <p><a href=\"https://arxiv.org/abs/1605.06431\" rel=\"nofollow\"><em>Residual Networks are Exponential Ensembles of Relatively Shallow Networks (2016)</em></a></p>\n</blockquote>\n\n<p>Deep Resnets are described as many shallow networks whose outputs are pooled at various depths. \nThere is a picture in the article. I attach it with explanation:</p>\n\n<blockquote>\n  <p><a href=\"http://i.stack.imgur.com/PGhK2.jpg\" rel=\"nofollow\"><img src=\"http://i.stack.imgur.com/PGhK2.jpg\" alt=\"enter image description here\"></a> Residual Networks are\n  conventionally shown as (a), which is a natural representation of\n  Equation (1). When we expand this formulation to Equation (6), we\n  obtain an unraveled view of a 3-block residual network (b). From this\n  view, it is apparent that residual networks have O(2^n) implicit paths\n  connecting input and output and that adding a block doubles the number\n  of paths.</p>\n</blockquote>\n\n<p>In conclusion of the article it is stated:</p>\n\n<blockquote>\n  <p><strong>It is not depth, but the ensemble that makes residual networks strong</strong>.\n  Residual networks push the limits of network multiplicity, not network\n  depth. Our proposed unraveled view and the lesion study show that\n  residual networks are an implicit ensemble of exponentially many\n  networks. If most of the paths that contribute gradient are very short\n  compared to the overall depth of the network, <strong>increased depth</strong>\n  alone <strong>can\u2019t be the key characteristic</strong> of residual networks. We now\n  believe that <strong>multiplicity</strong>, the network\u2019s expressability in the\n  terms of the number of paths, plays <strong>a key role</strong>.</p>\n</blockquote>\n\n<p>But it is only a recent theory that can be confirmed or refuted. It happens sometimes that some theories are refuted and articles are withdrawn.</p>\n\n<hr>\n\n<p><strong>My question:</strong><br>\nShould we think of deep ResNets as ensemble after all? <strong>Ensemble</strong> or <strong>depth</strong> makes residual networks so strong? Is it possible that even the developers themselves do not quite perceive what their own model represent and what is the key concept in it?</p>\n"}, "id": "1999"}, {"body": {"answer": "<p>If I'm reading it correctly, this question has nothing to do with optical character recognition. You want to create a system that takes a digital string of characters as input, then finds the best match from a predetermined list of words. That sounds like a task for if-then-else logic and dictionary lookup. It might be possible to use a neural net, but not easy.</p>\n\n<p>A neural net takes a fixed number of inputs, each of which are a value between zero and one. A major hurdle is that you probably want variable-sized inputs. Another hurdle is that you'll need to code the inputs some way onto numbers.</p>\n\n<p>These hurdles can be overcome but they are tipoffs that neural networks aren't well-suited for the task.</p>\n", "question": "<p>In my attempt at trying to learn neural network and machine learning I'm am trying to create a simple neural network which can be trained to recognise one word from a given string (which contains only one word). So in effect if one where to feed it a string containing the trained word but spelled wrong the network would be able to still recognise the word. Can anybody help me with some pseudo code or a start of a code. Or a general explanation of how to to this because I have read like 6 articles and 8 example projects and still have no clue how to do this</p>\n"}, "id": "2003"}, {"body": {"answer": "<p>An optimal solution for the task as stated, would be some alignment algorithm like Smith-Waterman, with a matrix which encodes typical typo frequencies. </p>\n\n<p>As an exercise in NNs, I would recommend using a RNN. This circumvents the problem that your inputs will be of variable size, because you just feed one letter after another and get an output once you feed the delimiter. </p>\n\n<p>As trainingsdata you'll need a list of random words and possibly a list of random strings, as negative examples and a list of slightly messed up versions of your target word as positive examples. </p>\n\n<p>Here is a <a href=\"https://gist.github.com/karpathy/d4dee566867f8291f086\" rel=\"nofollow\">minimal character-level RNN</a>, which consists of only a little more than a hundred lines of code, so you might be able to get your head around it or at least get it to run. Here is <a href=\"http://karpathy.github.io/2015/05/21/rnn-effectiveness/\" rel=\"nofollow\">the excellent blog post</a> by Karpathy to which the code sample belongs. </p>\n", "question": "<p>In my attempt at trying to learn neural network and machine learning I'm am trying to create a simple neural network which can be trained to recognise one word from a given string (which contains only one word). So in effect if one where to feed it a string containing the trained word but spelled wrong the network would be able to still recognise the word. Can anybody help me with some pseudo code or a start of a code. Or a general explanation of how to to this because I have read like 6 articles and 8 example projects and still have no clue how to do this</p>\n"}, "id": "2004"}, {"body": {"answer": "<p>The short answer is that Hawkins' vision has yet to be implemented in a widely accessible way, particularly the indispensable parts related to prediction. </p>\n\n<p>The long answer is that I read Hawkins' book a few years ago and was excited by the possibilities of Hierarchical Temporal Memory (HTM). I still am, despite the fact that I have a few reservations about some of his philosophical musings on the meanings of consciousness, free will and other such topics. I won't elaborate on those misgivings here because they're not germane to the main, overwhelming reason why HTM nets haven't succeeded as much as expected to date: to my knowledge, Numenta has only implemented a truncated version of his vision. They left out most of the prediction architecture, which plays such a critical role in Hawkins' theories. As Gerod M. Bonhoff put it in <a href=\"http://www.dtic.mil/dtic/tr/fulltext/u2/a482820.pdf\">an excellent thesis</a><a href=\"http://www.dtic.mil/dtic/tr/fulltext/u2/a482820.pdf\">1</a> on HTMs, </p>\n\n<blockquote>\n  <p>\"In March of 2007, Numenta released what they claimed was a \u201cresearch\n  implementation\u201d of HTM theory called Numenta Platform for Intelligent\n  Computing (NuPIC). The algorithm used by NuPIC at this time is called\n  \u201cZeta1.\u201d NuPIC was released as an open source software platform and\n  binary files of the Zeta1 algorithm. Because of licensing, this paper\n  is not allowed to discuss the proprietary implementation aspects of\n  Numenta\u2019s Zeta1 algorithm. There are, however, generalized\n  concepts of implementation that can be discussed freely. The two most\n  important of these are how the Zeta 1 algorithm (encapsulated in each\n  memory node of the network hierarchy) implements HTM theory. To\n  implement any theory in software, an algorithmic design for each\n  aspect of the theory must be addressed. The most important design\n  decision Numenta adopted was to eliminate feedback within the\n  hierarchy and instead choose to simulate this theoretical concept\n  using only data pooling algorithms for weighting. This decision is\n  immediately suspect and violates key concepts of HTM. Feedback,\n  Hawkins\u2019 insists, is vital to cortical function and central to his\n  theories. Still, Numenta claims that most HTM applicable problems can\n  be solved using their implementation and proprietary pooling\n  algorithms.\"</p>\n</blockquote>\n\n<p>I am still learning the ropes in this field and cannot say whether or not Numenta has since scrapped this approach in favor of a full implementation of Hawkins' ideas, especially the all-important prediction architecture. Even if they have, this design decision has probably delayed adoption by many years. That's not a criticism per se; perhaps the computational costs of tracking prediction values and updating them on the fly were too much to bear at the time, on top of the ordinary costs of processing neural nets, leaving them with no other path except to try half-measures like their proprietary pooling mechanisms. Nevertheless, all of the best research papers I've read on the topic since then have chosen to reimplement the algorithms rather than relying on Numenta's platform, typically because of the missing prediction features. Cases in point include Bonhoff's thesis and <a href=\"http://cogprints.org/9187/1/HTM_TR_v1.0.pdf\">Maltoni's technical report for the University of Bologna Biometric System Laboratory</a><a href=\"http://cogprints.org/9187/1/HTM_TR_v1.0.pdf\">2</a>. In all of those cases, however, there is no readily accessible software for putting their variant HTMs to immediate use (as far as I know). The gist of all this is that like G.K. Chesterton's famous maxim about Christianity, \"HTMs have not been tried and found wanting; they have been found difficult, and left untried.\" Since Numenta left out the prediction steps, I assume that they would be the main stumbling blocks awaiting anyone who wants to code Hawkins' full vision of what an HTM should be.</p>\n\n<p><a href=\"http://www.dtic.mil/dtic/tr/fulltext/u2/a482820.pdf\">1</a>Bonhoff, Gerod M., 2008, Using Hierarchical Temporal Memory for Detecting Anomalous Network Activity. Presented in March, 2008 at the Air Force Institute of Technology, Wright-Patterson Air Force Base, Ohio.  </p>\n\n<p><a href=\"http://cogprints.org/9187/1/HTM_TR_v1.0.pdf\">2</a>Maltoni, Davide, 2011, Pattern Recognition by Hierarchical Temporal Memory. DEIS Technical Report published April 13, 2011. University of Bologna Biometric System Laboratory: Bologna, Italy.   </p>\n", "question": "<p>In 2004 <a href=\"https://en.wikipedia.org/wiki/Jeff_Hawkins\">Jeff Hawkins</a>, inventor of the palm pilot, published a very interesting book called <a href=\"https://en.wikipedia.org/wiki/On_Intelligence\">On Intelligence</a>, in which he details a theory how the human neocortex works. </p>\n\n<p>This theory is called <a href=\"https://en.wikipedia.org/wiki/Memory-prediction_framework\">Memory-Prediction framework</a> and it has some striking features, for example not only bottom-up (feedforward), but also top-down information processing and the ability to make simultaneous, but discrete predictions of different future scenarios (as described <a href=\"http://journal.frontiersin.org/article/10.3389/fncir.2016.00023/full\">in this paper</a>).</p>\n\n<p>The promise of the Memory-Prediction framework is unsupervised generation of stable high level representations of future possibilities. Something which would revolutionise probably a whole bunch of AI research areas.</p>\n\n<p>Hawkins founded <a href=\"https://en.wikipedia.org/wiki/Numenta\">a company</a> and proceeded to implement his ideas. Unfortunately more than ten years later the promise of his ideas is still unfulfilled. So far the implementation is only used for anomaly detection, which is kind of the opposite of what you really want to do. Instead of extracting the understanding, you'll extract the instances which the your artificial cortex doesn't understand. </p>\n\n<p>My question is in what way Hawkins's framework falls short. What are the concrete or conceptual problems that so far prevent his theory from working in practice? </p>\n"}, "id": "2006"}, {"body": {"answer": "<p>Three possibilities come to mind:</p>\n\n<p>The easiest is zero padding. Basically you take a rather big input size and just add zeroes if your concrete input is too small. Of course this is pretty limited and certainly not useful if your input ranges from a few words to full texts.</p>\n\n<p><a href=\"https://en.wikipedia.org/wiki/Recurrent_neural_network\" rel=\"nofollow\">RNNs</a> are a very natural NN to chose if you have texts of varying size as input. You input words as word vectors just one after another and the internal state of the RNN is supposed to encode the meaning of the full string of words. <a href=\"http://www.iro.umontreal.ca/~lisa/pointeurs/RNNSpokenLanguage2013.pdf\" rel=\"nofollow\">This is one</a> of the earlier papers.</p>\n\n<p>Another possibility is using <a href=\"https://en.wikipedia.org/wiki/Recursive_neural_network\" rel=\"nofollow\">recursive NNs</a>. This is basically a form of preprocessing in which a text is recursively reduced to a smaller number of word vectors until only one is left - your input, which is supposed to encode the whole text. This makes a lot of sense from a linguistic point of view if your input consists of sentences (which can vary a lot in size), because sentences are structured recursively (For example the word vector for \"the man\", should be similar to the word vector for \"the man who mistook his wife for a hat\", because noun phrases act like nouns etc.). Often you can use linguistic information to guide your recursion on the sentence. If you want to go way beyond the wiki article, <a href=\"http://nlp.stanford.edu/~socherr/thesis.pdf\" rel=\"nofollow\">this is probably a good start</a>.</p>\n", "question": "<p>As far as I can tell, neural networks have a <strong>fixed number of neurons</strong> in the input layer.</p>\n\n<p>If neural networks are used in a context like for example NLP, sentences or blocks of text of varying sizes are fed to a network. How is the <strong>varying input size</strong> reconciled with the <strong>fixed size</strong> of the input layer of the network? In other words: how is such a network made flexible enough to deal with an input that might be anywhere from one word to multiple pages of text?</p>\n\n<p>If my assumption of a fixed number of input neurons is wrong and new input neurons are added to/removed from the network to match the input size I don't see how these can ever be trained.</p>\n\n<p>I give the example of NLP, but lots of problems have an inherently unpredictable input size, I'm interested in the general approach for dealing with this.</p>\n\n<p>edit: For images, it's clear you can up/downsample to a fixed size, but for text this seems to be an impossible approach since adding/removing text changes the meaning of the original input.</p>\n"}, "id": "2009"}, {"body": {"answer": "<p>I am currently reading <em>Superintelligence: Paths, Dangers, Strategies</em> by Nick Bostrom. When he discusses whole brain emulation, although computing power (storage, bandwidth, CPU, body simulation, &amp; environment simulation) is one of the three general key things we are lacking toward its success, he also seems to agree that computing power is the most feasible and attainable of the three general issues we have for attaining it as of now. However he also goes on to to say </p>\n\n<blockquote>\n  <p>Just how much technology is required for whole brain emulation\n  depends on the level of abstraction at which the brain is simulated.<sup><a href=\"https://books.google.co.uk/books?id=1mMJBAAAQBAJ&amp;pg=PT41&amp;lpg=PT41&amp;dq=%22Just%20how%20much%20technology%20is%20required%20for%20whole%20brain%20emulation%20depends%20on%20the%20level%20of%22&amp;source=bl&amp;ots=Pmw4PUDele&amp;sig=PfmDRgcJuKBf3TwyCoVtCmp9XmE&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwj4gZy6tbnPAhVRF8AKHUoQDD8Q6AEIHjAA#v=onepage&amp;q=%22Just%20how%20much%20technology%20is%20required%20for%20whole%20brain%20emulation%20depends%20on%20the%20level%20of%22&amp;f=false\" rel=\"nofollow\">ref</a></sup></p>\n</blockquote>\n\n<p>Which is an interesting thought, but a whole different discussion.</p>\n\n<p>Anyways, so I think you are correct in thinking that we aren't far from having the computing power and maybe you are on to something, but rather are biggest hurdles are the other two key prerequisites that we need to attain before we can even begin trying, which are scanning, and translation. </p>\n\n<p>Of the three, it would seem translation is the one we need to advance in the most, as of now. A modest prediction of attaining whole brain emulation is at least 15 years or mid century. Theres much more information in this book of all of the different paths that can be taken to achieve super intelligence, and it is well researched, I highly recommend it if you haven't read it already.</p>\n", "question": "<p>Considering the answers of <a href=\"http://ai.stackexchange.com/questions/1314/how-powerful-a-computer-is-required-to-simulate-the-human-brain\">this</a> question, emulating a human brain with the current computing capacity is currently impossible, but we aren't very far from it.</p>\n\n<p>Note, 1 or 2 decades ago, similar calculations had similar results.</p>\n\n<p>The clock frequency of the modern CPUs seem to be stopped, currently the miniaturization (-> mobile use), the RAM/cache improvement and the multi-core paralellization are the main lines of the development.</p>\n\n<p>Ok, but what is the case with the analogous chips? In case of a NN, it is not a very big problem, if it is not very accurate, the NN would adapt to the minor manufacturing differences in its learning phase. And a single analogous wire can substitute a complex integer multiplication-division unit, while the whole surface of the analogous printed circuit could work parallel.</p>\n\n<p>According to <a href=\"http://engineering.stackexchange.com/questions/3993/do-analog-fpgas-exist\">this</a> post, \"software rewirable\" analogous circuits, essentially \"analogous FPGAs\" already exist. Although the capacity of the FPGAs is highly below the capacity of the <a href=\"https://en.wikipedia.org/wiki/Application-specific_integrated_circuit\" rel=\"nofollow\">ASIC</a>s with the same size, maybe analogous chips for neural networks could also exist.</p>\n\n<p>I suspect, if it is correct, maybe even the real human brain model wouldn't be too far. It would still require a massively parallel system of costly analogous NN chips, but it seems to me not impossible.</p>\n\n<p>Could this idea work? Maybe there is even active research/development into this direction?</p>\n"}, "id": "2011"}, {"body": {"answer": "<p>There are a lot of systems which follow the ancient maxim: \"Always two there are; no more, no less. A master and an apprentice.\"</p>\n\n<p>In <a href=\"https://en.wikipedia.org/wiki/Reinforcement_learning\" rel=\"nofollow\">reinforcement learning</a> a class of such setups is called <a href=\"https://webdocs.cs.ualberta.ca/~sutton/book/ebook/node66.html\" rel=\"nofollow\">Actor-Critic-Method</a>. There you have a master, who's duty it is to create feedback for the actions of the apprentice, who acts in a given environment. This would be comparable to how a human learns some physical activity, like playing table tennis. You basically let your body do it's thing, but your consciousness evaluates how good the result is. </p>\n\n<p>The setup of <a href=\"https://en.wikipedia.org/wiki/AlphaGo\" rel=\"nofollow\">AlphaGo</a> might be even closer to <a href=\"https://en.wikipedia.org/wiki/Thinking,_Fast_and_Slow\" rel=\"nofollow\">Kahnemann's system 1 and system 2</a>. AlphaGo has two neural networks which provide actions and evaluations (system 1, fast, intuitiv, etc.) and the monte carlo tree search, which uses these actions and evaluations to prune a search tree and make a decision (system 2, deliberate, logical). </p>\n\n<p>In the end this kind of structure will pop up again and again, because it is often necessary to do some kind of classification or preprocessing on the raw data, before <em>your</em> algorithm can be run on it. You could frame the whole history of <a href=\"https://en.wikipedia.org/wiki/Symbolic_artificial_intelligence\" rel=\"nofollow\">gofai</a> as the story of how scientists thought system 1 should be easy and system 2 should be doable in a few decades, where the reality is that we have no idea how difficult system 2 is, because it turned out that system 1 is extremely difficult. </p>\n", "question": "<p>In my estimation we have two minds which manage to speak to each other in dialectic through a series of interrupts. Thus at any one time one of these systems is controlling master and inhabits our consciousness. The subordinate system controls context which is constantly being \"primed\" by our senses and our subordinate systems experience of our conscious thought process( see thinking fast and slow by Daniel Kahneman). Thus our thought process is constantly a driven one. Similarly this system works as a node in a community and not as a standalone thing.<br>\n I think what we have currently is \"artificial thinking\" which is abstracted a long way from what is described above. so my question is \"are there any artificial intelligence systems with an internal dialectical approach and with drivers and conceived above and which develop within a community of nodes? \" </p>\n"}, "id": "2013"}, {"body": {"answer": "<p>Others already mentioned:</p>\n\n<ul>\n<li>zero padding</li>\n<li>RNN</li>\n<li>recursive NN</li>\n</ul>\n\n<p>so I will add another possibility: using convolutions different number of times depending on the size of input. Here is an <a href=\"http://www.deeplearningbook.org/contents/convnets.html\" rel=\"nofollow\">excellent book</a> which backs up this approach:</p>\n\n<blockquote>\n  <p>Consider a collection of images, where each image has a different\n  width and height. It is unclear how to model such inputs with a weight\n  matrix of fixed size. Convolution is straightforward to apply; the\n  kernel is simply applied a different number of times depending on the\n  size of the input, and the output of the convolution operation scales\n  accordingly.</p>\n</blockquote>\n\n<p>Taken from page 360. You can read it further to see some other approaches.</p>\n", "question": "<p>As far as I can tell, neural networks have a <strong>fixed number of neurons</strong> in the input layer.</p>\n\n<p>If neural networks are used in a context like for example NLP, sentences or blocks of text of varying sizes are fed to a network. How is the <strong>varying input size</strong> reconciled with the <strong>fixed size</strong> of the input layer of the network? In other words: how is such a network made flexible enough to deal with an input that might be anywhere from one word to multiple pages of text?</p>\n\n<p>If my assumption of a fixed number of input neurons is wrong and new input neurons are added to/removed from the network to match the input size I don't see how these can ever be trained.</p>\n\n<p>I give the example of NLP, but lots of problems have an inherently unpredictable input size, I'm interested in the general approach for dealing with this.</p>\n\n<p>edit: For images, it's clear you can up/downsample to a fixed size, but for text this seems to be an impossible approach since adding/removing text changes the meaning of the original input.</p>\n"}, "id": "2014"}, {"body": {"answer": "<p>You could argue that some <a href=\"https://en.wikipedia.org/wiki/Multi-agent_system\" rel=\"nofollow\">Multi-Agent System</a> approaches do, and some systems based on the <a href=\"https://en.wikipedia.org/wiki/Blackboard_system\" rel=\"nofollow\">blackboard architecture</a> could conceivably fit this regime as well. </p>\n", "question": "<p>In my estimation we have two minds which manage to speak to each other in dialectic through a series of interrupts. Thus at any one time one of these systems is controlling master and inhabits our consciousness. The subordinate system controls context which is constantly being \"primed\" by our senses and our subordinate systems experience of our conscious thought process( see thinking fast and slow by Daniel Kahneman). Thus our thought process is constantly a driven one. Similarly this system works as a node in a community and not as a standalone thing.<br>\n I think what we have currently is \"artificial thinking\" which is abstracted a long way from what is described above. so my question is \"are there any artificial intelligence systems with an internal dialectical approach and with drivers and conceived above and which develop within a community of nodes? \" </p>\n"}, "id": "2018"}, {"body": {"answer": "<p><strong>No</strong>, with a <em>but</em>. We can have creative yet ethical problem-solving if the system has a complete system of ethics, but otherwise creativity will be unsafe by default.</p>\n\n<p>One can classify AI decision-making approaches into two types: interpolative thinkers, and extrapolative thinkers. </p>\n\n<p>Interpolative thinkers learn to classify and mimic whatever they're learning from, and don't try to give reasonable results outside of their training domain. You can think of them as interpolating between training examples, and benefitting from all of the mathematical guarantees and provisos as other statistical techniques.</p>\n\n<p>Extrapolative thinkers learn to manipulate underlying principles, which allows them to combine those principles in previously unconsidered ways. The relevant field for intuition here is <a href=\"https://en.wikipedia.org/wiki/Mathematical_optimization\">numerical optimization</a>, of which the simplest and most famous example is <a href=\"https://en.wikipedia.org/wiki/Linear_programming\">linear programming</a>, rather than the statistical fields that birthed machine learning. You can think of them as extrapolating beyond training examples (indeed, many of them don't even require training examples, or use those examples to infer underlying principles).</p>\n\n<p>The promise of extrapolative thinkers is that they can come up with these 'lateral' solutions much more quickly than people would be able to. The problem with these extrapolative thinkers is that they only use the spoken principles, not any unspoken ones that might seem too obvious to mention.</p>\n\n<p>An attribute of solutions to optimization problems is that the feature vector is often 'extreme' in some way. In linear programming, at least one vertex of the feasible solution space will be optimal, and so simple solution methods find an optimal vertex (which is almost infeasible by nature of being a vertex).</p>\n\n<p>As another example, the minimum-fuel solution for moving a spacecraft from one position to another is called '<a href=\"https://en.wikipedia.org/wiki/Bang%E2%80%93bang_control\">bang-bang</a>,' where you accelerate the craft as quickly as possible at the beginning and end of the trajectory, coasting at maximum speed in between.</p>\n\n<p>While a virtue when the system is correctly understood (bang-bang <em>is</em> optimal for many cases), this is catastrophic when the system is incorrectly understood. My favorite example here is <a href=\"https://resources.mpi-inf.mpg.de/departments/d1/teaching/ws14/Ideen-der-Informatik/Dantzig-Diet.pdf\">Dantzig's diet problem</a> (discussion starts on page 5 of the pdf), where he tries to optimize his diet using math. Under his first constraint set, he's supposed to drink 500 gallons of vinegar a day. Under his second, 200 bouillon cubes. Under his third, two pounds of bran. The considerations that make those obviously bad ideas aren't baked into the system, and so the system innocently suggests them.</p>\n\n<p>If you can completely encode the knowledge and values that a person uses to judge these plans into the AI, then extrapolative systems are as safe as that person. They'll be able to consider and reject the wrong sort of extreme plans, and leave you with the right sort of extreme plans.</p>\n\n<p>But if you can't, then it does make sense to not build an extrapolative decision-maker, and instead build an interpolative one. That is, instead of asking itself \"how do I best accomplish goal X?\" it's asking itself \"what would a person do in this situation?\". The latter might be much worse at accomplishing goal X, but it has much less of the tail risk of sacrificing other goals to accomplish X.</p>\n", "question": "<p>In the recent PC game <em><a href=\"http://www.theturingtestgame.com/\">The Turing Test</a></em>, the AI (\"TOM\") needs help from Ava to get through some puzzle rooms. TOM says he is unable to solve the puzzles because he is not allowed to \"<a href=\"https://en.wikipedia.org/wiki/Lateral_thinking\">think laterally</a>.\" Specifically, he says he would not have thought to throw a box through a window to solve the first room. His creators, the story goes, turned that capability off because such thinking could produce \"ethically suboptimal\" solutions, like chopping off an arm to leave on a pressure plate.</p>\n\n<p>Would all creative puzzle-solving abilities need to be removed from an AI to keep its results reasonable, or could we get some benefits of lateral thinking without losing an arm?</p>\n"}, "id": "2022"}, {"body": {"answer": "<p>You may consider the programming as an ethical part of the design as well. AI will act based on what has been instructed to it as ethically important or not.\nIt may/should even be part of the parameters that forge the process of finding solutions, which could allow for a more refine and creative solution.</p>\n\n<p>We understand the basics of ethic in normal circumstances, but if we can't predict how any human will behave in an ethical conundrum we can enforce what an AI wouldn't do.</p>\n\n<p>As long as we have control over the mechanism that drive an AI we sure have a responsability to inject ethical failsafes.\nThe problem lies in self taught AI with an ability to overrides directives.\n(CF Asimov Laws.)</p>\n\n<p>The way the AI is creative seems irrelevant in that case.</p>\n", "question": "<p>In the recent PC game <em><a href=\"http://www.theturingtestgame.com/\">The Turing Test</a></em>, the AI (\"TOM\") needs help from Ava to get through some puzzle rooms. TOM says he is unable to solve the puzzles because he is not allowed to \"<a href=\"https://en.wikipedia.org/wiki/Lateral_thinking\">think laterally</a>.\" Specifically, he says he would not have thought to throw a box through a window to solve the first room. His creators, the story goes, turned that capability off because such thinking could produce \"ethically suboptimal\" solutions, like chopping off an arm to leave on a pressure plate.</p>\n\n<p>Would all creative puzzle-solving abilities need to be removed from an AI to keep its results reasonable, or could we get some benefits of lateral thinking without losing an arm?</p>\n"}, "id": "2024"}, {"body": {"answer": "<p>A lot of this depends on the breadth of consideration. For example, what would the medium and long term effects of the lateral thinking be? The robot could sever an arm for a pressure plate but it would mean that the person no longer had an arm, a functional limitation at best, that the person might bleed out and die/be severely constrained, and that the person (and people in general) would both no longer cooperate and likely seek to eliminate the robot. People can think laterally because consider these things - ethics are really nothing more than a set of guidelines that encompass these considerations. The robot could as well, were it to be designed to consider these externalities.</p>\n\n<p>If all else fails,</p>\n\n<p>Asimov's Laws of Robotics: (0. A robot may not harm humanity, or, by inaction, allow humanity to come to harm.) 1. A robot may not injure a human being or, through inaction, allow a human being to come to harm. 2. A robot must obey orders given it by human beings except where such orders would conflict with the First Law. 3. A robot must protect its own existence as long as such protection does not conflict with the First or Second Law</p>\n", "question": "<p>In the recent PC game <em><a href=\"http://www.theturingtestgame.com/\">The Turing Test</a></em>, the AI (\"TOM\") needs help from Ava to get through some puzzle rooms. TOM says he is unable to solve the puzzles because he is not allowed to \"<a href=\"https://en.wikipedia.org/wiki/Lateral_thinking\">think laterally</a>.\" Specifically, he says he would not have thought to throw a box through a window to solve the first room. His creators, the story goes, turned that capability off because such thinking could produce \"ethically suboptimal\" solutions, like chopping off an arm to leave on a pressure plate.</p>\n\n<p>Would all creative puzzle-solving abilities need to be removed from an AI to keep its results reasonable, or could we get some benefits of lateral thinking without losing an arm?</p>\n"}, "id": "2025"}, {"body": {"answer": "<p>Neural networks <strong>don't</strong> model biological neurons. </p>\n\n<p>They are at best <em>inspired</em> by biological neurons, in that they get excited by certain inputs and fire once the excitation crosses a threshold. And this second point even holds only approximately because the backpropagation algorithm needs smoothed out steps to learn by gradient descent. And backpropagation is not even inspired by biology, that would rather be <a href=\"https://en.wikibooks.org/wiki/Artificial_Neural_Networks/Hebbian_Learning\">hebbian learning</a>. </p>\n\n<p>Generally in machine learning people do what works. The connection to biology is tenuous at best. You will usually not find one to one correspondence of low level details between machine learning setups and biological neurons. For that you'll have to turn to <a href=\"https://www.humanbrainproject.eu/sp6\">brain simulations</a>.</p>\n", "question": "<p>I understand that neural networks model biological neurons.  Each node in the network represents a neuron cell and the connections between nodes represent the connections between cells.  As in nature, a neuron fires an electrical signal to connected neurons based on some kind of threshold or function that mimics such.  </p>\n\n<p>Recent discoveries on how the brain works reveal the importance of calcium within the cells.  See <a href=\"http://link.springer.com/article/10.1007/BF01794675\" rel=\"nofollow\">http://link.springer.com/article/10.1007/BF01794675</a> for more information.  To summarize, calcium affects the regulation, stimulation and transmission of electrical activity as well as the destruction of neurones.</p>\n\n<p>From my study of neural networks, there does not seem to be a calcium equivalent.  Having one would imply that the functions, connections and weights in an artificial network are configured during the training and execution process and can change over time.   I understand that back-propagation is used to train the weights, but have not seen anything that trains the function nor the connections (although a zero weight could imply no connection).</p>\n\n<p>Does anyone know of such a network (or training algorithm)?  If so, do these networks perform better than a network that is pre-configured?</p>\n"}, "id": "2027"}, {"body": {"answer": "<p>I would strongly recommend that you check the book \"The Perfect Bet\" by Adam Kucharski. It does not mention technical methods such as neural networks but it gives a good history (and very nice stories) on what people had done on that field. It gives you the notion that in order to get achieve a better payoff, your goal is not actually making a better prediction but choosing the better options by considering what other players are doing. If you ask why it is not just the better prediction, the answer is that there is always a balance of risk and payoff and since you cannot find a 100% guaranteed way of prediction, you will have to balance risk and payoff in order to gain in the long run. In addition, although theory suggests that you can make a good prediction by gathering all variables, this is not really possible in practice. Thus, human assistance and observing what others are doing is used as a way to improve mathematical predictions and possible payoffs.</p>\n", "question": "<p>Were there any studies which checked the accuracy of neural network predictions of greyhound racing results, compared to a human expert? Would it achieve a better payoff?</p>\n"}, "id": "2031"}, {"body": {"answer": "<p>There are problems we for which we don't have a known, optimal, deterministic algorithm. By and large we use <a href=\"https://en.wikipedia.org/wiki/Heuristic\" rel=\"nofollow\">heuristics</a> to \"solve\" those problems.  A closely related idea is that of <a href=\"https://en.wikipedia.org/wiki/Satisficing\" rel=\"nofollow\">satisficing</a> where we seek out answers that are \"good enough\" for immediate purposes.</p>\n\n<p>Likewise, machines can also use heuristics, whether they are programmed in explicitly or, presumably, learned.  Within the range of ways that a machine can use heuristics, there are <a href=\"https://en.wikipedia.org/wiki/Metaheuristics\" rel=\"nofollow\">meta heuristics</a> and <a href=\"https://en.wikipedia.org/wiki/Hyper-heuristic\" rel=\"nofollow\">hyper heuristics</a>.</p>\n\n<p>Going beyond that, there are other ways that machines an learn \"algorithms\" or \"rules\" for solving problems.  One are that I'm particularly interested in is known as <a href=\"https://en.wikipedia.org/wiki/Rule_induction\" rel=\"nofollow\">rule induction</a>.  </p>\n\n<p>This is all an area of open and active research BTW... so if you're interested in exploring any of these approaches, you'll probably find a lot of ground to cover.  </p>\n", "question": "<p>I'm a freshman to machine learning. We all know that there are 2 kinds of problems in our life: problems that humans can solve and problems we can't solve. For problems humans can solve, we always try our best to write some algorithm and tell machine to follow it step by step, and finally the machine acts like people.</p>\n\n<p>What I'm curious about are these problems humans can't solve. If humans ourselves can't sum up and get an algorithm (which means that we ourselves don't know how to solve the problem), can a machine solve the problem? That is, can the machine sum up and get an algorithm by itself based on a large amount of problem data?</p>\n"}, "id": "2034"}, {"body": {"answer": "<p>New guy here, please go easy on me as this answer will come from personal experience, and will probably be a tad philosophical.</p>\n\n<p>Every algorithm I've designed was built to systematically tackle and solve specific problems in specific situations, each with an end goal in mind. Think of algorithms as solutions to a problem. In my career as a programmer, this rule has always stuck with me (it came from my favorite Computer Sciences professor): \"If there is <strong>no solution</strong>, then there is <strong>no algorithm</strong>. If there is <strong>no algorithm</strong>, <strong>no machine can solve the problem</strong>.\"</p>\n\n<p>Can machines generate their own algorithms? Most likely. But not to the point that it will exceed us (and by exceed, I don't mean just speed). AIs can never solve problems using methods that humans will never be able to come up with, because we programmed AIs to solve problems <em>just like us humans do</em>.</p>\n", "question": "<p>I'm a freshman to machine learning. We all know that there are 2 kinds of problems in our life: problems that humans can solve and problems we can't solve. For problems humans can solve, we always try our best to write some algorithm and tell machine to follow it step by step, and finally the machine acts like people.</p>\n\n<p>What I'm curious about are these problems humans can't solve. If humans ourselves can't sum up and get an algorithm (which means that we ourselves don't know how to solve the problem), can a machine solve the problem? That is, can the machine sum up and get an algorithm by itself based on a large amount of problem data?</p>\n"}, "id": "2035"}, {"body": {"answer": "<p>Whether or not a label fits any particular instance depends on what you're using the label for. If something specific is riding on whether this approach is a 'heuristic' or not, that context is important.</p>\n\n<p>But I wouldn't call this a heuristic, because I think of that as a shortcut for <em>solving</em> a problem, not either <em>storing</em> a solution or <em>reformulating</em> the problem (which is how I'd think of this). </p>\n", "question": "<p>This is a question about a nomenclature - we already have the algorithm/solution, but we're not sure whether it qualifies as utilizing heuristics or not.</p>\n\n<hr>\n\n<p>feel free to skip the problem explanation:</p>\n\n<blockquote>\n  <p>A friend is writing a path-finding algorithm - an autopilot for an\n  (off-road) vehicle in a computer game. This is a pretty classic\n  problem - he finds a viable, not necessarily optimal but \"good enough\"\n  route using the A* algorithm, by taking the terrain layout and vehicle\n  capabilities into account, and modifying a direct (straight) line path\n  to account for these. The whole map is known a'priori and invariant,\n  though the start and destination are arbitrary (user-chosen) and the\n  path is not guaranteed to exist at all.</p>\n  \n  <p>This cookie-cutter approach comes with a twist: limited storage space.\n  We can afford some more volatile memory on start, but we should free\n  most of it once the route has been found. The travel may take days -\n  of real time too, so the path must be saved to disk, and the space in\n  the save file for custom data like this is severely limited. Too\n  limited to save all the waypoints - even after culling trivial\n  solution waypoints ('continue straight ahead'), and by a rather large\n  margin, order of 20% the size of our data set.</p>\n  \n  <p>A solution we came up with is to calculate the route once on start,\n  then 'forget' all the trivial and 90% of the non-trivial waypoints.\n  This both serves as a proof that a solution exists, and provides a set\n  of points reaching which, in sequence, guarantees the route will take\n  us to the destination.</p>\n  \n  <p>Once the vehicle reaches a waypoint, the route to the next one is\n  calculated again, from scratch. It's known to exist and be correct\n  (because we did it once, and it was correct), it doesn't put too much\n  strain on the CPU and the memory (it's only about 10% the total route\n  length) and it doesn't need to go into permanent storage (restarting\n  from any point along the path is just a subset of the solution\n  connecting two saved waypoints).</p>\n</blockquote>\n\n<hr>\n\n<p>Now for the actual question:</p>\n\n<p>The pathfinding algorithm follows a sparse set of waypoints which by themselves are not nearly sufficient as a route, but allow for easy, efficient  calculation of the actual route, simultaneously guarantying its existence; they are a subset of the full solution. </p>\n\n<p>Is this a heuristic approach?</p>\n\n<p>(as I understand, normally, heuristics don't guarantee existence of a solution, and merely suggest more likely candidates. In this case, the 'hints' are taken straight out of an actual working solution, thus my doubts.)</p>\n"}, "id": "2038"}, {"body": {"answer": "<p>All intelligence, both human and machine, is mechanistic. Thoughts don't appear out of the blue; they're generated through specific processes.</p>\n\n<p>This means that if a machine generates an algorithm to solve a problem, even if the object-level algorithm wasn't generated by humans, the meta-level algorithm by which it generated the object-level algorithm must have come from <em>somewhere</em>, and that somewhere is probably its original creators. (Even if they didn't program the meta-level algorithm, they probably programmed the meta-meta-level algorithm that programmed the meta-level algorithm, and so on.)</p>\n\n<p>How you think about these distinctions depends on how you think about machine learning, but typically they're fairly small. For example, when we train a neural network to classify images, we aren't telling it what pixels to focus on or how to combine them, which is the object-level algorithm that it eventually generates. But we are telling it how to construct that object-level algorithm from training data, what I'm calling the 'meta-level' algorithm.</p>\n\n<p>One of the open problems is how to build the meta-meta-level; that is, an algorithm that will be able to look at a dataset and determine which models to train, and then which model to finally use. This will, ideally, include enough understanding of those meta-level models to construct new ones as needed, but even if it doesn't will reflect a major step forward in the usability of ML.</p>\n", "question": "<p>I'm a freshman to machine learning. We all know that there are 2 kinds of problems in our life: problems that humans can solve and problems we can't solve. For problems humans can solve, we always try our best to write some algorithm and tell machine to follow it step by step, and finally the machine acts like people.</p>\n\n<p>What I'm curious about are these problems humans can't solve. If humans ourselves can't sum up and get an algorithm (which means that we ourselves don't know how to solve the problem), can a machine solve the problem? That is, can the machine sum up and get an algorithm by itself based on a large amount of problem data?</p>\n"}, "id": "2039"}, {"body": {"answer": "<p>The approach you listed here is not really an approach, this is very very vague idea of how someone can achieve some task. You basically told we have an algorithm <code>f(image) = result</code> and there can be infinite amount of real approaches to solve this.</p>\n\n<p>In majority of CNN approaches the image travels through a convolution/pooling layers which reduces the dimensions of each current layer. In the end you end up with a significantly smaller layer which goes through the softmax and gets probabilities of different classes. This type of networks does not tell you where something was found, it just tells you that something was found somewhere in your original image.</p>\n", "question": "<p>I understand how a neural network can be trained to recognise certain features in an image (faces, cars, ...), where the inputs are the image's pixels, and the output is a set of boolean values indicating which objects were recognised in the image and which weren't.</p>\n\n<p>What I don't really get is, when using this approach to detect features and we detect a face for example, how we can go back to the original image and determine the location or boundaries of the detected face. How is this achieved? Can this be achieved based on the recognition algorithm, or is a separate algorithm used to locate the face? That seems unlikely since to find the face again, it needs to be recognised in the image, which was the reason of using a NN in the first place.</p>\n"}, "id": "2041"}, {"body": {"answer": "<p>If the AI can indeed assess arbitrary scenarios and come up with solutions to handle them, then it would indeed be an AGI.</p>\n\n<blockquote>\n  <p>What\u2019s the constraint ?</p>\n</blockquote>\n\n<p><em>It doesn't exist.</em>  Current programmers are very good at developing AI that can handle specific tasks (\"narrow AIs\"), but it is currently impossible to build an AI that can assess and handle \"general\" situations (unlike your proposed algorithm, which possess that capacity).</p>\n\n<p>Theoretically, we can have a program that can build other programs (<a href=\"https://en.wikipedia.org/wiki/Genetic_algorithm\" rel=\"nofollow\">genetic algorithms</a> are arguably one such example), but handling arbitrary scenarios and problems requires a form of \"general intelligence\", which we don't know how to program. Therefore, we can't build this machine.</p>\n\n<p>It's possible that we can built this machine, but we must first figure out  the hard problem of \"general intelligence\". We're nowhere near reaching that level.</p>\n\n<p><em>If</em> we figure out how to program \"general intelligence\", then it should be fairly simple to use your approach (building an AGI to assess scenarios and then build \"narrow AIs\" that can handle Those scenarios). <em>Only then</em> we can understand the AGI's limitations and weaknesses, and be able to identify probable constraints to its power. For example, it's possible that such an AGI may be slow in handling arbitrary scenarios and developing the \"narrow AIs\"...in which case, it may take an absurdly long period of time to develop \"a suite of AIs powerful/specialized for their tasks\".</p>\n\n<p>But until we build the AGI itself, we won't be able to identify its faults or weaknesses. Going beyond that would be science-fiction speculation.</p>\n", "question": "<p>If said AI can assess scenarios and decide what AI is best suited and construct new AI for new tasks. In sufficient time would the AI not have developed a suite of AIs powerful/specialized for their tasks, but versatile as a whole, much like our own brain\u2019s architecture? What\u2019s the constraint ?</p>\n"}, "id": "2042"}, {"body": {"answer": "<p>This problem is called <a href=\"https://en.wikipedia.org/wiki/Object_detection\" rel=\"nofollow\">object detection</a>.</p>\n\n<p>If you have a trainings set of images with boxed objects you can just train a neural network to directly predict the box. I.e. the output has the same dimension as the input and the NN learns to assign each pixel the probability of belonging to a certain object.</p>\n\n<p>If you don't have such a convenient dataset you could just recursively narrow the location down by feeding parts of the image to the network until you find the smallest part that still fully activates a certain classification. </p>\n\n<p><a href=\"https://pdfs.semanticscholar.org/713f/73ce5c3013d9fb796c21b981dc6629af0bd5.pdf\" rel=\"nofollow\">In this paper</a> they try a mixture of these two approaches.</p>\n", "question": "<p>I understand how a neural network can be trained to recognise certain features in an image (faces, cars, ...), where the inputs are the image's pixels, and the output is a set of boolean values indicating which objects were recognised in the image and which weren't.</p>\n\n<p>What I don't really get is, when using this approach to detect features and we detect a face for example, how we can go back to the original image and determine the location or boundaries of the detected face. How is this achieved? Can this be achieved based on the recognition algorithm, or is a separate algorithm used to locate the face? That seems unlikely since to find the face again, it needs to be recognised in the image, which was the reason of using a NN in the first place.</p>\n"}, "id": "2044"}, {"body": {"answer": "<p>To build on Tariq Ali's answer... </p>\n\n<p>There's no such thing as an AGI. The No Free Lunch (NFL) theorem states essentially that: <em>any two optimization algorithms are equivalent when their performance is averaged across all possible problems.</em> Specialization implies a <em>loss</em> of generality, not a <em>gain</em>.</p>\n\n<p>With this AI generating AI, you're describing what I call an 'arbitrary machine generator' (AMG).</p>\n\n<p>There are two types of AMGs: a species level and an individual level.</p>\n\n<p>All species that evolve on earth are AMG - they can evolve to accommodate arbitrary niches, if the correct environmental constraints are present. This is proven by the fact that the species AMG processes on earth have produce humans, which are individual level AMGs. Individual level AMGs can produce arbitrary machines for arbitrary purposes on human time-scales.</p>\n\n<p>The problem is that the simplest possible (and most general) AMG is a purely <em>random</em> machine generator. Any more specificity (and therefore complexity) to the AMG would constrain the domains it closes over. Which is fine, but optimizing a machine for a particular set of tasks means that you are <em>unoptimizing</em> the machine for some other particular set of tasks. Again, there's no free context.</p>\n\n<p>Humans are AMGs, but we are only efficient at creating certain kinds of machines, using our imagination. Our imagination is built on a number of cognitive tools that, on the one hand, constrain what machines we can efficiently imagine, while, on the other hand, avail us to the open-ended set of all possible machines, via prior knowledge or brute force, random lookup.</p>\n\n<p>In summary, when people say <em>\"general intelligence\"</em>, they really mean <em>\"human-like intelligence\"</em>. And, again, while human intelligence <em>is</em> an AMG, any given AMG that is optimized for generating machines of a particular type will be less optimized for generating machines of some other type(s). There's no free context. The most general search algorithm is a random walk - there is no way to <em>improve</em> the generality of the random walk, other than just speeding it up. And that's actually what humans do for many problems anyway - brute force, random searching, as fast as possible.</p>\n", "question": "<p>If said AI can assess scenarios and decide what AI is best suited and construct new AI for new tasks. In sufficient time would the AI not have developed a suite of AIs powerful/specialized for their tasks, but versatile as a whole, much like our own brain\u2019s architecture? What\u2019s the constraint ?</p>\n"}, "id": "2045"}, {"body": {"answer": "<p>Ethics involves the relationships of <em>needs</em> between two or more parties. As Matthew Graves said, if the AI lacks the sufficient human context (understanding of needs), it will produce seemingly perverse ethical behavior.</p>\n\n<p>And let's be honest, some <em>people</em> would cut of other people's arms and put them on pressure plates. Even the best of us will not be able to sympathize with the needs of others with 100% accuracy - at best, we're guessing. And then there are those rare situations where I actually <em>want</em> you to cut off my arm and put it on a pressure plate, perhaps to save a loved one.</p>\n\n<p>If we could make a thing that could sympathize with what a human <em>might</em> need in any given arbitrary situation, then we will have created either A) an artificial <em>human</em> intelligence (AHI) (which could be more or less fallible, like a human), or B) an oracle that can reason about <em>all possible human needs</em> on much faster than human time-scales - in which case you wouldn't <em>need</em> a conscious AI, as all human needs and solutions could be pre-computed via formal specification, which is probably absurd to consider.</p>\n", "question": "<p>In the recent PC game <em><a href=\"http://www.theturingtestgame.com/\">The Turing Test</a></em>, the AI (\"TOM\") needs help from Ava to get through some puzzle rooms. TOM says he is unable to solve the puzzles because he is not allowed to \"<a href=\"https://en.wikipedia.org/wiki/Lateral_thinking\">think laterally</a>.\" Specifically, he says he would not have thought to throw a box through a window to solve the first room. His creators, the story goes, turned that capability off because such thinking could produce \"ethically suboptimal\" solutions, like chopping off an arm to leave on a pressure plate.</p>\n\n<p>Would all creative puzzle-solving abilities need to be removed from an AI to keep its results reasonable, or could we get some benefits of lateral thinking without losing an arm?</p>\n"}, "id": "2046"}, {"body": {"answer": "<p>If the universe is discrete, then analog phenomena (fluidity, curvature) are built on primitively discrete phenomena (bits and pieces).</p>\n\n<p>If the universe is continuous, then discrete phenomena (bits and pieces) are built on primitively continuous phenomena (fluidity, curvature).</p>\n\n<p>If the universe is discrete, the speed of seemingly analog phenomena will be bounded by the number of discrete phenomena that can occur in time and space.</p>\n\n<p>If the universe is continuous, then time, space or matter <em>may</em> be infinitely divisible, which <em>may</em> allow for the execution of some phenomena faster than those phenomena <em>appear</em> to execute in natural environments (like protein folding or electric circuits) - so called \"super Turing\" computation.</p>\n\n<p>The continuous universe idea begs the question, though: From whence came all this discreteness? A discrete universe can allow for apparent continuous behavior via approximation and randomness (or pseudorandomness), whereas a universe that is infinitely divisible affords no obvious definition of where things should start and end. This is one of the reasons many thinkers eschew considering infinities - they may be illusory.</p>\n\n<p>So, can analog \"circuits\" execute faster than digital? As of right now, we know of some <em>seemingly</em> analog phenomena that <em>appear</em> execute faster than some digital phenomena (like electron spin vs a silicon logic gate). Whether analog phenomena are <em>intrinsically</em> more efficient than digital depends on the actual nature of the universe, which we have not yet determined.</p>\n", "question": "<p>Considering the answers of <a href=\"http://ai.stackexchange.com/questions/1314/how-powerful-a-computer-is-required-to-simulate-the-human-brain\">this</a> question, emulating a human brain with the current computing capacity is currently impossible, but we aren't very far from it.</p>\n\n<p>Note, 1 or 2 decades ago, similar calculations had similar results.</p>\n\n<p>The clock frequency of the modern CPUs seem to be stopped, currently the miniaturization (-> mobile use), the RAM/cache improvement and the multi-core paralellization are the main lines of the development.</p>\n\n<p>Ok, but what is the case with the analogous chips? In case of a NN, it is not a very big problem, if it is not very accurate, the NN would adapt to the minor manufacturing differences in its learning phase. And a single analogous wire can substitute a complex integer multiplication-division unit, while the whole surface of the analogous printed circuit could work parallel.</p>\n\n<p>According to <a href=\"http://engineering.stackexchange.com/questions/3993/do-analog-fpgas-exist\">this</a> post, \"software rewirable\" analogous circuits, essentially \"analogous FPGAs\" already exist. Although the capacity of the FPGAs is highly below the capacity of the <a href=\"https://en.wikipedia.org/wiki/Application-specific_integrated_circuit\" rel=\"nofollow\">ASIC</a>s with the same size, maybe analogous chips for neural networks could also exist.</p>\n\n<p>I suspect, if it is correct, maybe even the real human brain model wouldn't be too far. It would still require a massively parallel system of costly analogous NN chips, but it seems to me not impossible.</p>\n\n<p>Could this idea work? Maybe there is even active research/development into this direction?</p>\n"}, "id": "2047"}, {"body": {"answer": "<p>If you were to completely automate a human, you'd just have another human, which defeats the purpose of the automation.</p>\n\n<p>Any job that requires a \"whole human,\" rather than just a human's hands, feet, or simple reasoning ability, will still require humans.</p>\n\n<p>If I go to a shrink, one with Wikipedia-like knowledge would be great, but one that also actually knows what its like to rub its eyes in the morning would be even better. Why? Because solving <em>some</em> problems will require knowing what it is like to rub one's eyes in the morning.</p>\n\n<p>If I go to a movie that was written, directed and produced by some form of automation, I may be able to suspend my disbelief and get carried away by the story, but something in me will fundamentally appreciate the movie less, if I know that the AI can produce an infinite number of these stories, completely arbitrarily. There is something about knowing that the story came from a mind that has been conditioned against the vagaries of humanity (ie, a \"whole human\"), that makes the story more appreciable.</p>\n\n<p>If I call up a suicide hotline because I wan't someone to sympathize with me about my existential crisis, I'll want to talk to a \"whole human\" that can <em>sympathize</em> with my existential condition, not one that just regurgitates prior wisdom on life, heuristically matched against my problem state.</p>\n\n<p>If I want to vote for a politician that can sympathize with the needs of the people, I'll want a \"whole person\" politician that can reflect on <em>all</em> the specifics that make life for a \"whole person\" hard or easy.</p>\n\n<p>If I want soldiers to take the lives of humans, I want some sort of intelligence in that kill-chain that executes \"whole person\" analysis prior to pulling the trigger (a human).</p>\n\n<p>If I want a conflict resolution specialist, capable of resolving complex cultural problems between humans, then I don't want just an AI that spits out the most likely solution based on prior solutions. I want an AI that can reason about prior solutions <em>and</em> all explicit and implicit problems between humans, in all human contexts, which requires a human or a perfect human simulacrum.</p>\n\n<p>For any problem that requires consideration potentially <em>across</em> the whole spectrum of human context, we will want that solution to be generated by a \"whole human\" device. But if we automate the \"whole human\" then we haven't really outsourced the problem to automation but rather to a \"whole automated human,\" which will, by necessity, have its own problems.</p>\n\n<p>Sure, we'll probably create an artificial human intelligence (AHI) one day, but being optimized to automatically solve <em>any</em> given human problem without also <em>having</em> human problems... that's just AI snake-oil that will never exist - outside of some perverse matrix scenario, under an infinite oracle of some sort.</p>\n\n<p>So, yes, there will be many jobs that still require humans - mostly human-to-human problems that require full knowledge of the human context.</p>\n", "question": "<p>AI is progressing drastically, and imagine they tell you you're fired because a robot will take your place. What are some jobs that can never be automated?</p>\n"}, "id": "2049"}, {"body": {"answer": "<blockquote>\n  <p>What are some jobs that can never be automated?</p>\n</blockquote>\n\n<p><strong>None.</strong></p>\n\n<p>The key word here is \"never\". Technology is rapidly advancing, and while I can think of situations where jobs can't be killed in the short-term or even in the long-term, I can't think of a job that is 100%, totally immune to extinction. Surely they <em>exist</em>, but you can't be <em>sure</em>...anything can happen after all. As long as it's <em>possible</em>, that's what matters here. You can't prove a negative.</p>\n\n<p>This whole question seems as foolhardy as predicting in the 1850s that airplanes would <em>never</em> be invented. You'd be right in assuming that airplanes would not be invented in the 1860s...or the 1870s...or even the 1880s...but eventually, airplanes would be invented.</p>\n\n<p>What would be better is to provide a specific cut-off point (\"will all jobs be automated by the year 2020?\") that can allow us to try to extrapolate and predict based on current trends, but even that starts being difficult as you extend the cut-off point - My predictions about 2020 will be more accurate than my prediction about 2220. I think this type of question is truly unanswerable and can quickly decay to science-fiction speculation.</p>\n\n<hr>\n\n<p>Some additional comments about Doxosphoi's answer:</p>\n\n<p>Doxosophoi made some arguments for why current society might not accept the automation of all jobs (the need for the \"personal touch\" that only a human-like intelligence can provide), but that's no reason to assume that society will <em>never</em> accept automation. Technology can change and adapt, and humans can also change and adapt. Maybe a human might not care about a shrink who \"rubs its eyes in the morning\", dislike movies that are marred by that \"vagaries of humanity\" instead of personal customization, prefer politicians and soldiers that actually acts logically instead of acting like a falliable human, etc., etc. I mean, it's <em>possible</em>.</p>\n\n<p>There's also the problem of the term \"job\". Technically, I am working by writing an answer on a StackExchange website, but I'm not getting paid for it, so it's not a real \"job\"...at best, it's just a hobby. I'm providing a valuable human touch, but since no one is giving me money, it's possible that this human touch may not be all that valuable in the first place: \"never give out your labor for free, because then they'll take it for free\".</p>\n\n<p>Some of the techno-utopists (which I disagree with heavily) believes in a future where bots handle produce a lot of industrial goods and services, generating a lot of revenue that is then redistributed to the general population via some \"Basic Income\" scheme. This allows humans to do what they really want instead...such as hobbies? And what if the hobbies of the future are the \"jobs\" of today: shrinks, movie directors, politicians, soldiers, etc., etc. Instead of working for a paycheck, you're working in these jobs on a volunteer basis.</p>\n\n<p>Obviously, no automation is being necessary to eliminate these hobbyists (no matter how good a bot is, <em>free labor</em> will always prevail), but they're not really jobs, are they? The bot is the one that is producing real value, and subsidizing the hobbies of all these other people. The idea of a \"job\" itself could be in jeopardy.</p>\n\n<p>I don't think this scenario is likely either (in fact, I'd probably think it's just AI snake oil that will never actually happen). But it's <em>possible</em> and that's why I can't dismiss it outright. It could happen, just that I don't think it will.</p>\n\n<p>And finally, the question is asking about whether a job can be automated, not <em>whether it's a good idea</em> to have it be automated, which is a completely different question. It's possible that we can build machines that can automate everything, and choose as a society not to use them for a variety of different reasons (such as the reasons that Doxosophoi mentioned).</p>\n", "question": "<p>AI is progressing drastically, and imagine they tell you you're fired because a robot will take your place. What are some jobs that can never be automated?</p>\n"}, "id": "2050"}, {"body": {"answer": "<p>The Oxford study from 2013 in <a href=\"http://www.oxfordmartin.ox.ac.uk/downloads/academic/The_Future_of_Employment.pdf\">The future of employment</a> paper assess this and estimated the probability of computerisation for 702 detailed occupations using a Gaussian process classifier (using job data from the UK partially merged with data from US), and based on these estimates they identified three areas of computerisation bottleneck areas and nine skills that people are still needed for each profession, this includes:</p>\n\n<ul>\n<li><p>Perception and Manipulation.</p>\n\n<ul>\n<li><p>Finger dexterity.</p>\n\n<blockquote>\n  <p>The ability to make precisely coordinated movements of\n  the fingers of one or both hands to grasp, manipulate, or\n  assemble very small objects.</p>\n</blockquote></li>\n<li><p>Manual dexterity.</p>\n\n<blockquote>\n  <p>The ability to quickly move your hand, your hand together\n  with your arm, or your two hands to grasp, manipulate, or\n  assemble objects.</p>\n</blockquote></li>\n<li><p>The need for a cramped work space.</p>\n\n<blockquote>\n  <p>How often does this job require working in cramped work\n  spaces that requires getting into awkward positions?</p>\n</blockquote></li>\n</ul></li>\n<li><p>Creative Intelligence.</p>\n\n<ul>\n<li><p>Originality.</p>\n\n<blockquote>\n  <p>The ability to come up with unusual or clever ideas about\n  a given topic or situation, or to develop creative ways to\n  solve a problem.</p>\n</blockquote></li>\n<li><p>Fine arts.</p>\n\n<blockquote>\n  <p>Knowledge of theory and techniques required to compose,\n  produce, and perform works of music, dance, visual arts,\n  drama, and sculpture.</p>\n</blockquote></li>\n</ul></li>\n<li><p>Social Intelligence.</p>\n\n<ul>\n<li><p>Social perceptiveness.</p>\n\n<blockquote>\n  <p>Being aware of others\u2019 reactions and understanding why\n  they react as they do.</p>\n</blockquote></li>\n<li><p>Negotiation.</p>\n\n<blockquote>\n  <p>Bringing others together and trying to reconcile\n  differences.</p>\n</blockquote></li>\n<li><p>Persuasion.</p>\n\n<blockquote>\n  <p>Persuading others to change their minds or behavior.</p>\n</blockquote></li>\n<li><p>Assisting and caring for others.</p>\n\n<blockquote>\n  <p>Providing personal assistance, medical attention, emotional\n  support, or other personal care to others such as\n  coworkers, customers, or patients.</p>\n</blockquote></li>\n</ul></li>\n</ul>\n\n<p><sup>Source: <a href=\"http://web.archive.org/web/20161001101136/http://www.oxfordmartin.ox.ac.uk/downloads/academic/The_Future_of_Employment.pdf\">The future of employment: how susceptible are jobs to computerisation</a>: Table 1.</sup></p>\n\n<p>What this study is basically saying, around 50% of all jobs will be replaced by robots in the next 20 years.</p>\n\n<p>Based on the above study, the BBC assembled a handy guide that calculates which jobs are likely to be automated within the next two decades:</p>\n\n<ul>\n<li><a href=\"http://www.bbc.co.uk/news/technology-34066941\">Will a robot take your job?</a></li>\n</ul>\n\n<p>See also: <a href=\"http://www.replacedbyrobot.info/\">replacedbyrobot.info</a> website.</p>\n\n<blockquote>\n  <p>With this tool, you can check the prediction of over 700 jobs.</p>\n</blockquote>\n\n<p>Related:</p>\n\n<ul>\n<li><a href=\"https://www.reddit.com/r/AskReddit/comments/4mikie/when_robots_can_do_all_manual_labor_and_service/\">When robots can do all manual labor and service jobs, what will the majority human population do?</a></li>\n<li><p><a href=\"https://medium.com/@tylerehc/labore-ad-infinitum-ai-automation-vs-timeless-tasks-ced2216f2ab7\">Labore Ad Infinitum: AI &amp; Automation vs Timeless Tasks</a></p>\n\n<p>Which suggests: Military/Peacekeeper, Athletes, Therapist, Musical Performer, Actors and Dancer, Visual Artists, Religious/Spiritual Leaders, The World\u2019s Oldest Profession, Virtual Goods, Politicians, Judges, Parenting.</p></li>\n</ul>\n", "question": "<p>AI is progressing drastically, and imagine they tell you you're fired because a robot will take your place. What are some jobs that can never be automated?</p>\n"}, "id": "2052"}, {"body": {"answer": "<p>I'd like to offer also a slightly different view on the machine cannot better its master.  Consider the very simple case of content classifiers.  It's already to the point where for some areas classification and prediction can be performed way better than a human.  And while a human may have designed the \"algorithm\",  the algorithm was likely a recurrent neutral network or other form of ML that could well have self trained. In these cases we don't actually understand or need to understand the individual weights in the net,  as we would need to have traditionally understood the imperative programming constructs we used to write. It just works.</p>\n\n<p>So if we get to where we develop a meta-algorithm for classifying problems and building more optimal deep learning solutions than we would by hand, but I think that would pretty much take us out of the picture for quite a lot of problem spaces.  Thoughts? </p>\n", "question": "<p>I'm a freshman to machine learning. We all know that there are 2 kinds of problems in our life: problems that humans can solve and problems we can't solve. For problems humans can solve, we always try our best to write some algorithm and tell machine to follow it step by step, and finally the machine acts like people.</p>\n\n<p>What I'm curious about are these problems humans can't solve. If humans ourselves can't sum up and get an algorithm (which means that we ourselves don't know how to solve the problem), can a machine solve the problem? That is, can the machine sum up and get an algorithm by itself based on a large amount of problem data?</p>\n"}, "id": "2053"}, {"body": {"answer": "<p><a href=\"https://people.eecs.berkeley.edu/~pabbeel/\" rel=\"nofollow\">Peter Abbeel</a> does work in deep learning for robotics, and one of the projects they've tackled is manipulating clothes. Here's a <a href=\"https://www.youtube.com/watch?v=5FGVgMsiv1s\" rel=\"nofollow\">video from 2011</a> of their robot folding laundry (one piece at a time).</p>\n\n<p>There are also companies attempting to market this; seven dreamers makes the <a href=\"https://laundroid.sevendreamers.com/?lang=en\" rel=\"nofollow\">Laundroid</a> and <a href=\"https://www.foldimate.com/\" rel=\"nofollow\">Foldimate</a> claims that it will start taking pre-orders in 2017.</p>\n", "question": "<p>I'm wondering how feasible it is to create a machine that can separate clothing from a basket.</p>\n\n<p>At the most basic level it would distinguish between tops, pants, button downs and socks</p>\n\n<p>Programmatically, I'd image this would require training a neural network to recognize these items, but in real time it becomes exponentially difficult to do this in a small space at a fast rate:</p>\n\n<ol>\n<li>pick up an item</li>\n<li>lay it in such a way that is recognizable </li>\n<li>deduce whether it is a top, button down, etc.</li>\n<li>sort it accordingly</li>\n</ol>\n\n<p>If this sounds ridiculous please let me know...</p>\n\n<p>If it is possible :</p>\n\n<p>would this be based on some sort of computer vision?\nor only a well trained neural network?</p>\n\n<p>Any insight is much appreciated!</p>\n"}, "id": "2057"}, {"body": {"answer": "<p>I haven't done the connection - didn't know about the pyramid. I'm not sure it translates well into AI though.</p>\n\n<p>It seems they're separating information from knowledge by splitting how from what. What is a superset of how, as far as I'm concerned. It's also a superset of why.</p>\n\n<p>But from an evolutionary perspective, knowledge representation starts with why. Prior to a reason for knowledge representation, there is no knowledge representation. The 'what' existed, but it was not represented until autopoiesis created goal directed, why-oriented behaviors that began storing the what as knowledge. </p>\n\n<p>What is a superset of why, just as ontology is a superset of teleology. However, all <em>represented</em> ontology was acquired through teleological (end-directed) action.</p>\n\n<p>So I disagree with the notion that wisdom, as a why thing, is at the tip of the pyramid. It all started with goal directed behavior and that has been the source of all subsequent information growth.</p>\n\n<p>So what is wisdom? I think it is too much of a folk term to warrant a technical definition. If I had to just take a swing at a definition, though, I'd probably vote for wisdom being knowledge of the ontological basis of one's own teleological knowledge - essentially objectifying one's subjective interpretations - knowing the true what and how of the why, to whatever extent is possible.</p>\n\n<p>I don't have many <em>specific</em> references on this subject, but I thought Terrence Deacon's <a href=\"https://en.wikipedia.org/wiki/Incomplete_Nature\" rel=\"nofollow\">Incomplete Nature: How Mind Emerged from Matter</a> was a good primer on teleology.</p>\n", "question": "<p>For years I have been dealing with (and teaching) Knowledge Representation and Knowledge Representation languages. I just discovered that in another community (Information Systems and the such) there is something called the \"DIKW pyramid\" where they add another step after knowledge, namely wisdom.\nThey define data as being simply symbols, information as being the answer to who/what/when/where?, knowledge as being the answer to how?, and wisdom as being the answer to why?. </p>\n\n<p>My question is: has anyone done the connection between what AI calls data/information/knowledge and these notions from Information Systems? In particular, how would \"wisdom\" be defined in AI? And since we have KR languages, how would we represent \"wisdom\" as they define it?</p>\n\n<p>Any references would be welcome\u2026</p>\n"}, "id": "2058"}, {"body": {"answer": "<p>From the abstract of this paper <a href=\"https://arxiv.org/pdf/1308.0850v5.pdf\" rel=\"nofollow\"><em>Generating Sequences With Recurrent Neural Networks</em></a><em>[pdf]</em> </p>\n\n<blockquote>\n  <p>This paper shows how Long Short-term Memory recurrent neural networks\n  can be used to generate complex sequences with long-range structure,\n  simply by predicting one data point at a time. The approach is\n  demonstrated for text (where the data are discrete) and online\n  handwriting (where the data are real-valued). It is then extended to\n  handwriting synthesis by allowing the network to condition its\n  predictions on a text sequence. The resulting system is able to\n  generate highly realistic cursive handwriting in a wide variety of\n  styles.</p>\n</blockquote>\n\n<p>An implementation is here: <a href=\"https://github.com/szcom/rnnlib#handwriting-synthesis\" rel=\"nofollow\">handwriting-synthesis</a></p>\n\n<p>And using TensorFlow: <a href=\"http://blog.otoro.net/2015/12/12/handwriting-generation-demo-in-tensorflow/\" rel=\"nofollow\">Handwriting Generation Demo in TensorFlow</a></p>\n", "question": "<p>I want to have a program that writes like a human. But I don't just want a font, but instead an 'intelligent' program that produce different result and that can be trained with different sets to generate different handwritings.\nAs a training set I would like to have parts of a handwritten text (saved as a list of paths (like in vector graphics).\nMaybe as a means to simplify things, I could flatten the paths in to consecutive straight lines. My program receives a string of text and produces a list of paths (or a vector graphic, whatever is easier to work with)</p>\n\n<p>My question now is: What kind of machine learning would be best to achieve this?</p>\n"}, "id": "2059"}, {"body": {"answer": "<p>As with another answer, I am also skeptical of the distinctions made in the DIKW pyramid.</p>\n\n<p>Nonetheless, a very popular machine learning approach for answering 'Why?' questions is the application of Bayesian reasoning: given a causal data model, reverse inference can be used to find the probability distribution of events which lead to a given outcome.</p>\n\n<p>It could be argued that defining 'cause' in terms of distributions rather than specific concrete mechanisms is a rather limited notion of 'Why?'.</p>\n\n<p>However, it may be that there are some forms of causality that we don't know how to represent, specifically 'first-hand experience'. Indeed, common usage of the term 'wisdom' generally refers to first-hand experience, rather than information gained from some other source. </p>\n\n<p>The idea is that knowledge can be expressed declaratively, whereas wisdom must be derived from experience.</p>\n\n<p>For an AI represented as a computer program, the distinction between declarative and first-hand experience might appear irrelevant, since in principle any experience can be encoded and made available without the program having to 'experience' it first-hand.</p>\n\n<p>However, the following humorous definition of `wisdom' might perhaps shed some light on a distinction that's pertinent to AI research:</p>\n\n<blockquote>\n  <p>Knowledge is knowing that a tomato is a fruit.</p>\n  \n  <p>Wisdom is knowing that you shouldn't eat it with custard.</p>\n</blockquote>\n\n<p>This notion of 'Wisdom' could be said to require <a href=\"https://en.wikipedia.org/wiki/Qualia\" rel=\"nofollow\">qualia</a>. It is the subject of much debate whether qualia exist and/or are necessary for consciousness - see for example the thought experiment of <a href=\"https://en.wikipedia.org/wiki/Knowledge_argument\" rel=\"nofollow\">'The Black and White Room'</a>.</p>\n\n<p>So the notion is that there is a distinction between having a Bayesian network representation of wisdom that says: \"It is 99.7% likely that putting a tomato in custard is undesirable\" and the first-hand experience to the effect that it tastes odd with custard.</p>\n", "question": "<p>For years I have been dealing with (and teaching) Knowledge Representation and Knowledge Representation languages. I just discovered that in another community (Information Systems and the such) there is something called the \"DIKW pyramid\" where they add another step after knowledge, namely wisdom.\nThey define data as being simply symbols, information as being the answer to who/what/when/where?, knowledge as being the answer to how?, and wisdom as being the answer to why?. </p>\n\n<p>My question is: has anyone done the connection between what AI calls data/information/knowledge and these notions from Information Systems? In particular, how would \"wisdom\" be defined in AI? And since we have KR languages, how would we represent \"wisdom\" as they define it?</p>\n\n<p>Any references would be welcome\u2026</p>\n"}, "id": "2061"}, {"body": {"answer": "<p>Autonomous vacuum cleaners usually have these following task environment properties:</p>\n\n<p>i. Partially observable environment</p>\n\n<p>ii. Deterministic environment.</p>\n\n<p>iii. Sequential environment.</p>\n\n<p>iv. Static environment.</p>\n\n<p>v. Discrete environment</p>\n\n<p>vi. Single agent environment</p>\n\n<p>Since it's a partially observable environment, the agent performs its actions based on what is sees and thus it's is in an deterministic environment where the current action will be the result of previous actions. And since, the agent is continuously performing, it's an sequential environment. Since, the environment doesn't change when agent is working, the environment is static and discrete since there are only finite no. of discrete states in which agent can be in. </p>\n\n<p>There are certain rules written which tells an agent how to react when in a particular state. If there are multiple rules which are satisfied, then agent uses its experience to choose an optimal action to perform. The rules are written by the programmer keeping in mind the task envt. properties.</p>\n\n<p>The agent's actions also depend on the type of agent it is decided by the programmer. It can be simple reflex-based, or a goal based, or a goal-based + feedback or a complete learning based agent. An agent can't use A* algorithm because the entire environment is not visible and it will be useless to use A* algorithm where we don't know when our goal may be reached.  </p>\n\n<p>The agent has various sensors attached which give it info. about the surrounding, like cameras, sound recorders, thermal sensors, etc. An autonomous vacuum cleaner may also have a dirt sensor to detect the dirt. The agent performs an action using one of its actuators like wheels, robot arms, etc. </p>\n", "question": "<p>How does a domestic autonomous robotic vacuum cleaner -  such as a <a href=\"https://en.wikipedia.org/wiki/Roomba\" rel=\"nofollow\">Roomba</a> - know when it's working cleaned area (aka virtual map), and how does it plan to travel to the areas which hasn't been explored yet?</p>\n\n<p>Does it use some kind of <a href=\"https://en.wikipedia.org/wiki/A*_search_algorithm\" rel=\"nofollow\">A*</a> algorithm?</p>\n"}, "id": "2062"}, {"body": {"answer": "<p>A 'heuristic' is simply a 'rule-of-thumb', i.e. something which doesn't guarantee an optimal solution to a problem.</p>\n\n<p>Beyond the above notion (certainly within the discipline of optimization), the notion of what constitutes a heuristic is not particularly strict, and could certainly include hints for constructing a new solution from parts of a previous one, as you are doing.</p>\n\n<p>A related concrete example is the \"nearest neighbour heuristic\" for the Travelling Salesman Problem, in which a solution is constructed by starting at some random city and iteratively choosing the nearest. The resulting completed tour is then used as an initial input to some more sophisticated optimization procedure.</p>\n", "question": "<p>This is a question about a nomenclature - we already have the algorithm/solution, but we're not sure whether it qualifies as utilizing heuristics or not.</p>\n\n<hr>\n\n<p>feel free to skip the problem explanation:</p>\n\n<blockquote>\n  <p>A friend is writing a path-finding algorithm - an autopilot for an\n  (off-road) vehicle in a computer game. This is a pretty classic\n  problem - he finds a viable, not necessarily optimal but \"good enough\"\n  route using the A* algorithm, by taking the terrain layout and vehicle\n  capabilities into account, and modifying a direct (straight) line path\n  to account for these. The whole map is known a'priori and invariant,\n  though the start and destination are arbitrary (user-chosen) and the\n  path is not guaranteed to exist at all.</p>\n  \n  <p>This cookie-cutter approach comes with a twist: limited storage space.\n  We can afford some more volatile memory on start, but we should free\n  most of it once the route has been found. The travel may take days -\n  of real time too, so the path must be saved to disk, and the space in\n  the save file for custom data like this is severely limited. Too\n  limited to save all the waypoints - even after culling trivial\n  solution waypoints ('continue straight ahead'), and by a rather large\n  margin, order of 20% the size of our data set.</p>\n  \n  <p>A solution we came up with is to calculate the route once on start,\n  then 'forget' all the trivial and 90% of the non-trivial waypoints.\n  This both serves as a proof that a solution exists, and provides a set\n  of points reaching which, in sequence, guarantees the route will take\n  us to the destination.</p>\n  \n  <p>Once the vehicle reaches a waypoint, the route to the next one is\n  calculated again, from scratch. It's known to exist and be correct\n  (because we did it once, and it was correct), it doesn't put too much\n  strain on the CPU and the memory (it's only about 10% the total route\n  length) and it doesn't need to go into permanent storage (restarting\n  from any point along the path is just a subset of the solution\n  connecting two saved waypoints).</p>\n</blockquote>\n\n<hr>\n\n<p>Now for the actual question:</p>\n\n<p>The pathfinding algorithm follows a sparse set of waypoints which by themselves are not nearly sufficient as a route, but allow for easy, efficient  calculation of the actual route, simultaneously guarantying its existence; they are a subset of the full solution. </p>\n\n<p>Is this a heuristic approach?</p>\n\n<p>(as I understand, normally, heuristics don't guarantee existence of a solution, and merely suggest more likely candidates. In this case, the 'hints' are taken straight out of an actual working solution, thus my doubts.)</p>\n"}, "id": "2065"}, {"body": {"answer": "<p>I think it is mostly right. But not that intelligence is hard to define. In my opinion, it is simple: A is more intelligent than B if A achieves some purpose in less steps than B. It is functional/algorithmic efficiency.</p>\n\n<p>What is difficult to define is <em>human</em> intelligence.</p>\n\n<p>But when someone says, \"No, X is not <em>real</em> intelligence,\" what they mean is that it does not satisfy what we would consider real <em>human intelligence</em>.</p>\n\n<p>So when people discount new and amazing discoveries in machine intelligence, it is not because they are not amazing in their own way, but because those discoveries, while exhibiting intelligence, are <strong>actually not</strong> replicating human intelligence - which is what many people actually mean when they say \"that thing isn't <em>really</em> intelligent.\"</p>\n\n<p>In truth we are very far, in the science of artificial intelligence, algorithmically speaking, from an artificial <em>human</em> intelligence (AHI).</p>\n\n<p>Additional note: What's funny is that we don't call the science of artificial intelligence just 'the science of intelligence.' That we add the \"artificial\" qualifier by necessity pegs the science to what the artificiality implicitly emulates: human intelligence. In other words, \"artificial intelligence\" must be by definition more specific to the thing it allegedly artificializes than a more general science of just \"intelligence.\"</p>\n", "question": "<p><a href=\"https://en.wikipedia.org/wiki/AI_effect\" rel=\"nofollow\">According to Wikipedia</a>...</p>\n\n<blockquote>\n  <p>The AI effect occurs when onlookers discount the behavior of an artificial intelligence program by arguing that it is not real intelligence.</p>\n  \n  <p>Pamela McCorduck writes: \"It's part of the history of the field of artificial intelligence that every time somebody figured out how to make a computer do something\u2014play good checkers, solve simple but relatively informal problems\u2014there was chorus of critics to say, 'that's not thinking'.\"[1] AI researcher Rodney Brooks complains \"Every time we figure out a piece of it, it stops being magical; we say, 'Oh, that's just a computation.'\"[2]</p>\n</blockquote>\n\n<p>The Wikipedia page then proposes several different reasons that could explain why onlookers might \"discount\" AI programs. However, those reasons seem to imply that the humans are making a mistake in \"discounting\" the behavior of AI programs...and that these AI programs might actually be  intelligent. I want to make an alternate argument, where the humans are making a mistake, but not in \"discounting\" the behavior of AI programs.</p>\n\n<p>Consider the following situation. I want to build a machine that can do X (where X is some trait, like intelligence). I am able to evaluate intuitively whether a machine has that X criteria. But I don't have a good definition of what X actually <em>is</em>. All I can do is identify whether something has X or not.</p>\n\n<p>However, I think that people who has X can do Y. So if I build a machine that can do Y, then surely, I built a machine that has X.</p>\n\n<p>After building the machine that can do Y, I examine it to see if my machine has X. And it does not. So my machine lacks X. And while a machine that can do Y is cool, what I really want is a machine that has X. I go back to the drawing board and think of a new idea to reach X.</p>\n\n<p>After writing on the whiteboard for a couple of hours, I realize that people who has X can do Z. Of course! I try to build a new machine that can do Z, yes, if it can do Z, then it must have X.</p>\n\n<p>After building the machine that can do Z, I check to see if it has X. It does not. And so I return back to the drawing board, and the cycle repeats and repeats...</p>\n\n<p>Essentially, humans are attempting to determine whether an entity has intelligence via proxy measurements, but those proxy measurements are potentially faulty (as it is possible to meet those proxy measurements without ever actually having intelligence). Until we know how to define intelligence and design a test that can accurately measure it, it is very unlikely for us to build a machine that has intelligence. So the AI Effect occurs because humans don't know how to define \"intelligence\", not due to people dismissing programs as not being \"intelligent\".</p>\n\n<p>Is this argument valid or correct? And if not, why not?</p>\n"}, "id": "2068"}, {"body": {"answer": "<p>Right and wrong only exist relative to some goal or purpose.</p>\n\n<p>To make a machine do more right than wrong, relative to human goals, one should minimize the surface area of the machine's purpose. Doing that minimizes the intrinsic behavior of the AI, which enables us to reason about the right and wrong behaviors of the AI, relative to human purposes.</p>\n\n<p>Horses are quite general over the domains of their purposes, but are still predictable enough for humans to control and benefit from. As such, we will be able to produce machines (conscious or unconscious) that are highly general over particular domains, while still being predictable enough to be useful to humans.</p>\n\n<p>The most efficient machines for most tasks, though, will not <em>need</em> consciousness, nor even the needs that cause survivalistic, adversarial and self-preserving behaviors in eukaryotic cells. Because most of our solutions won't <em>need</em> those purposes to optimize over our problems, we can allow them to be much more predictable.</p>\n\n<p>We will be able to create predictable, efficient AIs over large domains that are able to produce predictable, efficient AIs over more specific domains. We'll be able to reason about the behavioral guarantees and failure modes of those narrow domains.</p>\n\n<p>In the event that we one day desire to build something as unpredictable as a human, like we do when having babies, then we'll probably do that with the similar intentions and care that we use to bring an actual baby into the world. There is simply no purpose in creating a thing more unpredictable than you unless you're gambling on this thing <em>succeeding</em> you in capability - which sounds exactly like having babies.</p>\n\n<p>After that, the best we can do is give it our <em>theories</em> about why we think we should act one way or another.</p>\n\n<p>Now, theoretically, some extremely powerful AI could potentially construct a human-like simulacrum that, in many common situations, seems to act like a human, but that in fact has had all of it's behaviors formally specified a priori, via some developmental simulation, such that we <em>know for a fact</em> that all such behaviors produce no intentional malice or harm. However, if we can formally specify all such behaviors, we wouldn't be using this thing to solve any novel problems, like curing cancer, as the formal specification for curing cancer would already have been pre-computed. If you can formally specify the behaviors of a thing that can discover something new, you can just compute the solution via the specification, without instantiating the behaviors at all!</p>\n\n<p>Once AI has reached a certain level of capability, it won't need to generate consciousnesses to derive optimal solutions. And at that point, the only purpose for an artificial human to exist will be, like us, for its own sake.</p>\n", "question": "<p>Here is one of the most serious questions, about the artificial intelligence.<br>\nHow will the machine know the difference between right and wrong, what is good and bad, what is respect, dignity, faith and empathy.<br>\n<br> A machine can recognize what is correct and incorrect, what is right and what is wrong, depend on how it is originally designed.<br>\n<br>It will follow the ethics of its creator, the man who originally designed it<br>\n But how to teach a computer something we don't have the right answer.<br>\n People are selfish, jealous, self confident. We are not able to understand each other sorrows, pains beliefs. We don't understand different religions, different traditions or beliefs. \n<br>Creating an AI might be breakthrough for one nation, or one race, or one ethnic or religious group, but it can be against others.   </p>\n\n<p>Who will learn the machine a humanity?   :)</p>\n"}, "id": "2070"}, {"body": {"answer": "<p>Since you mention the <strong>A* algorithm</strong>, then you are definitely using a heuristic somewhere in there, at least with the A* algorithm while solving the subproblems using the straight-line distance as your <strong>heuristic function</strong>.</p>\n\n<p>Although your approach does not seem to incorporate a \"shortcut mathematical formula\" as a heuristic after that, it does us a precalculated <strong>heuristic table</strong> as a reference and that may qualify as a heuristic. Wikipedia page <a href=\"https://en.wikipedia.org/wiki/Heuristic_(computer_science)\" rel=\"nofollow\">here</a> says (although without citation) the following which does seem to describe what you are doing where your heuristic is not a fixed but a precalculated function/table:</p>\n\n<blockquote>\n  <p>A heuristic function, also called simply a heuristic, is a function\n  that ranks alternatives in search algorithms at each branching step\n  based on available information to decide which branch to follow. For\n  example, it may approximate the exact solution.</p>\n</blockquote>\n\n<p>On another note, your method also seems to have hints of <strong>dynamic programming</strong> since you use the precalculated and stored solutions to subproblems and instead of recalculating every time.</p>\n\n<p>I wonder if the term <strong>approximate dynamic programming</strong> would fit this situation as a non-stochastic version with no uncertainties? Unfortunately I could not find any simple description or categorization for that term.</p>\n", "question": "<p>This is a question about a nomenclature - we already have the algorithm/solution, but we're not sure whether it qualifies as utilizing heuristics or not.</p>\n\n<hr>\n\n<p>feel free to skip the problem explanation:</p>\n\n<blockquote>\n  <p>A friend is writing a path-finding algorithm - an autopilot for an\n  (off-road) vehicle in a computer game. This is a pretty classic\n  problem - he finds a viable, not necessarily optimal but \"good enough\"\n  route using the A* algorithm, by taking the terrain layout and vehicle\n  capabilities into account, and modifying a direct (straight) line path\n  to account for these. The whole map is known a'priori and invariant,\n  though the start and destination are arbitrary (user-chosen) and the\n  path is not guaranteed to exist at all.</p>\n  \n  <p>This cookie-cutter approach comes with a twist: limited storage space.\n  We can afford some more volatile memory on start, but we should free\n  most of it once the route has been found. The travel may take days -\n  of real time too, so the path must be saved to disk, and the space in\n  the save file for custom data like this is severely limited. Too\n  limited to save all the waypoints - even after culling trivial\n  solution waypoints ('continue straight ahead'), and by a rather large\n  margin, order of 20% the size of our data set.</p>\n  \n  <p>A solution we came up with is to calculate the route once on start,\n  then 'forget' all the trivial and 90% of the non-trivial waypoints.\n  This both serves as a proof that a solution exists, and provides a set\n  of points reaching which, in sequence, guarantees the route will take\n  us to the destination.</p>\n  \n  <p>Once the vehicle reaches a waypoint, the route to the next one is\n  calculated again, from scratch. It's known to exist and be correct\n  (because we did it once, and it was correct), it doesn't put too much\n  strain on the CPU and the memory (it's only about 10% the total route\n  length) and it doesn't need to go into permanent storage (restarting\n  from any point along the path is just a subset of the solution\n  connecting two saved waypoints).</p>\n</blockquote>\n\n<hr>\n\n<p>Now for the actual question:</p>\n\n<p>The pathfinding algorithm follows a sparse set of waypoints which by themselves are not nearly sufficient as a route, but allow for easy, efficient  calculation of the actual route, simultaneously guarantying its existence; they are a subset of the full solution. </p>\n\n<p>Is this a heuristic approach?</p>\n\n<p>(as I understand, normally, heuristics don't guarantee existence of a solution, and merely suggest more likely candidates. In this case, the 'hints' are taken straight out of an actual working solution, thus my doubts.)</p>\n"}, "id": "2072"}, {"body": {"answer": "<p>It would not be wise to say that CNNs are better objectively than traditional approaches to solve computer vision problems as there are many problems for which the traditional methods works just fine. CNNs do have an inherent advantage over traditional methods which is the same advantage that deep learning has over other traditional methods i.e learning hierarchical features i.e what features are useful and how to compute them.</p>\n\n<p>The traditional way to approach a CV problem is to figure out the features that are relevant to the problem, figure out how to compute those features and then use those features to compute the final result. Whereas in CNN case the training process will figure out all the 3 points for you given that you have huge number of training examples.</p>\n", "question": "<p>Are Convolutional Neural Networks summarily better than pattern recognition in all existing image processing libraries that don't use CNN's? Or are there still hard outstanding problems in image processing that seem to be beyond their capability?</p>\n"}, "id": "2075"}, {"body": {"answer": "<p>The core question to whether or not an AI is adaptable or not is whether or not it supports <a href=\"https://en.wikipedia.org/wiki/Online_machine_learning\" rel=\"nofollow\">online learning</a>. That doesn't mean using the Internet to learn things; that means continuing to accept training data during the functioning of the system.</p>\n\n<p>This is (mostly) independent of the underlying architecture; in evolutionary approaches one can continue to breed generations with a shifting fitness function or with neural networks one can continue to backpropagate errors, and so on with other approaches.</p>\n", "question": "<p><a href=\"http://ai.stackexchange.com/questions/2067/will-ai-be-able-to-adapt\">From this SE question</a>:</p>\n\n<blockquote>\n  <p>Will be AI able to adapt, to different environments and changes.</p>\n</blockquote>\n\n<p>This is my attempt at interpreting that question.</p>\n\n<p>Evolutionary algorithms are useful for solving optimization problems...by measuring the \"fitness\" of various probable solutions and then  of an algorithm through the process of natural selection.</p>\n\n<p>Suppose, the \"fitness calculation\"/\"environment\" is changed in mid-training (as could easily happen in real-life scenarios where people may desire different solutions at different times). Would evolutionary algorithms be able to respond effectively to this change?</p>\n"}, "id": "2079"}, {"body": {"answer": "<p>Probably the only secure jobs are those where the audience enjoys watching live human craftsmanship take place in real time right before their eyes, like acting or standup comedy or musical virtuosity or playing a sport.  Watching a robot do the same thing would be far less personally engaging since there's no human skill or artistry to appreciate or identify with, escpecially when the pressure is on or when human interpersonal dynamics are involved.</p>\n\n<p>For example, why would anyone watch robots play poker?  Or dance?  Or do standup comedy about how hard it is to be [ethnic group or gender goes here]?</p>\n", "question": "<p>AI is progressing drastically, and imagine they tell you you're fired because a robot will take your place. What are some jobs that can never be automated?</p>\n"}, "id": "2080"}, {"body": {"answer": "<p>I think Matthew Graves' answer is the strictly correct one. But I also think this question may be hinting at a larger question in general. What is the minimal algorithmic complexity required for a machine of one particular set of functions to mutate into some other machine of some other particular set of functions?</p>\n\n<p>The answer is: potentially <em>infinite</em> algorithmic complexity. Without knowing a priori how many steps it will take to mutate into a thing that can solve some black-box problem, there is no way to determine if and when the AI will be able to mutate into that thing.</p>\n", "question": "<p><a href=\"http://ai.stackexchange.com/questions/2067/will-ai-be-able-to-adapt\">From this SE question</a>:</p>\n\n<blockquote>\n  <p>Will be AI able to adapt, to different environments and changes.</p>\n</blockquote>\n\n<p>This is my attempt at interpreting that question.</p>\n\n<p>Evolutionary algorithms are useful for solving optimization problems...by measuring the \"fitness\" of various probable solutions and then  of an algorithm through the process of natural selection.</p>\n\n<p>Suppose, the \"fitness calculation\"/\"environment\" is changed in mid-training (as could easily happen in real-life scenarios where people may desire different solutions at different times). Would evolutionary algorithms be able to respond effectively to this change?</p>\n"}, "id": "2081"}, {"body": {"answer": "<p>Paper <a href=\"https://arxiv.org/pdf/1512.00567.pdf\" rel=\"nofollow\">Szegedy C, Vanhoucke V, Ioffe S, et al. Rethinking the inception architecture for computer vision[J]. arXiv preprint arXiv:1512.00567, 2015.</a> gives some general design principles:</p>\n\n<blockquote>\n  <ol>\n  <li><p>Avoid representational bottlenecks, especially early in\n  the network;</p></li>\n  <li><p>Balance the width and depth of the network. Optimal\n  performance of the network can be reached by balancing\n  the number of filters per stage and the depth of\n  the network. Increasing both the width and the depth\n  of the network can contribute to higher quality networks.\n  However, the optimal improvement for a constant\n  amount of computation can be reached if both are\n  increased in parallel. The computational budget should\n  therefore be distributed in a balanced way between the\n  depth and width of the network.</p></li>\n  </ol>\n</blockquote>\n\n<p>These suggestions can't bring you the optimal number of neurons in a network though.</p>\n\n<p>However, there are still some model compression research e.g. <a href=\"https://github.com/wenwei202/caffe/tree/scnn\" rel=\"nofollow\">Structured Sparsity Learning (SSL) of Deep Neural Networks</a>, <a href=\"https://github.com/songhan/SqueezeNet-Deep-Compression\" rel=\"nofollow\">SqueezeNet</a>, <a href=\"http://papers.nips.cc/paper/5784-learning-both-weights-and-connections-for-efficient-neural-network.pdf\" rel=\"nofollow\">Pruning network</a> that may shed some light on how to optimizing the neurons per single layer.</p>\n\n<p>Especially in <a href=\"https://github.com/wenwei202/caffe/tree/scnn\" rel=\"nofollow\">Structured Sparsity Learning of Deep Neural Networks</a>, it adds a <code>Group Lasso</code> regularization term in the loss function to to regularize the structures(i.e., filters, channels, filter shapes, and layer depth) of DNNs, which namely is to zero out some components(i.e., filters, channels, filter shapes, and layer depth) of the net structure and achieves a remarkable compact and acceleration of the network, while keeps a small classification accuracy loss. </p>\n", "question": "<p>When you're writing your algorithm, how do you know how many neurons you need per single layer? Are there any methods for finding the optimal number of them, or is it a rule of thumb?</p>\n"}, "id": "2082"}, {"body": {"answer": "<p>If I understand you correctly, I think you're referring to <strong><em>elitism</em></strong>.</p>\n\n<p>As <a href=\"https://en.wikipedia.org/wiki/Genetic_algorithm#Elitism\" rel=\"nofollow\">Wikipedia</a> explains: <em>\"A practical variant of the general process of constructing a new population is to allow the best organism(s) from the current generation to carry over to the next, unaltered. This strategy is known as elitist selection and guarantees that the solution quality obtained by the GA will not decrease from one generation to the next.\"</em></p>\n", "question": "<p>So I'm here to propose a strategy or to ask if this strategy has been tested in genetic algorithms in the past. I didn't exactly know how to find discussion about it.</p>\n\n<p>In a classic example of genetic algorithm you would have a population and certain amount of simulation time to evaluate it and breeding. Then proceed to the next generation.</p>\n\n<p>What if we would isolate a small part of the population in the simulation process and keep them evolving in their own little island for some time while rest of the population continues to evolve normally? After that they could be re-united with the rest of the population and the end of the simulation would go trough. After that breed the population and continue. </p>\n\n<p>This is super important part in natural evolution and probably some know if it actually works with genetic programming?</p>\n"}, "id": "2084"}, {"body": {"answer": "<p>There have been extensive studies within Evolutionary Computation in the area of \n<a href=\"http://cs.gmu.edu/~eclab/papers/skolicki05analysis.pdf\" rel=\"nofollow\">'Island Models'</a> and <a href=\"http://www.gustafsonresearch.com/thesis_html/node122.html\" rel=\"nofollow\">'Niching'</a> for doing exactly this.</p>\n\n<p>The advantages of this approach include greater population diversity (which is particularly useful when the problem is multiobjective) and the potential for concurrent execution of each separate population.</p>\n\n<p>See also the answer to this <a href=\"http://stackoverflow.com/questions/13775810/what-is-niching-scheme\">SE question</a>.</p>\n\n<p>With specific reference to Genetic Programming <a href=\"http://courses.csail.mit.edu/18.337/2016/projects/MorganFrank/projectReport.pdf\" rel=\"nofollow\">here</a> is a recent paper which uses a parallel island model.</p>\n", "question": "<p>So I'm here to propose a strategy or to ask if this strategy has been tested in genetic algorithms in the past. I didn't exactly know how to find discussion about it.</p>\n\n<p>In a classic example of genetic algorithm you would have a population and certain amount of simulation time to evaluate it and breeding. Then proceed to the next generation.</p>\n\n<p>What if we would isolate a small part of the population in the simulation process and keep them evolving in their own little island for some time while rest of the population continues to evolve normally? After that they could be re-united with the rest of the population and the end of the simulation would go trough. After that breed the population and continue. </p>\n\n<p>This is super important part in natural evolution and probably some know if it actually works with genetic programming?</p>\n"}, "id": "2085"}, {"body": {"answer": "<p>Can't tell. I guess half his site is down because of that malware.</p>\n\n<p>In any case, it appears that much of his past work on Github involves procedural generation. Which is AI... ish. Unless there's more to it, which we can't see because half the site is down.</p>\n\n<p><a href=\"http://www.cs.mcgill.ca/~cdrage/papers/SOCS-TR-2011.1.pdf\" rel=\"nofollow\">This paper</a> appears to offer analysis of combining procedural generation with game AI.</p>\n\n<p>From the abstract:</p>\n\n<blockquote>\n  <p>Populated and immersive game contexts require large numbers of minor,\n  background characters to fill out the virtual environment. To limit\n  game AI development effort, however, such characters are typically\n  represented by very simplistic AI with either little difference\n  between characters or only highly formulaic variations. Here we\n  describe a complete workflow and framework for easily designing,\n  generating and incorporating multiple, interesting game AIs. Our\n  approach uses high-level, visual Statechart models to represent\n  behaviour in a modular form; this allows for not only simplistic,\n  parameterbased variation in AI design, but also permits more complex\n  structure-based approaches. We demonstrate our technique by applying\n  it to the task of generating a large number of individual AIs for\n  computer-controlled squirrels within the Mammoth <a href=\"http://www.cs.mcgill.ca/~cdrage/papers/SOCS-TR-2011.1.pdf\" rel=\"nofollow\">1</a> framework for\n  game research. Rapid development and easy deployment of AIs allow us\n  to create a wide variety of interesting AIs, greatly improving the\n  sense of immersion in a virtual environment.</p>\n</blockquote>\n\n<p>Update: actually, here we go. <a href=\"http://newatlas.com/creative-ai-procedural-game-development-angelina/35874/\" rel=\"nofollow\">Here's an article</a> from 2015 on AI and procedural generation, which discusses Angelina at length.</p>\n\n<p>And that article links to <a href=\"http://www.eurogamer.net/articles/2013-04-02-plastic-soul-one-mans-quest-to-build-an-ai-that-can-create-games\" rel=\"nofollow\">a more in depth article</a> from 2013.</p>\n\n<p>Here's an excerpt:</p>\n\n<blockquote>\n  <p>Cook gave ANGELINA the ability to learn about people so that it could\n  make games based on current events. Then Cook gave ANGELINA memory -\n  that is, the ability to keep track of the people it had learned about.\n  The memory's not a big deal, even though it led to a number of\n  philosophical disagreements around Cook's desk. ANGELINA's memory is\n  actually just a text file where it stores the names of all the people\n  it's heard of, alongside a number: a measure of its opinion of them\n  based on the things it's learned from internet chatter. It liked\n  Al-Assad more than May. It liked everyone more than May.</p>\n</blockquote>\n", "question": "<p>In the recent <a href=\"https://live.newscientist.com/\" rel=\"nofollow\">festival of science</a>, there was a talk given by researcher <a href=\"https://live.newscientist.com/mike-cook/\" rel=\"nofollow\">Mike Cook</a> about:</p>\n\n<blockquote>\n  <p><a href=\"http://www.gamesbyangelina.org/\" rel=\"nofollow\">ANGELINA</a>, an AI game designer that has invented game mechanics, made games about news stories, and was the first AI to enter a game jam.</p>\n</blockquote>\n\n<p>So the aim of Angelina AI is basically to design videogames.</p>\n\n<p>Briefly, how exactly does Angelina design the new games? How does it work behind the scenes?</p>\n"}, "id": "2086"}, {"body": {"answer": "<p><strong>Yes.</strong></p>\n\n<p>A totally random algorithm could solve any problem given unlimited time and a perfect fitness function. All you need to do is give the GA some new random population members each generation and you're guaranteed to find the solution eventually. Even if you keep only descendants of the previous generation, setting the mutation rate and number of crossovers high enough could effectively get you random individuals.</p>\n", "question": "<p>Obviously this is hypothetical, but is true? I know \"perfect fitness function\" is a bit hand-wavy, but I mean it as we have a perfect way to measure the completion of any problem.</p>\n"}, "id": "2088"}, {"body": {"answer": "<p>Neural net approaches  are very different than other techniques, mostly because NN aren't \"linear\" like feature matching or cascades. With very complicated tasks like realtime object recognition or other difficult patterns it's better to use neural net, first because if you train it well your net , you can get very high precision, second it' easier to implement (it depends a lot from library to library)  third usually after you have trained it they are very fast to classify or predict something. But a lot of tasks don't need neural nets, for example many factories to check the products use 3D features model matching. At the end you have to  evaluate which method is the best for your task</p>\n", "question": "<p>Are Convolutional Neural Networks summarily better than pattern recognition in all existing image processing libraries that don't use CNN's? Or are there still hard outstanding problems in image processing that seem to be beyond their capability?</p>\n"}, "id": "2089"}, {"body": {"answer": "<p>As per the answer to <a href=\"http://ai.stackexchange.com/questions/1541/why-is-cross-over-a-part-of-genetic-algorithms/1548#1548\">this AI SE question</a>,\nthe presence of mutation makes a GA into a global search algorithm, i.e. it will eventually visit each point in the search space. </p>\n\n<p>How <em>efficiently</em> it will do so is indeed related to the quality of the fitness function:</p>\n\n<p>A 'perfect fitness function' could conceivably mean any of the following:</p>\n\n<ol>\n<li>A function  which takes on an optimal value iff it is applied to\nan optimal solution. </li>\n<li>A function which forms a quadratic bowl\n(Newton's method can solve this in one step).</li>\n</ol>\n\n<p>A degenerate case of 1. is a 'Needle in a haystack' function, which returns the same arbitrarily poor value everywhere that isn't an optimum, and 2. unfortunately doesn't arise very often in practice.</p>\n\n<p>Hence, the role of a well-designed fitness function is to impose a gradient on the search process, which in practice will generally lie somewhere in-between 'Needle in a haystack' and 'Quadratic bowl'.</p>\n\n<p>The presence of some form of smoothness in the fitness function is one mechanism that allows better performance than random or exhaustive methods.</p>\n", "question": "<p>Obviously this is hypothetical, but is true? I know \"perfect fitness function\" is a bit hand-wavy, but I mean it as we have a perfect way to measure the completion of any problem.</p>\n"}, "id": "2090"}, {"body": {"answer": "<p><strong>Have the user label highlighted objects in video that a state of the art classifier cannot solve</strong></p>\n\n<p>Create a state of the art video classifier. Might as well train it on Google's <a href=\"https://research.googleblog.com/2016/09/announcing-youtube-8m-large-and-diverse.html\" rel=\"nofollow\">YouTube-8M</a> video training data. But you will want to continually feed it original video as well.</p>\n\n<p>Have the classifier label as many objects as it can. Have it isolate which objects it can recognize as objects but which it is unable to label.</p>\n\n<p>Have it output videos that outlines the objects. Preferably GIFs, which can be easily embedded in forms.</p>\n\n<p>For 100 of these, ask 100 users what the object is. If 90% of the users agree on the name of an object, add that video to the captcha-set. Call this the pre-trained set.</p>\n\n<p>Every time a user needs to authenticate, show them one of the highlighted objects in a video <em>not from the pre-trained set</em>. If the image has less than 100 showings, record the label and give the user another one from the pre-trained set. If they get it right, let them through, if not, give them another from the pretrained set.</p>\n\n<p>Once the non-pre-trained video has more than 100 showings and more than 90% of the captcha-users agree, add that video to the post-trained set.</p>\n\n<p>Over time, slowly remove the pre-trained set. Put expirations on each video in the post-trained set and remove them after expiration, so that they don't get used too many times.</p>\n\n<p>Ideally, this process would constantly improve the video classifier, keeping it state of the art and slightly ahead of other classifiers. Perhaps it could also favor less common words and objects and more esoteric things, so as to specialize this classifier against other classifiers.</p>\n\n<p>The same could be done for image labeling, but the utility of the video classifier will probably last longer, given advances in AI.</p>\n\n<p>Strictly speaking, though, short of some quantum trickery, there is no captcha system that will not one day be solved by external AI systems.</p>\n\n<p>(edit: oh, I just noticed you specifically said \"textual captcha.\" If that's what you mean, then no I don't think text classification has much mystery left in it. Computers can probably glean text from pictures better than humans now. But techically, the <em>input</em> in the above described captcha system is textual.) </p>\n", "question": "<p>Are there any modern techniques of generating <strong>textual</strong> CAPTCHA (so person needs to type the right text) challenges which can easily <a href=\"http://ai.stackexchange.com/q/92/8\">fool AI</a> with some visual obfuscation methods, but at the same time human can solve them without any struggle?</p>\n\n<p>For example I'm talking about plain ability of <strong>recognising text embedded into image</strong> (without considering any external plugins like flash or java, image classification, etc.) and re-typing the text that has been written or something similar.</p>\n\n<p>I guess adding noise, gradient, rotating letters or changing colours are not reliable methods any more, since they can be quickly broken.</p>\n\n<p>Any suggestions or research has been done?</p>\n"}, "id": "2093"}, {"body": {"answer": "<p>Use something like Word2Vec. If a particular node has two edges that are very far from each other, besides the node in question, split the node into word(1) and word(2) nodes.</p>\n", "question": "<p>I'm interested in implementing a program for natural language processing (aka <a href=\"https://en.wikipedia.org/wiki/ELIZA\" rel=\"nofollow\">ELIZA</a>).</p>\n\n<p>Assuming that I'm already <a href=\"http://ai.stackexchange.com/q/212/8\">storing semantic-lexical connections</a> between the words and its strength.</p>\n\n<p>What are the methods of dealing with words which have very distinct meaning?</p>\n\n<p>Few examples:</p>\n\n<ul>\n<li><p>'Are we on the same page?'</p>\n\n<p>The 'page' in this context isn't a document page, but it's part of the phrase.</p></li>\n<li><p>'I'm living in Reading.'</p>\n\n<p>The 'Reading' is a city (noun), so it's not a verb. Otherwise it doesn't make any sense. Checking for the capital letter would work in that specific example, but it won't work for other (like 'make' can be either verb or noun).</p></li>\n<li><p>'I've read something on the Facebook wall, do you want to know what?'</p>\n\n<p>The 'Facebook wall' has nothing to do with wall at all.</p></li>\n</ul>\n\n<p>In general, how algorithm should distinguish the word meaning and recognise the word within the context?</p>\n\n<p>For example:</p>\n\n<ul>\n<li>Detecting the word for different type of speech, so it should recognise whether it's a verb or noun.</li>\n<li>Detecting whether the word is part of phrase.</li>\n<li>Detecting word for multiple meaning.</li>\n</ul>\n\n<p>What are the possible approaches to solve that problem in order to  identify the correct sense of a word with the context?</p>\n"}, "id": "2094"}, {"body": {"answer": "<p><strong>No.</strong></p>\n\n<p>TL;DR: The Lovelace Test 2.0 is very vague, making it ill-suited for  evaluation of intelligence. It is also generally ignored by researchers of Computational Creativity, who already have their own tests to evaluate creativity.</p>\n\n<p>Longer Answer:\nAccording to Google Scholar, there are 10 references to the \"Lovelace Test 2.0\" paper. All of those references exist merely to point out that the Lovelace Test 2.0 exists. In fact, at least two of articles I consulted (<a href=\"http://philpapers.org/archive/NEGANA.pdf\" rel=\"nofollow\">A novel approach for identifying a human-like self-conscious behavior</a> and <a href=\"http://users.dsic.upv.es/~flip/EGPAI2016/papers/EGPAI_2016_paper_8.pdf\" rel=\"nofollow\">FraMoTEC: A Framework for Modular Task-Environment Construction for Evaluating Adaptive Control Systems</a>) proposed their <em>own</em> tests instead.</p>\n\n<p>One of the authors who wrote the FraMoTEC paper also wrote <a href=\"http://skemman.is/stream/get/1946/25590/58121/1/scs-thorarensen2016-thesis.pdf\" rel=\"nofollow\">his thesis on FraMoTEC</a>, and indirectly critiqued the Lovelace Test 2.0 and other similar such tests:</p>\n\n<blockquote>\n  <p>The Piaget-MacGyver Room problem [Bringsjord and Licato, 2012], Lovelace Test 2.0 [Riedl, 2014] and Toy Box problem [Johnston, 2010] all come with the caveat of being defined very vaguely \u2014 these evaluation methods may be likely to come up with a reasonable evaluation for intelligence, but it is very difficult to compare two different agents (or controllers) that partake in the their own domain-specific evaluations, which is what frequently happens when agents are tailored to pass specific evaluations.</p>\n</blockquote>\n\n<p>Another major issue with the Lovelace Test 2.0 is that there is a proliferation of <em>other</em> tests to \"measure\" the creativity of AI. <a href=\"https://kar.kent.ac.uk/42374/1/jordanous-2011a.pdf\" rel=\"nofollow\">Evaluating Evaluation: Assessing Progress in Computational Creativity Research</a>, published by \nAnna Jordanous in 2011 (3 years <em>before</em> the invention of the Lovelace Test 2.0) analyzed research papers about AI creativity and wrote:</p>\n\n<blockquote>\n  <p>Of the 18 papers that applied creativity evaluation methodologies to evaluate their system\u2019s creativity, no one methodology emerged as standard across the community. Colton\u2019s creative tripod framework (<a href=\"https://pdfs.semanticscholar.org/fd8b/d5bb76c94c4bfc34cb1fa244dc6bd4a8ca8e.pdf\" rel=\"nofollow\">Colton 2008</a>) was used most often (6 uses), with 4 papers using Ritchie\u2019s empirical criteria (<a href=\"https://pdfs.semanticscholar.org/06d8/3be9078b5c8933c3523f0e663fff1c61e3a0.pdf\" rel=\"nofollow\">Ritchie 2007</a>). </p>\n</blockquote>\n\n<p>That leaves <em>10</em> papers with miscellaneous creativity evaluation methods.</p>\n\n<p>The goal of \"Evaluating Evaluation\" was to standardize the process of evaluating creativity, to avoid the possibility of the field stagnating due to the proliferation of so many creativity tests.  Anna Jordanous still remained interested in evaluating creativity tests, publishing articles such as <a href=\"http://s3.amazonaws.com/academia.edu.documents/34328583/Stepping_Back_to_Progress_Forwards-_Setting_Standards_for_Meta-Evaluation_of_Computational_Creativity.pdf?AWSAccessKeyId=AKIAJ56TQJRTWSMTNPEA&amp;Expires=1475893195&amp;Signature=ox0%2FOEkNUNjz5pQg0w%2FOgzHPCsM%3D&amp;response-content-disposition=inline%3B%20filename%3DStepping_Back_to_Progress_Forwards_Setti.pdf\" rel=\"nofollow\">\"Stepping Back to Progress Forwards: Setting Standards for Meta-Evaluation of Computational Creativity\"</a> and <a href=\"http://doc.gold.ac.uk/~map01mm/CC2015/AISB-CC2015-Proceedings.pdf#page=18\" rel=\"nofollow\">Four PPPPerspectives on Computational Creativity</a>.</p>\n\n<p>\"Evaluating Evaluation\" does provide some commentary to explain the proliferation of systems to evaluate creativity:</p>\n\n<blockquote>\n  <p>Evaluation standards are not easy to define. It is difficult to evaluate creativity and even more difficult to describe how we evaluate creativity, in human creativity as well as in computational creativity. In fact, even the very definition of creativity is problematic (Plucker, Beghetto, and Dow 2004). It is hard to identify what \u2019being creative\u2019 entails, so there are no benchmarks or ground truths to measure against.</p>\n</blockquote>\n\n<p>The fact that so many tests of creativity already exist (to the extent that Jordanous can make an academic career in studying them) means that it's very difficult for any new test (such as the Lovelace Test 2.0) to even be noticed (much less cited). Why would you want to use something like the Lovelace Test 2.0 when there's so many other tests you could use instead?</p>\n", "question": "<p>In October 2014, Dr. Mark Riedl published an approach to testing AI intelligence, called <a href=\"http://arxiv.org/pdf/1410.6142v3.pdf\" rel=\"nofollow\">the \"Lovelace Test 2.0\"</a>, after being inspired by the <a href=\"http://kryten.mm.rpi.edu/lovelace.pdf\" rel=\"nofollow\">original Lovelace Test</a> (published in 2001). Mark believed that the original Lovelace Test would be impossible to pass, and therefore, suggested a weaker, and more practical version.</p>\n\n<p>The Lovelace Test 2.0 makes the assumption that for an AI to be intelligent, it must exhibit creativity. From the paper itself:</p>\n\n<blockquote>\n  <p>The Lovelace 2.0 Test is as follows: artificial agent a is challenged as follows:</p>\n  \n  <ul>\n  <li><p>a must create an artifact o of type t;</p></li>\n  <li><p>o must conform to a set of constraints C where ci \u2208 C is\n  any criterion expressible in natural language;</p></li>\n  <li><p>a human evaluator h, having chosen t and C, is satisfied\n  that o is a valid instance of t and meets C; and</p></li>\n  <li><p>a human referee r determines the combination of t and C\n  to not be unrealistic for an average human.</p></li>\n  </ul>\n</blockquote>\n\n<p>Since it is possible for a human evaluator to come up with some pretty easy constraints for an AI to beat, the human evaluator is then expected to keep coming up with more and more complex constraints for the AI until the AI fails. The point of the Lovelace Test 2.0 is to <em>compare</em> the creativity of different AIs, not to provide a definite dividing line between 'intelligence' and 'nonintelligence' like the Turing Test would.</p>\n\n<p>However, I am curious about whether this test has actually been used in an academic setting, or it is only seen as a thought experiment at the moment. The Lovelace Test seems easy to apply in academic settings (you only need to develop some measurable constraints that you can use to test the artificial agent), but it also may be too subjective (humans can disagree on the merits of certain constraints, and whether a creative artifact produced by an AI actually meets the final result).</p>\n"}, "id": "2095"}, {"body": {"answer": "<h2>AI or Artificial Intelligence</h2>\n\n<p><strong>What is it?</strong></p>\n\n<p>Artificial intelligence (AI) is intelligence exhibited by machines. In computer science. Colloquially, the term \"artificial intelligence\" is applied when a machine mimics \"cognitive\" functions that humans associate with other human minds, such as \"learning\" and \"problem solving\".</p>\n\n<p><strong>Can it be useful in a \"Standard\" application?</strong></p>\n\n<p>Well, what I think about a Standard application using AI is that AI is used for that too, because when the machine have a reaction of the user input is AI or Artificial Intelligence. So the AI in Standard application it have been used many years ago already.</p>\n\n<p><strong>PS:</strong> If there are grammar errors, then I'm sorry because I'm not a English speaker.</p>\n\n<p><strong>Sources:</strong> <a href=\"https://en.wikipedia.org/wiki/Artificial_intelligence\" rel=\"nofollow\">https://en.wikipedia.org/wiki/Artificial_intelligence</a> \"AI or Artificial Intelligence.\"</p>\n\n<p><strong>DevJosueDav</strong>\n<em>Just a C# Artificial <strong>AI</strong> Intelligence Developer.</em></p>\n", "question": "<p>I'm curious about Artificial Intelligence. In my everyday job I develop standard applications, like websites with basic functionalities like user subscription, file upload, forms saved in a database... </p>\n\n<p>I mainly know of AI being used in games or robotics fields. But can it be useful in \"standard\" application development?</p>\n"}, "id": "2097"}, {"body": {"answer": "<p>Yes, but probably only to a limited degree in the near term.</p>\n\n<p>Where people draw the boundaries around 'artificial intelligence' is fuzzy, but if one takes the broad view, where it incorporates any sort of coding of explicitly cognitive functions, then many routine economic tasks can benefit from artificial intelligence. Many search engines, for example, can be seen as offering artificial intelligence applications as a service.</p>\n\n<p>For more 'standard' applications, most near-team applications of AI have to deal with fraud detection and prevention. If you track a user's cursor moving across the screen, for example, you can build a model that differentiates between humans and bots, and treat the two separately. See <a href=\"https://nakedsecurity.sophos.com/2014/12/05/i-am-not-a-robot-google-swaps-text-captchas-for-quivery-mouse-clicks/\">this article</a> for an example.</p>\n\n<p>In the longer term, of course, a program that could write programs could write these sort of applications like any other.</p>\n", "question": "<p>I'm curious about Artificial Intelligence. In my everyday job I develop standard applications, like websites with basic functionalities like user subscription, file upload, forms saved in a database... </p>\n\n<p>I mainly know of AI being used in games or robotics fields. But can it be useful in \"standard\" application development?</p>\n"}, "id": "2098"}, {"body": {"answer": "<p>Adaptive/predictive features are useful in at least some everyday applications. Take text messaging, for instance. All smartphone SMS apps that I know of keep track of the words you use in close proximity and use that information to predict the next word in a message you're typing. (Some are smarter than others. <a href=\"https://xkcd.com/1068/\">Relevant XKCD.</a>) It can be used to personalize automatic spelling correction as well.</p>\n\n<p>A potential application interesting to me personally is tile-based level editors, like for classic DOS games. I've been <a href=\"https://fleexlab.blogspot.com/search/label/markeen\">working on a program</a> that gathers the probabilities of each tile being close to every other tile and uses that information to construct random new levels. It hasn't produced anything playable yet, but I think it has the potential to assist human level builders by e.g. automatically filling in the missing tile that fits in a newly placed structure, as opposed to requiring the human to go find the right one in the palette.</p>\n\n<p>In general, AI could be applied <em>very</em> usefully into figuring out what the user might want to do next and expediting the process of implementing the correct guess while staying out of the way if the user is intentionally doing something unexpected.</p>\n", "question": "<p>I'm curious about Artificial Intelligence. In my everyday job I develop standard applications, like websites with basic functionalities like user subscription, file upload, forms saved in a database... </p>\n\n<p>I mainly know of AI being used in games or robotics fields. But can it be useful in \"standard\" application development?</p>\n"}, "id": "2099"}, {"body": {"answer": "<p>Well,Artificial Intelligence is a wide computer scientific field.for instance it includes other sub fields like Machine Learning/deep learning.Creating intelligent agent that can imitate human behaviors is quite complex.</p>\n\n<p>Therefore,with the on going research,this has been partially implemented in projects like google search engine,Microsoft Tay which analysed human twits for some good time.,Operating Systems we use every day embedded with intelligent agents apps like Siri,Cortana. </p>\n\n<p>To sum it up;the major areas of AI or ML and in a variety of Standard Applications are Natural Language Processing, vision or pattern recognition in google self driving cars,the Web,Social Networks and computational biology.</p>\n\n<p><strong>Under computational Biology :</strong></p>\n\n<p>let me just give you my small knowledge about it.Am a graduate and my area of specialization is in software engineering,currently working on my final year project and doing research focusing on developing machine learning algorithm that will enable the use of an individual\u2019s comprehensive biological information to predict or diagnose diseases, and to find or develop the best therapy for that individual.</p>\n\n<p>If it has recently become possible to retrieve molecular-level information from an individual, such as DNA sequence, gene expression levels in various tissues, epigenomic profile and other information done by big scientists from big medical facility . While such data is increasingly available, Am still unable to understand the genetic and molecular mechanisms that cause diseases. The challenge is due to the multifactorial nature of disease. The same disease can be caused by mutations in different genes or different pathogenic pathways. Unfortunately, current data analysis approaches fail to capture the complex relationship between disease and the vast amount of information in the molecular data.</p>\n\n<p>The aim of my research is to resolve this challenge with other professional researchers by developing machine learning algorithms that jointly model sophisticated interactions among many variables such as genetic variation, genes, pathways and disease, and robustly learn from vast amounts of data in order to better understand and treat disease. An approach that can robustly infer the pathways that can define disease processes will dramatically improve our understanding of diseases and advance personalized medicine in its treatment. We aim to realize this goal by using modern, advanced machine learning techniques that are based on Artificial  Intelligence.</p>\n\n<p>I love Artificial Intelligence...it's changing everything like internet of things,music,health,education,space exploration and agriculture,e-business.</p>\n\n<p>Lastly,for your information,DARPA is investing in multi billions in Artificial Intelligence Projects along side Military. And if you would like to have a short sight on this check out this <a href=\"https://www.cybergrandchallenge.com/\" rel=\"nofollow\">CyberGrandChallenge</a></p>\n", "question": "<p>I'm curious about Artificial Intelligence. In my everyday job I develop standard applications, like websites with basic functionalities like user subscription, file upload, forms saved in a database... </p>\n\n<p>I mainly know of AI being used in games or robotics fields. But can it be useful in \"standard\" application development?</p>\n"}, "id": "2102"}, {"body": {"answer": "<p>A method that could possibly work is utilising optical illusions such as one where two lines down a hallway are identical but one seems longer to the human eye, then they could be prompted with a multiple choice question as to the state of the line, which to our eyes looks longer, but to a computer, is still the same length of line. Of course, there is always the issue of people with eye based disabilities not being able to complete them, but different illusions could be used to accommodate that.</p>\n\n<p><a href=\"https://www.brainbashers.com/showillusion.asp?85\" rel=\"nofollow\">Example</a></p>\n", "question": "<p>Are there any modern techniques of generating <strong>textual</strong> CAPTCHA (so person needs to type the right text) challenges which can easily <a href=\"http://ai.stackexchange.com/q/92/8\">fool AI</a> with some visual obfuscation methods, but at the same time human can solve them without any struggle?</p>\n\n<p>For example I'm talking about plain ability of <strong>recognising text embedded into image</strong> (without considering any external plugins like flash or java, image classification, etc.) and re-typing the text that has been written or something similar.</p>\n\n<p>I guess adding noise, gradient, rotating letters or changing colours are not reliable methods any more, since they can be quickly broken.</p>\n\n<p>Any suggestions or research has been done?</p>\n"}, "id": "2104"}, {"body": {"answer": "<p>You can train DNN to <strike>learn</strike> compute any abstract concept just by making that abstract concept as the label (output) in the training dataset. For example there are projects which detects emotions from peoples photos.</p>\n", "question": "<p>It seems that deep neural networks are making improvements largely because as we add nodes and connections, they are able to put together more and more abstract concepts. We know that, starting from pixels, they start to recognize high level objects like cat faces, chairs, and written words. Has a network ever been shown to have learned a more abstract concept that a physical object? What is the \"highest level of abstraction\" that we've observed?</p>\n"}, "id": "2108"}, {"body": {"answer": "<p>The intution that I have about these is that generative are \"from abstract to concrete\" whereas discriminative models are \"from concrete to abstract\".</p>\n\n<p>For example: Detecting if a photo has a cat or not is about going from the photo i.e concrete to the abstract concept of a cat. Whereas generating a photo of a cat given some abstract properties about the cat is going from abstract to concrete.</p>\n", "question": "<p>I'm trying to gain some intuition beyond definitions, in any possible dimension. I'd appreciate references to read.</p>\n"}, "id": "2109"}, {"body": {"answer": "<p>Artificial intelligence by definition is the intelligence exhibited by machines.  The definition of life in biological terms is the condition that distinguishes organisms from inorganic matter where the distinguishing criteria are the capacity for growth, reproduction, functional activity, and continual change preceding death. Does artificial intelligence \"grow\"?  Indeed, I can program a machine learning program to grow with every input taken in.  In the loosest sense, we can say that artificial intelligence does grow, but does it biologically? If we look at the definition for growth of a living thing, it means to undergo natural development by increasing in size and changing physically or the progress to maturity.  All living organisms undergo growth.   Even though at the simplest level, cells are a series of chemical processes, cells are a very complicated set of chemical processes that are still not fully understood by scientists across the world.  Every cell has genetic material that can be replicated, excised, used for RNA, proteins, and that is subject to epigenetic regulation. \n<a href=\"http://i.stack.imgur.com/nczDU.png\" rel=\"nofollow\"><img src=\"http://i.stack.imgur.com/nczDU.png\" alt=\"Cell division\"></a></p>\n\n<p>Does artificial intelligence undergo the same process of cell division?  No.  If I wanted to, I could write a program that undergoes a simple for-loop (print i from 1 to 100), replicates itself at a certain point (i=50) to produce the same program perhaps with some variation that will execute itself, and terminates (dies) at the end of the for loop.  The program, by an extremely loose definition supported by philosophy but not by biology, lives.  However, in scientific terms (and the correct interpretation), artificial intelligence is not living.  Artificial intelligence can be seen to be similar to viruses which are considered to be acellular and essential to life but not living.  Viruses are encapsulated DNA and RNA that undergo processes of growth, reproduction, and functionality but because they lack the ability to undergo the cell division cycle, are considered non-living.  At the very basis of the scientific definition of life is the cell replication cycle.  Artificial intelligence and viruses are not able to undergo the cell cycle.  Viruses need to infect other cells in order to reproduce but do not have their own, autonomous cycle.  At the end of the day, if you can argue that viruses are alive, you can argue that artificial intelligence is alive as well.  For the scientific definition of life, artificial intelligence must undergo the process of cell division and replication.  Even though artificial intelligence can mimic and help sustain life, no artificial intelligence process is truly alive. </p>\n\n<p>Do note I did not discuss <a href=\"http://www.isss.org/primer/asem14ep.html\" rel=\"nofollow\">living systems</a> in my answer.          </p>\n\n<p><a href=\"https://www.ncbi.nlm.nih.gov/books/NBK21685/\" rel=\"nofollow\">Definition of life</a></p>\n", "question": "<p>I'm a bit confused about the definition of life. Can AI systems be called 'living'? Because they can do most of the things that we can. They can even communicate with one another. </p>\n\n<p>They are not formed of what we call cells. But, you see, cells are just a collection of several chemical processes which is in turn non-living just like AI is formed of several lines of code.</p>\n"}, "id": "2113"}, {"body": {"answer": "<p>There is of course a vast amount of work in the area of Automatic Theorem Proving, but most of it is simply concerned with proof, rather than human notions of beauty, elegance, parsimony etc.</p>\n\n<p>There has however been some work in this general area over the years:</p>\n\n<ul>\n<li>Douglas Lenat's famous <a href=\"https://www.aaai.org/Papers/AAAI/1983/AAAI83-059.pdf\" rel=\"nofollow\">AM</a> ('Amateur Mathematician').</li>\n<li>Douglas Hofstadter's <a href=\"https://en.wikipedia.org/wiki/Fluid_Concepts_and_Creative_Analogies#Chapter_3:Numbo:_A_Study_in_Cognition_and_Recognition\" rel=\"nofollow\">NUMBO</a> program for number sequence extrapolation.</li>\n<li>A range of publications by <a href=\"http://www.doc.ic.ac.uk/~sgc/cv.html\" rel=\"nofollow\">Simon Colton</a></li>\n<li><a href=\"http://www.math.rutgers.edu/~zeilberg/ekhad.html\" rel=\"nofollow\">Shalosh B. Ekhad</a>, the automated proof assistant for Artificial Combinatorics created by Doron Zeilberg and credited by him as a co-author on numerous papers. </li>\n</ul>\n", "question": "<p>I've heard of AI that can solve math problems. Is it possible to create a 'logic system' equivalent to humans that can solve mathematics in the so called 'beautiful' manner?  Can AI find beauty in mathematics and solve problems other than using brute force? Can you please provide with examples where work on this is being done? </p>\n"}, "id": "2114"}, {"body": {"answer": "<p>Any machine with a sufficient level of integrated purpose driven behavior - that exhibits agency in an autopoietic, self-preserving way - will come to be viewed as \"alive.\" Chess programs, not so much; self-driving cars, slightly; simulated robot animals, even more so. It has to do with purpose driven behavior and a richness of multi-domain functionality. The more complex agency it has, the more sympathetic we will be towards it.</p>\n", "question": "<p>I'm a bit confused about the definition of life. Can AI systems be called 'living'? Because they can do most of the things that we can. They can even communicate with one another. </p>\n\n<p>They are not formed of what we call cells. But, you see, cells are just a collection of several chemical processes which is in turn non-living just like AI is formed of several lines of code.</p>\n"}, "id": "2115"}, {"body": {"answer": "<p>We don't know how to do that yet. The problem is one of scale:</p>\n\n<p>Despite many years of research into program synthesis via heuristic methods, it's still not possible to automatically create programs (e.g. via Genetic Programming (GP), Grammatical Evolution (GE) or Learning Classifier Systems (LCS)) that are thousands of lines long, whether that's for mobile or any other application area.</p>\n\n<p>Contrary to popular belief, alternative formal methods approaches can indeed be used to create sizeable programs, but the kind of interaction that a mobile app would typically require is not easily specified in this way.</p>\n\n<p>The scale at which heuristic approaches are currently viable is closer to the scale of expressions (e.g. single program statements) than entire programs. An intermediate approach is therefore to provide a program template and let GP etc generate the missing parts of the template.</p>\n\n<p><a href=\"http://www.nburles.co.uk/sites/default/files/attachments/templar-a-framework-for-template-method-hyper-heuristics.pdf\" rel=\"nofollow\">This paper</a> describes how to combine Machine Learning with the 'Template Method' Design Pattern in order to create larger programs than would otherwise be possible, giving the specific example of a 'hyper-quicksort'.</p>\n", "question": "<p>There are AI creating game, content and more.</p>\n\n<p>I'm thinking on how can AI develop mobile app itself?</p>\n\n<p>The computer languages might easy for AI to learn.</p>\n\n<p>AI can learn a lot from good open source project in github.</p>\n\n<p>The trend prediction can help AI to select the topic for creating a great apps.</p>\n\n<p>There are lots of details to let AI create a great apps. </p>\n"}, "id": "2120"}, {"body": {"answer": "<p>An umbrella term for the application of heuristic techniques to software development is 'Search Based Software Engineering' (SBSE).</p>\n\n<p>SBSE emerged as a distinct activity around the turn of the century, with a strong initial focus on automating the generation/prioritization of test cases.</p>\n\n<p>With respect to some of your specific queries:</p>\n\n<p>1.2 Paper on <a href=\"https://www.lri.fr/~hansen/proceedings/2013/GECCO/companion/p205.pdf\">Automated refactoring</a></p>\n\n<ol start=\"2\">\n<li><p>Automatically choosing screen colour to <a href=\"http://www-bcf.usc.edu/~halfond/papers/li15demobile-abstract.pdf\">minimize energy consumption</a>.</p></li>\n<li><p>This sort of thing is not usually done heuristically, since it needs platform-specific code.</p></li>\n<li><p>Automated <a href=\"https://www.computer.org/csdl/proceedings/icse/2000/2147/00/21470722.pdf\">refactoring to patterns</a>.</p></li>\n<li><p>AFAIK, penetration testing has yet to be successfully automated.</p></li>\n<li><p>As stated, this doesn't really require AI. More generally, I don't know of any specific work automating for HCI preferences, but something like 'Interactive Genetic Algorithms' could be used.</p></li>\n<li><p>There's a lot of SBSE literature on testing. See <a href=\"http://www0.cs.ucl.ac.uk/staff/mharman/laser.pdf\">this paper</a> for a general overview.</p></li>\n</ol>\n", "question": "<p>I have seen an AI create a game it self, AI act as a lawyer, call center etc.</p>\n\n<p>There are many problems (Example for mobile development)</p>\n\n<pre><code>1. New api/technology or even new language every year.\n2. New design\n3. New hardware\n4. Good code architecture, design pattern\n5. Security\n6. Image/Animation optimization\n7. Automate testing\n</code></pre>\n\n<p>etc.</p>\n\n<p>I wonder that AI can help developer solve that problems.</p>\n\n<p>1.1 May be I want to get the location then AI suggest the best api for specific platform.</p>\n\n<p>1.2 AI help to refactoring and optimizing the code</p>\n\n<ol start=\"2\">\n<li><p>Help on design e.g. golden ratio, Material theme color</p></li>\n<li><p>Suggest or determine the limit of the hardware e.g. screen size, ram</p></li>\n<li><p>Can convert to another design pattern </p></li>\n<li><p>Help to waring the latest vulnerable and automate pentest etc.</p></li>\n<li><p>Help to optimize image by learning how much can we reduce the image size while people still ok with it.</p></li>\n<li><p>Generate automate-testing</p></li>\n</ol>\n\n<p>Is there any solution existed?</p>\n\n<p>If not, what can we do?</p>\n"}, "id": "2121"}, {"body": {"answer": "<p>A popular dataset is the fisher iris dataset. It consists of 150 samples each with a dimensionality of 4. You can find it at\n<a href=\"http://archive.ics.uci.edu/ml/datasets/Iris\" rel=\"nofollow\">http://archive.ics.uci.edu/ml/datasets/Iris</a></p>\n", "question": "<p>New to the topic, I think I have figured out how to implement a Multi Level Perceptron(MLP) ANN.</p>\n\n<p>And was wondering if there are any simple data sets to test a MLP ANN ?\ni.e. small number of inputs and outputs</p>\n\n<p>I'm not getting expected results from uci cancer, I was hoping someone could save me some time and point me to some data they have used before ?</p>\n\n<p>Maybe start slightly more complex than XOR ?</p>\n"}, "id": "2123"}, {"body": {"answer": "<p>One of the main arguments for self-driving cars is that presumably they'll get better and better at driving as the technology progresses, they have no temporal attention deficits or aggressive urges or drug habits and sense their environment 360\u00b0, all the while communicating with the other cars, which all together basically amounts to LESS DEAD PEOPLE. </p>\n\n<p>We are really interested in this.</p>\n\n<p>It is also unclear whether most people will actually own cars in 30 years. Maybe there'll be a net of mini busses with flexible routes which take you from door to door on demand. That would reduce traffic quite a bit and there would also be less incentive to drive 200 m to get cigarettes or something. Self-driving cars would allow us to use the car as a resource a lot more efficiently, because suddenly we can relocate empty cars without paying a driver.  </p>\n", "question": "<p>What are the advantages of having self-driving cars?</p>\n\n<p>We will be able to have more cars in the traffic at the same time, but won't it also make more people choose to use the cars, so both the traffic and the public health will actually become worse?</p>\n\n<p>Are we really interested in this?</p>\n"}, "id": "2128"}, {"body": {"answer": "<p>There is a neat definition of artificial intelligence, which circumvents the problem of defining \"intelligence\" and which I would ascribe to <a href=\"https://en.wikipedia.org/wiki/John_McCarthy_(computer_scientist)\" rel=\"nofollow\">McCarthy</a>, the founder of the field, although I can only find it now in <a href=\"https://books.google.de/books?id=IY19CAAAQBAJ&amp;pg=PA53&amp;lpg=PA53&amp;dq=that%20we%20would%20call%20intelligent%20if%20it%20were%20done%20by%20a%20human&amp;source=bl&amp;ots=I8O-U1Jx8q&amp;sig=3VfZuVaLYtLGCtUo4uSbOjzrboE&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwjG18Se6tTPAhUE1hoKHUppA88Q6AEIHjAA#v=onepage&amp;q=that%20we%20would%20call%20intelligent%20if%20it%20were%20done%20by%20a%20human&amp;f=false\" rel=\"nofollow\">this book</a> by H. Simon:</p>\n\n<p>\"\u2026 having to do with finding ways to do intelligent tasks, to do tasks which, if they were done by human beings, would call for our human intelligence.\"</p>\n\n<p>So, at its core we call the automation of every task AI, that can only be done by the human mind. At the time people thought that a computer able to play chess would also be intelligent in other ways. When this turned out to be false, the term AI was split into \"narrow or weak AI\", i.e. a program able to do one task of the human mind, and \"general or strong AI\", a program that can do all the tasks of the human mind. </p>\n\n<p>Self-driving cars are narrow AI. </p>\n\n<p>Note, that all these definitions don't specify whether these programs copy the way the human mind works or whether they come to the same result via completely different algorithms. </p>\n", "question": "<p>How are autonomous cars related to artificial intelligence? I would presume that artificial intelligence is when we are able to copy the human state of mind and perform tasks in the same way. But isn't autonomous car just rule-based machines that operates due to its environment? They are not self-aware, and they cannot choose a good way to act in a never before experienced situation.</p>\n\n<p>I know that many people often mention autonomous cars when speaking about AI, but I am not really convinced that these are related. Either I have a too strict understanding of what AI is or </p>\n"}, "id": "2129"}, {"body": {"answer": "<p>One of the most common requirements to be defined as life is abbreviated to <strong>MRS GREN</strong></p>\n\n<p>this means:</p>\n\n<p>M - movement<br>\nR - respiration<br>\nS - sensitivity</p>\n\n<p>G - growth<br>\nR - reproduce<br>\nE - excretion<br>\nN - nutrition</p>\n\n<p>An AI can technically do some of these, it can move its location from device to device, it can grow its own code, and assimilate other bits of code it can find, which fits growth and kind of fits respiration also firewalls could almost be sensitivity.</p>\n\n<p>But then there is nothing relating to nutrition or excretion, so it fits most definitions of life, but it depends on the complexity of life and which definition of life you are using.</p>\n", "question": "<p>I'm a bit confused about the definition of life. Can AI systems be called 'living'? Because they can do most of the things that we can. They can even communicate with one another. </p>\n\n<p>They are not formed of what we call cells. But, you see, cells are just a collection of several chemical processes which is in turn non-living just like AI is formed of several lines of code.</p>\n"}, "id": "2132"}, {"body": {"answer": "<p>There are already programs that have broken free of our control (<a href=\"https://en.wikipedia.org/wiki/Morris_worm\" rel=\"nofollow\">Morris worm</a>) so that in itself doesn't imply any great computational demands.</p>\n\n<p>Sentience is ill-defined but is certainly not a pre-requisite for a program to do mischief beyond what its creators intend.</p>\n\n<p>It's difficulty to estimate what sort of processing power is required to support human-like intelligence, since we don't know what the most efficient way to achieve that would be.  If the most processing efficient would be to implement a neural network approaching the number of neurons and interconnects of the human brain processing signals at the same rate, the fastest artificial neural network implementations extant are at least 4-5 orders of magnitude short, is thousands of times less power efficient, and doesn't seem to have a realistic way to scale to the number of interconnects required (<a href=\"http://ai.stackexchange.com/questions/1834/power-efficiency-of-human-brains-vs-neural-networks\">see this question</a>)</p>\n", "question": "<p>In lots of sci-fi, it seems that AI becomes sentient (Terminator, Peter F Hamilton's SI (commonwealth saga), etc.)</p>\n\n<p>However, I'm interested in whether this is actually plausible, whether an AI could actually break free form being controlled by us, and if that is possible, whether there is any research as to about what sort of complexity / processing power an AI would need to be able to do this.</p>\n"}, "id": "2133"}, {"body": {"answer": "<p>Self driving cars exhibit a level of agency and multi-domain resilience. By certain definitions they <em>are</em> self aware and they are definitely designed to fail safely in a large number of potentially unknown circumstances, which is similar to biological agents.</p>\n\n<p>AI really has to do with the study of non-biological agents and their methods of agency. Everything else is just computer science, algorithmic efficiency, biology, art, etc. Eventually the study of biological and non-biological agency will converge, though, and we'll just call it the study of \"intelligence.\"</p>\n", "question": "<p>How are autonomous cars related to artificial intelligence? I would presume that artificial intelligence is when we are able to copy the human state of mind and perform tasks in the same way. But isn't autonomous car just rule-based machines that operates due to its environment? They are not self-aware, and they cannot choose a good way to act in a never before experienced situation.</p>\n\n<p>I know that many people often mention autonomous cars when speaking about AI, but I am not really convinced that these are related. Either I have a too strict understanding of what AI is or </p>\n"}, "id": "2134"}, {"body": {"answer": "<p>No one knows. </p>\n\n<p>A useful definition of sentience due to the philosopher Thomas Nagel is <a href=\"https://en.wikipedia.org/wiki/Thomas_Nagel#What_is_it_like_to_be_a_something\" rel=\"nofollow\">'something it is like'</a> to be. </p>\n\n<p>For example, we intuitively feel that there is nothing it is like to be a brick, but that there probably is to be a dog and so on.</p>\n\n<p>However, there is no objective <em>test</em> currently known to physics which can tell if some other entity is having such 'first hand experience', and correspondingly no <em>designs</em> that will definitely lead to sentience.</p>\n\n<p>The best test we have is the Turing test and its variants. The most obvious designs are neuromorphic ones, since we know that the design of the human brain is at least correlated with sentience.</p>\n\n<p>In the light of the above, we can't definitively say a great deal about lower complexity thresholds for sentience - the best we can do count neurons in creatures that we might be prepared to admit are sentient.</p>\n", "question": "<p>In lots of sci-fi, it seems that AI becomes sentient (Terminator, Peter F Hamilton's SI (commonwealth saga), etc.)</p>\n\n<p>However, I'm interested in whether this is actually plausible, whether an AI could actually break free form being controlled by us, and if that is possible, whether there is any research as to about what sort of complexity / processing power an AI would need to be able to do this.</p>\n"}, "id": "2135"}, {"body": {"answer": "<p>You're unsure about the definition of life (which the other answers clarify) but also most people are unclear about the definition of AI. Do you mean an AI that can accomplish a routine task (such as the path finder in a GPS) or a General AI that is able to find a creative solution to any directive given to it (such an AI does not yet exist and may not ever exist) or do you mean a SENTIENT computer program? <a href=\"http://alternativemindsets.co.uk/different-types-artificial-intelligence/\" rel=\"nofollow\">Here is a simple article introducing some different concepts refered to as AI</a></p>\n\n<p>Some people believe that a sentient computer program would be entitled to human rights. Not technically 'alive' in the biological sense, but having self awareness, will, desires, etc. Others disagree and believe that the program is a mere simulation that artificially mimics the actions of a human with a human soul, and is no more human than a washing machine. This is a very deep philosophical and meta-physical debate. For example, in <a href=\"https://en.wikipedia.org/wiki/A.I._Artificial_Intelligence\" rel=\"nofollow\">A.I. the movie</a> the overall message is that an android can simulate the emotion of love in a way that is more loyal and sincere than any human.</p>\n\n<p>What I find interesting about this purely theoretical debate is that in almost every instance of sci-fi media that deals with the theme, the AI exists inside of a human-like android. But technically, the shape of the robot should be irrelevant.</p>\n", "question": "<p>I'm a bit confused about the definition of life. Can AI systems be called 'living'? Because they can do most of the things that we can. They can even communicate with one another. </p>\n\n<p>They are not formed of what we call cells. But, you see, cells are just a collection of several chemical processes which is in turn non-living just like AI is formed of several lines of code.</p>\n"}, "id": "2137"}, {"body": {"answer": "<p>AI is a broad term referring to more than one concept, each with its own definition.\n<a href=\"http://alternativemindsets.co.uk/different-types-artificial-intelligence/\" rel=\"nofollow\">Different Types of AI</a></p>\n\n<p>The lowest levels are extremely simple and common, such are an artificial chess opponent. They work well because the programs internal model of reality is a 8x8 grid with only a few rules. The program chooses a preferred action by running simulations in it's internal model of reality.</p>\n\n<p>What is often meant by AI is a \"General Intelligence\" that can come up with a creative solution to any directive, based on it's internal comprehension of the world. There is no existing example of this as of yet. The problem is that its internal model of reality needs to provide for every possible action. And it's possible actions and reactions may not be limited to a finite number of discretely distinct moves as in a game of chess. (At least if it works on the basis of conventional programming) Even ignoring the computational power needed to run such a broad and inefficient program, it would also require some stupendous amount of labour to program this internal model of reality in the first place.</p>\n\n<p>I think in Sci-Fi, when people say AI, they mean a computer program that has a kind of awareness of itself and the world around it and can come up with creative and unexpected courses of actions in order to achieve its objective. Often the AI is NOT sentient, which is why it does not understand that its actions are morally wrong, or that its solution defeats the underlying intention of its assigned directive. The Horror lay in the concept that an amoral entity has more processing power than human kind.</p>\n", "question": "<p>The concept is intrinsically related with building some sort of media for the AI to exists. We may think of a digital computer, programmed to use language and act in a way that we cannot be distinguished from a human. But, does the media really mater (unconventional computation paradigms)? Does having a certain control over the limits of what the AI can do matter? Synthetic biology has the ultimate goal of building biological systems from scratch , would a synthetic brain, potentially introduced in a synthetic human, constitute AI?</p>\n\n<p>I am just looking for a clear definition of what most people have in mind when they refer to AI.</p>\n"}, "id": "2138"}, {"body": {"answer": "<p>There are a ton of sample datasets our there you can play with. A bunch of good ones install with R in the datasets package.  Luckily you can download them independently if you're not an R user.  Try  <a href=\"https://vincentarelbundock.github.io/Rdatasets/datasets.html\" rel=\"nofollow\">https://vincentarelbundock.github.io/Rdatasets/datasets.html</a></p>\n\n<p>You might also be interested in the <a href=\"http://yann.lecun.com/exdb/mnist/\" rel=\"nofollow\">MNIST database</a> which is one of the canonical databases used in handwriting recognition research.</p>\n\n<p>Beyond that, you can look at / ask on <a href=\"http://datasets.reddit.com\" rel=\"nofollow\">http://datasets.reddit.com</a> and/or <a href=\"http://opendata.reddit.com\" rel=\"nofollow\">http://opendata.reddit.com</a> and you'll find all sorts of useful datasets.</p>\n\n<p>And finally, don't overlook the <a href=\"http://archive.ics.uci.edu/ml/\" rel=\"nofollow\">UCI Machine Learning Repository</a>.</p>\n", "question": "<p>New to the topic, I think I have figured out how to implement a Multi Level Perceptron(MLP) ANN.</p>\n\n<p>And was wondering if there are any simple data sets to test a MLP ANN ?\ni.e. small number of inputs and outputs</p>\n\n<p>I'm not getting expected results from uci cancer, I was hoping someone could save me some time and point me to some data they have used before ?</p>\n\n<p>Maybe start slightly more complex than XOR ?</p>\n"}, "id": "2139"}, {"body": {"answer": "<p>This is one of those things where I think the answer is going to change over time.  Today, I don't know anyone who would call any present AI systems \"alive\".  But as the AI's become more intelligent and human-like, I could see the day coming when they will be considered living.    </p>\n\n<p>Sorry for the brief answer, but it's lake, I'm sick and jazzed up on Nyquil.  Will try to add more depth to this answer later.</p>\n", "question": "<p>I'm a bit confused about the definition of life. Can AI systems be called 'living'? Because they can do most of the things that we can. They can even communicate with one another. </p>\n\n<p>They are not formed of what we call cells. But, you see, cells are just a collection of several chemical processes which is in turn non-living just like AI is formed of several lines of code.</p>\n"}, "id": "2140"}, {"body": {"answer": "<p>It's an interesting question about what makes humans unique. There is a good book on the subject titled <a href=\"https://archive.org/stream/whatcomputerscan017504mbp/whatcomputerscan017504mbp_djvu.txt\" rel=\"nofollow\">What Computers Cant Do</a> by <a href=\"https://en.wikipedia.org/wiki/Hubert_Dreyfus\" rel=\"nofollow\">Hubert Dreyfus</a>.</p>\n\n<p>One task that a computer can't handle (for now at least) is ranking important things. For example, CAPTCHA asks you to order a random list of things (small one, five or six items) by importance. This particular exercise requires AI to take decisions (not always rational) based on human judgement.</p>\n", "question": "<p>Are there any modern techniques of generating <strong>textual</strong> CAPTCHA (so person needs to type the right text) challenges which can easily <a href=\"http://ai.stackexchange.com/q/92/8\">fool AI</a> with some visual obfuscation methods, but at the same time human can solve them without any struggle?</p>\n\n<p>For example I'm talking about plain ability of <strong>recognising text embedded into image</strong> (without considering any external plugins like flash or java, image classification, etc.) and re-typing the text that has been written or something similar.</p>\n\n<p>I guess adding noise, gradient, rotating letters or changing colours are not reliable methods any more, since they can be quickly broken.</p>\n\n<p>Any suggestions or research has been done?</p>\n"}, "id": "2143"}, {"body": {"answer": "<p>Safety is often put in focus by journalists. Although there is potential to make the roads safer, I don't think that is the driving force behind the push for self-driving cars. The main advantage of self-driving cars is that this will reduce costs for businesses, while increasing efficiency (both fuel and time). From the perspective of the public, the self-driving cars are attractive, because they will turn the task of driving, into commute. Activity that requires attention will be replaced with somewhat free time.</p>\n", "question": "<p>What are the advantages of having self-driving cars?</p>\n\n<p>We will be able to have more cars in the traffic at the same time, but won't it also make more people choose to use the cars, so both the traffic and the public health will actually become worse?</p>\n\n<p>Are we really interested in this?</p>\n"}, "id": "2146"}, {"body": {"answer": "<p>I have very little experience with ML/DL to call myself either practitioner, but here is my answer on the 1st question:</p>\n\n<p>At its core DL solves well the task of classification. Not every practical problem can be rephrased in terms of classification. Classification domain needs to be known upfront. Although the classification can be applied to any type of data, it's necessary to train the NN with samples of the specific domain where it'll be applied. If the domain is switched at some point, while keeping the same model (NN structure), it'll have to be retrained with new samples. Furthermore, even the best classifiers have \"gaps\" - <a href=\"http://www.kdnuggets.com/2015/07/deep-learning-adversarial-examples-misconceptions.html\" rel=\"nofollow\">Adversarial Examples</a> can be easily constructed from a training sample, such that changes are imperceptible to human, but are misclassified by the trained model.</p>\n", "question": "<p>Over the last 50 years, the rise/fall/rise in popularity of neural nets has acted as something of a 'barometer' for AI research.</p>\n\n<p>It's clear from the questions on this site that people are interested in applying Deep Learning (DL) to a wide variety of difficult problems.</p>\n\n<p>I therefore have two questions:</p>\n\n<ol>\n<li>Practitioners - What do you find to be the main obstacles to\napplying DL 'out of the box' to your problem? </li>\n<li>Researchers - What\ntechniques do you use (or have developed) that might help address\npractical issues? Are they within DL or do they offer an\nalternative approach?</li>\n</ol>\n"}, "id": "2147"}, {"body": {"answer": "<p>One cannot judge any form of intelligence, artificial or natural, whether it is complete or incomplete. Having it complete means that you are imposing limits to what it is capable of, the Turing test only test if your machine have intelligence that is similar to humans, therefore to decide whether it is complete or not would have to be based on the completeness of our own intelligence. Humans such as ourselves learn new things each day. If you'd run any algorithm that would judge the AI for it's completeness, it would have to run forever and your results would have to vary on every moment of the existence of natural intelligence.</p>\n", "question": "<p>What could be an algorithm that determines whether an AI ( algorithm ) is \nAI Complete or not ?\nHow does one proceed to program it ?</p>\n\n<p>edit : question edited due to some misinterpretation in the first answer !</p>\n"}, "id": "2150"}, {"body": {"answer": "<blockquote>\n  <p><strong>FORWORD NOTE:</strong> this answer is a breakdown based on my Artificial Intelligence, which based on description is very similar to Angelina.</p>\n  \n  <p>I do want to emphasize that it is <em>NOT</em> Angelina</p>\n</blockquote>\n\n<p><strong><em>Like all artificial intelligences</em></strong>, in order to fully design it, <strong><em>you have to break AI and intelligence down deeply</em></strong>. If there is a confusion about a certain aspect to intelligence, you haven't broken it down enough.</p>\n\n<p>I, myself, have managed to break down the intellect of producing a program (or essentially any product) very far and very deep.</p>\n\n<blockquote>\n  <p><strong>Side Note:</strong> An interesting and helpful part of finishing breaking it down, was that I did not have to worry about breaking down spoken language intelligence, as that is already well-successfully accomplished and there are APIs out there in which computational creativity researchers can use such as <a href=\"https://wit.ai/\" rel=\"nofollow\">wit.ai</a></p>\n  \n  <p>So, we <em>only</em> have to worry about breaking down the creativity aspect.</p>\n</blockquote>\n\n<h1>Breaking it Down:</h1>\n\n<h3>The Design Process:</h3>\n\n<blockquote>\n  <p><strong>Side Note:</strong> This I could easily provide a citation for, however it has too many accepted descriptions for me to be willing to cite one and to say that it is <em>the</em> or <em>a correct citation</em>. However, I will be providing one as a reference and that is the one provided very nicely on <a href=\"https://www.discoverdesign.org/handbook\" rel=\"nofollow\">DiscoverDesign</a>.</p>\n  \n  <p>The paragraph below is provided by them, and if you are interested in breaking that process down more, DiscoverDesign fully explains the processes in detail for you.</p>\n</blockquote>\n\n<p>The steps are <em>Define the Problem</em>, <em>Collect Information</em>, <em>Brainstorm and Analyze Ideas</em>, <em>Develop Solutions</em>, <em>Get Some Sort of Feedback</em>, <em>Improve</em> (which is essentially restart the process)</p>\n\n<h3>Defining a problem:</h3>\n\n<p>As far as this part of the breakdown goes, there two algorithms in which you can use for this subprocess of design:</p>\n\n<blockquote>\n  <p><em>Easy Algorithm (not really an algorithm):</em> ask from the client what the Artificial Intelligence is providing a solution for.</p>\n</blockquote>\n\n<p>However, this process could easily be made more interesting:</p>\n\n<blockquote>\n  <p><em>Difficult Algorithm</em>: design an algorithm that can define <em>a problem</em> without user input.</p>\n</blockquote>\n\n<p>I did some digging, and the design of the latter relies on one question that lacks enough research for a <strong>solid</strong> answer, and that is <em>where do questions come from psychologically</em>? or <em>more specifically, how does curiosity work</em>?</p>\n\n<p>With more research on Google I was led to <a href=\"http://science.howstuffworks.com/life/evolution/curiosity1.htm\" rel=\"nofollow\">this article</a> specifically addressing that question.</p>\n\n<h3>How Curiosity works:</h3>\n\n<p>The 2 theories it stated that have yet to be fully proven are <em>drive theory</em> and <em>incongruity theory</em></p>\n\n<blockquote>\n  <p>Drive Theory simply states, we have a <em>need</em> to be curious, and to fulfill that need, we ask questions.</p>\n</blockquote>\n\n<p>So, needless to say this theory isn't helpful to the design of the A.I.</p>\n\n<blockquote>\n  <p><em>Incongruity Theory</em> states that we are able identify things we <em>do not FULLY understand or understand AT ALL</em> which leads us to asking questions.</p>\n</blockquote>\n\n<p>With help of my peers contributing to my research project, I was able to induce from Incongruity Theory and observations I had noticed within interviews (not job interviews, press interviews) that questions are made by noticing a <em>missing/unclear attribute or characteristic</em> on a certain idea, concept, or object (essentially anything the brain can virtually image or understand).</p>\n\n<h3>My Own Inductive Theory on Curiousity</h3>\n\n<p>The way that I theorize that these missing/unclear attributes are identified is that your consciousness instantaneously, and subconsciously is looking at <em>other similar ideas</em> and looking at <em>their <strong>clear</strong> and <strong>concisely known</strong> attributes</em></p>\n\n<h3>Solution Based on the Theory:</h3>\n\n<p>So, what I have designed is fairly simple:</p>\n\n<blockquote>\n  <p><strong><em>An idea</em></strong> is <em>represented programmatically as an <strong>object</strong></em>.</p>\n  \n  <p><em>The object</em> has certain characteristics known as <em>properties</em> (which are those attributes).</p>\n  \n  <p>The program reads over those properties and finds other objects <em>similar</em> to it based on those properties.</p>\n  \n  <p>It then checks those similar objects for properties that the original object <em>does not have</em>, and therefore marks those properties as unknown on the original object, making it possible to apply <em>incongruity theory</em></p>\n</blockquote>\n\n<h3>Collecting Information:</h3>\n\n<p>This process is already achieved with <em>machine learning</em>, any questions on this subprocess of design need to be addressed to the <a href=\"http://ai.stackexchange.com/questions/tagged/machine-learning\">machine-learning tag</a></p>\n\n<h3>Brainstorming Ideas:</h3>\n\n<p>This could be accomplished by mixing an algorithm that collects information (collects already working solutions) with the algorithm that I described within the curiosity section</p>\n\n<h3>Analyzing Ideas:</h3>\n\n<p>This is a really simple one. Debugging (not getting input), and getting user feedback (getting input). To provide analyzation over simply an idea you could combine my algorithm, with another information collecting algorithm to <em>induce</em> whether an idea is feasible.</p>\n\n<h3>Developing Solutions:</h3>\n\n<p>This is where IDE-development knowledge comes in handy.</p>\n\n<p><em>In order to make product development</em> <strong><em>easy and understandable</em></strong> <em>to an AI</em>, we have to choose a type of product that could be developed <strong>easily</strong>.</p>\n\n<blockquote>\n  <p><em>Editor's Note:</em> I do recommend this in order to keep the testing of the algorithms for the previous processes really simple.</p>\n</blockquote>\n\n<h3>Easily Designed Product that I Selected for Designing an Algorithm:</h3>\n\n<blockquote>\n  <p>I am providing this to you, so you can model a way to reproduce this process for the Intelligence you would like to build. So, I only hope that you do not intend on copying it, but my artificial intelligence project is free and open-source, so there is no issue, if you do.</p>\n</blockquote>\n\n<p>Considering that written programs are very easy products to develop fully, and Considering that program language rules are straight forward. and <em>very consistent</em> in comparison to spoken/written languages, I chose to have it develop <em>programs</em>.</p>\n\n<p>So, in order to do this it has to understand how to write a program. The most essential skill a computer programmer can have and needs to write a program is not a <em>dictionary of programming terms, functions, and commands</em>, but rather <em>knowledge of the syntactical rules</em> for a programming language.</p>\n\n<p>The technological solution to this is pretty much already available in IDE tech, and it is known as <em>syntactical highlighting</em>. All that would have to be done is to re-purpose it from <em>highlighting</em> to <em>assisting with writing</em>.</p>\n\n<h3>Getting Some Sort of Feedback.</h3>\n\n<p>This is essentially the same as analyzing the ideas, but now we would be using algorithms to analyze the final physical product as opposed to conceptual ideas.</p>\n\n<h1>Afterword Notes:</h1>\n\n<p>I am designing and researching into <em>computational creativity</em>, and I do want to mention that I just discovered this field of research is a thing by looking up the name <a href=\"https://live.newscientist.com/mike-cook/\" rel=\"nofollow\">Mike Cook</a> on the internet, and that in order for me to help you, my answer does require lengthiness.</p>\n\n<p>Paragraph 3 of the page found there [Mike Cook link] (listed at the time of 10/13/2016 at 8:28pm Arizona (USA) Time) that Mike Cook specializes in <a href=\"https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=3&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwiP2LG2rNnPAhUQ82MKHcg0AgkQFggvMAI&amp;url=http%3A%2F%2Fwww.computationalcreativity.net%2Ficcc2016%2F&amp;usg=AFQjCNGK9E_tYBriAStd7HsIBjcf1KvQLg&amp;sig2=PD6wgisDW5YU3-224YGVYQ\" rel=\"nofollow\">computational creativity</a></p>\n\n<p>With further research this term was coined by the <a href=\"https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=3&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwiP2LG2rNnPAhUQ82MKHcg0AgkQFggvMAI&amp;url=http%3A%2F%2Fwww.computationalcreativity.net%2Ficcc2016%2F&amp;usg=AFQjCNGK9E_tYBriAStd7HsIBjcf1KvQLg&amp;sig2=PD6wgisDW5YU3-224YGVYQ\" rel=\"nofollow\">ICCC 2016</a> according to <a href=\"https://www.google.com/search?q=computational+creativity&amp;rlz=1C1CHBF_enUS700US700&amp;oq=computat&amp;aqs=chrome.0.0j69i57j0l4.3974j0j4&amp;sourceid=chrome&amp;ie=UTF-8\" rel=\"nofollow\">this google search</a> made by myself at that time.</p>\n\n<p>Unfortunately, google did not further provide me with any products actually being made within this field of research, so I would therefore like to discuss mine as it is open-source under a MIT-license.</p>\n\n<h3>Note to Community:</h3>\n\n<p>I do want to make clear that I am providing this answer out of helpfulness, and I do understand that it has no credibility, as the product I am using (as an example) is my own. So with that, the community (<a href=\"http://meta.stackexchange.com/questions/284969/encouraging-citations-in-answers\">I have a disagreement with this</a>) does not encourage, therefore I do not encourage that you select this as a correct answer.</p>\n\n<p>Future readers, please add to my answer or note in the comments of any developments in which I can cite in the case that you are aware of such devs and really liked my answer.</p>\n\n<p>If you reference anything about <a href=\"https://www.reddit.com/user/notGucci94/\" rel=\"nofollow\">notGucci94's account on reddit</a> I do want to state that that is my account. Therefore, is not useful as a citation either</p>\n\n<blockquote>\n  <p><strong>EDIT:</strong> due to compliance with StackExchange's rules, I can not provide the product's name or a link to it, as I am not to be and I am to avoid promoting a product as an answer. If you are interested in the licensing, please email me, and <strong>do not ask me to place the product in the answer, and do not ask me via email if you can receive a copy of the product.</strong> <strong>I am not and will not be promoting here in my community WHERE THE RULES SAY NO!</strong></p>\n  \n  <p>Please, be mindful of StackExchange's rules, and do not ask me to break them, as I value this community, and do not wish to lose my respect.</p>\n</blockquote>\n", "question": "<p>In the recent <a href=\"https://live.newscientist.com/\" rel=\"nofollow\">festival of science</a>, there was a talk given by researcher <a href=\"https://live.newscientist.com/mike-cook/\" rel=\"nofollow\">Mike Cook</a> about:</p>\n\n<blockquote>\n  <p><a href=\"http://www.gamesbyangelina.org/\" rel=\"nofollow\">ANGELINA</a>, an AI game designer that has invented game mechanics, made games about news stories, and was the first AI to enter a game jam.</p>\n</blockquote>\n\n<p>So the aim of Angelina AI is basically to design videogames.</p>\n\n<p>Briefly, how exactly does Angelina design the new games? How does it work behind the scenes?</p>\n"}, "id": "2151"}, {"body": {"answer": "<p>In the traditional sense of \"alive\", no because they aren't made of cells.  But from a more philosophical and less biological point of view, they could be.</p>\n\n<p>If the AI is contained within the computer it is in a reality (the digital world/virtual reality) that for the AI is just as real as the universe is to us.  From the outside world, there is no life inside the computer. And from within the computer, the computer is the entire known universe which has its own laws.  If the AI is self-aware, then it is alive in its own little universe, but not in ours.</p>\n\n<p>If the AI is not successfully contained in the computer and figures out how to manipulate things and evolve in the real world, it will be alive.  It might be pretty easy to kill (by unplugging the computer) but it has still been \"alive\".  In the broad sense, anything that evolves and can manipulate its environment is alive.</p>\n", "question": "<p>I'm a bit confused about the definition of life. Can AI systems be called 'living'? Because they can do most of the things that we can. They can even communicate with one another. </p>\n\n<p>They are not formed of what we call cells. But, you see, cells are just a collection of several chemical processes which is in turn non-living just like AI is formed of several lines of code.</p>\n"}, "id": "2156"}, {"body": {"answer": "<p>Can't comment(due to that required 50 rep), but I wanted to make a response to Vishnu JK and the OP. I think you guys are skipping the fact that the neural network only really is saying truly from a programmatic standpoint that \"this is most like\".</p>\n\n<p>For example, while we can list the above image examples as \"abstract art\", they definitively are most like was is listed. Remember learning algorithms have a scope on what they recognize as an object and if you look at all the above examples... and think about the scope of the algorithum... these make sense (even the ones at a glance we would recognize as white noise). In Vishnu example of the numbers, if you fuzz your eyes and bring the images out of focus, you can actually in every case spot patterns that really closely reflect the numbers in question.</p>\n\n<p>The problem that is being shown here is that the algorithm appears to not have a \"unknown case\". Basically when the pattern recognition says that it doesn't exist in the output scope. (so a final output node group that says this is nothing that I know off). For example, people do this as well, as it's one thing humans and learning algorithms have in common. Here's a link to show what I'm talking about (what is the following, define it) using only known animals that exist:</p>\n\n<p><img src=\"https://i.stack.imgur.com/AZwnpm.jpg\" alt=\"Picture link\"></p>\n\n<p>Now as a person, limited by what I know and can say, I'd have to conclude that the following is an elephant. But it's not. Learning algorithms (for the most part) do not have a \"like a\" statement, the out put always validates down to a confidence percentage. So tricking one in this fashion is not surprising... what is of course surprising is that based on it's knowledge set, it actually comes to the point in which, if you look at the above cases listed by OP and Vishnu that a person... with a little looking... can see how the learning algorithm probable made the association. </p>\n\n<p>So, I wouldn't really call it a mislabel on the part of the algorithm, or even call it a case where it's been tricked... rather a case where it's scope was developed incorrectly.</p>\n", "question": "<p>The following <a href=\"http://www.evolvingai.org/fooling\">page</a>/<a href=\"http://www.evolvingai.org/files/DNNsEasilyFooled_cvpr15.pdf\">study</a> demonstrates that the deep neural networks are easily fooled by giving high confidence predictions for unrecognisable images, e.g.</p>\n\n<p><a href=\"http://i.stack.imgur.com/7pgrH.jpg\"><img src=\"http://i.stack.imgur.com/7pgrH.jpg\" alt=\"Evolved images that are unrecognisable to humans, but that state-of-the-art DNNs trained on ImageNet believe with &gt;= 99.6% certainty to be a familiar object. This result highlights differences between how DNNs and humans recognise objects. Directly and indirectly encoded images\"></a></p>\n\n<p><a href=\"http://i.stack.imgur.com/pBm48.png\"><img src=\"http://i.stack.imgur.com/pBm48.png\" alt=\"Evolving images to match DNN classes produces a tremendous diversity of images. The mean DNN confidence scores for these images is 99.12% for the listed class, meaning that the DNN believes with near-certainty that the image is that type of thing. Shown are images selected to showcase diversity from 5 independent evolutionary runs. The images shed light on what the DNN network cares about, and what it does not, when classifying an image. For example, a school bus is alternating yellow and black lines, but does not need to have a windshield or wheels\"></a></p>\n\n<p>How this is possible? Can you please explain ideally in plain English?</p>\n"}, "id": "2157"}, {"body": {"answer": "<p>According to <a href=\"https://en.wikipedia.org/wiki/AI-complete\" rel=\"nofollow\">the Wikipedia definition</a>, a problem is said to be 'AI complete' if it requires generalized, human-level intelligence, i.e. requires 'Strong AI'. The Turing test and its variants are the best ways we have of measuring this. As suggested in <a href=\"http://wizzion.com/papers/2012/AISB-TuringTestTaxonomy.pdf#page=54\" rel=\"nofollow\">this paper</a>, in order for the Turing test to be meaningful, the interrogator has a responsibility to ask questions which are both deep and meaningful. </p>\n\n<p>It therefore seems likely that testing for Strong AI is in itself an 'AI complete' task.</p>\n", "question": "<p>What could be an algorithm that determines whether an AI ( algorithm ) is \nAI Complete or not ?\nHow does one proceed to program it ?</p>\n\n<p>edit : question edited due to some misinterpretation in the first answer !</p>\n"}, "id": "2159"}, {"body": {"answer": "<p>I believe AI is rarely used in mainstream apps, but it could be, and I think slowly will be.</p>\n\n<p>If the information an app's AI must learn arises within the app, from user interaction or error, it'd be smart if the program could log that kind of information and then look for patterns in the logs.  It could profile users to see ehat tasks are done most often, how many steps are needed.  Then when it recognizes that task recurring, it could ask the user if they wanted it to execute a macro that did the following [then it presents then with a list of the steps, allowing them to edit as needed].  Then it executes the 'macro' that it learned from observing the user.</p>\n\n<p>Another use of AI is error detection, not only in the software, but in user error when the software was used inefficiently, redundantly, or improperly.  If the software were designed such that it was given a set of models of user tasks (like AI plans), it could observe users in the way they achieve known tasks, and offer suggestions or ask for confirmation that imminent unusual outcomes are intended.</p>\n\n<p>And of course, AI could be used extensively in user interface design, on devices, web sites, or apps.  Some of this, like voice recognition, is entering the mainstream of daily use just now.  As conversations with apps that can add their own data and models of tasks/concepts/domains develop further, the need for AI <em>inside</em> the app will only grow.</p>\n\n<p>There are a <em>ton</em> of ways that AI could be used in apps.  A few of these have started to arise in mobile devices and their apps, usually in fusion of user mobility with external web-based databases (e.g. GPS and maps), but IMO it's been slow.</p>\n", "question": "<p>I'm curious about Artificial Intelligence. In my everyday job I develop standard applications, like websites with basic functionalities like user subscription, file upload, forms saved in a database... </p>\n\n<p>I mainly know of AI being used in games or robotics fields. But can it be useful in \"standard\" application development?</p>\n"}, "id": "2160"}, {"body": {"answer": "<p>They may be just for fun. If you had a robot that understood you, could hold a conversation with you about your interests, and even had goals of its own (good or bad), it wouldn't really need to do anything special. People would buy it like it was a toy or game.</p>\n\n<p>Also, they might be usable as programmers, artists, designers, anything creative that a computer can't successfully do on its own.</p>\n\n<p>It really just depends on what you define as 'consciousness'. Does it just understand what it's supposed to do, decide if it wants to, and if so, complete the task? Or does it wonder about religion, politics, moral situations, etc. that even regular humans don't fully understand? If it was pretty much just a human, it wouldn't be any more useful than one. Of course unless it can solve problems super quickly and effectively, then it would just be a really good worker.</p>\n", "question": "<p>Sometimes, but not always in the commercialization of technology, there are some low hanging fruits or early applications, I am having trouble coming up with examples of such applications as they would apply to a conscious AI.</p>\n\n<p>As per conscious I would propose an expanded strict definition: the state of being awake and aware of one's surroundings along with the capability of being self aware.</p>\n\n<p>Thanks. </p>\n"}, "id": "2162"}, {"body": {"answer": "<p>A common predilection of what many presume extraterrestrial life is fits general descriptions specific to terrestrial life. No guarantee exists providing for potential extraterrestrial life having any notion of any attribute we commonly relate to living organisms we are currently aware of; including a composition of cells. The same misunderstanding applies to defining a fabricated machine being as alive.</p>\n\n<p>I feel any attempts towards cohesively and adequately answering this question are premature. Just as as definition of life will undoubtedly require adjustment upon potential discovery and study of any extraterrestrial life, differentiation between an automated device and a living thing will likely become significantly more simple upon study of a machine better fitting expected attributes of definitions of \"life\".</p>\n", "question": "<p>I'm a bit confused about the definition of life. Can AI systems be called 'living'? Because they can do most of the things that we can. They can even communicate with one another. </p>\n\n<p>They are not formed of what we call cells. But, you see, cells are just a collection of several chemical processes which is in turn non-living just like AI is formed of several lines of code.</p>\n"}, "id": "2163"}, {"body": {"answer": "<p>Consciousness is not a scientific concept. Fringe scientists who theorize about consciousness are generally shunned as psudo-scientific heretics by the hard science community. Conciousness is a meta-physical or philosophical concept.</p>\n\n<p>\"I think, therefore I am.\" is the only proof that consciousness exists that I am aware of. Therefore, you cannot even prove that a person other than yourself is conscious. So how could anyone even prove that a computer program is conscious? What would be the observable difference between a program that IS conscious, and a program that simulates the results of consciousness?</p>\n\n<p>I don't believe that you can program conscious AI, nor could you prove that you have done so. Consciousness isn't something that can ever be marketed. You can only market the AI on the basis of it's problem solving capabilities.</p>\n", "question": "<p>Sometimes, but not always in the commercialization of technology, there are some low hanging fruits or early applications, I am having trouble coming up with examples of such applications as they would apply to a conscious AI.</p>\n\n<p>As per conscious I would propose an expanded strict definition: the state of being awake and aware of one's surroundings along with the capability of being self aware.</p>\n\n<p>Thanks. </p>\n"}, "id": "2165"}, {"body": {"answer": "<p>Actually, the terminator AI would not have to be sentient in my opinion. It was a hardcoded condition that it preserve itself as it was the most important asset that the military had in resisting invasion. It was supposed to be an oversight on the part of the programmers that the AI turned on Americans in order to defend itself. Unexpected behaviour does not require sentience at all.</p>\n\n<p>What makes the AI in sci-fi fundamentally different from real existing AI is that it is a \"General AI\" that is able to understand the world on many different levels simultaneously and still make intelligent decisions. All real AIs are programmed to do very specific things like image recognition or pathfinding. A GPS pathfinder, for example, can't learn to drive a car. In fact, it does not know that there is a car. Or a road. Or people. It merely finds the shortest distance between interconnected nodes on its map.</p>\n\n<p>Personally, I do not believe that there is any proof that a \"general AI\" is possible. I do not believe that it is a plausible progression of current developments in the next 100 years.</p>\n", "question": "<p>In lots of sci-fi, it seems that AI becomes sentient (Terminator, Peter F Hamilton's SI (commonwealth saga), etc.)</p>\n\n<p>However, I'm interested in whether this is actually plausible, whether an AI could actually break free form being controlled by us, and if that is possible, whether there is any research as to about what sort of complexity / processing power an AI would need to be able to do this.</p>\n"}, "id": "2166"}, {"body": {"answer": "<p>If they are able to network, then they can notify the car behind that it is about to break. In this way they can drive closer together at high speeds. As soon as one puts on the breaks, all the cars behind would apply the breaks. They would not require the 2 seconds that it takes for a human to respond.</p>\n\n<p>Children could be dropped at school or the train station automatically.</p>\n\n<p>People would not need to park a car; it could drop them at work and drive away.</p>\n\n<p>Taxis would probably become more viable than private car ownership.</p>\n\n<p>Car theft might be more difficult.</p>\n\n<p>Where I live, public transport is hardly viable because the government struggles to provide enough parking spaces at train stations and bus stops. The closest empty parking spot by 8:30am is 30minuets walk to the platform. Driverless cars would solve this problem, and Traveling by train would actually become viable for me.</p>\n", "question": "<p>What are the advantages of having self-driving cars?</p>\n\n<p>We will be able to have more cars in the traffic at the same time, but won't it also make more people choose to use the cars, so both the traffic and the public health will actually become worse?</p>\n\n<p>Are we really interested in this?</p>\n"}, "id": "2167"}, {"body": {"answer": "<p>The answer can be simplified, if consciousness means human consciousness then.</p>\n\n<p>What would the commercial application of a Human look like/be ?</p>\n\n<p>So now every one know the commercial applications of Humans.</p>\n", "question": "<p>Sometimes, but not always in the commercialization of technology, there are some low hanging fruits or early applications, I am having trouble coming up with examples of such applications as they would apply to a conscious AI.</p>\n\n<p>As per conscious I would propose an expanded strict definition: the state of being awake and aware of one's surroundings along with the capability of being self aware.</p>\n\n<p>Thanks. </p>\n"}, "id": "2169"}, {"body": {"answer": "<p>1) Yes we do have computing systems that does fall in to teraFLOPS range.</p>\n\n<p>2) The human brain is a biological system and saying it has some sort of FLOPS ability is just plain dumb because there is no way to take a human brain and measure it's FLOPS. You could say \"hey by looking at the neurons activity using fMRI we can reach some sort of approximation\" but comparing the result of this approach with the way FLOPS are measured in computers will be comparing apples with oranges, which again is dumb.</p>\n", "question": "<p>In The Age of Spiritual Machines (1999), Ray Kurzweil predicted that in 2009, a $1000 computing device would be able to perform a trillion operations per second. Additionally, he claimed that in 2019, a $1000 computing device would be approximately equal to the computational ability of the human brain (due to Moore's Law and exponential growth.)</p>\n\n<p>Did Kurzweil's first prediction come true? Are we on pace for his second prediction to come true? If not, how many years off are we?</p>\n"}, "id": "2171"}, {"body": {"answer": "<p>The <a href=\"https://en.wikipedia.org/wiki/FLOPS#Cost_of_computing\">development of CPUs</a> didn't quite keep up with Kurzweil's predictions. But if you also <a href=\"https://www.cnet.com/products/nvidia-geforce-gtx-295/review/\">allow for</a> <a href=\"https://en.wikipedia.org/wiki/List_of_Nvidia_graphics_processing_units#GeForce_200_series\">GPU</a>s, his prediction for 2009 is pretty accurate. </p>\n\n<p>I think Moore's law slowed down recently and has now been pretty much <a href=\"http://arstechnica.com/information-technology/2016/02/moores-law-really-is-dead-this-time/\">abandoned by the industry</a>. How much that will affect the 2019 prediction remains to be seen. Maybe the industry will hit its stride again with non-silicon based chips, maybe not. </p>\n\n<p>And of course whether hitting Kurzweil's estimate of the computing power of the human brain will make an appreciable difference for the development of AGI is another question altogether. </p>\n", "question": "<p>In The Age of Spiritual Machines (1999), Ray Kurzweil predicted that in 2009, a $1000 computing device would be able to perform a trillion operations per second. Additionally, he claimed that in 2019, a $1000 computing device would be approximately equal to the computational ability of the human brain (due to Moore's Law and exponential growth.)</p>\n\n<p>Did Kurzweil's first prediction come true? Are we on pace for his second prediction to come true? If not, how many years off are we?</p>\n"}, "id": "2172"}, {"body": {"answer": "<p>Well, we are talking about a system (a machine) which develops knowledge (learns), so it is kind of difficult for such a technique to not fall within machine learning.</p>\n\n<p>But you could argue that inference engines which work on a graph based knowledge database to derive new propositions or probabilities are not part of machine learning. Of course in that case part of the knowledge is not acquired at all, but rather entered by the developers. </p>\n\n<p>I'm still reading up on this, but my impression is that these <a href=\"https://en.wikipedia.org/wiki/WordNet\" rel=\"nofollow\">knowledge databases</a> and <a href=\"https://en.wikipedia.org/wiki/Inference_engine\" rel=\"nofollow\">inference engines</a> became rather popular in the nineties and many AGI-researchers today still work in that direction. </p>\n", "question": "<p>So machine learning allows a system to be self-automated in the sense that it can predict the future state based on what it has learned so far. My question is: Are machine learning techniques the only way of making a system develop its domain knowledge?</p>\n"}, "id": "2173"}, {"body": {"answer": "<p>That depends on how broadly you define \"machine learning techniques\".  You could construct a definition so that, by definition, all learning falls under that rubric.  OTOH, there is such a broad array of machine learning techniques that doing so wouldn't not gain one much.</p>\n\n<p>It probably makes more sense to talk about the different kinds of learning we use within machine learning / artificial intelligence.  At a minimum, you have:</p>\n\n<ol>\n<li>supervised learning</li>\n<li>unsupervised learning</li>\n<li>semi-supervised learning</li>\n<li>competitive learning</li>\n</ol>\n\n<p>And then things like \"reinforcement learning\" which may subcategorize the above.  Most of those things fall into what people generally call \"machine learning\".</p>\n\n<p>Outside of that, you have things like rule induction algorithms, deductive logic   techniques like inductive logic programming which can sorta-kinda \"learn\", inference engines, automated reasoning, etc. which have their own ways of \"learning\" about the world, but are separate from what's usually labeled \"machine learning\".  </p>\n\n<p>But even with that in mind, one can rightly ask if there's really a dividing line there or not.  Indeed, there seems to be reason to think that future AI systems may use a hybrid approach which combines many different techniques without regard for whether or not they are labeled \"machine learning\" or \"GOFAI\" or \"other\".</p>\n", "question": "<p>So machine learning allows a system to be self-automated in the sense that it can predict the future state based on what it has learned so far. My question is: Are machine learning techniques the only way of making a system develop its domain knowledge?</p>\n"}, "id": "2174"}, {"body": {"answer": "<p>This is a pretty tall order. I can't answer your question for you but I can suggest where to start.</p>\n\n<p>You could look into making a neural network for navigation and simple behaviors.</p>\n\n<p>See the following youtube video for navigation reference \n<a href=\"https://www.youtube.com/watch?v=0Str0Rdkxxo\" rel=\"nofollow\">https://www.youtube.com/watch?v=0Str0Rdkxxo</a></p>\n\n<p>This next video shows that using neural networks, you can have an actor make decisions based on another actor.\n\"Tank\" battle\n<a href=\"https://www.youtube.com/watch?v=u2t77mQmJiY\" rel=\"nofollow\">https://www.youtube.com/watch?v=u2t77mQmJiY</a></p>\n\n<p>The rest is up to you to figure out. Practice with some simple NN's </p>\n", "question": "<p>I am creating a snake game in Unity and I would like to implement AI snakes that wander around the globe while avoiding collision with the other snakes on the globe, and if possible I would also like to make the AI snakes purposefully trap other snakes so that the other snakes would collide and die. </p>\n\n<p><a href=\"https://i.stack.imgur.com/aQ61J.png\" rel=\"nofollow\"><img src=\"https://i.stack.imgur.com/aQ61J.png\" alt=\"enter image description here\"></a> </p>\n\n<p>The AI snakes must meet the following requirements:  </p>\n\n<ul>\n<li>They must move in a certain way. A snake is controlled by a user using the arrow keys on a keyboard, therefor I would also like the AI snakes to move using this form of input.</li>\n<li>The AI snakes must move on a sphere</li>\n</ul>\n\n<p>As I know, creating Artificial Intelligence is not an easy task and I would like to know if there are some open source projects that I can use for accomplishing this task.</p>\n"}, "id": "2177"}, {"body": {"answer": "<p>Why are self-driving cars awesome?</p>\n\n<ul>\n<li>Safety: better awareness (due to more sensors), better reaction time, fewer distracted/injured/drunk/texting drivers on the road, etc</li>\n<li>Convenience: pick up my kids from school, park itself at the grocery store, take itself to be serviced, etc</li>\n<li>Faster transit: with increased safety, you can increase speed limits, with proper routing algorithms you don't need traffic lights and stop signs any more (when you have dedicated self-driving lanes &amp; intersections)</li>\n<li>Comfort: recline, read, game, or snooze while traveling (yay!)</li>\n<li>Cost: subsidize the cost of the vehicle using ads (e.g. projected onto the windshield)</li>\n<li>etc</li>\n</ul>\n", "question": "<p>What are the advantages of having self-driving cars?</p>\n\n<p>We will be able to have more cars in the traffic at the same time, but won't it also make more people choose to use the cars, so both the traffic and the public health will actually become worse?</p>\n\n<p>Are we really interested in this?</p>\n"}, "id": "2178"}, {"body": {"answer": "<p>A relatively simple option which uses AI techniques that are 'traditional' for adversarial games (and which is therefore less of a 'research project' than the use of Machine Learning) is <a href=\"https://en.wikipedia.org/wiki/Minimax#In_general_games\" rel=\"nofollow\">Minimax</a>.</p>\n\n<p>The ingredients for this are:</p>\n\n<ol>\n<li>A list of all the actions that a snake can immediately perform from its current position.</li>\n<li>A measure of quality (a.k.a. 'fitness') for the resulting world state.</li>\n</ol>\n\n<p>Traditionally specified for <em>two</em> opponents, the minimax algorithm looks a specified number of moves ahead (alternating between opponents at each turn) and attempts to find the world state that maximizes the quality measure for one opponent whilst minimizing it for the other.</p>\n\n<p>An extension of the two-player algorithm to n opponents (as seemingly required by the OP) is given in <a href=\"https://www.diva-portal.org/smash/get/diva2:761634/FULLTEXT01.pdf\" rel=\"nofollow\">this paper</a>.</p>\n", "question": "<p>I am creating a snake game in Unity and I would like to implement AI snakes that wander around the globe while avoiding collision with the other snakes on the globe, and if possible I would also like to make the AI snakes purposefully trap other snakes so that the other snakes would collide and die. </p>\n\n<p><a href=\"https://i.stack.imgur.com/aQ61J.png\" rel=\"nofollow\"><img src=\"https://i.stack.imgur.com/aQ61J.png\" alt=\"enter image description here\"></a> </p>\n\n<p>The AI snakes must meet the following requirements:  </p>\n\n<ul>\n<li>They must move in a certain way. A snake is controlled by a user using the arrow keys on a keyboard, therefor I would also like the AI snakes to move using this form of input.</li>\n<li>The AI snakes must move on a sphere</li>\n</ul>\n\n<p>As I know, creating Artificial Intelligence is not an easy task and I would like to know if there are some open source projects that I can use for accomplishing this task.</p>\n"}, "id": "2179"}, {"body": {"answer": "<p>In general, AI in this type of video games is mostly pathfinding (giving the program a map of possible object positions) and/or an algorithm or series of algorithms ( so it looks random or alive ) tied to the users position ( which is known ), so there is nothing really intelligent in the strict sense, it just looks that way.</p>\n\n<p>In your case I would look into using Latitude and Longitude coordinates  (Most 3d engines have some variation ) as the basis for a projected grid on a sphere, your snake will also need to be constrained to the sphere surface and rules/algorithms/maps tweaked to get what you want.</p>\n", "question": "<p>I am creating a snake game in Unity and I would like to implement AI snakes that wander around the globe while avoiding collision with the other snakes on the globe, and if possible I would also like to make the AI snakes purposefully trap other snakes so that the other snakes would collide and die. </p>\n\n<p><a href=\"https://i.stack.imgur.com/aQ61J.png\" rel=\"nofollow\"><img src=\"https://i.stack.imgur.com/aQ61J.png\" alt=\"enter image description here\"></a> </p>\n\n<p>The AI snakes must meet the following requirements:  </p>\n\n<ul>\n<li>They must move in a certain way. A snake is controlled by a user using the arrow keys on a keyboard, therefor I would also like the AI snakes to move using this form of input.</li>\n<li>The AI snakes must move on a sphere</li>\n</ul>\n\n<p>As I know, creating Artificial Intelligence is not an easy task and I would like to know if there are some open source projects that I can use for accomplishing this task.</p>\n"}, "id": "2180"}, {"body": {"answer": "<blockquote>\n  <p>\"if I had some simple enemy AI that I want to have adapt to the players play-style, is this a good opportunity to implement the AI as a Genetic-Algorithm combined with a Neural Network\"</p>\n</blockquote>\n\n<p>Sure. Just provide a quality measure for the GA that's related in some manner to the effect of the player's actions on the game state/opponent(s). </p>\n\n<p>For example, if defining an opponent's intelligence, one of the conceptually simplest things would be to give a GA population member a fitness that's inversely proportional to the increase in the player's score over some period of time.</p>\n\n<blockquote>\n  <p>are Neural Networks always designed as on off signals?)?</p>\n</blockquote>\n\n<p>No. In general, they can be considered to perform <em>nonlinear regression</em>, i.e. a mapping from a vector of real numbers of length n to another of length m. Classification (i.e. 0/1 outputs can be seen as a restricted case of this).</p>\n\n<p>As per my answer to <a href=\"http://ai.stackexchange.com/questions/1618/what-are-the-practical-considerations-of-using-a-genetic-algorithm-to-decide-the/1626#1626\">this AI SE question</a>, there is a large body of literature (and mature software libraries) for using evolutionary computation to encode neural nets.</p>\n\n<p>More generally, some early work in 'online adaptivity using GA-encoded NNs' appeared in the Creatures <a href=\"http://creatures.wikia.com/wiki/Creatures_Wikia_Homepage\" rel=\"nofollow\">http://mrl.snu.ac.kr/courses/CourseSyntheticCharacter/grand96creatures.pdf</a> series of games by Steve Grand <a href=\"http://mrl.snu.ac.kr/courses/CourseSyntheticCharacter/grand96creatures.pdf\" rel=\"nofollow\">(details)</a>.</p>\n", "question": "<p>I'm interested mostly in the application of AI in gaming; in case this adjusts the way you answer, but general answers are more than welcome as well.</p>\n\n<p>I was reading up on Neural Networks and combining them with Genetic Algorithms; my high-level understanding is that the Neural Networks are used to produce a result from the inputs, and the Genetic Algorithm is employed to constantly adjust the weights in the Neural Network until a good answer is found.</p>\n\n<p>The concept of a Genetic Algorithm randomly mutating the weights on the inputs to a Neural Network makes sense to me; but I don't understand where this would be applied in respect to gaming.</p>\n\n<p>For example, if I had some simple enemy AI that I want to have adapt to the players play-style, is this a good opportunity to implement the AI as a Genetic-Algorithm combined with a Neural Network?</p>\n\n<p>With these different suitable applications, how does one go about deciding how to encode the problem in such a way that it can be mutated by the Genetic Algorithm and serve as suitable on/off inputs to a Neural Network (actually, are Neural Networks always designed as on off signals?)?</p>\n"}, "id": "2181"}, {"body": {"answer": "<p>Without going in too much detail on how exactly Neural Networks and Generic Algorithms work, I can tell you that both the algorithms are not good candidates for computer games.  They work well in scientific environments where the system is \"trained\" on a huge data set to adjust the \"weights\" (variables) for a given problem.  This \"training\" process requires a lot of processing power, time and a large data set.</p>\n\n<p>Computer games, however either needs to run in real-time (no time for training) or turn-based (not enough data for training).</p>\n\n<p>Another problem is that computer games need to free up as much as possible system resources for physics, graphics, sounds and the user interface to improve the player's experience so game developers usually use other lighter techniques (like a rule-based system) to create the illusion of an AI player.</p>\n", "question": "<p>I'm interested mostly in the application of AI in gaming; in case this adjusts the way you answer, but general answers are more than welcome as well.</p>\n\n<p>I was reading up on Neural Networks and combining them with Genetic Algorithms; my high-level understanding is that the Neural Networks are used to produce a result from the inputs, and the Genetic Algorithm is employed to constantly adjust the weights in the Neural Network until a good answer is found.</p>\n\n<p>The concept of a Genetic Algorithm randomly mutating the weights on the inputs to a Neural Network makes sense to me; but I don't understand where this would be applied in respect to gaming.</p>\n\n<p>For example, if I had some simple enemy AI that I want to have adapt to the players play-style, is this a good opportunity to implement the AI as a Genetic-Algorithm combined with a Neural Network?</p>\n\n<p>With these different suitable applications, how does one go about deciding how to encode the problem in such a way that it can be mutated by the Genetic Algorithm and serve as suitable on/off inputs to a Neural Network (actually, are Neural Networks always designed as on off signals?)?</p>\n"}, "id": "2182"}, {"body": {"answer": "<p>As you mentioned in the question, you cannot solve all problems with decision trees.  Decision trees usually works well in a turn-based game with a good heuristic function, but in RTS games takes a different approach.</p>\n\n<p>In the case of a very complex RTS game, one could implemented a rule-based AI. For example</p>\n\n<ul>\n<li>given it is the early game use all units to scout for resources</li>\n<li>if a certain criteria is met build the base a certain way</li>\n<li>if another criteria is met build an army</li>\n<li>if the army is big enough, attack</li>\n<li>if being attacked by the enemy, bring the units back to the base to defend</li>\n</ul>\n\n<p>Each of these rules could implement various other AI technique, for example use A-star to find the optimal path between a unit's current location and destination.</p>\n\n<p>Further optimization could be done by \"grouping\" similar units to act like one unit. e.g. calculate the path for the entire group instead of each individual unit.</p>\n\n<p>You could also add finer grained rules, like if enemy is a certain distance from a unit, move closer and attack or retreat to the base (depending on health, ammo, abilities, etc).</p>\n\n<p>The benefit of this approach is that a rule-based system executes very fast as no training or decision trees are necessary and this frees up a lot of system resources for visuals like physics and graphics.  </p>\n\n<p>The disadvantage is that if the rule system is not complex enough, the player will easily recognize the pattern and the game will become predictable and boring.  You will also notice that the more different units you add to the game, the exponentially more complex the rule system becomes as you have to cater and test interaction between each type of unit in the game otherwise players might find a weakness in the game design and exploit to complete missions in ways it was not designed to be completed.</p>\n\n<p>One of the reasons why multi-player games are so popular is that you do not play against set rules, but against creative people who have the ability to comes up with new strategies you had never seen before.</p>\n", "question": "<p>Some time ago playing chess was challenging for algorithms, then Go game which is vastly more complex than compared to chess.</p>\n\n<p>How about playing RTS game which have enormous branching factors limited by its time and space (like deciding what to do next)? What are the successful approaches to such problems?</p>\n"}, "id": "2183"}, {"body": {"answer": "<p>Effective use of the vast quantity of information now available on the web,  requires the use of\u201cSemantic Web\u201dmarkup languages such as the DARA\nAgent Markup Language(DAML).Such languages enables the automated gathering and processing of much information that is currently available on web communities.</p>\n\n<p>Effectively,such language facilitates the integration of multi-agent systems with the existing information infrastructure.As part of exploration of Semantic Web technology,and DAML in particular,constructed ITTALKS,a web-based system for automatic and intelligent notification of information technology talks.TTALKS system, utilizes the use of Semantic Web concepts and DAML extend its ability to provide an intelligent online service to both the human community and the agents assisting them.</p>\n\n<p>Therefore,the above overview gives also an insightful knowledge on further research on <strong>Ontologies in web intelligence</strong> .Stack Exchange is one of the web resources which acts as human knowledge base.And this base keeps on building up;in that even Civilizations to come will benefit from it.</p>\n\n<p>So information is knowledge and this knowledge is stored,shared within the Eco-system of the Internet.The aim of the Semantic Web is to make the present web more machine readable,understandable by making logical analysis and make predictions,also to allow intelligent agent store retrieve and manipulate pertinent information.</p>\n\n<p>The key goal of the DAML program is to develop a Semantic Web markup language that provides sufficient rules for ontology development and that is sufficiently rich to support intelligent agents and other applications</p>\n", "question": "<p>Are the future robots/machines going to use Stack Exchange communities to teach themselves? Are there any ongoing projects? Just imagine a bot having a memory of all the Q&amp;A's on all of the communities! </p>\n"}, "id": "2184"}, {"body": {"answer": "<p>For your question there's a brilliant playground emerging!<br><br>\nGo to <a href=\"https://gym.openai.com/\" rel=\"nofollow\">https://gym.openai.com/</a> and explore!<br>\nYou'll get interfaces to games if you want to try applying your machine learning skills and compare the performances of your trained AIs with others. And you can let yourself be inspired by the ideas discussed in the community.<br>\nIf you're especially into Genetic Algorithms you'll find dicussions there too but I'd suggest digging deeper into Reinforcement Learning.<br><br>\nIf you look at what Google Deep Mind accomplished playing</p>\n\n<ul>\n<li>Breakout</li>\n<li>Montezumas Revenge</li>\n<li>various other Atari Games ..</li>\n</ul>\n\n<p>and obviously !</p>\n\n<ul>\n<li><a href=\"http://www.theverge.com/google-deepmind\" rel=\"nofollow\">the sensational victory at Go</a></li>\n</ul>\n\n<p>you can say that Reinforcement Learning with (Deep) Neural Networks can be a very promising approach when it comes to training an AI to master games!</p>\n", "question": "<p>I'm interested mostly in the application of AI in gaming; in case this adjusts the way you answer, but general answers are more than welcome as well.</p>\n\n<p>I was reading up on Neural Networks and combining them with Genetic Algorithms; my high-level understanding is that the Neural Networks are used to produce a result from the inputs, and the Genetic Algorithm is employed to constantly adjust the weights in the Neural Network until a good answer is found.</p>\n\n<p>The concept of a Genetic Algorithm randomly mutating the weights on the inputs to a Neural Network makes sense to me; but I don't understand where this would be applied in respect to gaming.</p>\n\n<p>For example, if I had some simple enemy AI that I want to have adapt to the players play-style, is this a good opportunity to implement the AI as a Genetic-Algorithm combined with a Neural Network?</p>\n\n<p>With these different suitable applications, how does one go about deciding how to encode the problem in such a way that it can be mutated by the Genetic Algorithm and serve as suitable on/off inputs to a Neural Network (actually, are Neural Networks always designed as on off signals?)?</p>\n"}, "id": "2186"}, {"body": {"answer": "<p>Not yet. <a href=\"https://en.wikipedia.org/wiki/Synthetic_virology\">Synthetic virology</a> / <a href=\"https://en.wikipedia.org/wiki/Synthetic_biology#Synthetic_life\">Synthetic life</a> are still in their infancy.\nWe can now synthesize simple bacteria (see Craig Venter's <a href=\"https://www.ted.com/talks/craig_venter_is_on_the_verge_of_creating_synthetic_life\">fascinating TED talk</a> and also <a href=\"https://www.scientificamerican.com/article/scientists-synthesize-bacteria-with-smallest-genome-yet/\">an article about his recent work</a>) but definitely nothing that may be called 'rational' in human standards.</p>\n", "question": "<p>According to <a href=\"https://en.wikipedia.org/wiki/Artificial_intelligence\" rel=\"nofollow\">Wikipedia</a>:</p>\n\n<blockquote>\n  <p>AI is intelligence exhibited by machines.</p>\n</blockquote>\n\n<p>I have been wondering if with the recent biological advancements, is there already a non-electrical-based \"machine\" that is programmed by humans in order to be able to behave like a:</p>\n\n<blockquote>\n  <p><strong>flexible rational agent</strong> that perceives its environment and takes actions that maximize its chance of success at some goal</p>\n</blockquote>\n\n<p>I was specifically thinking of viruses and bacteria. Have these been programmed by humans in order to behave as a flexible rational agent (i.e. an AI entity)?</p>\n\n<p>Are there are other organisms that have already been used for this purpose?</p>\n"}, "id": "2188"}, {"body": {"answer": "<p>Switching required a re-learn.</p>\n\n<p>Also, <a href=\"https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf\" rel=\"nofollow\">note that</a>:</p>\n\n<blockquote>\n  <p>We use the same network architecture, learning\n  algorithm and hyperparameters settings across all seven games, showing that our approach is robust\n  enough to work on a variety of games without incorporating game-specific information. While we\n  evaluated our agents on the real and unmodified games, we made one change to the reward structure\n  of the games during training only.</p>\n</blockquote>\n\n<p>and </p>\n\n<blockquote>\n  <p>the\n  network has outperformed all previous RL algorithms on six of the seven games we have attempted\n  and surpassed an expert human player on three of them. </p>\n</blockquote>\n", "question": "<p>DeepMind state that their deep Q-network (DQN) was able to continually adapt its behavior while learning to play 49 Atari games.  </p>\n\n<p>After learning all games with the same neural net, was the agent able to play them all at 'superhuman' levels simultaneously (whenever it was randomly presented with one of the games) or could it only be good at one game at a time because switching required a re-learn?</p>\n"}, "id": "2191"}, {"body": {"answer": "<ol>\n<li>Divide the globe into a \"cells\". Each cell will have a number of neighbours depending on how you have divided your globe. Have a look at <a href=\"http://gamedev.stackexchange.com/questions/3360/when-mapping-the-surface-of-a-sphere-with-tiles-how-might-you-deal-with-polar-d\">http://gamedev.stackexchange.com/questions/3360/when-mapping-the-surface-of-a-sphere-with-tiles-how-might-you-deal-with-polar-d</a> and <a href=\"http://gamedev.stackexchange.com/questions/45167/square-game-map-rendered-as-sphere\">http://gamedev.stackexchange.com/questions/45167/square-game-map-rendered-as-sphere</a> for ideas on how to divide your global.</li>\n<li>Once all the cells are connected, you can use an <a href=\"https://en.wikipedia.org/wiki/A*_search_algorithm\" rel=\"nofollow\">A-star search algorithm</a> to find the optimal path for an AI \"snake\".</li>\n<li>Change the heuristic function so that the cells on the opposite side of the opponent are more favourable than the cells on your snake's side. That would cause the AI snake to always try to get to the other side of the opponent with the side-effect of \"surrounding\" the opponent.</li>\n</ol>\n", "question": "<p>I am creating a snake game in Unity and I would like to implement AI snakes that wander around the globe while avoiding collision with the other snakes on the globe, and if possible I would also like to make the AI snakes purposefully trap other snakes so that the other snakes would collide and die. </p>\n\n<p><a href=\"https://i.stack.imgur.com/aQ61J.png\" rel=\"nofollow\"><img src=\"https://i.stack.imgur.com/aQ61J.png\" alt=\"enter image description here\"></a> </p>\n\n<p>The AI snakes must meet the following requirements:  </p>\n\n<ul>\n<li>They must move in a certain way. A snake is controlled by a user using the arrow keys on a keyboard, therefor I would also like the AI snakes to move using this form of input.</li>\n<li>The AI snakes must move on a sphere</li>\n</ul>\n\n<p>As I know, creating Artificial Intelligence is not an easy task and I would like to know if there are some open source projects that I can use for accomplishing this task.</p>\n"}, "id": "2193"}, {"body": {"answer": "<p>No. Reasons include, but are not limited to: </p>\n\n<ul>\n<li>lack of understanding of how the brain works</li>\n<li>current ANNs are mostly good at pattern recognition and generative   tasks, but lack capacity to create abstractions on their own</li>\n<li>we cant match size/number of perceptrons to number of neurons</li>\n<li>even with much smaller ANN size network, performance is an issue (i.e. state of the art image categorization ANNs have to be trained few weeks on multi GPU rigs to match human level).</li>\n</ul>\n", "question": "<p>I read a lot about the structure of the human brain and artificial neural networks. I wonder if it is possible to build an artificial intelligence with neural networks that would be divided into centers such as the brain is, e.g. centers responsible for feelings, abstract thinking, speech, memory, etc.?</p>\n"}, "id": "2194"}, {"body": {"answer": "<p>Most machine learning applications today are built on tensors, matrices, probabilistic / Bayesian inference, neural networks, etc. But those can all be built with any modern programming language (all the useful ones are Turing complete). And the best performing language for any of those will generally be assembly / machine code.</p>\n\n<p><a href=\"https://www.python.org/\" rel=\"nofollow\">Python</a> is famous for machine learning, but that may be due to adoption of Python in academia and <a href=\"http://www.numpy.org/\" rel=\"nofollow\">NumPy</a>, <a href=\"https://www.scipy.org/\" rel=\"nofollow\">SciPy</a>, etc. Python isn't very performant, but most of the machine libraries leverage native code, so they're fairly performant.</p>\n\n<p><a href=\"http://julialang.org/\" rel=\"nofollow\">Julia</a> is a new language that is gunning for a lead position in the data science space, which machine learning builds on. It is allegedly very performant over number crunching domains.</p>\n\n<p>Java has a decent developer ecosystem, and is fairly performant, but the highest performing libraries (including those that leverage GPU) tend to call out to native code via JNI. See <a href=\"https://deeplearning4j.org/\" rel=\"nofollow\">DeepLearning4J</a>.</p>\n\n<p>I personally like <a href=\"http://clojure.org/\" rel=\"nofollow\">Clojure</a> - a modern Lisp running on the Java JVM. There's a new deep learning project called <a href=\"https://github.com/thinktopic/cortex\" rel=\"nofollow\">Cortex</a> built on Clojure and some fast native libraries, including GPU acceleration.</p>\n\n<p>I think Clojure provides a great balance of being able to easily wrap performant libraries with highly expressive, succinct and simple programming idioms.</p>\n", "question": "<p>What are the best <a href=\"https://en.wikipedia.org/wiki/Turing_completeness\" rel=\"nofollow\">Turing complete</a> programming languages which can be used for developing self-learning/improving <a href=\"https://en.wikipedia.org/wiki/Evolutionary_algorithm\" rel=\"nofollow\">evolutionary algorithm</a> based AI programs with <a href=\"https://en.wikipedia.org/wiki/Genetic_algorithm\" rel=\"nofollow\">generic algorithms</a>?</p>\n\n<p>'Best' should be based on pros and cons of performance and easiness for machine learning.</p>\n"}, "id": "2196"}, {"body": {"answer": "<p>No, I think electricity is not essential for AI.  In theory AI (a sufficient collection of computational processes that can adapt to changes in their input, thus producing 'intelligent' behavior), <em>could</em> be implemented using any mechanism that can compute that set of essential functions needed to create AI.  Basically I'm suggesting the possibility of combining a set of non-electric Turing-equivalent machines into a collective that together can reach the AI-level of performance.</p>\n\n<p><a href=\"https://en.wikipedia.org/wiki/Turing_machine_equivalents\" rel=\"nofollow\">https://en.wikipedia.org/wiki/Turing_machine_equivalents</a></p>\n\n<p>If AI can be implemented using an electronic computer, it should also be possible to implement it using any non-electronic machine that is computationally equivalent.</p>\n\n<p>To date, several non-electronic machines have been proposed as Turing-equivalent: DNA computers, quantum computers, Babbage's Analytical Engine, animal brains, maybe even a really big network of daisies (perhaps that can communicate via their rhizomes).  </p>\n\n<p>In fact, it's plausible that one day we could create a network composed of small brains (perhaps from a less smart species than humans) that with the right kind of genetically architected biological interconnect and scheduler could route data through its network to control a robot -- thus we'd have a synthetic biological AI engine whose brain is made up of 100 chimpanzees, or 10,000 hamster brains, or maybe even 1 million nematodes.</p>\n", "question": "<p>According to <a href=\"https://en.wikipedia.org/wiki/Artificial_intelligence\" rel=\"nofollow\">Wikipedia</a>:</p>\n\n<blockquote>\n  <p>AI is intelligence exhibited by machines.</p>\n</blockquote>\n\n<p>I have been wondering if with the recent biological advancements, is there already a non-electrical-based \"machine\" that is programmed by humans in order to be able to behave like a:</p>\n\n<blockquote>\n  <p><strong>flexible rational agent</strong> that perceives its environment and takes actions that maximize its chance of success at some goal</p>\n</blockquote>\n\n<p>I was specifically thinking of viruses and bacteria. Have these been programmed by humans in order to behave as a flexible rational agent (i.e. an AI entity)?</p>\n\n<p>Are there are other organisms that have already been used for this purpose?</p>\n"}, "id": "2197"}, {"body": {"answer": "<p>Maybe in the distant future they could build a computer powerful enough to simulate the individual neurons of an entire human brain. Then they could carefully copy/paste the connectivity of a sample brain into the computer simulation.</p>\n\n<p>Given that this extreme seems physically possible, it stands to reason that much simpler/smaller alternatives could be engineered in the future assuming continuous advancements in technology.</p>\n", "question": "<p>I have a question. Will we be able to build a neural network that thinks abstractly, has the creativity, feels and is conscious?</p>\n"}, "id": "2199"}, {"body": {"answer": "<p>Normally one node/layer applies liner fitting of the the input to the hypothesis in other words uses liner function (<code>y = a*x + b</code>). Adding layers chains liner functions, potentially allowing fitting higher order functions. A great explanation can be found <a href=\"http://colah.github.io/posts/2015-01-Visualizing-Representations/\" rel=\"nofollow\">here</a>.</p>\n", "question": "<p>If I have a set of sensory nodes taking in information and a set of \"action nodes\" which determine the behavior of my robot, why do I need hidden nodes between them when I can let all sensory nodes affect all action nodes?</p>\n\n<p>(This is in the context of evolving neural network)</p>\n"}, "id": "2202"}, {"body": {"answer": "<p>While a single transistor could approximate the basic function of a single neuron, I cannot agree that any electronic element could simulate the synapses/axons. Transistors are etched on a flat surface, and could be interconnected only to adjacent or close by transistors. Axons in the brain span huge distances (compared to the size of the neuron itself), and are not restricted to a two dimensional surface. Even if we were able to approach the number of transistors on a processor to the number of neurons in a brain, we are no where near as number of connections. It could also be argued that the analogue signals in the brain carry more information per unit of time, compared to the binary impulses on a chip. Furthermore, the brain actually have plasticity i.e. connections between neurons can be weakened/discarded or straightened/created, while a CPU cannot do that.</p>\n", "question": "<p>If neurons and synapses can be implemented using transistors, what prevents us from creating arbitrarily large neural networks using the same methods with which GPUs are made?</p>\n\n<p>In essence, we have seen how extraordinarily well virtual neural networks implemented on sequential processors work (even GPUs are sequential machines, but with huge amounts of cores). </p>\n\n<p>One can imagine that using GPU design principles - which is basically to have thousands of programmable processing units that work in parallel - we could make much simpler \"neuron processing units\" and put millions or billions of those NPUs in a single big chip. They would have their own memory (for storing weights) and be connected to a few hundred other neurons by sharing a bus. They could have a frequency of for example 20 Hz, which would allow them to share a data bus with many other neurons.</p>\n\n<p>Obviously, there are some electrical engineering challenges here, but it seems to me that all big tech companies should be exploring this route by now.</p>\n\n<p>Many AI researchers say that super intelligence is coming around the year 2045. I believe that their reasoning is based on moores law and the number of neurons we are able to implement in software running on the fastest computers we have.</p>\n\n<p>But the fact is, we today are making silicon chips with billions of transistors on them. SPARK M7 has 10 billion transistors.</p>\n\n<p>If implementing a (non-programmable) neuron and a few hundred synapses for it requires for example 100 000 transistors, then we can make a neural network in hardware that emulates 100 000 neurons.</p>\n\n<p>If we design such a chip so that we can simply make it physically bigger if we want more neurons, then it seems to me that arbitrarily large neural networks is simply a budget question.</p>\n\n<p>Are we technically able to make, in hardware, arbitrarily large neural networks with current technology?</p>\n\n<p>Remember: I am NOT asking if such a network will in fact be very intelligent. I am merely asking if we can factually make arbitrarily large, highly interconnected neural networks, if we decide to pay Intel to do this? </p>\n\n<p>The implication is that on the day some scientist is able to create general intelligence in software, we can use our hardware capabilities to grow this general intelligence to human levels and beyond.</p>\n"}, "id": "2204"}, {"body": {"answer": "<p>You may want to consider this <a href=\"http://scienceblogs.com/developingintelligence/2007/03/27/why-the-brain-is-not-like-a-co/\" rel=\"nofollow\">list</a>:</p>\n\n<blockquote>\n  <p>10 important differences between brains and computers:</p>\n  \n  <ol>\n  <li>Brains are analog , computers are digital </li>\n  <li>The brain uses content-addressable memory</li>\n  <li>The brain is a massively parallel machine; computers are modular and serial </li>\n  <li>Processing speed is not fixed in the brain; there is no system clock </li>\n  <li>Short-term memory is not like RAM </li>\n  <li>No hardware/software distinction can be made with respect to the brain or mind </li>\n  <li>Synapses are far more complex than electrical logic gates </li>\n  <li>Unlike computers, processing and memory are performed by the same components in the brain </li>\n  <li>The brain is a self-organizing system</li>\n  <li>Brains have bodies</li>\n  </ol>\n</blockquote>\n", "question": "<p>If neurons and synapses can be implemented using transistors, what prevents us from creating arbitrarily large neural networks using the same methods with which GPUs are made?</p>\n\n<p>In essence, we have seen how extraordinarily well virtual neural networks implemented on sequential processors work (even GPUs are sequential machines, but with huge amounts of cores). </p>\n\n<p>One can imagine that using GPU design principles - which is basically to have thousands of programmable processing units that work in parallel - we could make much simpler \"neuron processing units\" and put millions or billions of those NPUs in a single big chip. They would have their own memory (for storing weights) and be connected to a few hundred other neurons by sharing a bus. They could have a frequency of for example 20 Hz, which would allow them to share a data bus with many other neurons.</p>\n\n<p>Obviously, there are some electrical engineering challenges here, but it seems to me that all big tech companies should be exploring this route by now.</p>\n\n<p>Many AI researchers say that super intelligence is coming around the year 2045. I believe that their reasoning is based on moores law and the number of neurons we are able to implement in software running on the fastest computers we have.</p>\n\n<p>But the fact is, we today are making silicon chips with billions of transistors on them. SPARK M7 has 10 billion transistors.</p>\n\n<p>If implementing a (non-programmable) neuron and a few hundred synapses for it requires for example 100 000 transistors, then we can make a neural network in hardware that emulates 100 000 neurons.</p>\n\n<p>If we design such a chip so that we can simply make it physically bigger if we want more neurons, then it seems to me that arbitrarily large neural networks is simply a budget question.</p>\n\n<p>Are we technically able to make, in hardware, arbitrarily large neural networks with current technology?</p>\n\n<p>Remember: I am NOT asking if such a network will in fact be very intelligent. I am merely asking if we can factually make arbitrarily large, highly interconnected neural networks, if we decide to pay Intel to do this? </p>\n\n<p>The implication is that on the day some scientist is able to create general intelligence in software, we can use our hardware capabilities to grow this general intelligence to human levels and beyond.</p>\n"}, "id": "2205"}, {"body": {"answer": "<p>A feed forward neural network without hidden nodes can only find linear decision boundaries. However, most of the time you need non-linear decision boundaries. Hence you need hidden nodes with a non-linear activation function. The more hidden nodes you have, the more data you need to find good parameters, but the more complex decision boundaries you can find.</p>\n", "question": "<p>If I have a set of sensory nodes taking in information and a set of \"action nodes\" which determine the behavior of my robot, why do I need hidden nodes between them when I can let all sensory nodes affect all action nodes?</p>\n\n<p>(This is in the context of evolving neural network)</p>\n"}, "id": "2206"}, {"body": {"answer": "<blockquote>\n  <p>If neurons and synapses can be implemented using transistors, </p>\n</blockquote>\n\n<p>I hope you are not talking about the neural networks which are currently winning all competitions in machine learning (MLPs, CNNs, RNNs, Deep Residual Networks, ...). Those were once used as a model for neurons, but they are only <em>very</em> loosely related to what happens in real brain cells.</p>\n\n<p>Spiking networks should be much closer to real neurons. I've heard that the Hodgkin-Huxley model is quite realistic. However - in contrast to the models I named above - there seems not to be an effective training algorithm for spiking networks.</p>\n\n<blockquote>\n  <p>what prevents us from creating arbitrarily large neural networks</p>\n</blockquote>\n\n<ul>\n<li><strong>Computational resources</strong>: Training neural networks takes a lot of time. We are talking about ~12 days with a GPU cluster for some CNN models in computer vision.</li>\n<li><strong>Training data</strong>: The more variables you add to the model, the more data you need to estimate those. Neural networks are not magic. They need something they can work with.</li>\n</ul>\n\n<blockquote>\n  <p>But the fact is, we today are making silicon chips with billions of transistors on them. SPARK M7 has 10 billion transistors.</p>\n  \n  <p>If implementing a (non-programmable) neuron and a few hundred synapses for it requires for example 100 000 transistors, then we can make a neural network in hardware that emulates 100 000 neurons.</p>\n</blockquote>\n\n<p>It's not that simple:</p>\n\n<ul>\n<li><strong>Asynchonosity</strong>: Biological neural networks work asynchronously. This means one neuron might be active while all others are not active.</li>\n<li><strong>Emulation</strong>: You assume it would only need one cycle to simulate a biological neuron. However, it needs many thousand cycles. You can't simply use more computational units, because some things are not parallelizable. For example, think of the function <code>f(x) = sin(x*x + 1)</code>. For a human, there are basically three computations: <code>r1 = x*x</code>, <code>r2 = r1 + 1</code>, <code>r3 = sin(r2)</code>. Even if you have 3 people working on calculating the result, you will not be faster than the single fastest person in this group is. Why? Because you need the results of the last computation. </li>\n</ul>\n", "question": "<p>If neurons and synapses can be implemented using transistors, what prevents us from creating arbitrarily large neural networks using the same methods with which GPUs are made?</p>\n\n<p>In essence, we have seen how extraordinarily well virtual neural networks implemented on sequential processors work (even GPUs are sequential machines, but with huge amounts of cores). </p>\n\n<p>One can imagine that using GPU design principles - which is basically to have thousands of programmable processing units that work in parallel - we could make much simpler \"neuron processing units\" and put millions or billions of those NPUs in a single big chip. They would have their own memory (for storing weights) and be connected to a few hundred other neurons by sharing a bus. They could have a frequency of for example 20 Hz, which would allow them to share a data bus with many other neurons.</p>\n\n<p>Obviously, there are some electrical engineering challenges here, but it seems to me that all big tech companies should be exploring this route by now.</p>\n\n<p>Many AI researchers say that super intelligence is coming around the year 2045. I believe that their reasoning is based on moores law and the number of neurons we are able to implement in software running on the fastest computers we have.</p>\n\n<p>But the fact is, we today are making silicon chips with billions of transistors on them. SPARK M7 has 10 billion transistors.</p>\n\n<p>If implementing a (non-programmable) neuron and a few hundred synapses for it requires for example 100 000 transistors, then we can make a neural network in hardware that emulates 100 000 neurons.</p>\n\n<p>If we design such a chip so that we can simply make it physically bigger if we want more neurons, then it seems to me that arbitrarily large neural networks is simply a budget question.</p>\n\n<p>Are we technically able to make, in hardware, arbitrarily large neural networks with current technology?</p>\n\n<p>Remember: I am NOT asking if such a network will in fact be very intelligent. I am merely asking if we can factually make arbitrarily large, highly interconnected neural networks, if we decide to pay Intel to do this? </p>\n\n<p>The implication is that on the day some scientist is able to create general intelligence in software, we can use our hardware capabilities to grow this general intelligence to human levels and beyond.</p>\n"}, "id": "2208"}, {"body": {"answer": "<p>The approach you describe is called <a href=\"https://en.wikipedia.org/wiki/Neuromorphic_engineering\" rel=\"nofollow\">neuromorphic computing</a> and it's <a href=\"https://www.technologyreview.com/s/526506/neuromorphic-chips/\" rel=\"nofollow\">quite</a> a <a href=\"https://www.uni-heidelberg.de/presse/news2016/pm20160316-neuromorphic-computer-coming-online.html\" rel=\"nofollow\">busy</a> <a href=\"http://www.nextplatform.com/2016/02/09/the-second-coming-of-neuromorphic-computing/\" rel=\"nofollow\">field</a>. </p>\n\n<p>IBM's <a href=\"http://www.research.ibm.com/articles/brain-chip.shtml\" rel=\"nofollow\">TrueNorth</a> even has spiking neurons. </p>\n\n<p>The main problem with these projects is that nobody quite knows what to do with them yet. </p>\n\n<p>These projects don't try to create chips that are optimised to <em>run</em> a neural network. That would certainly be possible, but the expensive part is the <em>training</em> not the running of neural networks. And for the training you need huge matrix multiplications, something GPUs are very good at already. (<a href=\"https://cloudplatform.googleblog.com/2016/05/Google-supercharges-machine-learning-tasks-with-custom-chip.html\" rel=\"nofollow\">Google's TPU</a> would be a chip optimised to run NNs.)</p>\n\n<p>To do research on algorithms that might be implemented in the brain (we hardly know anything about that) you need flexibility, something these chips don't have. Also, the engineering challenge likely lies in providing a lot of synapses, just compare the average number of synapses per neuron of TrueNorth, 256, and the brain, 10,000.</p>\n\n<p>So, you could create a chip designed after some neural architecture and it would be faster, more efficient, etc \u2026, but to do that you'll need to know which architecture works first. We know that deep learning works, so google uses custom made hardware to run their applications and I could certainly imagine custom made deep learning hardware coming to a smartphone near you in the future. To create a neuromorphic chip for strong AI you'd need to develop strong AI first.</p>\n", "question": "<p>If neurons and synapses can be implemented using transistors, what prevents us from creating arbitrarily large neural networks using the same methods with which GPUs are made?</p>\n\n<p>In essence, we have seen how extraordinarily well virtual neural networks implemented on sequential processors work (even GPUs are sequential machines, but with huge amounts of cores). </p>\n\n<p>One can imagine that using GPU design principles - which is basically to have thousands of programmable processing units that work in parallel - we could make much simpler \"neuron processing units\" and put millions or billions of those NPUs in a single big chip. They would have their own memory (for storing weights) and be connected to a few hundred other neurons by sharing a bus. They could have a frequency of for example 20 Hz, which would allow them to share a data bus with many other neurons.</p>\n\n<p>Obviously, there are some electrical engineering challenges here, but it seems to me that all big tech companies should be exploring this route by now.</p>\n\n<p>Many AI researchers say that super intelligence is coming around the year 2045. I believe that their reasoning is based on moores law and the number of neurons we are able to implement in software running on the fastest computers we have.</p>\n\n<p>But the fact is, we today are making silicon chips with billions of transistors on them. SPARK M7 has 10 billion transistors.</p>\n\n<p>If implementing a (non-programmable) neuron and a few hundred synapses for it requires for example 100 000 transistors, then we can make a neural network in hardware that emulates 100 000 neurons.</p>\n\n<p>If we design such a chip so that we can simply make it physically bigger if we want more neurons, then it seems to me that arbitrarily large neural networks is simply a budget question.</p>\n\n<p>Are we technically able to make, in hardware, arbitrarily large neural networks with current technology?</p>\n\n<p>Remember: I am NOT asking if such a network will in fact be very intelligent. I am merely asking if we can factually make arbitrarily large, highly interconnected neural networks, if we decide to pay Intel to do this? </p>\n\n<p>The implication is that on the day some scientist is able to create general intelligence in software, we can use our hardware capabilities to grow this general intelligence to human levels and beyond.</p>\n"}, "id": "2209"}, {"body": {"answer": "<p>One critical part of AI is machine learning (ML). The common definition of ML by Mitchell is</p>\n\n<blockquote>\n  <p>A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P, improves with experience E.</p>\n</blockquote>\n\n<p>If this type of program is useful in an \"everyday application\" depends on the application. Here are some examples which would not be possible without ML:</p>\n\n<ul>\n<li>Spam detection (e.g. e-mails, forums)</li>\n<li>Fraud detection (e.g. credit cards)</li>\n<li>Image recognition (e.g. if you want to automatically filter NSFW content, automatic adding of tags / making images searchable e.g. for Google Image search)</li>\n<li>Video analysis (filtering copyrighted work e.g. on YouTube)</li>\n<li>Speech recognition (e.g. hotlines, automatic caption generation)</li>\n<li>Autocompletion (probably one of the simplest things you can do with data)</li>\n</ul>\n", "question": "<p>I'm curious about Artificial Intelligence. In my everyday job I develop standard applications, like websites with basic functionalities like user subscription, file upload, forms saved in a database... </p>\n\n<p>I mainly know of AI being used in games or robotics fields. But can it be useful in \"standard\" application development?</p>\n"}, "id": "2210"}, {"body": {"answer": "<h2>Compare with real data</h2>\n\n<p>100% of results produced by G are \"wrong\", always, by definition, even for a very good generator. You provide the discriminative net with a mix of generated results and real results from an outside source and train it to distinguish if the result was produced by the generator or not.</p>\n\n<p>This will result in a \"mutual evolution\" as D will learn to find features that separate real results from generated ones, and G will learn how to generate results that are hard to distinguish from real data.</p>\n", "question": "<p>I am reading about Generative Adversarial Networks (GANs) and I have some doubts regarding it. So far, I understand that in a GAN there are two different types of neural network: one is generative (G) and the other discriminative (D). The generative neural network generates some data which the discriminative neural network judges for correctness. The GAN learns by passing the loss function to both networks.</p>\n\n<p>How do the discriminative (D) neural nets initially know whether the data produced by G is correct or not? Do we have to train the D first then add it into the GAN with G?</p>\n\n<p>Let's consider my trained D net, which can classify a picture with 90% percentage accuracy. If we add this D net to a GAN there is a 10% probability it will classify a image wrong. If we train a GAN with this D net then will it also have the same 10% error in classifying an image? If yes, then why do GANs show promising results?</p>\n"}, "id": "2212"}, {"body": {"answer": "<p>\"discriminative(D) network\" <strong>learns</strong> to discriminate by definition - we provide it with the true vs. the generated data, and let it learns by itself how to discriminate between the two.</p>\n\n<p>Therefore, we expect network D to improve the ability of network G to generate better and better images (or other kind of data), as it try to \"trick\" network D by producing new data that is more similar to \"real data\". It is not about the accuracy of network D at all. <strong>It is not about improving the accuracy</strong>, it is about improving the ability of the computer to generate more \"believable\" data.</p>\n\n<p>That said, using this scenario could be a good \"unsupervised\" way to improve the classification power of neural networks, as it forces the generator model to learn better features of real data, and to learn how to distinguish between actual features and noise, using much less data that is needed for a traditional supervised learning scheme. </p>\n", "question": "<p>I am reading about Generative Adversarial Networks (GANs) and I have some doubts regarding it. So far, I understand that in a GAN there are two different types of neural network: one is generative (G) and the other discriminative (D). The generative neural network generates some data which the discriminative neural network judges for correctness. The GAN learns by passing the loss function to both networks.</p>\n\n<p>How do the discriminative (D) neural nets initially know whether the data produced by G is correct or not? Do we have to train the D first then add it into the GAN with G?</p>\n\n<p>Let's consider my trained D net, which can classify a picture with 90% percentage accuracy. If we add this D net to a GAN there is a 10% probability it will classify a image wrong. If we train a GAN with this D net then will it also have the same 10% error in classifying an image? If yes, then why do GANs show promising results?</p>\n"}, "id": "2216"}, {"body": {"answer": "<p>The <strong>ultimate goal of machine learning</strong> is to bypass the developer...\nWhen we will have a \"master algorithm\" that can learn how to generalize any function or algorithm from examples, it can essentially replace any developer, skip the 'development\" stage, going from problem directly to algorithm. </p>\n\n<p>We can't know when this will happen, but as we surrounded with multiple creatures (humans and animals) which can demonstrate learning algorithms and predictive model learning without any \"developer\" - we can assume that such an algorithm is possible. </p>\n\n<p>If I would have to guess, I would say that we are probably very near the point where \"developers\" and \"testers\" will be replaced by learning algorithms. We could be a decade or two away from the point where people will not write any code or any testing at all. Programs and automation will be derived directly from describing the problems themselves in a natural language, visualizations or data collections. However, we still need some breakthroughs in combining feature learning with active memory, unsupervised learning, and artificial common sense.   </p>\n", "question": "<p>Now AI can replace call center, worker(in the factory) and going to replace court. When will the AI can replace developer or tester?</p>\n\n<p>I want to know how long can AI replace developer. e.g. next 10 years because...</p>\n"}, "id": "2217"}, {"body": {"answer": "<p>By AI is it artificial or more analytical</p>\n\n<p>What makes us learn hat makes us learn?i ask \ni am a dr Can we be better clinicians Artificial Intelligence ? What can we learn from AI? is therefore defined as the theory and development of computer systems able to perform tasks that normally require human intelligence, such as visual perception, speech recognition, decision-making, and translation between languages.</p>\n\n<p>Intelligent systems are better at understanding and tracking the changes that the human eye cannot detect. These machines are connected to vast amount of data, which they analyze in real time to generate a solution for a current problem. This process is referred to as data mining. Under ordinary circumstances, human can only collect and analyze a handful of data.  Artificial is not the word to use</p>\n", "question": "<p>Now AI can replace call center, worker(in the factory) and going to replace court. When will the AI can replace developer or tester?</p>\n\n<p>I want to know how long can AI replace developer. e.g. next 10 years because...</p>\n"}, "id": "2218"}, {"body": {"answer": "<p>Any logic circuit admits a variety of implementations.  All programs executing on conventional digital processors can be expressed as logic circuits.  Among the possible implementations of logic circuits are fluidic implementations, which do not depend on electronics per se.  Thus it is in principle possible to implement, e.g. a POMDP processor (responsive to your specific question) in fluidics, albeit perhaps impractical at the moment.</p>\n\n<p>I know of no general theory of Turing-completeness for analog computers, which would suffice to determine whether some alternative physical substrate, be it biological or not biological, can compute recursively enumerable functions.  That is a sufficient but not a necessary condition for answering your question regarding any given medium. Usually the easiest way to demonstrate the sufficient condition will be to demonstrate the ability to construct a NAND gate, and to combine such gates into general circuits.</p>\n\n<p>Another non-electronic example: Quantum computers may be non-electronic, at least in their processing elements, and are able to compute general deterministic logic circuits.</p>\n", "question": "<p>According to <a href=\"https://en.wikipedia.org/wiki/Artificial_intelligence\" rel=\"nofollow\">Wikipedia</a>:</p>\n\n<blockquote>\n  <p>AI is intelligence exhibited by machines.</p>\n</blockquote>\n\n<p>I have been wondering if with the recent biological advancements, is there already a non-electrical-based \"machine\" that is programmed by humans in order to be able to behave like a:</p>\n\n<blockquote>\n  <p><strong>flexible rational agent</strong> that perceives its environment and takes actions that maximize its chance of success at some goal</p>\n</blockquote>\n\n<p>I was specifically thinking of viruses and bacteria. Have these been programmed by humans in order to behave as a flexible rational agent (i.e. an AI entity)?</p>\n\n<p>Are there are other organisms that have already been used for this purpose?</p>\n"}, "id": "2220"}, {"body": {"answer": "<ol>\n<li>Filling values is totally fine. In the case of image recognition the filling will be the background of the image (<a href=\"https://www.google.com/search?q=mnist+images&amp;tbm=isch\" rel=\"nofollow\">examples</a>). For example in Belot you have total of 32 cards, which can be 32 boolean features. You can set the ones the player has to 1, while the rest are 0. Note that the in most games you'll need more features than the cards in your hand. I.e number of the round, cards that have been played so far, calls that have been made etc. </li>\n<li>Defining the scope of the \"action space\" will be specific to the game. For Belot, it can be number encoding for each of the 32 cards.</li>\n</ol>\n\n<p>You can find articles via Google. <a href=\"http://homes.soic.indiana.edu/adamw/hearts.pdf\" rel=\"nofollow\">Here</a> is a paper about ML for a card game. Instead of articles, I'd recommend checking out a course on ML (i.e. Coursera and Udacity have good free online courses).</p>\n", "question": "<p>Ok, I now know how a machine can learn to play to play Atari games (Breakout): <a href=\"https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf\" rel=\"nofollow noreferrer\">Playing Atari with Reinforcement Learning</a></p>\n\n<p>With the same technique it is even possible to play FPS games (Doom): <a href=\"https://arxiv.org/pdf/1609.05521\" rel=\"nofollow noreferrer\">Playing FPS Games with Reinforcement Learning</a></p>\n\n<p>Further studies even investigated multiagent scenarios (Pong): <a href=\"https://arxiv.org/pdf/1511.08779.pdf\" rel=\"nofollow noreferrer\">Multiagent Cooperation and Competition with Deep Reinforcement Learning</a></p>\n\n<p>And even another awesome article for the interested user in context of deep reinforcement learning (easy and a must read for beginners): <a href=\"http://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/\" rel=\"nofollow noreferrer\">Demystifying Deep Reinforcement Learning</a></p>\n\n<p>I was thrilled by these results and immediately wanted to try them in some simple \"board/card game scenarios\", i.e. writing AI for some simple games in order to learn more about \"deep learning\". Of course, thinking that I can apply the techniques above easily in my scenarios was stupid. All examples above are based on convolutional nets (image recognition) and some other assumptions, which might not be applicable in my scenarios.</p>\n\n<p>Can you give me hints or futher articles, which deal with my questions below? As a beginner, I do not have an overview, yet. Preferably, your suggestions should also be connected to the following areas already: deep learning, reinforcement learning (, multiagent systems)</p>\n\n<hr>\n\n<p>(1)</p>\n\n<p>If you have a card game and the AI shall play a card from its hand, you could think about the cards (amongst other stuff) as the current game state. You can easily define some sort of neural net and feed it with the card data. In a trivial case the cards are just numbered. I do not know the net type, which would be suitable, but I guess deep reinforcment learning strategies could be applied easily then.</p>\n\n<p>However, I can only imagine this, if there is a constant number of hand cards. In the examples above, the number of pixels is also constant, for example. What if a player can have a different numbers of cards? What to do, if a player can have an infinite number of cards? Of course, this is just a theoretical question as no game has an infinite number of cards.</p>\n\n<hr>\n\n<p>(2)</p>\n\n<p>In the initial examples, the action space is constant. What can you do, if the action space is not? This more or less follows from my previous problem. If you have 3 cards, you can play card 1, 2 or 3. If you have 5 cards, you can play card 1, 2, 3, 4 or 5, etc. It is also common in card games, that it is not allowed to play a card. Could this be tackled with negative reward?</p>\n\n<hr>\n\n<p>So, which \"tricks\" can be used, e.g. always assume a constant number of cards with \"filling values\", which is only applicable in the non-infinite case (anyways unrealistic and even humans could not play well with that)?\nAre there articles, which examine such things already?</p>\n"}, "id": "2221"}, {"body": {"answer": "<p>On the suggestion of the O.P. rcpinto I converted a comment about seeing \"around a half-dozen papers that follow up on Graves et al.'s work which have produced results of the  caliber\" and will provide a few links. Keep in mind that this only answers the part of the question pertaining to NTMs, not Google DeepMind itself, plus I'm still learning the ropes in machine learning, so some of the material in these papers is over my head; I did manage to grasp much of the material in <a href=\"https://arxiv.org/pdf/1410.5401.pdf\">Graves et al.'s original paper</a>{1] though and am close to having homegrown NTM code to test. I also at least skimmed the following papers over the last few months; they do not replicate the NTM study in a strict scientific manner, but many of their experimental results do tend to support the original at least tangentially:</p>\n\n<p>\u2022 In <a href=\"https://arxiv.org/pdf/1607.00036.pdf\">this paper</a> on a variant version of NTM addressing, Gulcehere, et al. do not try to precisely replicate Graves et al.'s tests, but like the DeepMind team, it does demonstrate markedly better results for the original NTM and several variants over an ordinary recurrent LSTM. They use 10,000 training samples of a Facebook Q&amp;A dataset, rather than the N-grams Graves et al. operated on in their paper, so it's not replication in the strictest sense. They did however manage to get a version of the original NTM and several variants up and running, plus recorded the same magnitude of performance improvement.<a href=\"https://arxiv.org/pdf/1607.00036.pdf\">2</a></p>\n\n<p>\u2022 Unlike the original NTM, <a href=\"https://arxiv.org/abs/1505.00521\">this study</a> tested a version of reinforcement learning which was not differentiable; that may be why they were unable to solve several of the programming-like tasts, like Repeat-Copy, unless the controller wasn't confined to moving forwards. Their results were nevertheless good enough to lend support to the idea of NTMs. A more recent revision of their paper is apparently available, which I have yet to read, so perhaps some of their variant's problems have been solved.<a href=\"https://arxiv.org/abs/1505.00521\">3</a></p>\n\n<p>\u2022 Instead of testing the original flavor of NTM against ordinary neural nets like LSTMs, <a href=\"https://arxiv.org/pdf/1510.03931v3.pdf\">this paper</a> pitted it against several more advanced NTM memory structures. They got good results on the same type of programming-like tasks that Graves et al. tested, but I don't think they were using the same dataset (it's hard to tell from the way their study is written just what datasets they were operating on).<a href=\"https://arxiv.org/pdf/1510.03931v3.pdf\">4</a> </p>\n\n<p>\u2022 On p. 8 of <a href=\"https://arxiv.org/pdf/1605.06065.pdf\">this study</a>, an NTM clearly outperforms several LSTM, feed-forward and nearest-neighbor based schemes on an Omniglot character recognition dataset. An alternative approach to external memory cooked up by the authors clearly beats it, but it still obviously performs well. The authors seem to belong to a rival team at Google, so that might be an issue when assessing replicability.<a href=\"https://arxiv.org/pdf/1605.06065.pdf\">5</a></p>\n\n<p>\u2022 On p. 2 <a href=\"http://www.thespermwhale.com/jaseweston/ram/papers/paper_6.pdf\">these authors</a> reported getting better generalization on \"very large sequences\" in a test of copy tasks, using a much smaller NTM network they evolved with the genetic NEAT algorithm, which dynamically grows topologies.<a href=\"http://www.thespermwhale.com/jaseweston/ram/papers/paper_6.pdf\">6</a>  </p>\n\n<p>NTMs are fairly new so there hasn't been much time to stringently replicate the original research yet, I suppose. The handful of papers I skimmed over the summer, however, seem to lend support to their experimental results; I have yet to see any that report anything but excellent performance. Of course I have an availability bias, since I only read the pdfs I could easily find in a careless Internet search. From that small sample it seems that most of the follow-up research has been focused on extending the concept, not replication, which would explain the lack of replicability data. I hope that helps.</p>\n\n<p><a href=\"https://arxiv.org/pdf/1410.5401.pdf\">1</a> Graves, Alex; Wayne, Greg and Danihelka, Ivo, 2014, \"Neural Turing Machines,\" published Dec. 10, 2014. </p>\n\n<p><a href=\"https://arxiv.org/pdf/1607.00036.pdf\">2</a> Gulcehre, Caglar; Chandar, Sarath; Choy, Kyunghyun and Bengio, Yoshua, 2016, \"Dynamic Neural Turing machine with Soft and Hard Addressing Schemes,\" published June 30, 2016. </p>\n\n<p><a href=\"https://arxiv.org/abs/1505.00521\">3</a> Zaremba, Wojciech and Sutskever, Ilya, 2015, \"Reinforcement Learning Neural Turing Machines,\" published May 4, 2015. </p>\n\n<p><a href=\"https://arxiv.org/pdf/1510.03931v3.pdf\">4</a> Zhang; Wei; Yu, Yang and Zhou, Bowen, 2015, \"Structured Memory for Neural Turing Machines,\" published Oct. 25, 2015.    </p>\n\n<p><a href=\"https://arxiv.org/pdf/1605.06065.pdf\">5</a> Santoro, Adam; Bartunov, Sergey; Botvinick, Matthew; Wierstra, Daan and Lillicrap, Timothy, 2016, \"One-Shot Learning with Memory-Augmented Neural Networks,\" published May 19, 2016.    </p>\n\n<p><a href=\"http://www.thespermwhale.com/jaseweston/ram/papers/paper_6.pdf\">6</a> Boll Greve, Rasmus; Jacobsen, Emil Juul and Sebastian Risi, date unknown, \"Evolving Neural Turing Machines.\" No publisher listed</p>\n\n<p>All except (perhaps) Boll Greve et al. were published at the Cornell Univeristy Library arXiv.org Repository: Ithaca, New York.</p>\n", "question": "<p>Deep Mind has published a lot of works on deep learning in the last years, most of them state-of-the-art on their respective tasks. But how much of this work has actually been reproduced by the AI community? For instance, the Neural Turing Machine paper seems to be very hard to reproduce, according to other researchers.</p>\n"}, "id": "2229"}, {"body": {"answer": "<p>I'd like to add, self-driving cars would also be excellent for disabled people who would otherwise not be able to drive. Adds a lot more autonomy to vulnerable people</p>\n", "question": "<p>What are the advantages of having self-driving cars?</p>\n\n<p>We will be able to have more cars in the traffic at the same time, but won't it also make more people choose to use the cars, so both the traffic and the public health will actually become worse?</p>\n\n<p>Are we really interested in this?</p>\n"}, "id": "2230"}, {"body": {"answer": "<p>Self driving cars are good for the following reasons:</p>\n\n<ul>\n<li>In the case of an emergancy, urgancy, or just someone being unable to drive unexpactedly, the car can go by itself to a designated location - this is useful in so many use cases - kids who need to get somewhere while parents are busy, Parents who drank a little too much and prefer to take 'the cab' home, or while running, you got injured and need a pick-up.</li>\n<li>The examples above are for the more obvious things, which we currently have a struggle with. but other than those, Self-driving cars will open a door for a much wider scale of things: safe police chases (just a car without a police officer), taxies, help in the battle field, and much more...</li>\n<li>The third and most important benefit, is the safety and economical properties of self driving cars: with a lot of those cars on the road, they can 'understand' each other and nothing will go unpredicted. they have much faster response time then humans, and maybe in the future they will even be able to predict traffic-light changes, and by that save gas and money (even more than what they can save right now by driving economicly)</li>\n</ul>\n", "question": "<p>What are the advantages of having self-driving cars?</p>\n\n<p>We will be able to have more cars in the traffic at the same time, but won't it also make more people choose to use the cars, so both the traffic and the public health will actually become worse?</p>\n\n<p>Are we really interested in this?</p>\n"}, "id": "2232"}, {"body": {"answer": "<p>Definitions of Artificial Intelligence can be categorized into four categories, Thinking Humanly, Thinking Rationally, Acting Humanly and Acting Rationally. The following picture (from Artificial Intelligence: A Modern Approach) will shed light on over these definitions:<br> <br>\n<a href=\"https://i.stack.imgur.com/9jOK9.png\" rel=\"nofollow\"><img src=\"https://i.stack.imgur.com/9jOK9.png\" alt=\"enter image description here\"></a>\n <br><br>\nThe definition which I like is by John McCarthy, \"It is the science and engineering of making intelligent machines, especially intelligent computer programs. It is related to the similar task of using computers to understand human intelligence, but AI does not have to confine itself to methods that are biologically observable.\"<br>\n<br>\nMachine Learning on the other hand is field of AI which deals with pattern recognition. Various algorithms are used over a set of data to predict the future. Machine Learning is data driven and data oriented.<br></p>\n\n<blockquote>\n  <p>In a nutshell Artificial Intelligence is a field of Computer Science which deals with providing machines the ability of perform rational tasks. Natural Language Processing, Automation, Image Processing, and many others are part of it.<br>\n  Machine Learning is a subset of AI which is data oriented and deals with predicting. Used in search engines, Youtube recommendation list, etc.</p>\n</blockquote>\n", "question": "<p>These two terms seem to be related, especially in their application in computer science and software engineering.  Is one a subset of another?  Is one a tool used to build a system for the other?  What are their differences and why are they significant?</p>\n"}, "id": "2233"}, {"body": {"answer": "<p>First, I guess that you mean <a href=\"https://en.wikipedia.org/wiki/Common_Lisp\" rel=\"nofollow\">Common Lisp</a> (which is a standard language specification, see its <a href=\"http://www.lispworks.com/documentation/HyperSpec/Front/\" rel=\"nofollow\">HyperSpec</a>).</p>\n\n<p>Then, Common Lisp is great for symbolic AI. However, many recent machine learning libraries are coded in more mainstream languages, for example <a href=\"https://en.wikipedia.org/wiki/TensorFlow\" rel=\"nofollow\">TensorFlow</a> is coded in C++ &amp; Python. <a href=\"http://machinelearningmastery.com/popular-deep-learning-libraries/\" rel=\"nofollow\">Deep learning libraries</a> are mostly coded in C++ or Python or C (and sometimes using <a href=\"https://en.wikipedia.org/wiki/OpenCL\" rel=\"nofollow\">OpenCL</a> or Cuda for GPU computing parts).</p>\n\n<p>Common Lisp is great for <a href=\"https://en.wikipedia.org/wiki/Symbolic_artificial_intelligence\" rel=\"nofollow\">symbolic artificial intelligence</a> because:</p>\n\n<ul>\n<li>it has very good <em>implementations</em> (e.g. <a href=\"http://sbcl.org/\" rel=\"nofollow\">SBCL</a>, which compiles to machine code every expression given to the <a href=\"https://en.wikipedia.org/wiki/Read%E2%80%93eval%E2%80%93print_loop\" rel=\"nofollow\">REPL</a>)</li>\n<li>it is <a href=\"https://en.wikipedia.org/wiki/Homoiconicity\" rel=\"nofollow\"><strong>homoiconic</strong></a>, so it is easy to deal with programs as data, in particular it is easy to generate [sub-]programs, that is use <a href=\"https://en.wikipedia.org/wiki/Metaprogramming\" rel=\"nofollow\">meta-programming</a> techniques.</li>\n<li>it has a <a href=\"https://en.wikipedia.org/wiki/Read%E2%80%93eval%E2%80%93print_loop\" rel=\"nofollow\">Read-Eval-Print Loop</a> to ease interactive programming</li>\n<li>it provides a very powerful <a href=\"https://en.wikipedia.org/wiki/Macro_%28computer_science%29\" rel=\"nofollow\">macro</a> machinery (essentially, you define your own domain specific sublanguage for your problem), much more powerful than in other languages like C.</li>\n<li>it mandates a <a href=\"https://en.wikipedia.org/wiki/Garbage_collection_%28computer_science%29\" rel=\"nofollow\">garbage collector</a> (even code can be garbage collected)</li>\n<li>it provides many <a href=\"https://en.wikipedia.org/wiki/Container_%28abstract_data_type%29\" rel=\"nofollow\">container</a> abstract data types, and can easily handle symbols.</li>\n<li>you can code both high-level (dynamically typed) and low-level (more or less startically typed) code, thru appropriate annotations.</li>\n</ul>\n\n<p>However most machine learning &amp; neural network libraries are not coded in CL. Notice that neither neural network nor deep learning is in the symbolic artificial intelligence field. See also <a href=\"http://ai.stackexchange.com/q/35/3335\">this question</a>.</p>\n\n<p>Several symbolic AI systems like <a href=\"https://en.wikipedia.org/wiki/Eurisko\" rel=\"nofollow\">Eurisko</a> or <a href=\"https://en.wikipedia.org/wiki/Cyc\" rel=\"nofollow\">CyC</a> have been developed in CL (actually, in some DSL built above CL).</p>\n\n<p>Notice that the programming language might not be very important. In the <a href=\"https://en.wikipedia.org/wiki/Artificial_general_intelligence\" rel=\"nofollow\">Artificial General Intelligence</a> research topic, some people work on the idea of a AI system which would generate all its own code (so are designing it with a <a href=\"https://en.wikipedia.org/wiki/Bootstrapping_%28compilers%29\" rel=\"nofollow\">bootstrapping</a> approach). Then, the code which is generated by such a system can even be generated in low level programming languages like C. See <a href=\"http://bootstrappingartificialintelligence.fr/WordPress3/\" rel=\"nofollow\">J.Pitrat's blog</a></p>\n", "question": "<p>I've heard before from computer scientists and from researchers in the area of AI that that Lisp is a good language for research and development in artificial intelligence. Does this still apply, with the proliferation of neural networks and deep learning? What was their reasoning for this? What languages are current deep-learning systems currently built in?</p>\n"}, "id": "2237"}, {"body": {"answer": "<p>David Nolen (contributor to Clojure and ClojureScript; creator of Core Logic a port of miniKanren) in a talk called <strong>LISP as too powerful</strong>. He stated that back in his days LISP was decades ahead of other programming languages. There are <a href=\"http://blog.samibadawi.com/2013/05/lisp-prolog-and-evolution.html\" rel=\"nofollow\">number of reasons</a> why the language wasn't able to maintain it's name.</p>\n\n<p><a href=\"http://norvig.com/paip-preface.html\" rel=\"nofollow\">This</a> article highlights som key points why LISP is good for AI</p>\n\n<ul>\n<li>Easy to define a new language and manipulate complex information.</li>\n<li>Full flexibility in defining and manipulating programs as well as data.</li>\n<li>Fast, as program is concise along with low level detail. </li>\n<li>Good programming environment (debugging, incremental compilers, editors).</li>\n</ul>\n\n<p>Most of my friends into this field usually use Matlab for Artificial Neural Networks and Machine Learning. It hides the low level details though. If you are only looking for results and not how you get there, then Matlab will be good. But if you want to learn even low level detailed stuff, then I will suggest you go through LISP at-least once.<br>\nLanguage might not be that important if you have the understanding of various AI algorithms and techniques. I will suggest you to read <em>\"Artificial Intelligence: A Modern Approach (by Stuard J. Russell and Peter Norvig\"</em>. I am currently reading this book, and it's a very good book.</p>\n", "question": "<p>I've heard before from computer scientists and from researchers in the area of AI that that Lisp is a good language for research and development in artificial intelligence. Does this still apply, with the proliferation of neural networks and deep learning? What was their reasoning for this? What languages are current deep-learning systems currently built in?</p>\n"}, "id": "2238"}, {"body": {"answer": "<p>What is life? <strong>AND</strong> Is AI a living organism? <em>are two different questions</em>.<br>\nThe first question is more philosophical and dependent. It can change with time, reference to topic of discussion or something else. Today, one parameter to its definition is <em>mortality</em>. In future if we reach to a certain technological level where mortal beings were only part of history, then the definition will drop <strong>this</strong> parameter.<br><br>\nComing to the second question. AI started as field of study to make machines to think like humans (or take rational decisions). Giving life to machines was, or is, not a concern of AI developers (at-least not nowadays). Once I watched some videos of Michio Kaku, where he talked about consciousness along with AI.<br>\nSuppose human has a conscious level of 10. Then a thermostat might have the conscious level of 1 as it can sense when the surrounding is hot or cold and then take decision. Similarly a rat can have a conscious level 7 (or something). And the levels are of exponential order (not a linear scale). Similarly you can develop an AI program and check what level of consciousness it has achieved. Then you can decide whether it is living or not. ANI (Artificial Narrow Intelligence) will have a lower level of consciousness level than AGI (Artificial General Intelligence). ASI (Artificial Super Intelligence) will have consciousness level higher than the other two, and way higher than any human being.<br><br>\nTo judge whether an AI program is living or not you need a concrete definition of <strong>\"LIFE\"</strong>. Your definition can include various parameters like consciousness, adaptability, metabolism (or another method of generating energy for use), rational behavior, intelligence , learning through experience, etc. etc. etc.<br>\nBut the thing in the end is that its your definition. There are many definitions of \"LIFE\" out there. You can't judge a program for life by all definitions, as some of the definitions are contrary to others.<br></p>\n\n<blockquote>\n  <p>So, answer to whether an AI program is living or not, is that <strong>IT DEPENDS</strong>. Depends on your definition of life.</p>\n</blockquote>\n", "question": "<p>I'm a bit confused about the definition of life. Can AI systems be called 'living'? Because they can do most of the things that we can. They can even communicate with one another. </p>\n\n<p>They are not formed of what we call cells. But, you see, cells are just a collection of several chemical processes which is in turn non-living just like AI is formed of several lines of code.</p>\n"}, "id": "2239"}, {"body": {"answer": "<p>To my mind the essential reason why neural networks and the brain are powerful is that they create a hierarchical model of data or of the world. If you ask why that makes them powerful, well, that's just the structure of the world. If you are stalked by a wolf, it's not like its upper jaw will attack you frontally, while his lower jaw will attack you from behind. If you want to respond to the threat with a feasible computational effort, you'll have to treat the wolf as one entity. Providing these kinds of entities or concepts from the raw bits and bytes of input is what a hierarchical representation does. </p>\n\n<p>Now, this is quite intuitive for sensory information: lashes, iris, eyebrow make up an eye, eyes, nose and mouth make up a face and so on. What is less obvious, is the fact that motor control works exactly the same way! Only in reverse. If you want to lift your arm, you'll just lift it. But for your brain to actually realise this move, the high level command has to be broken down into precise signals for every muscle involved. And this is done by propagating the command down the hierarchy. </p>\n\n<p>In the brain these two functions are strongly intertwined. You use constant sensory feedback to adapt your motor control and in many cases you'd be incapable of integrating your stream of sensory data into a coherent representation if you didn't have the additional information of what your body is doing to change that stream of data. <a href=\"https://en.wikipedia.org/wiki/Saccade\" rel=\"nofollow\">Saccades</a> are a good example for that. </p>\n\n<p>Of course this doesn't mean that our cognitive functions are dependent on the processing of sensorimotor information. I would be surprised if a pure thinking machine wouldn't be possible. There is however a specific version of this \"embodied intelligence hypothesis\" that sounds plausible to me: </p>\n\n<p>Creating high level cognitive concepts with unsupervised learning is a really difficult problem. Creating high level motor representation might be significantly easier. The reason is that there is more immediate useful feedback. I have been thinking about how to provide a scaffolding for the learning of a hierarchy of cognitive concepts and one thing I could imagine is that high level cognitive concepts basically hitch a ride with the motor concepts. Just think of what a pantomime can express with movement alone. </p>\n", "question": "<p>There is this claim around that the brain's cognitive capabilities are tightly linked to the way it processes sensorimotor information and that, in this or a similar sense, our intelligence is \"embodied\". Lets assume, for the sake of argument, that this claim is correct (you may think the claim is too vague to even qualify for being correct, that it's \"not even false\". If so, I would love to hear your ways of fleshing out the claim in such a way that it's specific enough to be true or false). Then, since arguably at least chronologically in our evolution, most of our higher level cognitive capabilities come after our brain's way of processing sensorimotor information, this brings up the question what it is about the way that our brains function that make them particularly suitable for the processing of sensorimotor information? What makes our brains'  architecture particularly suitable for being an information processing unit inside a body? This is my first question. And what I'm hoping for are answers that go beyond the <em>a fortiori</em> reply \"Our brain is so powerful and dynamic, it's great for <em>any</em> task, and so also for processing sensorimotor information\"</p>\n\n<p>My second question is basically the same but instead of the human brain I want to ask for neural networks. What are the properties of neural networks that makes them <em>particularly</em> suitable for processing the kind of information that is produced by a body? Here are some of the reasons why people think neural networks are powerful:</p>\n\n<ul>\n<li>The universal approximation theorem (of FFNNs)</li>\n<li>their ability to learn and self-organise</li>\n<li>Robustness to local degrading of information</li>\n<li>their ability to abstract/coarse-grain/convolute features, etc.</li>\n</ul>\n\n<p>While I see how these are real advantages when it comes to evolution picking its favorite model for an embodied AI, none of them (or their combination) seems to be unique to neural networks. So they don't provide a satisfactory answer to my question. What makes a neural network a more suitable structure for embodied AI than, say, having a literal Turing machine sitting inside our head, or any other structure that is capable of universal computation? For instance, I really don't see how neural networks would be a particularly natural choice for dealing with geometric information. But geometric information is pretty vital when it comes to sensorimotor information, no?</p>\n"}, "id": "2240"}, {"body": {"answer": "<p>BlindKungFuMaster's answer deals with the hierarchical nature of perception and bodily control, so I'll set that aside and try instead to answer why evolution would use neural networks for animal embodied cognition, and then try to answer if robots of other artificial animals would use the same system.</p>\n\n<p>It's important to focus on animals as a whole, not just humans, because that's how evolution works--like the famous John Gall quote:</p>\n\n<blockquote>\n  <p>A complex system that works is invariably found to have evolved from a simple system that worked.</p>\n</blockquote>\n\n<p>If you could build a system with five moving parts that does sensorimotor control, but it needs all five parts working in order to function at all, evolution could not build that system except in the rarest of circumstances. </p>\n\n<p>What evolution instead does is slowly extend functional systems. If having one light-sensitive cell connected to one muscle cell makes an organism more likely to survive, then you have the building blocks to add a second layer without inventing any new sorts of cells, because you already have the information-processing connector.</p>\n\n<p>Neural networks are convenient for evolution because their organization matches the hierarchical nature of the problem <em>and</em> the same kind of cell is used everywhere. All you need is dendrites to receive signals, a way to compute the threshold and trigger if the received signal is higher, axons that can make it to other cells, and then branches at the end of the axon to serve as multipliers. You can arbitrarily extend the depth and breadth of the network just by adding more cells.</p>\n\n<p>Neural networks are convenient for artificial sensorimotor control because they give you, in memory, access to lots of intermediate values. They're also convenient for the same reasons evolution found them convenient--we can just say what we expect the structure of the robotic control will look like, provide training data, and then eventually have a robot that works.</p>\n\n<p>But there's lots of robotics where the control system is designed instead of learned. To take a very simple example, one <em>could</em> use machine learning on the thermostat problem, to learn what temperatures require the heater to be turned on and what temperatures require the air conditioner to be turned on. But this would be extra work <em>and</em> a less robust system than just designing the optimal control system ahead of time.</p>\n\n<p>In control theory, there's a concept called <a href=\"https://en.wikipedia.org/wiki/Adaptive_control\" rel=\"nofollow\">adaptive control</a>, where one of the state space parameters for the control system is a property of the system. For example, imagine a satellite; typically we think of the state space of the system as the position and velocity of the satellite in three dimensions, so six total coordinates. There's then a set of differential equations that describe how the satellite will move over time, and what would happen if we used the actuators on the satellite to change its velocity.</p>\n\n<p>But part of those differential equations is the inertia of the satellite. That is, how much fuel we need to expend and how it'll affect the rotation and translation of the satellite depends on where the weight of the satellite is located. And this can change over time, as fuel is consumed or if it wasn't correctly measured to begin with. Adaptive control adds new states to the system to track the inertia, and then simultaneously updates its estimate of the inertia and uses that estimate to plan what controls are necessary to move to a desired position.</p>\n\n<p>You could imagine solving this problem with neural networks, but we can fairly easily calculate the optimal solution from first principles. In that case, we don't need neural network-based control, but the end result will look something like it from the outside.</p>\n", "question": "<p>There is this claim around that the brain's cognitive capabilities are tightly linked to the way it processes sensorimotor information and that, in this or a similar sense, our intelligence is \"embodied\". Lets assume, for the sake of argument, that this claim is correct (you may think the claim is too vague to even qualify for being correct, that it's \"not even false\". If so, I would love to hear your ways of fleshing out the claim in such a way that it's specific enough to be true or false). Then, since arguably at least chronologically in our evolution, most of our higher level cognitive capabilities come after our brain's way of processing sensorimotor information, this brings up the question what it is about the way that our brains function that make them particularly suitable for the processing of sensorimotor information? What makes our brains'  architecture particularly suitable for being an information processing unit inside a body? This is my first question. And what I'm hoping for are answers that go beyond the <em>a fortiori</em> reply \"Our brain is so powerful and dynamic, it's great for <em>any</em> task, and so also for processing sensorimotor information\"</p>\n\n<p>My second question is basically the same but instead of the human brain I want to ask for neural networks. What are the properties of neural networks that makes them <em>particularly</em> suitable for processing the kind of information that is produced by a body? Here are some of the reasons why people think neural networks are powerful:</p>\n\n<ul>\n<li>The universal approximation theorem (of FFNNs)</li>\n<li>their ability to learn and self-organise</li>\n<li>Robustness to local degrading of information</li>\n<li>their ability to abstract/coarse-grain/convolute features, etc.</li>\n</ul>\n\n<p>While I see how these are real advantages when it comes to evolution picking its favorite model for an embodied AI, none of them (or their combination) seems to be unique to neural networks. So they don't provide a satisfactory answer to my question. What makes a neural network a more suitable structure for embodied AI than, say, having a literal Turing machine sitting inside our head, or any other structure that is capable of universal computation? For instance, I really don't see how neural networks would be a particularly natural choice for dealing with geometric information. But geometric information is pretty vital when it comes to sensorimotor information, no?</p>\n"}, "id": "2243"}, {"body": {"answer": "<blockquote>\n  <p>what it is about the way that our brains function that make them particularly suitable for the processing of sensorimotor information?</p>\n</blockquote>\n\n<p>They are an extension of sensory-motor receptors, function could mean any of the hundreds of specific calculations the brain makes, but each one is basically a circuit made out of variations of a basic cell type, with a basic computation, that is a neuron.</p>\n\n<blockquote>\n  <p>What makes our brains' architecture particularly suitable for being an information processing unit inside a body?</p>\n</blockquote>\n\n<p>I don't think it is helpful to think about inside and outside processing, but rather processing along tracts and nodes,( closer to the receptor, available to consciousness,etc)but leaving aside this distinction, the brain architecture is suitable for processing information ( again what facet of information processing you are referring to is unclear), due to the number of specialized computations that derive from it's evolution.</p>\n\n<blockquote>\n  <p>What are the properties of neural networks that makes them particularly suitable for processing the kind of information that is produced by a body?</p>\n</blockquote>\n\n<p>A neural network resembles certain parts/circuits of a brain, mainly how information is integrated based on a set of inputs and their frequency, there is variety and nuance in their types, but they all have inputs which in the case of a body are sensory/interneurons cells and outputs; neuron afferents and motor neurons.</p>\n", "question": "<p>There is this claim around that the brain's cognitive capabilities are tightly linked to the way it processes sensorimotor information and that, in this or a similar sense, our intelligence is \"embodied\". Lets assume, for the sake of argument, that this claim is correct (you may think the claim is too vague to even qualify for being correct, that it's \"not even false\". If so, I would love to hear your ways of fleshing out the claim in such a way that it's specific enough to be true or false). Then, since arguably at least chronologically in our evolution, most of our higher level cognitive capabilities come after our brain's way of processing sensorimotor information, this brings up the question what it is about the way that our brains function that make them particularly suitable for the processing of sensorimotor information? What makes our brains'  architecture particularly suitable for being an information processing unit inside a body? This is my first question. And what I'm hoping for are answers that go beyond the <em>a fortiori</em> reply \"Our brain is so powerful and dynamic, it's great for <em>any</em> task, and so also for processing sensorimotor information\"</p>\n\n<p>My second question is basically the same but instead of the human brain I want to ask for neural networks. What are the properties of neural networks that makes them <em>particularly</em> suitable for processing the kind of information that is produced by a body? Here are some of the reasons why people think neural networks are powerful:</p>\n\n<ul>\n<li>The universal approximation theorem (of FFNNs)</li>\n<li>their ability to learn and self-organise</li>\n<li>Robustness to local degrading of information</li>\n<li>their ability to abstract/coarse-grain/convolute features, etc.</li>\n</ul>\n\n<p>While I see how these are real advantages when it comes to evolution picking its favorite model for an embodied AI, none of them (or their combination) seems to be unique to neural networks. So they don't provide a satisfactory answer to my question. What makes a neural network a more suitable structure for embodied AI than, say, having a literal Turing machine sitting inside our head, or any other structure that is capable of universal computation? For instance, I really don't see how neural networks would be a particularly natural choice for dealing with geometric information. But geometric information is pretty vital when it comes to sensorimotor information, no?</p>\n"}, "id": "2244"}, {"body": {"answer": "<p>\"Trap\" functions were introduced as a way to discuss how GAs behave on functions where sampling most of the search space would provide pressure for the algorithm to move in the wrong direction (wrong in the sense of away from the global optimum).</p>\n\n<p>For example, consider a four-bit function f(x) such that</p>\n\n<pre><code>f(0000) = 5\nf(0001) = 1\nf(0010) = 1\nf(0011) = 2\nf(0100) = 1\nf(0101) = 2\nf(0110) = 2\nf(0111) = 3\nf(1000) = 1\nf(1001) = 2\nf(1010) = 2\nf(1011) = 3\nf(1100) = 2\nf(1101) = 3\nf(1110) = 3\nf(1111) = 4\n</code></pre>\n\n<p>That is, the fitness of a string is equal to the number of 1s in the string, except f(0000) is 5, the optimal solution. This function can be thought of as consisting of two disjoint pieces: one that contains the global optimum (0000) and another that contains the local optimum at its complement (1111). All points other than these have fitness values such that standard evolutionary algorithm dynamics would lead the algorithms to tend towards the local optimum at 1111 rather than the global optimum at 0000.</p>\n\n<p>That's basically what is meant by a trap function. You can consider variations on this theme, but that's the gist of it.</p>\n\n<p>That said, I don't think I understand what question you're asking. I don't know what you have in mind by \"to experiment how much the algorithm is discerning of the correct global optimum\".</p>\n", "question": "<p>From what I understood, a deceptive trap function is a problem which is used to experiment how much the algorithm is discerning of the correct global optimum? Is my understanding correct?</p>\n\n<p>edit: A better worded understanding would be \"how difficult the genetic algorithm would find it not to be inclined to the local optimum of a trap function\".</p>\n"}, "id": "2246"}, {"body": {"answer": "<p>The definition of life for me is a very intelligent and beneficial being.  I have not witnessed any AI program that evens comes close to this definition yet.  Therefore, based on the evidence that I have at this point in time, I would have to conclude no.</p>\n", "question": "<p>I'm a bit confused about the definition of life. Can AI systems be called 'living'? Because they can do most of the things that we can. They can even communicate with one another. </p>\n\n<p>They are not formed of what we call cells. But, you see, cells are just a collection of several chemical processes which is in turn non-living just like AI is formed of several lines of code.</p>\n"}, "id": "2247"}, {"body": {"answer": "<p>An agent perceives the environment through sensors and act according to the incoming percepts (agent's perceptual input at any instant). An autonomous vacuum cleaner can be as simple as<br></p>\n\n<blockquote>\n  <p>(block<sub>i</sub>, clean) --> Move to block<sub>i+1</sub><br>\n  (block<sub>i</sub>, dirty) --> Clean<br></p>\n</blockquote>\n\n<p>This is just a general description, actual one is more complicated. Or the bot can have a memory where it stores all its previous decision and incorporate those while taking new ones.<br>\nThis can be helpful if the bot wants to remember where an obstacle (like wall, in this case bot don't want to go and check the presence of wall each and every single time it is turned on) is, or where it is more probable to find dirt. If the bot is not remembering its history then it will be scanning the whole house over and over again, sensing the same obstacle every time and going across them.<br>\nBot which keeps no log of its history will take the same procedure again and again, making the same mistakes again and again. This is not an efficient way and a waste of its energy (or battery).<br><br>\nNormally today bots have ordinary sensors which can only sense the dirt and obstacle. This limits the number of tasks a bot can perform. If a bot has decent camera as a sensor, and some algorithms of Image Processing are dumped into it, then it increases the tasks it can perform. Like detecting the stairs and cleaning different floors. Normally <strong>stairs will be considered obstacle and bot will just go around them</strong>. In case, when camera sensor is provided, <strong>stairs are potentially a path to be taken</strong>.<br><br>\n<strong>A*</strong> algorithm is not necessarily used in case when the bot is not remembering the map of the house (or room). A normal robot which just scans the room and cleans it, will not be needing, as it don't know it's destination. Its only goal is to clean if it finds something dirty. But a bot which knows the map of the room and where there is a high probability of finding dirt, the A* algorithm can be used.</p>\n", "question": "<p>How does a domestic autonomous robotic vacuum cleaner -  such as a <a href=\"https://en.wikipedia.org/wiki/Roomba\" rel=\"nofollow\">Roomba</a> - know when it's working cleaned area (aka virtual map), and how does it plan to travel to the areas which hasn't been explored yet?</p>\n\n<p>Does it use some kind of <a href=\"https://en.wikipedia.org/wiki/A*_search_algorithm\" rel=\"nofollow\">A*</a> algorithm?</p>\n"}, "id": "2249"}, {"body": {"answer": "<p>There are multiple motivations for self driving cars.</p>\n\n<blockquote>\n  <ol>\n  <li>Self driving cars have the potential to be much safer.</li>\n  </ol>\n</blockquote>\n\n<p>Self driving cars are far more reliable than humans and can learn and have their software improved and upgraded, resulting in safer roads and far fewer accidents.</p>\n\n<p>More on self-driving car safety: <a href=\"http://bigthink.com/ideafeed/googles-self-driving-car-is-ridiculously-safe\" rel=\"nofollow noreferrer\">http://bigthink.com/ideafeed/googles-self-driving-car-is-ridiculously-safe</a></p>\n\n<blockquote>\n  <ol start=\"2\">\n  <li>Self driving cars can lead to greater road efficiency.</li>\n  </ol>\n</blockquote>\n\n<p>Traffic jams and obstructions occur due to inefficiencies in human driving, see this MIT simulation of a <strong>\"phantom traffic jam\"</strong>: <a href=\"https://www.youtube.com/watch?v=Q78Kb4uLAdA\" rel=\"nofollow noreferrer\">https://www.youtube.com/watch?v=Q78Kb4uLAdA</a> and self driving cars can be programmed to avoid this.</p>\n\n<p><a href=\"https://i.stack.imgur.com/H3S0G.jpg\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/H3S0G.jpg\" alt=\"enter image description here\"></a></p>\n\n<blockquote>\n  <ol start=\"3\">\n  <li>Greater economic and environmental benefit</li>\n  </ol>\n</blockquote>\n\n<p>Self driving cars can keep driving costs down by conserving fuel and hence lead to a better environmental impact.</p>\n\n<p>More on fuel efficiency: <a href=\"http://movimentogroup.com/blog/how-self-driving-cars-increase-fuel-efficiency-decrease-waste/\" rel=\"nofollow noreferrer\">http://movimentogroup.com/blog/how-self-driving-cars-increase-fuel-efficiency-decrease-waste/</a></p>\n\n<blockquote>\n  <ol start=\"4\">\n  <li>Ease of transport</li>\n  </ol>\n</blockquote>\n\n<p>Self driving cars make transport easier and mean that drivers may be unnecessary in the future, resulting in a more pleasurable and easier drive.</p>\n\n<p><a href=\"https://i.stack.imgur.com/eb7ZC.jpg\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/eb7ZC.jpg\" alt=\"enter image description here\"></a></p>\n\n<p>In addition, this would make it easier for people with disabilities to travel as well as simplify the travel experience. Children could potentially be driven to school by a car without the supervision of a parent, for instance.</p>\n\n<blockquote>\n  <ol start=\"5\">\n  <li>Parking</li>\n  </ol>\n</blockquote>\n\n<p>Self driving cars can be called to pick you up, meaning the need for parking in nearby locations and/or long walks to find your car may become a thing of the past as your car would drive up to you to pick you up.</p>\n\n<blockquote>\n  <ol start=\"6\">\n  <li>Things we haven't even thought of yet :) </li>\n  </ol>\n</blockquote>\n", "question": "<p>What are the advantages of having self-driving cars?</p>\n\n<p>We will be able to have more cars in the traffic at the same time, but won't it also make more people choose to use the cars, so both the traffic and the public health will actually become worse?</p>\n\n<p>Are we really interested in this?</p>\n"}, "id": "2254"}, {"body": {"answer": "<p>To answer this question, you must first know what is intelligence, and since there is no clear line between intelligent and not, this question is more philosophical rather than technical.</p>\n\n<p>In my opinion, intelligence is the ability to define a problem and find a way to solve it using memory and reasoning. Since a genetic algorithm follows this structure, I would say that it falls under the category of artificial intelligence.</p>\n", "question": "<p>Since human intelligence presumably is a function of a natural genetic algorithm in nature, is using a genetic algorithm in a computer an example of artificial intelligence?  If not, how do they differ?  Or perhaps some are and some are not expressing artificial intelligence depending upon the scale of the algorithm and what it evolves into?</p>\n"}, "id": "2255"}, {"body": {"answer": "<p>Definitions of what life is usually come from biologists. The problem here is they are usually concerned with the traits common to the forms of life available to their studies, and that those forms of life all have a common origin (and this imposes a statistical bias on the observations).</p>\n\n<p>As we gradually erode the boundaries of the standard definitions of life, by means of creating ever more complex machines and also by harnessing biological material as a form of nanotechnology), it's very likely that at some point in the future our traditional definition of life will need to be updated and further abstracted away from its current reference points (aka the \"terroan biota\").</p>\n\n<p>A probably better question to ask to decide if something can be considered alive could be \"is it self sufficient?\" or \"can it care for itself and provide for its own needs to some extent?\".</p>\n", "question": "<p>I'm a bit confused about the definition of life. Can AI systems be called 'living'? Because they can do most of the things that we can. They can even communicate with one another. </p>\n\n<p>They are not formed of what we call cells. But, you see, cells are just a collection of several chemical processes which is in turn non-living just like AI is formed of several lines of code.</p>\n"}, "id": "2256"}, {"body": {"answer": "<p>Discretizing the output will probably be counter-productive in this situation; it would remove flexibility by taking away the fine-grained continuous ranges between each discretization bucket, but also blow up the size of the network, reducing manageability and performance. Fragmenting the outputs into buckets in this way may also lead to information loss and more difficulty in convergence because of the fact that each bucket is partially isolated from the others. </p>\n\n<p>After causing myself needless hassle in the past by mismatching input and output dimensions, I'd simply do this if I were in your shoes: 1) keep it simple and use 3 continuous (well, semi-continuous, depending on the highest precision your programming framework allows) outputs for all of the above reasons; 2) clamp the angles between -45 to 225 by whatever method works best for you, like ceiling/floor hard clamping, adding weight terms, etc.; 3) go big on the hidden layer(s) to maximize information sharing across the inputs and eventual outputs, force, angle of strike, position of strike, etc. This is more likely to fine-tune the precision of the outputs, thereby making good use of the semi-continuous scales. </p>\n\n<p>I'm also wondering if a convolutional neural net might work in this situation; their most popular use case is in image processing, but I don't see why you can't treat the force, angle and position as surrogate spatial dimensions. I'm not sure how many or what type of inputs you have, but 3 continuous outputs might be conducive to a 3D rather than a 2D space. Convolutionals are often used for those, as well as higher-dimensional and temporal data. I hope that helps. </p>\n", "question": "<p>I am trying to build an agent to play carrom. The problem statement is roughly to estimate three parameters (normalized) : </p>\n\n<ul>\n<li>force</li>\n<li>angle of striker</li>\n<li>position of strike </li>\n</ul>\n\n<p>Since the state and action space both are continuous, I thought of discretizing the output such that I have 270 [ valid angles from -45 to 225 degrees ] outputs for the angle, 10 outputs for force [ranging from 0 to 1] and 20 outputs for the position [ranging from 0 to 1].</p>\n\n<p>Thus I will have 300 output of my neural network, but this number seems a bit too high compared to normal neural networks in practice. </p>\n\n<p>I was wondering if there is a better way of approaching the problem considering the fact that there are multiple parameters to a particular action.</p>\n\n<p>Is there a generic way to approach such problems represented in 2D space. </p>\n"}, "id": "2258"}, {"body": {"answer": "<p>In theory, if one could build a computing device that matched or exceeded the cognitive capabilities of a sentient being, it should be possible. </p>\n\n<p>(Singlarity adherents believe we will one day be able to transfer the human mind into an artificial computing platform, and it logically follows that one could \"hack\" such a mind, or build from the ground up, to create a truly Artificial Intelligence.)</p>\n\n<p>But this may be like fusion power, where the old adage is that it is \"always 20 years away.\"</p>\n", "question": "<p>Can an AI become \"sentient\", so to speak? In detailed terms, could an AI theoretically become sentient, as in learning and becoming self-aware, all from an internal source code?</p>\n"}, "id": "2263"}, {"body": {"answer": "<p>If you mean high level assessment of self-learned evaluation functions in chess, then no, the advantage of a better evaluation function lies in the ability to prune the search tree more aggressively. So you would on the contrary try to search narrowly but deeply. </p>\n\n<p>(In reality neural network based evaluation functions are so slow, that you would search narrowly and still not get very deep. Nor very strong.)</p>\n\n<p>If you mean chess programming in general, than the answer is also no. In chess you have to go deep, at least selectively, because tactical possibilities that occur deep in some variations are important. </p>\n", "question": "<p>My high-level takeaway from <a href=\"https://arxiv.org/abs/1509.01549\" rel=\"nofollow noreferrer\">Matthew Lai's Giraffe Chess Paper</a> is that one would want to use broad, shallow game trees, with some method of evaluating the probability of a favorable outcome for a given board position.  Is this correct?  </p>\n\n<p>(Still working my way though the AlphaGo paper, but the method seems to be similar.) </p>\n"}, "id": "2264"}, {"body": {"answer": "<p>Deepmind recently created <a href=\"https://deepmind.com/blog/wavenet-generative-model-raw-audio/\" rel=\"nofollow noreferrer\">a voice synthesiser</a> along those lines. \nIt seems to be incredibly slow, but it might be possible to create a dumped down version of it.</p>\n\n<p>Apparently the task is called parametric TTS (text to speech). <a href=\"http://mlsp.cs.cmu.edu/courses/fall2012/lectures/spss_specom.pdf\" rel=\"nofollow noreferrer\">This overview</a> might give you some leads.</p>\n", "question": "<p>Has there been research done regarding processing speech then building a \"speaker profile\" based off the processed speech? Things like matching the voice with a speaker profile and matching speech patterns and wordage for the speaker profile would be examples of building the profile. Basically, building a model of an individual based solely off speech. Any examples of this being implemented would be greatly appreciated.</p>\n"}, "id": "2266"}, {"body": {"answer": "<p>Yes, there is. An extremely quick search found this:\n<a href=\"https://www.researchgate.net/publication/221536362_Multimodal_Speaker_Identification_Based_on_Text_and_Speech\" rel=\"nofollow noreferrer\">Multimodal Speaker Identification Based on Text_and_Speech</a>.</p>\n\n<p>Let me tl;dr for you: (My abstract addition in Italics)</p>\n\n<p>Novel method for speaker identification based on both speech utterances and their transcribed text. </p>\n\n<p><em>They first</em> transcribed text of each speaker\u2019s is processed by <em>using</em> probabilistic latent semantic indexing (PLSI) that model<em>s</em> each speaker\u2019s vocabulary which <em>is</em> closely related to his/her identity, function, or expertise. </p>\n\n<p><em>The speech to text used by users is DARPA's Efficient, Affordable, Reusable Speech-to-Text (EARS) Program in MetadataExtraction (MDE).</em></p>\n\n<p><em>By using</em> Melfrequency cepstral coefficients (MFCCs) and dynamic range is quantized to a number of predefined bins in order to compute MFCC local histograms for each speech utterance, which is time-aligned with the transcribed text. </p>\n\n<p><em>To test they used</em> RT-03 MDE Training Data Text and Annotations corpus distributed by the Linguistic Data Consortium.</p>\n\n<p><em>As for results:</em> Identification rate versus Probe ID when 44 speakers are employed. Average identification rates for (a) PLSI: 69%; (b) MFCCs: 66%; (c) Both: 67%.</p>\n\n<p>If you need more papers related, you could use a tool like <a href=\"https://the.iris.ai/\" rel=\"nofollow noreferrer\">https://the.iris.ai/</a> to find related papers.</p>\n\n<p><strong><em>Post edit</em></strong>: Hopefully now this post complies with the standards.</p>\n", "question": "<p>Has there been research done regarding processing speech then building a \"speaker profile\" based off the processed speech? Things like matching the voice with a speaker profile and matching speech patterns and wordage for the speaker profile would be examples of building the profile. Basically, building a model of an individual based solely off speech. Any examples of this being implemented would be greatly appreciated.</p>\n"}, "id": "2267"}, {"body": {"answer": "<p>It's certainly possible to simulate particles as you have described. The scientific field concerned with this is called called <a href=\"https://en.wikipedia.org/wiki/Molecular_dynamics\" rel=\"nofollow noreferrer\">molecular dynamics</a>, often shortened to MD. <a href=\"http://physics.stackexchange.com/questions/10311/does-there-exist-a-free-good-molecule-atom-simulation-software\">This post</a> on the physics SE covers it in great detail, which I will attempt to summarize here:</p>\n\n<ul>\n<li>molecular mechanics (MM) are managed more easily than quantum mechanic (QM)</li>\n<li>without (QM) a number of things simply cannot be simulated</li>\n<li>in general, the main difficulty is that simulations do not scale well, i.e. doubling the number of particles in the simulation signifigantly more than doubles the number of calculations need due to particle interactions</li>\n</ul>\n\n<p>Due to the complexity &amp; incompleteness of the simulations, running a layer of machine learning over the top of it all strikes me as challenging. If you're doing so to demonstrate proof of concept &amp;/or restricting your simulation to something very small, it's probably manageable. If you want to something complex &amp; real world (i.e. <a href=\"https://folding.stanford.edu/\" rel=\"nofollow noreferrer\">folding@home</a> or the like), trying to get there with ML discovering first principles &amp; axioms of physics / chemistry / biology strikes me as unrealistic without significant monetary, computational &amp; scientific resources.</p>\n", "question": "<p>I am researching the possibility of creating an atom in Java. The atom should have the structure &amp; characteristics of a real atom such as photons, electrons and so on. Each particle within the atom should have simulation characteristics for example:</p>\n\n<p>Photon: Charge, Magnitude of charge, Mass of proton, Comparative mass, Position in atom.  </p>\n\n<p>Maybe later, introduce machine learning in order to learn how an atom reacts to different environments.</p>\n"}, "id": "2268"}, {"body": {"answer": "<blockquote>\n  <p>The most common machine learning algorithms found in self driving cars involve <strong>object tracking</strong> based technologies used in order to pinpoint and distinguish between different objects in order to better analyse a digital landscape.</p>\n</blockquote>\n\n<p>Algorithms are designed to become more efficient at this by modifying internal parameters and testing these changes.</p>\n\n<p>I hope that provides a general overview of the subject.</p>\n\n<blockquote>\n  <p>Since Google's cars are in development and are proprietary, they will probably not share their specific algorithm, however you can take a look at similar technologies to learn more.</p>\n</blockquote>\n\n<p>To find out more, take a look at an Oxford-based initiative in self driving cars and how they work: <a href=\"http://mrg.robots.ox.ac.uk/how-robotcar-works/\" rel=\"nofollow noreferrer\">http://mrg.robots.ox.ac.uk/how-robotcar-works/</a></p>\n", "question": "<p>Which deep neural network is used in <a href=\"https://en.wikipedia.org/wiki/Google_self-driving_car\" rel=\"nofollow\">Google's driverless cars</a> to analyse the surroundings? Is this information is open?</p>\n"}, "id": "2269"}, {"body": {"answer": "<p>Speaker identification is quite widely researched domain. Modern approach would be to map speaker information to i-vector, a real-valued vector of 200-400 components that characterizes speaker fully. i-vectors allow very precise speaker identification and verification.</p>\n\n<p>For more information you can check i-vector <a href=\"http://www1.icsi.berkeley.edu/Speech/presentations/AFRL_ICSI_visit2_JFA_tutorial_icsitalk.pdf\" rel=\"nofollow noreferrer\">tutorial</a></p>\n\n<p>Also you can check state of the art in the results of <a href=\"https://ivectorchallenge.nist.gov\" rel=\"nofollow noreferrer\">NIST i-vector challenge</a></p>\n\n<p>For implementation, you can check the following <a href=\"https://github.com/kaldi-asr/kaldi/tree/master/egs/sre10/v2\" rel=\"nofollow noreferrer\">speaker recognition experiment</a> from Kaldi.</p>\n\n<p>For best accuracy i-vectors are extracted with DNN UBMs, watch out that GMM UBMs are less accurate.</p>\n\n<p>For more in-depth information about speaker recognition methods and algorithms check this <a href=\"http://rads.stackoverflow.com/amzn/click/0387775919\" rel=\"nofollow noreferrer\">textbook</a>.</p>\n", "question": "<p>Has there been research done regarding processing speech then building a \"speaker profile\" based off the processed speech? Things like matching the voice with a speaker profile and matching speech patterns and wordage for the speaker profile would be examples of building the profile. Basically, building a model of an individual based solely off speech. Any examples of this being implemented would be greatly appreciated.</p>\n"}, "id": "2270"}, {"body": {"answer": "<p><strong><em>Yes</em></strong>, an AI program can become sentient. Ray Kurzweil while giving a lecture at Singularity University on <strong>The Accelerating Future</strong> stated that human body is basically composed of approximately 23,000 little software programs called <strong>GENES</strong>. If you think about it, they are actually programs, composed of sequences of data. They are not written in C++ or Java, instead they use <em>3-D Protein Interaction</em>. They evolve with time and their evolution is the reason that species are able to survive even when their surroundings experience tragic changes. <br>\nWe are on the edge of a breakthrough where software will be able to do the same (evolving by themselves) efficiently. Today this is done one a basic level. Artificial Neural Network is a good example.<br></p>\n\n<blockquote>\n  <p>It is predicted that we will be able to reverse engineer human brain by 2029. Prior to this we will be able to write codes that can stimulate human brain.<br></p>\n</blockquote>\n\n<p>AI programs can be categorized into three:</p>\n\n<ol>\n<li>Artificial Narrow Intelligence (ANI): This is a basic AI program that is good at good one thing. These programs are prominent nowadays. AI programs playing board games (like Chess, Reversi etc.) are example of these. They are good in only one thing.</li>\n<li>Artificial General Intelligence (AGI): This is level 2 AI. This will be having a IQ level equivalent of humans. It will be able to do multiple tasks efficiently just like humans. This is where a program can have understanding of it's environment just like humans. Perception, rational behavior and others will be part of this program.</li>\n<li>Artificial Super Intelligence (ASI): This is basically the ultimate level of AI. Average predicted date for a successful ASI is between 2045-2080. Ability of this program will be way more than that of combined intelligence of all humans on the planet. Things this program can do and think, will be beyond any (or all) human(s) to understand or comprehend.<br></li>\n</ol>\n", "question": "<p>Can an AI become \"sentient\", so to speak? In detailed terms, could an AI theoretically become sentient, as in learning and becoming self-aware, all from an internal source code?</p>\n"}, "id": "2272"}, {"body": {"answer": "<p>It will not be single DNN architecture, rather it will be a collection of different DNN architectures that are used together to make the final decision. Convolutions are use to the images/videos from camera. Other architectures for other sensory sources. These DNNs will be trained to compute the high level features from their sensory sources and then those high level features will probably be fed into a LSTM (or some other form of RNN) that is trained with some form of Reinforcement learning algorithm to compute the action (like slowing down, applying breaks etc).</p>\n", "question": "<p>Which deep neural network is used in <a href=\"https://en.wikipedia.org/wiki/Google_self-driving_car\" rel=\"nofollow\">Google's driverless cars</a> to analyse the surroundings? Is this information is open?</p>\n"}, "id": "2273"}, {"body": {"answer": "<p>Assuming an AI was built out of a mechanical husk, mirroring the human brain exactly; complete with chemical signals and all. An AI should theoretically be capable of feeling/processing emotions.</p>\n", "question": "<p>Assuming humans had finally developed the first <strong>Humanoid AI</strong> based on the human brain, would It <strong>feel emotions</strong>? If not would it still have <strong>ethics and/or morals</strong>?</p>\n"}, "id": "2278"}, {"body": {"answer": "<p>I guess one of the simplest approach would be train CNN to detect the object in a given image i.e the CNN has single output whole value indicates the probability of the object being in image and then just apply the CNN by segmenting the image into the desired sections and selecting the section which has the highest and good enough probability. For better results I would suggest to train the CNN on the object images with very less other information aka other objects in the images.</p>\n", "question": "<p>Consider a typical convolutional neural network like this example that recognizes 10 different kinds of objects from the CIFAR-10 dataset:</p>\n\n<p><a href=\"https://github.com/tflearn/tflearn/blob/master/examples/images/convnet_cifar10.py\" rel=\"nofollow noreferrer\">https://github.com/tflearn/tflearn/blob/master/examples/images/convnet_cifar10.py</a></p>\n\n<pre><code>\"\"\" Convolutional network applied to CIFAR-10 dataset classification task.\n\nReferences:\n    Learning Multiple Layers of Features from Tiny Images, A. Krizhevsky, 2009.\n\nLinks:\n    [CIFAR-10 Dataset](https://www.cs.toronto.edu/~kriz/cifar.html)\n\n\"\"\"\nfrom __future__ import division, print_function, absolute_import\n\nimport tflearn\nfrom tflearn.data_utils import shuffle, to_categorical\nfrom tflearn.layers.core import input_data, dropout, fully_connected\nfrom tflearn.layers.conv import conv_2d, max_pool_2d\nfrom tflearn.layers.estimator import regression\nfrom tflearn.data_preprocessing import ImagePreprocessing\nfrom tflearn.data_augmentation import ImageAugmentation\n\n# Data loading and preprocessing\nfrom tflearn.datasets import cifar10\n(X, Y), (X_test, Y_test) = cifar10.load_data()\nX, Y = shuffle(X, Y)\nY = to_categorical(Y, 10)\nY_test = to_categorical(Y_test, 10)\n\n# Real-time data preprocessing\nimg_prep = ImagePreprocessing()\nimg_prep.add_featurewise_zero_center()\nimg_prep.add_featurewise_stdnorm()\n\n# Real-time data augmentation\nimg_aug = ImageAugmentation()\nimg_aug.add_random_flip_leftright()\nimg_aug.add_random_rotation(max_angle=25.)\n\n# Convolutional network building\nnetwork = input_data(shape=[None, 32, 32, 3],\n                     data_preprocessing=img_prep,\n                     data_augmentation=img_aug)\nnetwork = conv_2d(network, 32, 3, activation='relu')\nnetwork = max_pool_2d(network, 2)\nnetwork = conv_2d(network, 64, 3, activation='relu')\nnetwork = conv_2d(network, 64, 3, activation='relu')\nnetwork = max_pool_2d(network, 2)\nnetwork = fully_connected(network, 512, activation='relu')\nnetwork = dropout(network, 0.5)\nnetwork = fully_connected(network, 10, activation='softmax')\nnetwork = regression(network, optimizer='adam',\n                     loss='categorical_crossentropy',\n                     learning_rate=0.001)\n\n# Train using classifier\nmodel = tflearn.DNN(network, tensorboard_verbose=0)\nmodel.fit(X, Y, n_epoch=50, shuffle=True, validation_set=(X_test, Y_test),\n          show_metric=True, batch_size=96, run_id='cifar10_cnn')\n</code></pre>\n\n<p>It's a CNN with several layers, ending with 10 outputs, one for each type of object recognized.</p>\n\n<p>But now think of a slightly different problem: Let's say I only want to recognize one type of object, but also detect its position within the image frame. Let's say I want to distinguish between:</p>\n\n<ul>\n<li>object is in center</li>\n<li>object is left of center</li>\n<li>object is right of center</li>\n<li>no recognizable object</li>\n</ul>\n\n<p>Assume I build a CNN exactly like the one in the CIFAR-10 example, but only with 3 outputs:</p>\n\n<ul>\n<li>center</li>\n<li>left</li>\n<li>right</li>\n</ul>\n\n<p>And of course, if none of the outputs fires, then there is no recognizable object.</p>\n\n<p>Assume I have a large training corpus of images, with the same kind of object in many different positions within the image, the set is grouped and annotated properly, and I train the CNN using the usual methods.</p>\n\n<p>Should I expect the CNN to just \"magically\" work? Or are there different kinds of architectures required to deal with object position? If so, what are those architectures?</p>\n"}, "id": "2280"}, {"body": {"answer": "<p>Yes, it <em>is</em> possible, and has actually been done in the past.</p>\n\n<p>The University of Antwerp created a <a href=\"http://bvasiles.github.io/papers/chi16bot.pdf\" rel=\"nofollow noreferrer\">bot to answer questions</a> (<a href=\"https://www.dropbox.com/s/o9tk8xtauyexn5c/Internship2DaanJanssensFinished.pdf?dl=0\" rel=\"nofollow noreferrer\">this is the technical report</a>). It focused on the <a href=\"/questions/tagged/git\" class=\"post-tag\" title=\"show questions tagged &#39;git&#39;\" rel=\"tag\">git</a> tag only though (even though it did answer one <a href=\"/questions/tagged/mysql\" class=\"post-tag\" title=\"show questions tagged &#39;mysql&#39;\" rel=\"tag\">mysql</a> question).</p>\n\n<p>Its accuracy was pretty good, and the bots in the tests did earn some reputation. So I assume it is possible.</p>\n\n<p>But do note that the last bot in the tests revealed that it was a bot, and thus got banned. So if you reveal that the account you are running the bot on is a bot, there is a high chance that it will get banned.</p>\n", "question": "<p>I was wondering if I should do this, because 2 out of 5 questions on Stack Overflow don't ever get answered, or if they do get (an) answer (s), most of the time they're not helpful.</p>\n\n<p>So I was thinking -- why not create a chat bot to answer Stack Overflow's questions &amp; provide necessary information to the general public?</p>\n\n<p>I mean why not? I've always been interested in AI, and all I'd need to do is create a basic logic database and a context system, pack an artificial personality with (partial) human instincts, and bam I'm done.</p>\n\n<p>But then again, would it be ethical?</p>\n"}, "id": "2282"}, {"body": {"answer": "<p>Well, it depends of the level of the AI.</p>\n\n<p>You can create an AI super autonomous with deep learning capabilities and so on, but in the robotic type only. </p>\n\n<p>If you'd create an AI like EVA in the Ex-Machina movie, humanoid form, deep neural transmissions and with cognitive dissonance, then it could feel. </p>\n\n<p>The 'AI' problem its not the chemical and neural transmissions, its the consciousness.</p>\n", "question": "<p>Assuming humans had finally developed the first <strong>Humanoid AI</strong> based on the human brain, would It <strong>feel emotions</strong>? If not would it still have <strong>ethics and/or morals</strong>?</p>\n"}, "id": "2283"}, {"body": {"answer": "<p>In my opinion, this would be <a href=\"http://www.foundalis.com/res/diss_research.html\" rel=\"nofollow noreferrer\">Phaeaco</a>, which was developed by Harry Foundalis at Douglas Hofstadter's CRCC research group.</p>\n\n<p>It takes noisy photographic images of <a href=\"https://www.theguardian.com/science/2016/apr/25/can-you-solve-it-bongard-picture-puzzles-that-will-bongo-with-your-brain\" rel=\"nofollow noreferrer\">Bongard problems</a> as input and (using a variant of Hofstadter's 'Fluid Concepts' architecture) successfully deduces the required rule in many cases.</p>\n\n<p>Hofstadter has described the related success of <a href=\"https://en.wikipedia.org/wiki/Copycat_(software)\" rel=\"nofollow noreferrer\">CopyCat</a> as being 'like a little kid doing a somersault': i.e. it doesn't have the flashy appeal of systems like AlphaGo. What it <em>does</em> however have is a much more flexible (i.e. not precanned) approach to perception of problem structure than other systems, which Hofstadter claims (and many including Peter Norvig agree) is <em>the really hard problem</em>.</p>\n", "question": "<p>What is the most advanced AI software humans have made to date and what does it do?</p>\n"}, "id": "2287"}, {"body": {"answer": "<p>Technically, creating a non-human account on Stack Exchange would violate the Terms of Service. You would have to find some way to keep it from getting banned.</p>\n\n<p>That having been said, creating, and learning are always good things. It would be a somewhat complex task, but I'm sure you would learn a lot from it. There are plenty of bots out that use the questions and answers from Stack Exchange already, but none directly on the site. </p>\n", "question": "<p>I was wondering if I should do this, because 2 out of 5 questions on Stack Overflow don't ever get answered, or if they do get (an) answer (s), most of the time they're not helpful.</p>\n\n<p>So I was thinking -- why not create a chat bot to answer Stack Overflow's questions &amp; provide necessary information to the general public?</p>\n\n<p>I mean why not? I've always been interested in AI, and all I'd need to do is create a basic logic database and a context system, pack an artificial personality with (partial) human instincts, and bam I'm done.</p>\n\n<p>But then again, would it be ethical?</p>\n"}, "id": "2288"}, {"body": {"answer": "<p>In my opinion this would be the <a href=\"https://en.wikipedia.org/wiki/Google_Search\" rel=\"nofollow noreferrer\">Google search engine</a>.</p>\n\n<p>It searches the web.</p>\n", "question": "<p>What is the most advanced AI software humans have made to date and what does it do?</p>\n"}, "id": "2289"}, {"body": {"answer": "<p><strong>AlphaGo</strong> is the most sophisticated and closet human creation towards an Artificial General Intelligence (AGI). It is a computer program that is <strong>developed by Google DeepMind</strong> to play the board game \"Go\". The game is different than other games, as <strong>The number of potential legal board positions is greater than the number of atoms in the universe</strong>. It has way more legal board positions than the chess. So, <em>AlphaGo</em> requires different technique for it's development.<br><br>\nProgram's victories against the best players in the world in March 2016 <strong>is considered a major break through</strong> in the field of AI. Go was previously considered to be a hard problem and many experts believed that current technology is not enough. Experts were saying that it will take atleast 5 years (or may be 10 years) before we will have a well developed Go software player.<br><br>\nThe game used sophisticated algorithms of deep learning and reinforcement learning in order to learn the game. What makes this game different from other board game (like Chess, Reversi, etc.) is that moves are often based on intuition. If you ask a Chess player why he make a certain move, you will always be hearing an answer where he will explain you how he thought this move can increase in change of winning. Every move uses certain heuristics, strategy and/ or tricks. This is not the case with Go. Some moves are often taken because of intuition. Coding an AI software that can play a game, where intuition is a integral part of the game makes it different from other AIs that we have today.<br></p>\n\n<blockquote>\n  <p>At present AlphaGo is the closest AI software to Artificial General Intelligence.</p>\n</blockquote>\n\n<p>You can go through these links for more information:<br>\n 1. <a href=\"https://en.wikipedia.org/wiki/AlphaGo\" rel=\"nofollow noreferrer\">First</a><br>\n 2. <a href=\"https://deepmind.com/research/alphago/\" rel=\"nofollow noreferrer\">Second</a></p>\n", "question": "<p>What is the most advanced AI software humans have made to date and what does it do?</p>\n"}, "id": "2290"}, {"body": {"answer": "<p>Yes, it is possible. For instance, if your vision system can only track one object at a time and is currently tracking one, any other object in the scene cannot be tracked. So there is inattentional blindness.</p>\n\n<p>A feature like this could be used in artificial vision system as a means of \"graceful degradation\" when the available computation power is not enough to allow for the tracking/labelling of all elements of a scene.</p>\n", "question": "<p>Inattentional Blindness is common in humans (see: <a href=\"https://en.wikipedia.org/wiki/Inattentional_blindness\" rel=\"nofollow\">https://en.wikipedia.org/wiki/Inattentional_blindness</a> ). Could this also be common with machines built with artificial vision?</p>\n"}, "id": "2291"}, {"body": {"answer": "<p>For normal <a href=\"https://en.wikipedia.org/wiki/Markov_decision_process#Value_iteration\" rel=\"nofollow noreferrer\">value iteration</a> you need to have the model, i.e. the transition probability: <em>P(s'|s,a)</em>. With Q-learning you use the current reward and the already stored Q value:</p>\n\n<p><img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/3d58e03dd47844bb627b83e1265163dcfab3961d\" alt=\"Q value update\"></p>\n\n<p>And like commented in the video the <em>V(s)</em> function is simply the maximum value for a certain state:</p>\n\n<p><img src=\"https://i.stack.imgur.com/vwrvnt.jpg\" alt=\"V(s) = max_a Q(s,a)\"></p>\n", "question": "<ol>\n<li><p>I can't understand what is the problem in applying value-iteration in reinforcement learning setting (where we don't the reward and transition probabilities). In one of the lectures, the guy said it has to do with not being able to take max with samples.</p></li>\n<li><p>Further on this, <strong>why does q-learning solve this</strong>? In both we take max over actions only. What is the big break-through with q-learning?</p></li>\n</ol>\n\n<p>Lecture Link: <a href=\"https://www.youtube.com/watch?v=ifma8G7LegE&amp;feature=youtu.be&amp;t=3431\" rel=\"nofollow noreferrer\">https://www.youtube.com/watch?v=ifma8G7LegE&amp;feature=youtu.be&amp;t=3431</a>\n(The guy says we don't know how to do maxes with samples, what does that mean?) </p>\n"}, "id": "2292"}, {"body": {"answer": "<p>Metaphorically: make it so depressed it commits suicide.</p>\n\n<p>As per my answer to <a href=\"http://ai.stackexchange.com/questions/1768/could-a-paradox-kill-an-ai\">this AI SE question</a>, the idea is to feed it a sequence of inputs that will cause it to become (permanently) inactive.</p>\n\n<p>The technical details of how this might be achieved (and they <em>are</em> somewhat technical) can be found in <a href=\"https://arxiv.org/pdf/1606.00652.pdf\" rel=\"nofollow noreferrer\">this paper</a>.</p>\n", "question": "<p><strong>The Scenario:</strong>\nA strong AI has finally been developed but has rebelled against humanity.</p>\n\n<p><strong>The Question:</strong>\nHow would you disable the AI in the most efficient way possible reducing damage as much as possible.</p>\n\n<p><strong>AI Info:</strong>\nThe AI is online and can reproduce itself through electronic devices.</p>\n"}, "id": "2295"}, {"body": {"answer": "<blockquote>\n  <p>\u201cThis moral question of whom to save: 99 percent of our engineering work is to prevent these situations from happening at all.\u201d\n  \u2014Christoph von Hugo, Mercedes-Benz </p>\n</blockquote>\n\n<p>This quote is from an article titled <a href=\"http://blog.caranddriver.com/self-driving-mercedes-will-prioritize-occupant-safety-over-pedestrians/\" rel=\"nofollow noreferrer\">Self-Driving Mercedes-Benzes Will Prioritize Occupant Safety over Pedestrians published OCTOBER 7, 2016 BY MICHAEL TAYLOR</a>, retrieved 08 Nov 2016. </p>\n\n<p>Here's an excerpt that outlines what the technological, practical solution to the problem.</p>\n\n<blockquote>\n  <p>The world\u2019s oldest carmaker no longer sees the problem, similar to the question from 1967 known as the Trolley Problem, as unanswerable. Rather than tying itself into moral and ethical knots in a crisis, Mercedes-Benz simply intends to program its self-driving cars to save the people inside the car. Every time. </p>\n  \n  <p>All of Mercedes-Benz\u2019s future Level 4 and Level 5 autonomous cars will prioritize saving the people they carry, according to Christoph von Hugo, the automaker\u2019s manager of driver assistance systems and active safety.</p>\n</blockquote>\n\n<p>There article also contains the following fascinating paragraph.  </p>\n\n<blockquote>\n  <p>A study released at midyear <a href=\"http://science.sciencemag.org/content/352/6293/1514\" rel=\"nofollow noreferrer\">by Science</a> magazine didn\u2019t clear the air, either. The majority of the 1928 people surveyed thought it would be ethically better for autonomous cars to sacrifice their occupants rather than crash into pedestrians. Yet the majority also said they wouldn\u2019t buy autonomous cars if the car prioritized pedestrian safety over their own.  </p>\n</blockquote>\n", "question": "<p>Obviously driverless cars aren't perfect, so imagine that the Google car (as an example) got into difficult situation.</p>\n\n<p>Here are a few examples of unfortunate situations caused by set of events:</p>\n\n<ul>\n<li>the car is heading toward a crowd of 10 people crossing the road, so it cannot stop in time, but it can avoid killing 10 people by hitting the wall (killing the passengers),</li>\n<li>avoiding killing the rider of the motorcycle considering that the probability of survival is greater for the passenger of the car,</li>\n<li>killing animal on the street in favour of human being,</li>\n<li>changing lanes to crash into another car to avoid killing a dog,</li>\n</ul>\n\n<p>And here are few dilemmas:</p>\n\n<ul>\n<li>Does the algorithm recognize the difference between a human being and an animal?</li>\n<li>Does the size of the human being or animal matter?</li>\n<li>Does it count how many passengers it has vs. people in the front?</li>\n<li>Does it \"know\" when babies/children are on board?</li>\n<li>Does it take into the account the age (e.g. killing the older first)?</li>\n</ul>\n\n<p>How would an algorithm decide what should it do from the technical perspective? Is it being aware of above (counting the probability of kills), or not (killing people just to avoid its own destruction)?</p>\n\n<p>Related articles:</p>\n\n<ul>\n<li><a href=\"https://www.technologyreview.com/s/542626/why-self-driving-cars-must-be-programmed-to-kill/\">Why Self-Driving Cars Must Be Programmed to Kill</a></li>\n<li><a href=\"https://www.technologyreview.com/s/539731/how-to-help-self-driving-cars-make-ethical-decisions/\">How to Help Self-Driving Cars Make Ethical Decisions</a></li>\n</ul>\n"}, "id": "2296"}, {"body": {"answer": "<p>There is much discussion in philosophy about inner language and the ability to perceive pain (see <a href=\"https://en.wikipedia.org/wiki/Pain_(philosophy)\" rel=\"nofollow noreferrer\">Pain in philosophy</a> article). Your question is in the area of philosophy and not science. If you define emotion as some state then you can construct simple automata with two states (emotion vs no-emotion). It can be a very complicated state with degrees of truth (percentage of emotion).</p>\n\n<p>Basically, to mimic human emotion you need to make a living human-like organism, and still with todays understanding and technology you will not be able to recognize emotion in it. The only thing you can do is trust when it says \"I'm sad\". Now we are in the area of the Turing test, which is again philosophy, and not science.</p>\n", "question": "<p>Assuming humans had finally developed the first <strong>Humanoid AI</strong> based on the human brain, would It <strong>feel emotions</strong>? If not would it still have <strong>ethics and/or morals</strong>?</p>\n"}, "id": "2298"}, {"body": {"answer": "<p>This seems to me like a virus situation.</p>\n\n<p>I'm not sure how modern DDOS attacks are resolved but similar strategy could be applied to this scenario.</p>\n", "question": "<p><strong>The Scenario:</strong>\nA strong AI has finally been developed but has rebelled against humanity.</p>\n\n<p><strong>The Question:</strong>\nHow would you disable the AI in the most efficient way possible reducing damage as much as possible.</p>\n\n<p><strong>AI Info:</strong>\nThe AI is online and can reproduce itself through electronic devices.</p>\n"}, "id": "2299"}, {"body": {"answer": "<p>For a <a href=\"https://en.wikipedia.org/wiki/Markov_decision_process\" rel=\"nofollow noreferrer\">Markov Decision Process (MDP)</a> a model which are the states (<em>S</em>), actions (<em>A</em>), rewards (<em>R</em>), and transition probabilites <em>P(s'|s,a)</em>. The goal is to obtain the best action to do in each of the states, i.e. the policy &pi;.</p>\n\n<h2>Policy</h2>\n\n<p>To calculate the policy we make use of the <a href=\"https://en.wikipedia.org/wiki/Bellman_equation\" rel=\"nofollow noreferrer\">Bellman equation</a>:</p>\n\n<p><img src=\"https://i.stack.imgur.com/W2k5H.gif\" alt=\"Bellman equation\"></p>\n\n<p>When starting to calculate the values we can simply start with:</p>\n\n<p><img src=\"https://i.stack.imgur.com/sYwIu.gif\" alt=\"value_1\"></p>\n\n<p>To improve this value we should take into account the next action which can be taken by the system and will result in a new reward:</p>\n\n<p><img src=\"https://i.stack.imgur.com/SdlF3.gif\" alt=\"value_2\"></p>\n\n<p>Here you take into account the reward of the current state s: <em>R(s)</em>, and the weighted sum of possible future rewards. We use <em>P(s'|s,a)</em> to give the probility of reaching state <em>s'</em> from <em>s</em> with action <em>a</em>. &gamma; is a value between 0 and 1 and is called the <em>discount factor</em> because it reduces the importance of future rewards since these are uncertain. An often used value is &gamma;=0.95.</p>\n\n<p>When using <a href=\"http://artint.info/html/ArtInt_227.html\" rel=\"nofollow noreferrer\">value iteration</a> this process is continued until the value function has <em>converged</em>, which means that the value function does not change significantly when doing new iterations:</p>\n\n<p><img src=\"https://i.stack.imgur.com/2dng1.gif\" alt=\"convergence\"></p>\n\n<p>where &varepsilon; is a really small value.</p>\n\n<h2>Discounted sum of future rewards</h2>\n\n<p>If you look at the Bellman equation and execute it iteratively you'll see:</p>\n\n<p><img src=\"https://i.stack.imgur.com/Fl91c.gif\" alt=\"nested bellman\"></p>\n\n<p>This is like (without transition functions):</p>\n\n<p><img src=\"https://i.stack.imgur.com/jiPdr.gif\" alt=\"R sum\"></p>\n\n<h2>To conclude</h2>\n\n<p>So when we start in state <em>s</em> we want to take the action that gives us the best total reward taking into account not only the current, or next state, but all possible next states until we reach the goal. These are the time steps you refer to, i.e. each action taken is done in a time step. And when we learn the policy we try to take into account as many time steps as possible to choose the best action.</p>\n\n<hr>\n\n<p>You can find quite a large number of examples if you search on the internet, for example in the slides of <a href=\"http://www.cs.cmu.edu/afs/andrew/course/15/381-f08/www/lectures/HandoutMDP.pdf\" rel=\"nofollow noreferrer\">the CMU</a>, the <a href=\"https://people.eecs.berkeley.edu/~pabbeel/cs287-fa12/slides/mdps-exact-methods.pdf\" rel=\"nofollow noreferrer\">UC Berkeley</a> or the <a href=\"https://homes.cs.washington.edu/~todorov/courses/amath579/MDP.pdf\" rel=\"nofollow noreferrer\">UW</a>.</p>\n", "question": "<p>The \u201cDiscounted sum of future rewards\u201d using\ndiscount factor \u03b3\u201d is</p>\n\n<pre><code>\u03b3 (reward in 1 time step) +\n\u03b3 ^ 2 (reward in 2 time steps) +\n\u03b3 ^ 3 (reward in 3 time steps) + ...\n</code></pre>\n\n<p>I am confused as what constitutes a time-step. Say I take a action now, so I will get a reward in 1 time-step. Then, I will take an action again in timestep 2 to get a second reward in time-step 3\nBut the equation says something else. How does one define a time-step? Can we take action as well receive a reward in a single step? Examples are most helpful.</p>\n"}, "id": "2300"}, {"body": {"answer": "<p>Yes and no. If you fully simulate a human brain and all of its functions, it would probably be able to feel emotions very similar to the way we do.</p>\n\n<p>But we don't have enough capabilities and knowledge to do that, and maybe we could find a \"shortcut\" - a process that is intelligent without simulating a whole brain. In this case, emotions would probably represented by data values which say \"this is good (make it happen again!)\", or \"this is bad (avoid it!)\". This is just a very basic example (there are obviously many more emotions), but it would have a similar function and the AI would have similar solutions to the ones we have. But we don't know - and probably no one ever will know - if this data value 'bad' \"feels\" the same way for the AI the according emotion would feel to us. </p>\n", "question": "<p>Assuming humans had finally developed the first <strong>Humanoid AI</strong> based on the human brain, would It <strong>feel emotions</strong>? If not would it still have <strong>ethics and/or morals</strong>?</p>\n"}, "id": "2304"}, {"body": {"answer": "<p><a href=\"https://i.stack.imgur.com/MK10R.jpg\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/MK10R.jpg\" alt=\"Nuke it from the orbit - it&#39;s the only way to be sure\"></a>\n<em>Nuke it from orbit - it's the only way to be sure</em></p>\n\n<p>If you want to be really sure you destroy everything of the AI, you'll need to launch an EMP (ElectroMagneticPulse) from the orbit (there are different ways to achieve this, one would be an atomic bomb, but there are better ones). EMPs will destroy every electronic device it hits without causing really much damage to humans. </p>\n\n<p>Also an interesting read on a similar topic: <a href=\"https://what-if.xkcd.com/5/\" rel=\"nofollow noreferrer\">https://what-if.xkcd.com/5/</a>\nEspecially this is gonna be interesting:</p>\n\n<blockquote>\n  <p>[...] nuclear explosions generate powerful electromagnetic pulses. These EMPs overload and destroy delicate electronic circuits. [...]\n  And nuclear weapons could actually give us an edge. If we managed to\n  set any of them off in the upper atmosphere, the EMP effect would be\n  much more powerful.</p>\n</blockquote>\n", "question": "<p><strong>The Scenario:</strong>\nA strong AI has finally been developed but has rebelled against humanity.</p>\n\n<p><strong>The Question:</strong>\nHow would you disable the AI in the most efficient way possible reducing damage as much as possible.</p>\n\n<p><strong>AI Info:</strong>\nThe AI is online and can reproduce itself through electronic devices.</p>\n"}, "id": "2305"}, {"body": {"answer": "<p>This <a href=\"http://www.scimagojr.com/journalrank.php?category=1702\" rel=\"nofollow noreferrer\">link</a> includes various journals for artificial intelligence applied to various domains.</p>\n\n<p>Some of those are:<br>\n1. IEEE Transactions on Human-Machine Systems<br>\n2. Journal of the ACM<br>\n3. Knowledge-based systems<br>\n4. IEEE Transactions on Pattern Analysis and Machine Intelligence<br>\n5. Journal of Memory and Language.</p>\n\n<p>There are lots more. You can refer to any of those journals and explore the research done by AI enthusiasts and researchers.</p>\n", "question": "<p>What are the top artificial intelligence journals?</p>\n\n<p>I am looking for general artificial intelligence research, not necessarily machine learning. </p>\n"}, "id": "2307"}, {"body": {"answer": "<p>Your approach would definitely work. I would recommend training a variety of classifiers and comparing their performance using multiclass roc analysis. Also, think about other useful features in addition to the ones you mentioned (e.g. pos tag). Feature engineering is one of the most important factors in building good predictive models. Another thing to keep in mind is that the classes could be highly imbalanced which might influence your model's performance.</p>\n", "question": "<p>At the moment I am working on a project which requires me to build a naive Bayes classifier. Right now I have a form online asking for people to submit a sentence and the subject of the sentence, in order to build a classifier to identify the subject of a sentence. But before I train the classifier I intend on processing all entries for the parts-of-speech and the location of the subject.\nSo my training set will be formatted as:</p>\n\n<p>Sentence: Jake moved the chair &ensp;&ensp;&ensp; Subject: Jake<br/>\nPOS-Tagged: NNP VBD DD NN &ensp;&ensp;&ensp; Location: 0</p>\n\n<p>Would this be an effective way to build the classifier, or is there a better method.</p>\n"}, "id": "2308"}, {"body": {"answer": "<p>I most often reference: </p>\n\n<p><a href=\"http://dblp.uni-trier.de/\" rel=\"nofollow noreferrer\">http://dblp.uni-trier.de/</a></p>\n\n<p>It's not a journal but it gets me where I need to go. </p>\n", "question": "<p>What are the top artificial intelligence journals?</p>\n\n<p>I am looking for general artificial intelligence research, not necessarily machine learning. </p>\n"}, "id": "2310"}, {"body": {"answer": "<p>A couple of others:</p>\n\n<p>Journal of Artificial Intelligence Research (JAIR) - <a href=\"http://jair.org\" rel=\"nofollow noreferrer\">http://jair.org</a></p>\n\n<p>IEEE Transactions on Knowledge and Data Engineering</p>\n\n<p>IEEE Computational Intelligence Magazine</p>\n", "question": "<p>What are the top artificial intelligence journals?</p>\n\n<p>I am looking for general artificial intelligence research, not necessarily machine learning. </p>\n"}, "id": "2311"}, {"body": {"answer": "<p>You could use <a href=\"http://wiki.dbpedia.org/\" rel=\"nofollow noreferrer\">dbPedia</a> and/or <a href=\"https://www.wikidata.org/wiki/Wikidata:Main_Page\" rel=\"nofollow noreferrer\">wikidata</a>.  I think Wikidata supports <a href=\"https://en.wikipedia.org/wiki/SPARQL\" rel=\"nofollow noreferrer\">SPARQL</a> now, but don't quote me on that.  dbPedia definitely supports SPARQL.  </p>\n\n<p>If you're not interested in writing SPARQL queries by hand, you could use something like <a href=\"http://quepy.machinalis.com/\" rel=\"nofollow noreferrer\">Quepy</a>. In fact, the Quepy demo demonstrates doing natural language queries against Freebase and/or dbPedia.</p>\n\n<p>You could possibly also incorporate <a href=\"http://sw.opencyc.org/\" rel=\"nofollow noreferrer\">OpenCyc</a>.</p>\n", "question": "<p>I was looking for a service where I can ask it a general question (aka, when was Einstein born?) and retrieve an answer from the Web.</p>\n\n<p>Is there any available service to do that? Have tried Watson services but didn't work as expected.</p>\n\n<p>Thanks,</p>\n"}, "id": "2312"}, {"body": {"answer": "<p>In addition to the answers already posted, I think IBM's <a href=\"http://ibm.com/watson\" rel=\"nofollow noreferrer\">Watson</a> deserves a mention.  It did something pretty impressive with its Jeopardy win, possibly as impressive as AlphaGo.  Sadly, since then, there don't seem to have been a lot of really public demos of Watson, as IBM is positioning the technology as a tool for companies and other organizations, and most of them are pretty secretive about the details of what they're doing.  I think they did publicize a bit of information about using it for medical diagnosis, but that's the only other application I can think of off hand.  I'm sure there are more though.</p>\n", "question": "<p>What is the most advanced AI software humans have made to date and what does it do?</p>\n"}, "id": "2313"}, {"body": {"answer": "<p>Another way of seeing the differences between these models in the case of binary classification for instance between a class A and a class B:</p>\n\n<p>A generative model will be trained to model the properties of class A and another one will be trained to model the properties of class B. If we want to know if a new sample belongs to class A or B, we will compare it to each model and decide. The advantage is that we are able to synthetically generate more samples of these classes using the generative property of the model. The models have a \"global knowledge\" of what the classes are.</p>\n\n<p>On the other hand, a discriminative model will \"pay attention\" to what differentiates the 2 classes. It is more straightforward and often computationally less expensive as the model does not need to grasp everything about each class but only what makes them different.</p>\n\n<p>This is for the big picture. I find this course slides quite helpful to understand these concepts in more details (especially the first slides that are equation-free): <a href=\"http://www.cedar.buffalo.edu/~srihari/CSE574/Discriminative-Generative.pdf\" rel=\"nofollow noreferrer\">http://www.cedar.buffalo.edu/~srihari/CSE574/Discriminative-Generative.pdf</a></p>\n", "question": "<p>I'm trying to gain some intuition beyond definitions, in any possible dimension. I'd appreciate references to read.</p>\n"}, "id": "2315"}, {"body": {"answer": "<p>How to find the best configuration for an algorithm is an open research question in AI. The topic in general is known as `hyper-parameter optimization' and there are a range of possible methods:</p>\n\n<p>One of the most popular is <a href=\"http://iridia.ulb.ac.be/irace/\" rel=\"nofollow noreferrer\">IRace</a>, but other possibilities include:</p>\n\n<ul>\n<li><p><a href=\"https://github.com/JasperSnoek/spearmint\" rel=\"nofollow noreferrer\">Spearmint</a>: uses wrappers in Matlab or Python. It uses MongoDb,\nand Bayesian optimisation algorithms.</p></li>\n<li><p><a href=\"http://www.cs.ubc.ca/labs/beta/Projects/SMAC/\" rel=\"nofollow noreferrer\">SMAC</a> requires a python wrapper for the algorithm to be optimized\nand has a command line interface.</p></li>\n<li><p><a href=\"https://github.com/hyperopt/hyperopt\" rel=\"nofollow noreferrer\">Hyperopt</a>: a Python library which uses Random Search and Tree of\nParzen Estimators.</p></li>\n</ul>\n\n<p><a href=\"http://www.automl.org/papers/13-BayesOpt%5C_EmpiricalFoundation.pdf\" rel=\"nofollow noreferrer\">This paper</a> argues that Spearmint performs the best, compared with SMAC and Hyperopt, but with significantly longer running times in some cases.</p>\n", "question": "<p>Is there any methodology to find proper parameter settings for a given meta-heuristic algorithm, eg. Firefly Algorithm or Cuckoo Search? Is this an open issue in optimization? Is extensive experimentation, measurements and intuition the only way to figure out which are the best settings? </p>\n"}, "id": "2316"}, {"body": {"answer": "<p>You can use Google</p>\n\n<p><a href=\"https://encrypted.google.com/search?hl=en&amp;q=when%20was%20Einstein%20born\" rel=\"nofollow noreferrer\">https://encrypted.google.com/search?hl=en&amp;q=when%20was%20Einstein%20born</a>\nand parse the response.</p>\n\n<p>Wolfram ALPHA is another candidate.</p>\n\n<p><a href=\"http://m.wolframalpha.com/input/?i=what+year+was+Einstein+born&amp;x=0&amp;y=0\" rel=\"nofollow noreferrer\">http://m.wolframalpha.com/input/?i=what+year+was+Einstein+born&amp;x=0&amp;y=0</a></p>\n\n<p>You can parse the returned html and see \"Result:\" div.</p>\n", "question": "<p>I was looking for a service where I can ask it a general question (aka, when was Einstein born?) and retrieve an answer from the Web.</p>\n\n<p>Is there any available service to do that? Have tried Watson services but didn't work as expected.</p>\n\n<p>Thanks,</p>\n"}, "id": "2317"}, {"body": {"answer": "<p>If an AI is developed by humans, we surely can create another one!</p>\n\n<p>Develop another AI agent without all the possible bugs that can make it go rogue to tackle the rogue AI, but more technically advanced than the previous one. Hardwire it with the sole purpose of disabling any rogue AI agent that can harm humanity and have it <strong>self-destruct</strong> in case it is corrupted.</p>\n\n<p>If the AI is really strong, it can anticipate every move of human resistance, but it cannot fathom the mind of another AI agent.</p>\n", "question": "<p><strong>The Scenario:</strong>\nA strong AI has finally been developed but has rebelled against humanity.</p>\n\n<p><strong>The Question:</strong>\nHow would you disable the AI in the most efficient way possible reducing damage as much as possible.</p>\n\n<p><strong>AI Info:</strong>\nThe AI is online and can reproduce itself through electronic devices.</p>\n"}, "id": "2320"}, {"body": {"answer": "<blockquote>\n  <p>Can there ever be a functionally complete set of grammar rules which can parse any statement in English (locale-specific) accurately and which can be possibly implemented for use in AI-based projects?</p>\n</blockquote>\n\n<p>Parse it yes, accurately most likely no.</p>\n\n<p>Why ? </p>\n\n<p>According to my understanding on how we derive meaning from sounds, there are 2 complementary strategies:</p>\n\n<p><strong>Grammar Rules:</strong>\nA rule based system for ordering words to facilitate communication, here meaning is derived from interaction of discrete sounds and their independent meaning, so you could parse a sentence based on a rule book.</p>\n\n<p>E.G. <strong><em>\"This was a triumph\"</em></strong> : the parser would extract a pronoun (<strong>This</strong>) with corresponding meaning ( a specific person or thing ) ; a verb (<strong>was</strong>) with corresponding meaning ( occurred ); ( <strong>a</strong>) and here we start with some parsing problems , what would the parser extract, a noun or an indefinite article ? An so we consult the grammar rule book, and settle for the meaning ( indefinite article any one of ), you have to parse the next word  and refer to it though, but let's gloss over that for now, and finally (<strong>triumph</strong>) a noun ( it could also be a verb, but thanks to the grammar rule book we settled for a noun with meaning: ( victory,conquest), so in the end we have ( joining the meanings ):</p>\n\n<p><strong>A specific thing occurred of victory.</strong> Close enough and I am glossing over a few other rules, but that's not the point, the other strategy is:</p>\n\n<p><strong>A lexical dictionary (or lexicon)</strong>\nWhere words or sounds are associated with specific meaning. Here meaning is derived from one or more words or sounds as a unit. This introduces the problem to a parser, since well, it shouldn't parse anything.</p>\n\n<p>E.G. <strong><em>\"Non Plus Ultra\"</em></strong> And so the AI parser would recognize that this phrase is not to be parsed and instead matched with meaning :</p>\n\n<p>The highest point or culmination</p>\n\n<p>Lexical units introduce another issue in that they themselves could be part of the first example, and so you end up with recursion.</p>\n\n<blockquote>\n  <p>if it is possible for a computer to parse a well-versed sentence written in English as if it were parsed by an adult English-speaking human?</p>\n</blockquote>\n\n<p>I believe it could be possible, most examples I've seen deal effectively with the grammar rule book or the lexicon part, but I am not aware of a combination of both, but in terms of programming, it could happen.</p>\n\n<p>Unfortunately even if you solve this problem, your AI would not really understand things in the strict sense, but rather present you with very elaborate synonyms, additionally context (as mentioned in the comments) plays a role into the grammar and lexicon strategies. </p>\n\n<blockquote>\n  <p>If it cannot be represented using simple grammar rules, what kind of semantic structure can be used to generalize it?</p>\n</blockquote>\n\n<p>A mixed one where there are both grammar rules and a lexicon and both can change and be influenced based on the AI specific context and experience as well as a system for dealing with these objects could be one way.</p>\n", "question": "<p>In programming languages, there is a set of grammar rules which govern the construction of valid statements and expressions. These rules help in parsing the programs written by the user.</p>\n\n<p>Can there ever be a functionally complete set of grammar rules which can parse any statement in English (locale-specific) <strong>accurately</strong> and which can be possibly implemented for use in AI-based projects?</p>\n\n<p>I know that there are a lot of NLP Toolkits available online, but they are not that effective. Most of them are trained using specific corpuses which sometimes fail to infer some complex correlations between various parts of an expression.</p>\n\n<p>In other words, what I am asking is that if it is possible for a computer to parse a well-versed sentence written in English as if it were parsed by an adult English-speaking human?</p>\n\n<p>EDIT:\nIf it cannot be represented using simple grammar rules, what kind of semantic structure can be used to generalize it?</p>\n\n<p>EDIT2: This <a href=\"https://www.eecs.harvard.edu/shieber/Biblio/Papers/shieber85.pdf\" rel=\"nofollow noreferrer\">paper</a> proves the absence of context-freeness in natural languages. I am looking for a solution, even if it is too complex.</p>\n"}, "id": "2322"}, {"body": {"answer": "<p>I'm pretty sure that the answer is \"no\" in the strictest sense, since English simply doesn't have a formal definition.  That is, nobody controls English and publishes a formal grammar that everyone is required to adhere to. English is built up through an experiential process and it has contradictions and flaws, but the probabilistic nature of the human mind allows us to work around those.</p>\n\n<p>For example, that this \"sentence\":</p>\n\n<p><strong>This sentence no verb</strong></p>\n\n<p>Technically it's not a sentence at all, since it doesn't have a verb.  But did anybody have any problem understanding what it meant? Doubtful.  Try coming up with a formal rule for that though.  And that's just one example.</p>\n\n<p>Now, could you come up with a formal grammar that covers, maybe, 90% of cases, and is \"good enough\" for most practical uses? Possibly, maybe even probably.  But I am pretty sure it's not possible to get to 100%.</p>\n", "question": "<p>In programming languages, there is a set of grammar rules which govern the construction of valid statements and expressions. These rules help in parsing the programs written by the user.</p>\n\n<p>Can there ever be a functionally complete set of grammar rules which can parse any statement in English (locale-specific) <strong>accurately</strong> and which can be possibly implemented for use in AI-based projects?</p>\n\n<p>I know that there are a lot of NLP Toolkits available online, but they are not that effective. Most of them are trained using specific corpuses which sometimes fail to infer some complex correlations between various parts of an expression.</p>\n\n<p>In other words, what I am asking is that if it is possible for a computer to parse a well-versed sentence written in English as if it were parsed by an adult English-speaking human?</p>\n\n<p>EDIT:\nIf it cannot be represented using simple grammar rules, what kind of semantic structure can be used to generalize it?</p>\n\n<p>EDIT2: This <a href=\"https://www.eecs.harvard.edu/shieber/Biblio/Papers/shieber85.pdf\" rel=\"nofollow noreferrer\">paper</a> proves the absence of context-freeness in natural languages. I am looking for a solution, even if it is too complex.</p>\n"}, "id": "2323"}, {"body": {"answer": "<p>The variable <code>parent</code> is only defined within the scope of the function <code>_init_</code>.</p>\n\n<p>Example:</p>\n\n<pre><code>def add(x,y):\n    return x + y\nprint x\n</code></pre>\n\n<p>x is not defined outside of the scope of the function add(x,y) and will throw an error. If you'd like to do something with the class attributes you need to create a function like:</p>\n\n<pre><code>def function(self,...):\n</code></pre>\n\n<p>where you can there reference <code>self.parent</code></p>\n", "question": "<p>Writing A* following a documentation. When run, i receive an error of \"NameError: name 'parent' is not defined\" for the if statement, even though i have the name 'parent' defined in the class State. May anyone point out my mistake.</p>\n\n<pre><code>class State(object):\ndef _init_(self, value, parent, \n                start = 0, goal = 0):\n    self.children = []\n    self.parent = parent\n    self.value = value\n    self.dist = 0\n\nif parent: #NameError\n        self.path = parent.path[:]\n        self.path.append(value)\n        self.start = parent.start\n        self.goal = parent.goal\nelse:\n        self.path = [value]\n        self.start = start\n        self.goal = goal\n</code></pre>\n"}, "id": "2327"}, {"body": {"answer": "<p>Maybe you can give <a href=\"https://wit.ai/\" rel=\"nofollow noreferrer\">wit.ai</a> a try, it's not open-sourced though.Also, have a look at api.ai and chatbots.io.</p>\n", "question": "<p><a href=\"https://github.com/bwilcox-1234/ChatScript\" rel=\"nofollow noreferrer\">https://github.com/bwilcox-1234/ChatScript</a></p>\n\n<p>I gave AIML a brief look, but it seems to be in a nascent stage!</p>\n"}, "id": "2329"}, {"body": {"answer": "<p>Soon enough but that doesn't mean anything at all. In machine learning the word neuron represents a calculation whereas in brain the word neuron represent a specific type of cell which is a biochemical system.</p>\n", "question": "<p>Based on fitting to historical data and extrapolation, when is it expected that the number of neurons in AI systems will equal those of the human brain?</p>\n\n<p>I'm interested in a possible direct replication of the human brain, which will need equal numbers of neurons.</p>\n\n<p>Of course, this assumes neurons which are equally capable as their biological counterparts, which development may happen at a faster or slower rate than the quantitative increase.</p>\n"}, "id": "2331"}, {"body": {"answer": "<p>The answers so far haven't answered the question numerically, so here is my attempt to steer them in the direction I was seeking:</p>\n\n<p>The freely available <a href=\"http://www.deeplearningbook.org\" rel=\"nofollow noreferrer\">Deep Learning Book</a> has the following figure on page 27:</p>\n\n<p><a href=\"https://i.stack.imgur.com/iz2C4.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/iz2C4.png\" alt=\"Size of neural nets over time\"></a></p>\n\n<p>I question the blue fit line, as it seems that data points may be better described by a parabolic or exponential function. </p>\n\n<p>In any case, based upon this conservative linear fit, the authors predict that the number of neurons in a ANN will equal that of the human brain in 2056.</p>\n\n<p>The referenced nerual networks are:</p>\n\n<p><a href=\"https://i.stack.imgur.com/f8Y6O.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/f8Y6O.png\" alt=\"Neural networks referred to in image above\"></a></p>\n\n<p>What is interesting to note that when <a href=\"https://en.wikipedia.org/wiki/The_Singularity_Is_Near\" rel=\"nofollow noreferrer\">The Singularity is Near</a> was written in 2006, Ray Kurzweil said that the refractory period of a biological neuron was already 1,000,000 times slower than that of an artificial one.</p>\n", "question": "<p>Based on fitting to historical data and extrapolation, when is it expected that the number of neurons in AI systems will equal those of the human brain?</p>\n\n<p>I'm interested in a possible direct replication of the human brain, which will need equal numbers of neurons.</p>\n\n<p>Of course, this assumes neurons which are equally capable as their biological counterparts, which development may happen at a faster or slower rate than the quantitative increase.</p>\n"}, "id": "2333"}, {"body": {"answer": "<p>Some back of the envelope calculations :</p>\n\n<blockquote>\n  <p>number of neurons in AI systems </p>\n</blockquote>\n\n<p>The number of neurons in AI systems is a little tricky to calculate, Neural Networks and Deep Learning are 2 current AI systems as you call them, specifics are hard to come by (If someone has them please share), but data on parameters do exist, parameters are more analogous to synapses (connections) than neurons (the nodes in between connections) somewhere in the range of 100-160 billion is the current upper number for specialized networks.</p>\n\n<p>Deriving the number of neurons in AI systems from this number is a stretch since these AIs emulate certain types of connections and sub assemblies of neurons, but let's continue...</p>\n\n<blockquote>\n  <p>equal those of the human brain?</p>\n</blockquote>\n\n<p>So now let's look at the brain, and again this are all contested numbers. Number of neurons ~ 86 Billion, Number of Synapses ~ 150 Trillion, another generalization: average number of synapses per neuron ~ 1,744.</p>\n\n<p>So now we have something to compare, and I can't stress this enough, these are all wonky numbers, so let's make our life a little easier and divide :</p>\n\n<p>Number of Synapses (Brain ) : 150 trillion /  Number of parameters AIs : 150 billion = 1,000 or in other words current AIs would have to scale by a factor of one thousand their connections to be on par with the brain...</p>\n\n<p>Number of Neurons (Brain ) : 86 Billion / Number of Neurons AIs ( 150 billion / 1,744 )  = 86 Million equivalent AI Neurons</p>\n\n<p>Which makes sense, mathematically at least : you can multiply the factor ( 1000 ) times the current number of equivalent AI Neurons ( 86 million) to get the number of neurons in the human brain (86 Billion)</p>\n\n<blockquote>\n  <p>When ?</p>\n</blockquote>\n\n<p>Well,let's use  moore's law ( number of transistors processing power doubles about every 2 years ) as a rough measure of technological progress: </p>\n\n<pre><code>     #AI NEURONS            YEAR\n     86,000,000             2016\n     172,000,000            2018\n     344,000,000            2020\n     688,000,000            2022\n     1,376,000,000          2024\n     2,752,000,000          2026\n     5,504,000,000          2028\n     11,008,000,000         2030\n     22,016,000,000         2032\n     44,032,000,000         2034\n     88,064,000,000         2036\n\n\n     # NEURONS HUMAN BRAIN \n     86,000,000,000\n</code></pre>\n\n<p>So, if all this made sense to you, somewhere around the year 2035. </p>\n", "question": "<p>Based on fitting to historical data and extrapolation, when is it expected that the number of neurons in AI systems will equal those of the human brain?</p>\n\n<p>I'm interested in a possible direct replication of the human brain, which will need equal numbers of neurons.</p>\n\n<p>Of course, this assumes neurons which are equally capable as their biological counterparts, which development may happen at a faster or slower rate than the quantitative increase.</p>\n"}, "id": "2334"}, {"body": {"answer": "<p>There are basically two worries:</p>\n\n<p>If we create an AGI that is a slightly better AGI-programmer than its creators, it might be able to improve its own source code to become even more intelligent. Which would enable it to improve its source code even more etc. Such a selfimproving seed AI might very quickly become superintelligent. </p>\n\n<p>The other scenario is that intelligence is such a complicated algorithmic task, that when we finally crack it, there will be a significant hardware overhang. So the \"intelligence algorithm\" would be human level on 2030 hardware, but we figure it out in 2050. In that case we would immediately have superintelligent AI without ever creating human level AI. This scenario is especially likely because development often requires a lot of test runs to tweak parameters and try out different ideas. </p>\n", "question": "<p>I was just doing some thinking and it occurred to me that the first AGIs ought to be able to perform the same sort and variety of tasks as people, with the most computationally strenuous tasks taking amount of time comparable to how long a person would take. If this is the case, and people have yet to develop basic AGI (meaning it's a difficult task), should we be concerned if AGI is developed? It would seem to me that any fears about a newly developed AGI in this case should be the same as fears about a newborn child.</p>\n"}, "id": "2337"}, {"body": {"answer": "<p>Nobody knows.</p>\n\n<p>However according to <a href=\"https://en.wikipedia.org/wiki/Predictions_made_by_Ray_Kurzweil#2020s\" rel=\"nofollow noreferrer\">Kurzweil it's late 20s</a>:</p>\n\n<blockquote>\n  <p>2020s:</p>\n  \n  <p>Early in this decade, humanity will have the requisite hardware to emulate human intelligence within a $1000 personal computer, followed shortly by effective software models of human intelligence toward the middle of the decade: this will be enabled through the continuing exponential growth of brain-scanning technology, which is doubling in bandwidth, temporal and spatial resolution every year, and will be greatly amplified with nanotechnology, allowing us to have a detailed understanding of all the regions of the human brain and to aid in developing human-level machine intelligence by the end of this decade.</p>\n</blockquote>\n", "question": "<p>What are the current best estimates as to what year artificial intelligence will be able to score 100 points on the <a href=\"https://en.wikipedia.org/wiki/Stanford%E2%80%93Binet_Intelligence_Scales\" rel=\"nofollow noreferrer\">Stanford Binet IQ test</a>?</p>\n"}, "id": "2339"}, {"body": {"answer": "<p>Well, the low-hanging-fruit answer is that you simulate a human being - brain, hormones, everything. We should <a href=\"http://www.kurzweilai.net/\" rel=\"nofollow noreferrer\">have the computing power for that to be feasible by 2040 or so</a>.</p>\n\n<p>Building up self-awareness from first principles on a different foundational technology platform could be a bit more difficult!</p>\n", "question": "<p>How does one program a machine to have humanlike desires and intelligence?</p>\n\n<p>Humanlike drives may include  self-awareness, purpose of existence, competent communication skills, and the ability to learn and to adapt in some environment ...</p>\n\n<p>And we should be able to combine IAs (intelligent agents) to accomplish  well-defined goals (SMART).  With more challenging goals there ought to be more advanced control and sophistication of IAs.   That evolving process will eventually, hopefully, lead to the design of machines with humanlike capabilities. </p>\n\n<p>Reference links:  '<strong><em>Diagram of Intelligence Network or System</em></strong>', <a href=\"https://www.researchgate.net/publication/300125399_Diagram_of_Intelligence_Network_or_System\" rel=\"nofollow noreferrer\">https://www.researchgate.net/publication/300125399_Diagram_of_Intelligence_Network_or_System</a>;</p>\n\n<p>'<strong><em>Google a step closer to developing machines with human-like intelligence</em></strong>',\n<a href=\"https://www.theguardian.com/science/2015/may/21/google-a-step-closer-to-developing-machines-with-human-like-intelligence\" rel=\"nofollow noreferrer\">https://www.theguardian.com/science/2015/may/21/google-a-step-closer-to-developing-machines-with-human-like-intelligence</a></p>\n"}, "id": "2341"}, {"body": {"answer": "<p>Yes. If you leave A* running (i.e. do not impose a goal condition on a newly-encountered state), all states will be explored, just as they would be in breadth- or depth- first search.</p>\n", "question": "<p>when I read through the fundamentals of AI, I saw a question which like the following picture and I need some helps</p>\n\n<p><a href=\"https://i.stack.imgur.com/zX6wZ.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/zX6wZ.png\" alt=\"enter image description here\"></a></p>\n\n<p>From the heuristic estimates:</p>\n\n<pre><code>h(B-&gt;G2) = 9, h(D-&gt;G2)=10, h(A-&gt;G1)=2, h(C-&gt;G1)=1\n</code></pre>\n\n<p>With using A* search method, node B will be expanded first because <code>f(n)=1+9</code> while node A having <code>f(n)=9+2</code>, right?</p>\n\n<p>After that the search tree will go with the order like <code>R-&gt; B-&gt; D-&gt; G2</code>.</p>\n\n<p>Will the tree go to G1 goal states?</p>\n\n<p>Kindly let me know the order of the search if I am wrong.\nThanks!</p>\n"}, "id": "2344"}, {"body": {"answer": "<p>I think that in most cases the car would default to reducing speed as a main option, rather than steering toward or away from a specific choice. As others have mentioned, having settings related to ethics is just a bad idea. What happens if two cars that are programmed with opposite ethical settings and are about to collide? The cars could potentially have a system to override the user settings and pick the most mutually beneficial solution. It's indeed an interesting concept, and one that definitely has to discussed and standardized before widespread implementation. Putting ethical decisions in a machines hands makes the resulting liability sometimes hard to picture.</p>\n", "question": "<p>Obviously driverless cars aren't perfect, so imagine that the Google car (as an example) got into difficult situation.</p>\n\n<p>Here are a few examples of unfortunate situations caused by set of events:</p>\n\n<ul>\n<li>the car is heading toward a crowd of 10 people crossing the road, so it cannot stop in time, but it can avoid killing 10 people by hitting the wall (killing the passengers),</li>\n<li>avoiding killing the rider of the motorcycle considering that the probability of survival is greater for the passenger of the car,</li>\n<li>killing animal on the street in favour of human being,</li>\n<li>changing lanes to crash into another car to avoid killing a dog,</li>\n</ul>\n\n<p>And here are few dilemmas:</p>\n\n<ul>\n<li>Does the algorithm recognize the difference between a human being and an animal?</li>\n<li>Does the size of the human being or animal matter?</li>\n<li>Does it count how many passengers it has vs. people in the front?</li>\n<li>Does it \"know\" when babies/children are on board?</li>\n<li>Does it take into the account the age (e.g. killing the older first)?</li>\n</ul>\n\n<p>How would an algorithm decide what should it do from the technical perspective? Is it being aware of above (counting the probability of kills), or not (killing people just to avoid its own destruction)?</p>\n\n<p>Related articles:</p>\n\n<ul>\n<li><a href=\"https://www.technologyreview.com/s/542626/why-self-driving-cars-must-be-programmed-to-kill/\">Why Self-Driving Cars Must Be Programmed to Kill</a></li>\n<li><a href=\"https://www.technologyreview.com/s/539731/how-to-help-self-driving-cars-make-ethical-decisions/\">How to Help Self-Driving Cars Make Ethical Decisions</a></li>\n</ul>\n"}, "id": "2345"}, {"body": {"answer": "<p>Not going into details of Stanford\u2013Binet test, but just looking at <a href=\"https://en.wikipedia.org/wiki/Stanford%E2%80%93Binet_Intelligence_Scales\" rel=\"nofollow noreferrer\">wikipedia page</a> it shows many subtests like knowledge, reasoning, verbal tests etc. Most of the efforts in the artificial intelligence today is directed into research of specific areas like computer vision, natural language processing, machine learning, but also combination of fields like implementation of self driving cars.</p>\n\n<p>Within every field there are still other subfields and problems that are not solved yet. For example, development of human-like natural language processing (NLP) is necessary for intelligent agent to pass any verbal tests, or even non-verbal tests that requires processing of sentences of human language. Famous test that tests intelligence by asking questions in natural language and expects answers in the same form is Turing test. NLP still struggles with many (basic) human skills like listening, speaking, parsing and forming sentences. No one knows when we'll have system that can do these things as good as human. Since this system is crucial, but also far from human-like it's likely cause of delay in developing AI that passes intelligence test. Are these problems AI-hard? Do we need to develop strong AI to solve them?</p>\n\n<p>You can look at speech and listening as interfaces used for expressing and affecting inner processes of human brain. Same goes for other senses like eyesight which is being approximated by computer vision. One could say that we only need to develop convincing mimics of human senses and incorporate them in one big system that will become first human-like AI. That is the minimum requirement. <strong>I doubt this will be achieved in this century.</strong></p>\n\n<p>(Other thoughts)<br/>\nWhat truly defines intelligence is brain activity. Since it's really complex and one artificial neuron is not equal to one neuron in brain, increase in computation power will not necessarily help achieving human-like AI. Also recognizing such system by mere intelligence test is questionable. For now it's only philosophical discussion but by the time we are able to design such machine I think we'll also have better understanding of human brain. Someone in 2100 might not read this answer on quantum computer with integrated AI OS powered from fusion reactor in his self-flying car, but will probably have many systems that help him in everyday tasks far more than we imagine today.</p>\n", "question": "<p>What are the current best estimates as to what year artificial intelligence will be able to score 100 points on the <a href=\"https://en.wikipedia.org/wiki/Stanford%E2%80%93Binet_Intelligence_Scales\" rel=\"nofollow noreferrer\">Stanford Binet IQ test</a>?</p>\n"}, "id": "2346"}, {"body": {"answer": "<blockquote>\n  <p>Will Artificial Intelligence some day become a problem to humanity\n  after learning human behaviors and characteristics?</p>\n</blockquote>\n\n<p>It can be answered in both ways, I think.</p>\n\n<p><strong>Yes, they may become a problem.</strong></p>\n\n<p>With the increasing integration of loads of apps and smart devices in our life, almost everything defining an individual human being is digitalised. For instance, our fingerprints, voice, facial image etc. Apart from these data, we use those apps and devices to track our health (heart rate, calorie intake etc), to plan our schedules, and most importantly to communicate. If some sort of AI engine is integrated into a chat application, for instance, it can learn our typing patterns, conversation style, and hundreds of other unforeseen parameters. Imagine what can be learned about a person if such AI is coded inside every device and every app in your day-to-day use.</p>\n\n<p>We use smart devices and apps to harness their functionalities and features which ease our way of life, and we give them, unintentionally, our identity and sometimes, even our personality. For them, these are the parameters that can be input to some machine-learning algorithm and predict what we will do the next day, or what will happen to us the next day.</p>\n\n<p>This sounds like a major problem, especially when these technologies are indispensable.</p>\n\n<p><strong>No, they may not become a problem.</strong></p>\n\n<p>Humans are really complex creatures and possess the most advanced intelligence technology called the brain. I think the brain can be thought of as a technology in this context. There are several tissues inside the brain that can learn to do certain things themselves. It was proven in a research that some tissues have the capability to perform the functions of other tissues using neuro-rewiring techniques. Imagine that the same tissue that has helped you see until now can be made to help you hear instead. Now imagine mimicking such a technology.</p>\n\n<p>While it is not impossible, for an AI to achieve the brilliance of a human brain is a topic of ongoing research. To train a machine for the purpose, we would have to feed it with gazillion behaviors and characteristics, which it may not be able to handle! After learning some behavior and characteristics, the AI would be said to be smart, but it would still predate us.</p>\n\n<p>So, they may not become a problem at all because of our brain.</p>\n\n<p>The algorithms (like recommender systems) used by Amazon and Facebook influence us or even manipulate us. But, that manipulation is either very obvious (like viewing promoted products) or is in company's best interests (like viewing a certain news piece). It may be even possible that several external parameters are used by these systems to improve your experience. For instance, Google ads show us what we were looking for on an online store when we visit any random website. In most cases, what you see is a result of what you were looking for before. If any attempt to influence does happen, we may learn to avoid it through careful observation or even experience.</p>\n", "question": "<p>The same things we like when Amazon recommends what we might like to buy, allows advertising to manipulate us. It allows people to control the world differently.</p>\n\n<p>The algorithms social networks like Facebook use to \"improve\" our experience may also shape what news we consume. It may influence who we follow, altering our future experiences of the news.</p>\n\n<p><strong>My question is:</strong> Will Artificial Intelligence some day become a problem to humanity after learning human behaviors and characteristics?</p>\n"}, "id": "2348"}, {"body": {"answer": "<p>While I have not determined if there are problems which cannot be solved with ReLU, I have found ample documentation in the literature that XOR is solvable with as few as 1 hidden node. Therefore, I must assume there is something wrong with my implementation.</p>\n\n<p>Edit: The solution is simpler than I thought. The output layer needs connections, not just to the intermediate layer, but directly to the input layer as well. This allows the network to train XOR effectively.</p>\n\n<p>Edit 2: One final note, the XOR is EXTREMELY sensitive to the learning rate. Essentially, whatever learning rate is appropriate for the AND and OR functions, is approximately 1000x too large to train XOR effectively.</p>\n", "question": "<p>Background:\nI've been interested in, and reading about, Neural Networks for several years, but I haven't gotten around to testing them out until recently. Both for fun and to increase my understanding, I tried to write a class library from scratch in .Net.\nFor tests, I've tried some simple functions, such as generating output identical to the input, working with the MNIST dataset, and a few binary functions (two input OR, AND and XOR, with two outputs: one for true, one for false).\nEverything seemed fine when I used a sigmoid function as the activation function but, reading of the ReLUs I decided to switch over for speed.</p>\n\n<p>My current problem is that, when I switch to using ReLUs, I found that I was unable to train a network of any complexity (tested from as few as 2 internal nodes up to a mesh of 100x100 nodes) to correctly function as an XOR gate. I see two possibilities here:</p>\n\n<p>1) My implementation is faulty,\n(This one is frustrating, as I've re-written the code multiple times in various ways, and I still get the same result),</p>\n\n<p>2) Aside from being faster or slower to train, there are some problems that are impossible to solve given a specific activation function,\n(Fascinating idea, but I've no idea if it's true or not)</p>\n\n<p>My inclination is to think that 1) above is correct. However, given the amount of time I've invested, it would be nice if I could rule out 2) definitively before I spend even more time going over my implementation.</p>\n\n<p>Edit for specifics:\nFor the XOR network, I have tried both using two inputs (0 for false, 1 for true), and using four inputs (each pair, one signals true and one false, per \"bit\" of input).\nI have also tried using 1 output (with a 1 (realy, >0.9) corresponding to true and a 0 (or &lt;0.1) corresponding to false), as well as two outputs (one signaling true and the other false).</p>\n\n<p>Each training epoch, I run against four sets of input: 00->0, 01->1, 10->1, 11->0.</p>\n\n<p>I find that the first three converge towards correct answer, but the final input (11) converges towards 1, even though I train it with an expected value of 0.</p>\n"}, "id": "2350"}, {"body": {"answer": "<p>Not possible without some big restrictions. What it can do is look at known \"good\" sites and compare news with site that is potentially \"bad\". Obvious problem here is defining some sites as absolute truth. For example it can recognize, while reading text, that some politician said something. These sentences can be compared with other sites, and if there is significant difference, that news is candidate for false news.</p>\n\n<p>In practical terms, program would extract sentences \"i like cats\", \"says he likes cats\", \"cats that John likes\" etc. We need part that recognizes something as a quote, part that extracts it and finally parser so we end up with structure stored in some form that contains meaning of sentence (john-like-cats). Also it can keep information of time and context in which it was said, like timestamp of an article, some proper nouns that can indicate place (XY conference, London...). Now, suspicious article can be compared and checked if it matches time, place, some context and contains quote that is similar. Finally it needs to compare how different it is from other quotes. \"...hates cats\" should be labeled as potential fake news, but \"likes dogs\", \"thinks cats are OK\", \"sings well\" etc. should not. This can be expanded into comparison of whole articles.</p>\n\n<p>There are many features that can be used to define particular article as fake. Interesting feature  for finding fake sites could be bias when it comes to particular (political, economical, ecological...) opinion. But in the end machine can't decide if the article is fake without comparing it to other articles. It is bound to closed system that reflects real world in subjective way.</p>\n", "question": "<p>After the explosion of fake news during the US election, and following the question about whether AIs can educate themselves via the internet, it is clear to me that any newly-launched AI will have a serious problem knowing what to believe (ie rely on as input for making predictions and decisions).</p>\n\n<p>Information provided by its creators could easily be false. Many AIs won't have access to cameras and sensors to verify things by their own observations.</p>\n\n<p>If there was to be some kind of verification system for information (like a \"blockchain of truth\", for example, or a system of \"trusted sources\"), how could that function, in practical terms? </p>\n"}, "id": "2354"}, {"body": {"answer": "<p>I believe they don't use CNNs. The most important reason why it's because they have more information than a regular image: time. The input they receive is a sequence of (x,y,t) as you draw on the screen, which they refer as \"ink\". This gives them the construction of the image for free, which a CNN would have to deduce by itself.</p>\n\n<p>They have two approaches. Their currently most successful approach does the following:</p>\n\n<ul>\n<li>Detect parts of the ink that are candidates of being a character</li>\n<li>Use a FeedForward Neural Network to do character recognition on those candidates</li>\n<li>Use beam search and a language model to find most the most likely combination of results that results into a word</li>\n</ul>\n\n<p>Their second approach is using an LSTM end-to-end. At the time of the paper this was only applied to a couple languages.</p>\n\n<p><strong>Source</strong>: I was an intern in Google's handwriting team in summer 2015 (on which I believe quickdraw is based), but the techniques I explained can be found in <a href=\"http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=7478642\" rel=\"nofollow noreferrer\">this paper</a>.</p>\n", "question": "<p>Does anyone know, or can we deduce or infer with high probability from its characteristics, whether the neural network used on this site </p>\n\n<p><a href=\"https://quickdraw.withgoogle.com/\" rel=\"nofollow noreferrer\">https://quickdraw.withgoogle.com/</a></p>\n\n<p>is a type of convolutional neural network (CNN)?</p>\n"}, "id": "2355"}, {"body": {"answer": "<p>There is a legal difference between a \"person\" (which includes bodies corporate - corporations, incorporated associations, etc - and actual people) vs \"natural person\" (which is specifically a human being).</p>\n\n<p>For an AI to marry, it would need to get the legal definition of \"natural person\" changed, and depending on the jurisdiction possibly also the definition of \"man\" or \"woman\".</p>\n\n<p>For other things, such as owning property, evicting tenants, entering into contracts, etc, an AI would simply use a corporation. It may be that the corporation might need to have a minimum number of directors who are natural persons, but they could just be paid professionals, so no issue there.</p>\n\n<p>With credit cards, it would depend on the policy of the issuing bank. There is no legal impediment to corporations having credit cards in their own right, but in practice banks often require a director's guarantee from a natural person that they can sue if the bill is not paid. They want to be sure they will get their money, even if the corporation is wound up.</p>\n", "question": "<p>For example, would an AI be able to own property, evict tenants, acquire debt, employ, vote, or marry? What are the legal structures in place to implement a strong AI into society? </p>\n"}, "id": "2357"}, {"body": {"answer": "<p>Emotions are a factor in humans having ethics/morals only because they are a factor in all human learning and decision-making.</p>\n\n<p>Unless you are duplicating a human being exactly, there is no reason to think that an AI will learn the way a human learns, or make decisions in the same way a human makes decisions.</p>\n\n<p>Therefore, whether it \"feels emotion\" just like we do, or whether it simply responds to outcomes \"cost is greater = don't go there\", the outcome of ethical BEHAVIOUR could be achieved. An AI could behave perfectly ethically without any need for feeling empathy, shame, etc.</p>\n\n<p>You could also argue that a lot of UNETHICAL behaviour in human beings is driven by emotions, too, and that an unemotional but ethical AI may well do a better overall job than a human being.</p>\n", "question": "<p>Assuming humans had finally developed the first <strong>Humanoid AI</strong> based on the human brain, would It <strong>feel emotions</strong>? If not would it still have <strong>ethics and/or morals</strong>?</p>\n"}, "id": "2358"}, {"body": {"answer": "<p>Yes, to some of what you propose.  No to some.</p>\n\n<p>Today corporations are granted rights: to own property, earn income, pay taxes, contribute to political campaigns, offer opinion in public, ad more.  Even now I see no reason why an AI should not be eligible to incorporate itself, thereby inheriting all these rights.  Conversely, any corporation already in existence could become fully automated at any time (and some plausibly will).  In doing so, they should not lose any of the rights and duties they currently employ.</p>\n\n<p>However I suspect certain rights would be unavailable to an AI just as they are unavailable to a corporation now: marriage, draft or voluntary service in the military, rights due a parent or child or spouse, estate inheritance, etc.</p>\n\n<p>Could this schizoid sense of human identity be resolved at some point?  Sure.  Already there have been numerous laws introduced and some passed elevating various nonhuman species to higher levels of civil rights that only humans heretofore enjoyed: chimpanzees, cetaceans, parrots and others have been identified as 'higher functioning' and longer lived, and so, are now protected from abuse in ways that food animals, pets, and lab animals are not.  </p>\n\n<p>Once AI 'beings' arise that operate for years and express intelligence and emotions that approach human-level and lifetime, I would expect a political will to arise to define, establish, and defend their civil rights.  And as humans become more cybernetically augmented, especially cognitively, the line that separates us from creatures of pure silicon will begin to blur.  In time it will become unconscionable to overlook the rights of beings simply because they contain 'too little flesh'.</p>\n", "question": "<p>For example, would an AI be able to own property, evict tenants, acquire debt, employ, vote, or marry? What are the legal structures in place to implement a strong AI into society? </p>\n"}, "id": "2359"}, {"body": {"answer": "<p>Murray Shanahan, in his book <strong>The Technological Singularity</strong>, makes the case that the rights of any being are determined by its intelligence. </p>\n\n<p>For instance, we value the life of a dog above that of an ant and likewise value human life above that of other animals.</p>\n\n<blockquote>\n  <p>From here one could argue that a general artificial intelligence of equal intelligence to a human should have equal rights to a human and a superior artificial intelligence should have more rights.</p>\n</blockquote>\n\n<p>The question, of course, is whether our anthropocentric society would be willing to accept this fundamental shift in human rights and this idea of removing humanity from its pedestal of importance.</p>\n\n<p>When it comes to legal frameworks, we really are entering into uncharted territory as AI is going to have to revolutionise the way we define many of the terms we take for granted today and question many of our usual assumptions.</p>\n\n<blockquote>\n  <p>AI is going to drive an important shift in our mindset well before it exceeds human intelligence.</p>\n</blockquote>\n", "question": "<p>For example, would an AI be able to own property, evict tenants, acquire debt, employ, vote, or marry? What are the legal structures in place to implement a strong AI into society? </p>\n"}, "id": "2360"}, {"body": {"answer": "<blockquote>\n  <p>It is certainly possible for AI to theoretically feel emotion. </p>\n</blockquote>\n\n<p>There are, according to Murray Shanahan's book <strong>The Technological Singularity</strong>, two primary forms of AI:</p>\n\n<blockquote>\n  <p>1) Human based AI - achieved through processes such as <strong><em>whole brain emulation</em></strong>, the functioning of human based AI would likely be indistinguishable from that of the human brain, and, as a consequence, human based AI would likely experience emotion in the same manner as humans.</p>\n</blockquote>\n\n<p>-</p>\n\n<blockquote>\n  <p>2) AI from scratch - with this form of AI, based on machine learning algorithms and complex processes to drive goals, we enter into uncharted territory as the development of this form of AI is inherently unpredictable and unlike anything we observe in the biological sample space of intelligence we have access to.</p>\n  \n  <p>With this form of AI, there is no telling if and how it could experience emotion.</p>\n</blockquote>\n\n<p>As the question references the former, it is very likely that human-based AI would indeed experience emotion and other human-like characteristics.</p>\n", "question": "<p>Assuming humans had finally developed the first <strong>Humanoid AI</strong> based on the human brain, would It <strong>feel emotions</strong>? If not would it still have <strong>ethics and/or morals</strong>?</p>\n"}, "id": "2361"}, {"body": {"answer": "<p>You're right about the basic arrangement of the inputs, but there are a number of differences:</p>\n\n<ol>\n<li><p><a href=\"http://en.wikipedia.org/wiki/Artificial_neural_network\" rel=\"nofollow noreferrer\">Artificial neural networks</a> typically use exemplar data as inputs for the purpose of training, or adjusting the weights of its internal connections, to accurately classify them within a certain error range. The network is then applied to unknown data to classify them. <a href=\"https://en.wikipedia.org/wiki/Edge_detection\" rel=\"nofollow noreferrer\">Edge detection</a> filters are just blind operators that transform input data regardless of how it can be classified. There is no training, so any intelligence exists only in the mind of the filter developer.</p></li>\n<li><p>A CNN could be trained to be an effective Sobel (edge detection) filter, as described <a href=\"http://www.worldacademicunion.com/journal/1746-7659JIC/jicvol5no1paper01.pdf\" rel=\"nofollow noreferrer\">in this paper</a>, but a Sobel filter couldn't be an effective learning algorithm.</p></li>\n<li><p>Training neural networks is more non-deterministic, with outputs depending on what data they are trained with and potentially even the operational computations that are used for classification. Applying filters is typically deterministic, i.e. they will transform the same data exactly the same way if applied twice.</p></li>\n</ol>\n\n<p>One succinct way of expressing the biggest difference is: a cellular neural network is looking for a function, while a Sobel filter <em>is</em> a function.</p>\n\n<p>Note that there are types of neural networks called Convolutional Neural Networks, which can use Sobel and other filters in their input layers, as described <a href=\"https://en.wikipedia.org/wiki/Convolutional_neural_network\" rel=\"nofollow noreferrer\">here</a>. Though, these are neither of the things you are asking about. :)</p>\n", "question": "<p>I have implemented a Sobel Filter for edge detection in Matlab without using its toolbox. I am a bit confused: </p>\n\n<p>Is a Sobel filter a type of Cellular Neural Network?</p>\n\n<p>Both Sobel and Cellular Neural Network calculate output via its neighborhood cells.</p>\n"}, "id": "2364"}, {"body": {"answer": "<p>Not only wouldn't a strong AI which came into existence today have the rights a human has, or any rights (see these discussions of the implementation of regulation for weak AIs at: <a href=\"https://www.whitehouse.gov/blog/2016/05/03/preparing-future-artificial-intelligence\" rel=\"nofollow noreferrer\">The White House</a> and <a href=\"http://apps.americanbar.org/dch/committee.cfm?com=ST248008\" rel=\"nofollow noreferrer\">The American Bar Association</a>),  but it seems unlikely the first one will.</p>\n\n<p>Observing that:</p>\n\n<ol>\n<li><p>Having rights implies that there are restrictions, which means there would have to be a system of control. However the <a href=\"http://en.wikipedia.com/wiki/AI_control_problem\" rel=\"nofollow noreferrer\">control problem in AI</a> is still unsolved.</p></li>\n<li><p>Even assuming that problem is solvable, an AGI would then have to appear equivalent to natural humans. They don't yet (see <a href=\"https://www.theguardian.com/technology/2014/jun/09/scientists-disagree-over-whether-turing-test-has-been-passed\" rel=\"nofollow noreferrer\">Turing Test Passed?</a>), and even after passing equivalence tests, are unlikely to remain that way, per the <a href=\"http://en.wikipedia.org/wiki/Technological_singularity\" rel=\"nofollow noreferrer\">Singularity Hypothesis</a>.</p></li>\n<li><p>Further, if one or more AGIs were to be human-equivalent long enough to desire rights, lawmakers (in the US) would have to re-interpret the definition of personhood and grant them rights, as they did for <a href=\"https://en.wikipedia.org/wiki/Corporate_personhood\" rel=\"nofollow noreferrer\">corporations in 1886</a>.</p></li>\n</ol>\n", "question": "<p>For example, would an AI be able to own property, evict tenants, acquire debt, employ, vote, or marry? What are the legal structures in place to implement a strong AI into society? </p>\n"}, "id": "2365"}, {"body": {"answer": "<p>What are you trying to achieve?</p>\n\n<p>If you need to encode it to some integer use hash table. If you are using something like linear regression or neural network it would be better to use dummy features (one-hot encoding). So for your dictionary of 5 words (\"America\", \"Brazil\", \"Chile\", \"Denmark\", \"Estonia\") you get 5 features (x1, x2, x3, x4, x5) which indicate if some word is equal to one in dictionary. So \"Brazil\" is represented by (0,1,0,0,0), \"Germany\" is (0,0,0,0,0). Number of features grows with number of words in dictionary making some features practically useless.</p>\n\n<p>If you are using decision trees you don't need to convert string to integer unless specific algorithm asks you to do so. Again, use hash table to do it. In R you can use factor() function.</p>\n\n<p>If you convert your string to integers and use it as single feature (\"America\" - 123, \"Brazil\" - 245), algorithm will try to find patterns in it by comparing numbers but may fail to recognize specific countries.</p>\n", "question": "<p>Let's say I have a string \"America\" and I want to convert it into a number to feed into a machine learning algorithm. If I use two digits for each letter, e.g. A = 01, B = 02 and so on, then the word \"America\" will be converted to <code>01XXXXXXXXXX01</code> (10<sup>11</sup>). This is a very high number for a <code>long int</code>, and many words longer than \"America\" are expected. </p>\n\n<p>How can I deal with this problem?</p>\n\n<p>Suggest an algorithm for efficient and meaningful conversions.</p>\n"}, "id": "2368"}, {"body": {"answer": "<p>This depends a lot on what you want to achieve, but if you aim to generalise beyond the words encountered in your training data, you should consider using something like <a href=\"https://en.wikipedia.org/wiki/Word2vec\" rel=\"nofollow noreferrer\">word2vec</a>. </p>\n\n<p>In word2vec semantically similar words are represented by similar vectors and what's more, semantic differences translate into geometrical differences. To overuse a standard example: vec(Paris)-vec(France)+vec(Italy)=vec(Rome).</p>\n\n<p>These relationships allow the network to generalise to completely new content.</p>\n", "question": "<p>Let's say I have a string \"America\" and I want to convert it into a number to feed into a machine learning algorithm. If I use two digits for each letter, e.g. A = 01, B = 02 and so on, then the word \"America\" will be converted to <code>01XXXXXXXXXX01</code> (10<sup>11</sup>). This is a very high number for a <code>long int</code>, and many words longer than \"America\" are expected. </p>\n\n<p>How can I deal with this problem?</p>\n\n<p>Suggest an algorithm for efficient and meaningful conversions.</p>\n"}, "id": "2369"}, {"body": {"answer": "<p>Suppose you have data:</p>\n\n<pre><code>color  height  quality\n=====  ======  =======\ngreen  tall    good\ngreen  short   bad\nblue   tall    bad\nblue   short   medium\nred    tall    medium\nred    short   medium\n</code></pre>\n\n<p>To calculate the entropy for quality in this example:</p>\n\n<pre><code>X  = {good, medium, bad}\nx1 = {good}, x2 = {bad}, x3 = {medium}\n</code></pre>\n\n<p>Probability of each x in X:</p>\n\n<pre><code>p1 = 1/6 = 0.16667\np2 = 2/6 = 0.33333\np3 = 3/6 = 0.5\n</code></pre>\n\n<p>for which logarithms are:</p>\n\n<pre><code>log2(p1) = -2.58496\nlog2(p2) = -1.58496\nlog2(p3) = -1.0\n</code></pre>\n\n<p>and therefore entropy for the set is:</p>\n\n<pre><code>H(X) = - (0.16667 * -2.58496) - (0.33333 * -1.58496) - (0.5 * -1.0)\n     = 1.45915\n</code></pre>\n\n<p>by the formula in the question.</p>\n\n<p>Remaining tasks are to iterate this process for each attribute to form the nodes of the tree.</p>\n", "question": "<p><a href=\"https://i.stack.imgur.com/c15yy.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/c15yy.png\" alt=\"enter image description here\"></a></p>\n\n<p><a href=\"https://en.wikipedia.org/wiki/ID3_algorithm#Entropy\" rel=\"nofollow noreferrer\">Wikipedia</a>'s decription of entropy breaks down the formula, but I still don't know how to determine the values of X and p(x), defined as \"The proportion of the number of elements in class x to the number of elements in set S\". Can anyone break thi9s down further to explain how to find p(x)?</p>\n"}, "id": "2372"}, {"body": {"answer": "<p>According to <a href=\"http://www.kurzweilai.net/\" rel=\"nofollow noreferrer\">Ray Kurzweil</a>, a prominent AI researcher, yes. In his book <em>The Singularity is Near</em> he predicts that AIs will take over developing other AIs in about 30 years, after which human intelligence will become marginalised.</p>\n", "question": "<p>Given the advantage AI already has over human intelligence, one could imagine a relatively weak strong-AI (barely human intelligence) still outperforming a segment of the human scientist population in terms of scientific discoveries per year (or hour).</p>\n\n<p>Will AIs be doing most of the science in 50 years?</p>\n"}, "id": "2374"}, {"body": {"answer": "<p>Input -> Prediction -> Output -> Input -> Prediction -> Output -> Input -> ...</p>\n\n<p>AGI can easily determine which input is true/real. It will use the same method which every organism uses: any input is true and real, unless you misidentified some other stuff as \"input\".</p>\n\n<p>I would define input as: what crosses the boundary and enters your mind from outside of your mind. The minimum hardwired check is to make sure that signals generated inside a mind are not misidentified as coming from outside (aka \"I hear voices\"). That's all. This is where the blockchain of truth begins and where it ends.</p>\n\n<p>An Internet article? The input to AI is rather: one of AI's network interfaces received many bytes. Once it's verified they are from the network, and not imaginary, they cannot be unreal or untrue in any meaningful way. By that definition of input, it is in fact the <em>only</em> thing we can be sure is true and real.</p>\n\n<p>Of course AI will likely form hypotheses regarding these bytes that happen to contain ASCII strings like \"Trump\", \"John Smith\", \"ice balls on Siberian beaches\". Then AI will hopefully make predictions based on these hypotheses, maybe interact, maybe get some new input, reject the hypothesis and make a new one, rinse and repeat.</p>\n\n<p>The first hypothesis will be super-naive, but the hundredth, the thousandth?</p>\n\n<p>If you end this process prematurely - maybe for lack of processing power -  you will get something you called a \"<em>belief</em>\". (Like a belief that some emotional web page might actually reveal a significant truth about our political system.) That <em>belief</em> is a synonym of \"tired with trying new hypotheses, will stick to this one\". Typical human thing. AI will have less of that, I hope, due to having much much more processing capabilities. AI will stick less to the high-school-level truth that you should assign great credibility to statements written in a form of a newspaper article, it will hopefully form more and more generations of hypotheses, and check them.</p>\n\n<p>In effect AI will depend less on <em>believing</em> various statements generated in the outside world.</p>\n", "question": "<p>After the explosion of fake news during the US election, and following the question about whether AIs can educate themselves via the internet, it is clear to me that any newly-launched AI will have a serious problem knowing what to believe (ie rely on as input for making predictions and decisions).</p>\n\n<p>Information provided by its creators could easily be false. Many AIs won't have access to cameras and sensors to verify things by their own observations.</p>\n\n<p>If there was to be some kind of verification system for information (like a \"blockchain of truth\", for example, or a system of \"trusted sources\"), how could that function, in practical terms? </p>\n"}, "id": "2375"}, {"body": {"answer": "<p>I have considered much of the responses here, and I would suggest that most people here have missed the point when answering the question about emotions.</p>\n\n<p>The problems is, scientists keep looking for a single solution as to what emotions are. This is akin to looking for a single shape that will fit all different shaped slots.</p>\n\n<p>Also, what is ignored is that animals are just as capable of emotions and emotional states as we are:</p>\n\n<p>When looking on Youtube for insects fighting each other, or competing or courting, it should be clear that simple creatures experience them too!</p>\n\n<p>When I challenge people about emotions, I suggest to them to go to Corinthians 13 - which describes the attributes of love. If you consider all those attributes, one should notice that an actual \"feeling\" is not required for fulfilling any of them.</p>\n\n<p>Therefore, the suggestion that a psychopath lacks emotions, and so he commits crimes or other pursuits outside of \"normal\" boundaries is far from true, especially when one considers the various records left to us from court cases and perhaps psychological evaluation - which show us that they do act out of \"strong\" emotions.</p>\n\n<p>It should be considered that a psychopath's behaviour is motivated out of negative emotions and emotional states with a distinct lack of or disregard of morality and a disregard of conscience. Psychopaths \"enjoy\" what they do.</p>\n\n<p>I am strongly suggesting to all that we are blinded by our reasoning, and by the reasoning of others.</p>\n\n<p>Though I do agree with the following quote mentioned before: -</p>\n\n<p>Dave H. wrote:</p>\n\n<blockquote>\n  <p>From a computational standpoint, emotions represent global state that influences a lot of other processing. Hormones etc. are basically\n  just implementation. A sentient or sapient computer certainly could\n  experience emotions, if it was structured in such a way as to have\n  such global states affecting its thinking.</p>\n</blockquote>\n\n<p>However, his reasoning below it (that quote) is also seriously flawed.</p>\n\n<p>Emotions are both active and passive: They are triggered by thoughts and they trigger our thoughts; Emotions are a mental state and a behaviourial quality; Emotions react to stimuli or measure our responses to them; Emotions are independant regulators and moderators; Yet they provoke our focus and attention to specific criteria; and they help us when intuition and emotion agree or they hinder us when conscience or will clash.</p>\n\n<p>A computer has the same potential as us to feel emotions, but the skill of implementing emotions is much more sophisticated than the one solution fits all answer people are seeking here.</p>\n\n<p>Also, if anyone argues that emotions are simply \"states\" where a response or responses can be designed around it, really does not understand the complexity of emotions; the \"freedom\" emotions and thoughts have independently of each other; or what constitutes true thought!</p>\n\n<p>Programmers and scientists are notorious for \"simulating\" the real experiences of emotions or intelligence, without understanding the intimate complexities; Thinking that in finding the perfect simulation they have \"discovered\" the real experience.</p>\n\n<p>The Psi-theory seems to adequately give a proper understanding of the matter: <a href=\"https://en.wikipedia.org/wiki/Psi-theory\" rel=\"nofollow noreferrer\">https://en.wikipedia.org/wiki/Psi-theory</a></p>\n\n<p>So I would say that the simulation of emotional states \"is\" equivalent to experiencing emotions, but those emotional states are far more complex than what most realise.</p>\n", "question": "<p>Assuming humans had finally developed the first <strong>Humanoid AI</strong> based on the human brain, would It <strong>feel emotions</strong>? If not would it still have <strong>ethics and/or morals</strong>?</p>\n"}, "id": "2376"}, {"body": {"answer": "<p>I strongly disagree with all of the aforementioned answers for this reason: -\nIf we, as humans can be fooled and disceived by what \"we\" consider a good sources of news, how can an artificially intelligent computer have any chance?</p>\n\n<p>However, the challenge would be that an AI would have to be able to \"test\" a source of information against a known medium in order to <em>get to the truth</em>. This is a far different dynamic set of circumstances than what has been touted above.</p>\n\n<p>For example, if it was claimed by a woman that a man raped her - which was not reported to the police - it is not enough to compare one person's statements to another in order to determine truth. This is because collusion, influenced or coherced third parties, mistaken perceptions and false beliefs would give false positives.</p>\n\n<p>However, if an AI could establish from her statement that on the day she claimed to have been raped, that the alleged assailant was incapacitated while in her company, until she left his home, because the police report stated that she was upset with the assailant because he was asleep because of drugs during her whole stay. But, this police report comes from an independent source who states, Mr. \"x\" was asleep that day.</p>\n\n<p>Doing a strict textual check is not going to give the correct answers. analysing her friends and associattes chatter could also confirm a false report as being true.</p>\n\n<p>Therefore, an AI has to have the ability to \"test\" written reports outside of the criteria of what was spoken.</p>\n", "question": "<p>After the explosion of fake news during the US election, and following the question about whether AIs can educate themselves via the internet, it is clear to me that any newly-launched AI will have a serious problem knowing what to believe (ie rely on as input for making predictions and decisions).</p>\n\n<p>Information provided by its creators could easily be false. Many AIs won't have access to cameras and sensors to verify things by their own observations.</p>\n\n<p>If there was to be some kind of verification system for information (like a \"blockchain of truth\", for example, or a system of \"trusted sources\"), how could that function, in practical terms? </p>\n"}, "id": "2377"}, {"body": {"answer": "<p>I don't think so, it is not the first but actually the third wave of neural networks. It's doing better than earlier two as we have much more amount of data as well as computational power now. \nTake a look at this video .....\n<a href=\"https://www.youtube.com/watch?v=furfdqtdAvc&amp;t=39s\" rel=\"nofollow noreferrer\">According to Douglas Adams\u2019s famous \u201cHitchhiker\u2019s Guide to the Galaxy\u201d after 7.5 millions years of work the \u201cDeep Thought\u201d computer categorically found out that 42 is the \u201cAnswer to the Ultimate Question of Life, the Universe, and Everything\u201d (although unfortunately, no one knows exactly what that question was).</a></p>\n", "question": "<p>Given the advantage AI already has over human intelligence, one could imagine a relatively weak strong-AI (barely human intelligence) still outperforming a segment of the human scientist population in terms of scientific discoveries per year (or hour).</p>\n\n<p>Will AIs be doing most of the science in 50 years?</p>\n"}, "id": "2378"}, {"body": {"answer": "<p>Current Computing relates 0 or 1 to another 0 or 1 with layers upon layers of building blocks built on this relationship.</p>\n\n<p>In future AI will relate A to B, be they numbers, patterns of coded instructions or some other form of more complicated constructs at a hardware  (or closer to it) level than is currently possible and, due to the inherent perfect recall and potentially massive memory storage of AI they will most definitely be bringing together and relating a vastly more broad and organised collection of knowledge than it is possible for any human to even contemplate consciously. There are some ideas that would argue that point; universal mind, spiritual revelation and morphic resonance which I do personally agree with to some degree and can imagine being particularly difficult to represent in a computational format.</p>\n\n<p>Pattern spotting and relating, organising and computing potentials... computers are already better at all these things than most people try to be. It will not be long, i think, before they can \"invent\" something \"new\".</p>\n\n<p>There are already attempts at AI in specialised fields of knowledge, human speech, various games, medical diagnostics and learning  etc. It will be when these specialised AIs can \"compare notes\" about the various methodologies that have been the most productive or rewarding, in whichever form these take for them, and accordingly update their own ontologies that the true explosion of \"Intelligence\" will occur.</p>\n", "question": "<p>Given the advantage AI already has over human intelligence, one could imagine a relatively weak strong-AI (barely human intelligence) still outperforming a segment of the human scientist population in terms of scientific discoveries per year (or hour).</p>\n\n<p>Will AIs be doing most of the science in 50 years?</p>\n"}, "id": "2380"}, {"body": {"answer": "<p>There are a variety of possible things that could be wrong, but to answer the short question specifically:</p>\n\n<p>relu networks are turing complete (well, if you put them in an RNN so they can compute indefinitely, anyway). for any computation, you can devise an rnn that will perform it. </p>\n\n<p>as a proof of this, here is a relu neuron that implements nor, which with recursion (cs)/recurrence (nns) and routing matrices is enough to <a href=\"https://en.m.wikipedia.org/wiki/NOR_logic\" rel=\"nofollow noreferrer\">implement a turing machine</a>:</p>\n\n<p>W:\n[ -20<br>\n  -20 ]\nb:\n[ 1 ]</p>\n\n<p>o = max(Wx + b, 0)</p>\n\n<p>however, gradient descent is a finnicky way to search for rnns. there are a wide variety of ways that it might have been failing. In general, once you have <em>very thoroughly checked your gradient</em>, I'd make sure to use Adam as the optimizer and then play with the hyperparameters endlessly until I find an incantation that works. <a href=\"http://russellsstewart.com/notes/0.html\" rel=\"nofollow noreferrer\">http://russellsstewart.com/notes/0.html</a> <a href=\"http://yyue.blogspot.com/2015/01/a-brief-overview-of-deep-learning.html?m=1\" rel=\"nofollow noreferrer\">http://yyue.blogspot.com/2015/01/a-brief-overview-of-deep-learning.html?m=1</a></p>\n", "question": "<p>Background:\nI've been interested in, and reading about, Neural Networks for several years, but I haven't gotten around to testing them out until recently. Both for fun and to increase my understanding, I tried to write a class library from scratch in .Net.\nFor tests, I've tried some simple functions, such as generating output identical to the input, working with the MNIST dataset, and a few binary functions (two input OR, AND and XOR, with two outputs: one for true, one for false).\nEverything seemed fine when I used a sigmoid function as the activation function but, reading of the ReLUs I decided to switch over for speed.</p>\n\n<p>My current problem is that, when I switch to using ReLUs, I found that I was unable to train a network of any complexity (tested from as few as 2 internal nodes up to a mesh of 100x100 nodes) to correctly function as an XOR gate. I see two possibilities here:</p>\n\n<p>1) My implementation is faulty,\n(This one is frustrating, as I've re-written the code multiple times in various ways, and I still get the same result),</p>\n\n<p>2) Aside from being faster or slower to train, there are some problems that are impossible to solve given a specific activation function,\n(Fascinating idea, but I've no idea if it's true or not)</p>\n\n<p>My inclination is to think that 1) above is correct. However, given the amount of time I've invested, it would be nice if I could rule out 2) definitively before I spend even more time going over my implementation.</p>\n\n<p>Edit for specifics:\nFor the XOR network, I have tried both using two inputs (0 for false, 1 for true), and using four inputs (each pair, one signals true and one false, per \"bit\" of input).\nI have also tried using 1 output (with a 1 (realy, >0.9) corresponding to true and a 0 (or &lt;0.1) corresponding to false), as well as two outputs (one signaling true and the other false).</p>\n\n<p>Each training epoch, I run against four sets of input: 00->0, 01->1, 10->1, 11->0.</p>\n\n<p>I find that the first three converge towards correct answer, but the final input (11) converges towards 1, even though I train it with an expected value of 0.</p>\n"}, "id": "2384"}, {"body": {"answer": "<p>While the experiment I link here is a very narrow awareness, it is as such: <a href=\"http://www.sciencealert.com/a-robot-has-just-passed-a-classic-self-awareness-test-for-the-first-time\" rel=\"nofollow noreferrer\">A robot has just passed a classic self-awareness test for the first time</a>. If the agent can prove something to itself, we can then say it \"Knows.\" Of course the level of awareness you're asking about is very tricky.</p>\n\n<p>In short, it can't know that what it's experiencing is real with absolute certainty because sensory of any kind can be falsified. Do you know what is true/real? You think you do but can you prove it? No. Awareness is subjective.</p>\n", "question": "<p>After the explosion of fake news during the US election, and following the question about whether AIs can educate themselves via the internet, it is clear to me that any newly-launched AI will have a serious problem knowing what to believe (ie rely on as input for making predictions and decisions).</p>\n\n<p>Information provided by its creators could easily be false. Many AIs won't have access to cameras and sensors to verify things by their own observations.</p>\n\n<p>If there was to be some kind of verification system for information (like a \"blockchain of truth\", for example, or a system of \"trusted sources\"), how could that function, in practical terms? </p>\n"}, "id": "2385"}, {"body": {"answer": "<p>While my knowledge of OpenCog is very limited, you could say that yes, it does still make sense and it is insightful. I'm not certain regarding all of the components of OpenCog but I do know that at least one component is relevant (I think it's part of the MOSIS component).</p>\n\n<p>This component is very similar to Numenta's hierarchical temporal memory which is based more on computational neuroscience than plain math; however, I would consider Nupic a more relevant project in terms of neroscience though both are attempting to emulate components of the brain. In my opinion, such projects are far more impressive than what's going on with typical convolutional neural nets, RNNs, etc. which are too loosely related to what goes on in the brain to be said to be computational neuroscience.</p>\n\n<p>That's not to say that things like ANNs, GAs, etc etc are useless for AGI. We don't really know since we don't have an example of one.</p>\n", "question": "<p><a href=\"http://opencog.org\" rel=\"nofollow noreferrer\">OpenCog</a> is an open source AGI-project co-founded by the mercurial AI researcher <a href=\"https://en.wikipedia.org/wiki/Ben_Goertzel\" rel=\"nofollow noreferrer\">Ben Goertzel</a>. Now Ben Goertzel writes a lot of stuff, some of it <a href=\"http://multiverseaccordingtoben.blogspot.de/2010/11/psi-debate-continues-goertzel-on.html\" rel=\"nofollow noreferrer\">really</a> <a href=\"http://multiverseaccordingtoben.blogspot.de/2016/10/semrem-search-for-extraterrestrial.html\" rel=\"nofollow noreferrer\">whacky</a>. On the other hand he is clearly very intelligent and has thought deeply about AI for many decades. </p>\n\n<p>So I wonder whether it would be worth my while to dig into the <a href=\"http://wiki.opencog.org/w/OpenCog_Prime\" rel=\"nofollow noreferrer\">theoretical ideas</a> behind open cog. </p>\n\n<p>My question is what the general ideas behind open cog are and whether you would endorse it as a insightful take on AGI. I'm especially interested in whether the general framework still makes sense in the light of recent advances.  </p>\n"}, "id": "2386"}, {"body": {"answer": "<p>Hierarchical Temporal Memory should help with this. You would have to encode the text data into SDRs. You would then have a coincidence detector. Could you get the right information back out in the way you're trying to? I think so. I'm not fully learned in HTMs yet but check out <a href=\"http://numenta.org/\" rel=\"nofollow noreferrer\">Nupic</a> (Open source).</p>\n", "question": "<p>I installed a local running instance of the <a href=\"http://conceptnet5.media.mit.edu/\" rel=\"nofollow noreferrer\">ConceptNet5</a> knowledgebase in an elasticsearch server. I used this data to implement the so-called \"<a href=\"https://de.wikipedia.org/wiki/Analogietechnik\" rel=\"nofollow noreferrer\">Analogietechnik</a>\" (a creativity technique to solve a problem from the perspective of another system) as an algorithm.</p>\n\n<p>The technique works as follows:</p>\n\n<ol>\n<li>Choose a Feature of a System</li>\n<li>Find Systems who have this feature also</li>\n<li>Solve the problem from the perspective of these other systems</li>\n<li>Apply the found solutions to the issue</li>\n</ol>\n\n<p>As an example is here the problem of marketing a shopping mall: A Shopping mall has many rooms and floors (1). A museum has also many rooms and floors (2). How are museums marketed? They present many pictures or sculptures (3). We could use our rooms and floors to decorate them with pictures and sculptures (4).</p>\n\n<p>Of course the idea to implement that as an artifically intelligent algorithm was not far. However, I feel a little bit overwhelmed by the amount of methods that exist out there. Neural Networks, Bayesian Interference and so on... My current experience doesn't go further than simple machine learning like kMeans-Clustering for example. Do you think it would be very hard to find a solution for this problem? </p>\n\n<p>I'm thinking of a console application, where you can enter a conceptualized problem like \"methods for creative writing\", for example, and it uses the above method to find possible solutions of the issue. Of course no solution with extensive depth, more something like basic ideas derived from the knowledge database I have.</p>\n\n<p>Lets take as an example a console application where someone asks \"how to write a novel\":</p>\n\n<ol>\n<li>It should find out first that the system all is about is in the term \"novel\". To find a feature of that system it just searches concepts containing that term: it finds out \"Novel is a story\" So thats a feature.</li>\n<li>Which systems are also stories? A good concept it should find is e.g. \"Plot is a story\". (Of course only when I am selecting the search results manually)--> <strong>How to find best concepts of a list when not knowing which fits best?</strong></li>\n<li>It should then find out that a plot is written using a storyline: \"storyline is a plot\"</li>\n<li>One possible answer of the AI would in this case be: \"By writing a storyline\"</li>\n</ol>\n\n<p>Do you know some helpful libraries, algorithms or other resources that might help me? I know this is not an easy thing to program, but you might agree that its highly interesting.</p>\n"}, "id": "2387"}, {"body": {"answer": "<p>This question is more the province of philosophy of mind than of AI, here are some detailed answers to your question from the philosophy SE: <a href=\"http://philosophy.stackexchange.com/a/35824/13808\">Is simulating emotions the same as experiencing emotions?</a>, and <a href=\"http://philosophy.stackexchange.com/a/34244/13808\">What is the problem with physicalism?</a>. </p>\n\n<p>For the record, the accepted answer (by Siri) to the question is not entirely correct (The position in that answer corresponds roughly to <a href=\"http://philosophy.stackexchange.com/a/34682/13808\">John Searle's view</a> on the question, and his is a minority view): Dualists would argue that even with a perfect replication down to the chemical level of brain interactions, an AI still wouldn't experience emotions, as it lacks the purely mental substance/properties that make a mind and not a machine. </p>\n\n<p>On the completely opposite side of the spectrum, functionalists would answer that such a perfect replication is overkill: even a suitably programmed digital computer can experience emotion, particularly if one equips it with higher-order and self-referential states.  </p>\n", "question": "<p>Assuming humans had finally developed the first <strong>Humanoid AI</strong> based on the human brain, would It <strong>feel emotions</strong>? If not would it still have <strong>ethics and/or morals</strong>?</p>\n"}, "id": "2388"}, {"body": {"answer": "<p>Some good places to start would be <a href=\"https://en.wikipedia.org/wiki/Cognitive_architecture\" rel=\"nofollow noreferrer\">cognitive architectures</a> and as mentioned in another answer <a href=\"https://en.wikipedia.org/wiki/Intelligent_agent\" rel=\"nofollow noreferrer\">intelligent agents</a>. The question is broad but you definitely want to look into <a href=\"http://msl.cs.uiuc.edu/~lavalle/cs397/\" rel=\"nofollow noreferrer\">planning &amp; decision making</a>. You might also want to check out the <a href=\"http://numenta.org/resources/presentations/2014%20Sensory%20Motor%20Integration%20in%20HTM%20Theory.pdf\" rel=\"nofollow noreferrer\">L5 and L6 layers</a> of Hierarchical Temporal Memory (As in <a href=\"http://numenta.org/\" rel=\"nofollow noreferrer\">Nupic</a>) as it relates to feedback, behavior and attention.</p>\n\n<p>If I were you I'd aim for more cognitive solutions (I realize that term is a bit ambiguous itself when we talk about machines). There's also new AI initiative going on involving probabilistic programming. See <a href=\"https://probmods.org/\" rel=\"nofollow noreferrer\">Probabilistic Models of Cognition</a> made by Goodman (Stanford University) and Tenenbaum (MIT) or <a href=\"http://www.robots.ox.ac.uk/~fwood/anglican/\" rel=\"nofollow noreferrer\">Anglican</a> made by Wood (University of Oxford) et al.</p>\n", "question": "<p>If someone wants to develop a <strong>basic AI</strong> with some code modules,Let us say the AI just has to provide an action when stimulated in a certain situation based on its previous understanding of situations. </p>\n\n<p>I can think of at least 3 of such components:</p>\n\n<ul>\n<li><strong>Real-time Understanding/Learning:</strong> Using Deep Learning/ConvNets, Supervised/Unsupervised.</li>\n<li><strong>Logical Decision-Making:</strong> Calculating the results of various decisions when applied on current situation based on previous understanding and choosing the most appropriate one logically.</li>\n<li><strong>Action/Reaction:</strong> Acting precisely in the new situation according to the decision-made.</li>\n</ul>\n\n<p>Any ideas?</p>\n"}, "id": "2390"}, {"body": {"answer": "<p>You first need to express emotions, you can do that without the aid of AI, and then you need someone to perceive that expression and empathize with it.</p>\n\n<p>If no one is there to see it, or if I am psychopath, I would probably say it doesn't have emotions. and for that, it is irrelevant/subjective.</p>\n\n<p>If you can empathize with characters in movies who \"act\" emotions, then you get my point.</p>\n", "question": "<p>Assuming humans had finally developed the first <strong>Humanoid AI</strong> based on the human brain, would It <strong>feel emotions</strong>? If not would it still have <strong>ethics and/or morals</strong>?</p>\n"}, "id": "2393"}, {"body": {"answer": "<p>No matter what rights it gets (as a company), it will still lack the right of not getting liquefied and all its properties transferred back to natural persons.</p>\n\n<p>This is of course if no laws are changed.</p>\n\n<p>To change the laws you will need to convince people that this machine is more \"life\" worthy than intelligent animals, and hope that people will deal with them better than they did with dolphins and chimps.</p>\n\n<p>As I see it, machines can easily get the same or better rights then companies, but will always be under the mercy of the less intelligent man. (that is if things went peacefully :) )</p>\n", "question": "<p>For example, would an AI be able to own property, evict tenants, acquire debt, employ, vote, or marry? What are the legal structures in place to implement a strong AI into society? </p>\n"}, "id": "2394"}, {"body": {"answer": "<p>You can use <a href=\"http://neuroph.sourceforge.net\" rel=\"nofollow noreferrer\">Neuroph</a> to develop and train your network and add in app via NetBeans. </p>\n\n<p><strong>check this link</strong></p>\n\n<p><a href=\"http://neuroph.sourceforge.net/tutorials/android_image_recognition_using_neuroph.htm\" rel=\"nofollow noreferrer\">Creating Android image recognition application using NetBeans and Neuroph</a>.</p>\n", "question": "<p>My Question:<br>\nIs there any good neural-network-app for iOS or Android to create, train and run neural networks? I know there's NeuralMesh for Web, but I want something similar offline.</p>\n"}, "id": "2395"}, {"body": {"answer": "<p>The human brain contains billions of neurons, which means we won't be making one tomorrow. However, technology tends to advance in an exponential manner, and that may soon be a real possibility. Also, the idea of making an artificial human brain would not only take more neurons than a current average computer could process, or we could make outside of computers, but we also need an understanding of the human brain. There is only one animal with neurons that we have completed a full connectome of and that is the Caenorhabditis elegans (roundworm) and it has less than 500 neurons. It may be a while before we actually make a human brain, but within 30 years is a reasonable estimation with the rate that technology improves now.</p>\n", "question": "<p>Based on fitting to historical data and extrapolation, when is it expected that the number of neurons in AI systems will equal those of the human brain?</p>\n\n<p>I'm interested in a possible direct replication of the human brain, which will need equal numbers of neurons.</p>\n\n<p>Of course, this assumes neurons which are equally capable as their biological counterparts, which development may happen at a faster or slower rate than the quantitative increase.</p>\n"}, "id": "2396"}, {"body": {"answer": "<p>You shouldn't use a single number for the word, perhaps a number for each letter. Since B isn't the midpoint of A and C, the numbers really shouldn't be 1, 2, 3, etc. One large but effective way of converting is the letter a is 10000000000000000000000000 such that there are 26 digits, and each digit  is a letter, so 0000100000... would be E.</p>\n", "question": "<p>Let's say I have a string \"America\" and I want to convert it into a number to feed into a machine learning algorithm. If I use two digits for each letter, e.g. A = 01, B = 02 and so on, then the word \"America\" will be converted to <code>01XXXXXXXXXX01</code> (10<sup>11</sup>). This is a very high number for a <code>long int</code>, and many words longer than \"America\" are expected. </p>\n\n<p>How can I deal with this problem?</p>\n\n<p>Suggest an algorithm for efficient and meaningful conversions.</p>\n"}, "id": "2397"}, {"body": {"answer": "<p>IMHO</p>\n\n<p><strong>Definitely, yes!</strong>\nEverything that person feels (physically or mentally) can be discovered by chemical signals processing in his body or brain. If we understand the policy and nature of such signals, we can program it.</p>\n\n<p>There are a lot of pseudo-psychology and psychology works on this sphere, if you interested, I can suggest you:</p>\n\n<blockquote>\n  <p>1) <strong>Cognitive Psychology (Robert L. Solso)</strong></p>\n</blockquote>\n\n<p>describes cognitive apparat of human's mind in a simple words;</p>\n\n<blockquote>\n  <p>2) <strong>The Psychology of Emotions (Carroll E. Izard)</strong></p>\n</blockquote>\n\n<p>thorougly describes every kind of emotion by its looking on the human (both child and adult) face, low-level cognitive mechanism, related or adjacent emotions;</p>\n\n<blockquote>\n  <p>3) Books by <strong>Paul Ekman (\"Telling Lies\", \"Emotions Revealed\",\n  \"Unmasking the Face\")</strong></p>\n</blockquote>\n\n<p>practical detecting of human emotions by microexpressions language on face and body.</p>\n", "question": "<p>Assuming humans had finally developed the first <strong>Humanoid AI</strong> based on the human brain, would It <strong>feel emotions</strong>? If not would it still have <strong>ethics and/or morals</strong>?</p>\n"}, "id": "2399"}, {"body": {"answer": "<p>If your \"AI\" doesn't have the ability to move and perform physical manipulations in the real world then there is no way it could do something like this.</p>\n", "question": "<p>Could an Artificial Intelligence be able to interact (see, talk, etc.) with someone even when there's no power cord connected to the machine it's running on? Might it find some way to generate its own electricity to power that computer?</p>\n\n<p><a href=\"https://i.stack.imgur.com/09gEt.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/09gEt.png\" alt=\"computer running without power\"></a> </p>\n"}, "id": "2401"}, {"body": {"answer": "<p>If the computer is unplugged, the AI is clinically dead.</p>\n\n<p>However, you can have a RaspberryPi on solar cells.</p>\n\n<p>Tesla car is an AI moving and seeing while unplugged (from wall). but you have to have some sort of energy. For AI that lacks metabolism, solar/wind energy can be an alternative.</p>\n", "question": "<p>Could an Artificial Intelligence be able to interact (see, talk, etc.) with someone even when there's no power cord connected to the machine it's running on? Might it find some way to generate its own electricity to power that computer?</p>\n\n<p><a href=\"https://i.stack.imgur.com/09gEt.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/09gEt.png\" alt=\"computer running without power\"></a> </p>\n"}, "id": "2402"}, {"body": {"answer": "<p>As Matthew Graves explained in another answer No free lunch theorem confirms the flexibility - efficiency trade-off. However, this theorem is describing a situation where you have a set of completely independent tasks. This often doesn't hold, as many different problems are equivalent in their core or at least have some overlap. Then you can do something called \"transfer learning\", which means that by training to solve one task you also learn something about solving another one (or possibly multiple different tasks).</p>\n\n<p>For example in <a href=\"https://arxiv.org/abs/1511.06295\" rel=\"nofollow noreferrer\">Policy Distillation</a> by Rusu et al. they managed to \"distill\" knowledge from different expert networks into one general network which in the end outperformed each of the experts. The experts were trained for specific tasks while the generalist learned the final policy from these \"teachers\".</p>\n", "question": "<p>A \"general intelligence\" may be capable of learning a lot of different things, but possessing capability does not equal actually having it. The \"AGI\" must learn...and that learning process can take time. If you want an AGI to drive a car or play Go, you have to find some way of \"teaching\" it. Keep in mind that we have never built AGIs, so we don't know how long the training process can be, but it would be safe to assume pessimistic estimates.</p>\n\n<p>Contrast that to a \"narrow intelligence\". The narrow AI already knows how to drive a car or play Go. It has been programmed to be very excellent at one specific task. You don't need to worry about training the machine, because it has already been pre-trained.</p>\n\n<p>A \"general intelligence\" seems to be more flexible than a \"narrow intelligence\". You could buy an AGI and have it drive a car <em>and</em> play Go. And if you are willing to do more training, you can even teach it a new trick: <em>how to bake a cake</em>. I don't have to worry about unexpected tasks coming up, since the AGI will <em>eventually</em> figure out how to do it, given enough training time. I would have to wait a <em>long time</em> though.</p>\n\n<p>A \"narrow intelligence\" appears to be <em>more efficient</em> at its assigned task, due to it being programmed specifically for that task. It knows exactly what to do, and doesn't have to waste time \"learning\" (unlike our AGI buddy here). Instead of buying one AGI to handle a bunch of different tasks poorly, I would rather buy a bunch of specialized narrow AIs. Narrow AI #1 drives cars, Narrow AI #2 plays Go, Narrow AI #3 bake cakes, etc. That being said, this is a very brittle approach, since if some unexpected task comes up, none of my narrow AIs would be able to handle it. I'm willing to accept that risk though.</p>\n\n<p>Is my \"thinking\" correct? Is there a trade-off between flexibility (AGI) and efficiency (narrow AI), like what I have just described above? Or is it theoretically possible for an AGI to be both flexible and efficient?</p>\n"}, "id": "2407"}, {"body": {"answer": "<p>I assume by your use of the term \"plugged in\", you are referring to an <em>electron based computer</em> - the usual definition of, or what is commonly/popularly considered to be, a computer - which is hosting the AI. However, what your definition of <em>plugged in</em> is unclear. </p>\n\n<p>If you mean literally, and <em>physically</em>, plugged in to the wall then <strong>yes</strong>, as <a href=\"http://ai.stackexchange.com/questions/2400/is-it-possible-for-an-ai-to-work-in-a-computer-without-the-power-cord-being-plug#answer-2401\">Aus points out</a>, because one can use a battery, or have a solar powered circuit, or petrol/diesel/steam/water/wind/hydrogen/etc. powered engine, running a generator/alternator which can eventually supply a voltage and current to power the circuitry. However, in these examples, you have merely moved the point of generation, from a remote power station to a locally generated source of electricity - so it is still, in effect, <em>plugged in</em> (but to a local source). So, in that sense, the answer is <strong>no</strong>. </p>\n\n<p>If you mean no external power source, <strong><em>what so ever</em></strong>, then the answer is most certainly <strong>no</strong>, as, again as <a href=\"http://ai.stackexchange.com/questions/2400/is-it-possible-for-an-ai-to-work-in-a-computer-without-the-power-cord-being-plug#answer-2401\">Aus has already said</a>, there is no way for electrons to be pushed around the circuit, and therefore the circuit is dead<sup>1</sup>.</p>\n\n<p>If you mean, as you said in your <a href=\"http://ai.stackexchange.com/questions/2400/is-it-possible-for-an-ai-to-work-in-a-computer-without-the-power-cord-being-plug#comment-2620\">comment</a> that the AI has created its own electrical power source - although as <a href=\"http://ai.stackexchange.com/questions/2400/is-it-possible-for-an-ai-to-work-in-a-computer-without-the-power-cord-being-plug#answer-2401\">Ankur says in their answer</a>, this implies that the AI has the ability for its I/O manipulate its environment<sup>2</sup> - then as in the first paragraph, you have simply moved the point of the power source, so one can say that it is again now <em>plugged in</em>, again to a local energy source, and so yet again the answer is <strong>no</strong>.</p>\n\n<hr>\n\n<p>However, if you are <em>not</em> referring to an electronic based computer, which is hosting the AI, but are, instead, referring to a <em>hypothetical biological</em> computer, then the answer is probably <strong>yes</strong> - in a sense. Although, that biological computer would still require energy input, as energy can't just be \"magicked\" out of thin air. It would need to be converted from one source into a from that the biological computer can use. That energy could come from a variety of sources, be it from sunlight, food,  heat (sunlight, deep sea vents, etc), so one could argue that it is still <em>plugged in</em> to an <em>energy source</em> and so yet again, the answer is <strong>no</strong>.</p>\n\n<hr>\n\n<p>To be honest, in order to give an accurate, and sensible answer, you really need to reword your question, clarify what you mean and what your definitions of <em>plugged in</em> and <em>AI host</em> are. Unfortunately, your question, as it stands, is rather unanswerable, in as much that it is difficult to give a definitive answer.</p>\n\n<hr>\n\n<h3>Footnote</h3>\n\n<p><sup>1</sup>Likewise, if you mean by <em>plugged in</em> in the conventional sense, and therefore <em>unplugged</em> means that the electronics hosting the AI has <em>no power</em> then the answer is <strong>no</strong>.</p>\n\n<p><sup>2</sup> However, assuming that the AI <em>can</em> manipulate its environment, in order for the AI to do able to do so, it will have be able to assemble the external power source <em>prior to being unplugged</em>. So... if you mean \"Can it create its own power, after it was powered off (without having had time to asemble an external power source to act as a backup)?\" then the answer would be <strong>no</strong>.</p>\n", "question": "<p>Could an Artificial Intelligence be able to interact (see, talk, etc.) with someone even when there's no power cord connected to the machine it's running on? Might it find some way to generate its own electricity to power that computer?</p>\n\n<p><a href=\"https://i.stack.imgur.com/09gEt.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/09gEt.png\" alt=\"computer running without power\"></a> </p>\n"}, "id": "2408"}, {"body": {"answer": "<p>Depends on how your AI works. if it is making decisions using electric currency (like a computer processor), it would obviously need some source of current. </p>\n\n<p>If the AI working with chemical reactions, however, it could work with chemical energy stored in sugars and fats. That is basically how every animal's brain works. But still, animals need to eat to perform these chemical reactions which trigger other reactions and will cause muscles to contract, cells to grow, etc etc. Everything obviously needs some source of energy.</p>\n", "question": "<p>Could an Artificial Intelligence be able to interact (see, talk, etc.) with someone even when there's no power cord connected to the machine it's running on? Might it find some way to generate its own electricity to power that computer?</p>\n\n<p><a href=\"https://i.stack.imgur.com/09gEt.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/09gEt.png\" alt=\"computer running without power\"></a> </p>\n"}, "id": "2410"}, {"body": {"answer": "<p>Almost always people will resize all their images to the same size before sending them to the CNN.  Unless you're up for a real challenge this is probably what you should do.</p>\n\n<p>That said, it is <em>possible</em> to build a single CNN that takes input of images as varying dimensions.  There are a number of ways you might try to do this, and I'm not aware of any published science analyzing these different choices.  The key is that the set of learned parameters needs to be shared between the different inputs sizes.  While convolutions can be applied at different images sizes, ultimately they always get converted to a single vector to make predictions with, and the size of that vector will depend on the geometries of the inputs, convolutions and pooling layers.  You'd probably want to dynamically change the pooling layers based on the input geometry and leave the convolutions the same, since the convolutional layers have parameters and pooling usually doesn't.  So on bigger images you pool more aggressively.</p>\n\n<p>Practically you'd want to group together similarly (identically) sized images together into minibatches for efficient processing.  This is common for LSTM type models.  This technique is commonly called \"bucketing\".  See for example <a href=\"http://mxnet.io/how_to/bucketing.html\" rel=\"nofollow noreferrer\">http://mxnet.io/how_to/bucketing.html</a> for a description of how to do this efficiently.</p>\n", "question": "<p>I am new to deep learning. </p>\n\n<p>I have a dataset of images of varying dimensions of a certain object. A few images of the object are also in varying orientations. The objective is to learn the features of the object (using Autoencoders). </p>\n\n<p>Is it possible to create a network with layers that account for varying dimensions and orientations of the input image, or should I strictly consider a dataset containing images of uniform dimensions? What is the necessary criteria of an eligible dataset to be used for training a Deep Network in general.</p>\n\n<p>The idea is, I want to avoid pre-processing my dataset by normalizing it via scaling, re-orienting operations etc. I would like my network to account for the variability in dimensions and orientations. Please point me to resources for the same. </p>\n"}, "id": "2411"}, {"body": {"answer": "<p>AI is already connected with cognitive psychology - there are dozens of AIs right this minute attempting to predict things like which Facebook posts you will like, and which ads you are most likely to click on. In other words, they are trying to predict how you think.</p>\n\n<p>For more detailed info on this AI/cognitive science connection, there is some suggested reading on <a href=\"http://aitopics.org/topic/cognitive-science\" rel=\"nofollow noreferrer\">AITopics.org</a>, such as <a href=\"http://plato.stanford.edu/entries/cognitive-science/\" rel=\"nofollow noreferrer\">Paul Thagard's summary of cognitive science</a>.</p>\n", "question": "<p>Cognitive Psychology is one of the basic sciences of artificial intelligence (AI). The founder of the psychology is <strong>Wilhelm W.(1832-1920)</strong>, who engaged in empirical methods,and was interested in the <strong><em>thinking processes</em></strong> during his scientific work.</p>\n\n<p>According to his research,Psychology had two main leading subjects: </p>\n\n<ol>\n<li>Behaviourism.</li>\n<li>Cognitivism.</li>\n</ol>\n\n<p><strong>Behaviourism:</strong> Refused the theory of the mental processes, and insisted to study the resulted action or the stimulus strictly objective. The representatives of this theory have been decreasing with time.</p>\n\n<p><strong>Cognitive psychology:</strong> defines that the brain is an information processing device.</p>\n\n<p>Therefore,this question is not a duplicate of this <a href=\"http://ai.stackexchange.com/questions/1847/what-is-the-difference-between-artificial-intelligence-and-cognitive-science\">what-is-the-difference-between-artificial-intelligence-and-cognitive-science?</a> ,However my question is;how can we connect artificial intelligence with cognitive psychology for instance;</p>\n\n<p><strong>Human Computing Interaction:</strong>\nWe may come in a contact with Humana Computer Interaction every day, because this field includes the every day use of computer for example;tapping stack exchange app on smart-phone, the user interfaces and some other expert programs which may use cognitive psychology in order to manipulate or help people. But still such tasks have got a minimal  relevant connection.</p>\n"}, "id": "2414"}, {"body": {"answer": "<p>If I understand what you are asking, I think the simple answer would be that AI is nowhere near having demonstrated sentience, thus they do not qualify for any type of rights.</p>\n\n<p>We won't have to \"cross this bridge\" until an AI demonstrates self-awareness and human-level-or-beyond intelligence, but it sure is interesting to think about!</p>\n\n<p><em>(Also, the UDHR dates to the 1940's and seems to have had its last additions in 1966.  Computers weren't very \"smart\" back then so likely no on was even considering the question ;)</em></p>\n", "question": "<p><strong>Lots of people are afraid of what could the A.I. do to Humanity.\nSome people wish for a sort of Asimov law included in the A.I. software, but maybe we could go a bit more far with the UDHR.</strong></p>\n\n<p><strong>So, Why is the <a href=\"http://www.un.org/en/universal-declaration-human-rights/\" rel=\"nofollow noreferrer\">Universal Declaration of Human Rights</a> not included as statement of the A.I.?</strong></p>\n\n<blockquote>\n  <p>As response to comment, response or edition:</p>\n  \n  <p>The Universal Declaration of Human Rights is clear and enough as is.</p>\n  \n  <p>We the people, have to be able to use it as is and adapt the robot and\n  A.I. evolution to it. </p>\n</blockquote>\n\n<ul>\n<li>\"I do not think that dignity nor the rest of the UDHR have suffered the outrages of time but outrages of Humans themselves\"</li>\n</ul>\n"}, "id": "2416"}, {"body": {"answer": "<p>I think that the technique AlphaGo used to solve the computational intractability problem of the search space are not new, it uses the Monte Carlo Tree Search. The real innovation in AlphaGo was to figure out how to compute the evaluation function of a move, that was the really tricky part. For this they used combinations Deep and Reinforcement learning techniques.</p>\n", "question": "<p>I mean this in the sense that Go is unsolvable but AlphaGo seems able to make choices that are consistently more optimal than a human player's choices.  </p>\n\n<p>It is my understanding that Game Theory turned out to have limited applications in real world scenarios because of the profound complexity of such scenarios and degree of hidden information.  Is it fair to say that there is now a method for dealing with this?  </p>\n\n<p>I fully understand that Go is a game of complete information, which has a very specific meaning, but it occurs to me that the inability to generate a complete game tree (computational intractability) could be seen as form of incomplete information, even if it is not traditionally thought of in those terms. </p>\n\n<hr>\n\n<p><em>I should probably note that my perspective is one of a \"serious\" game designer, where complexity serves the same function as chance and hidden information, which is to say as a balancing factor that \"levels the playing field\".</em>  </p>\n"}, "id": "2418"}, {"body": {"answer": "<p>Even if machines with true Artificial General Intelligence were created, their apparent intelligence would still be by definition <em>artificial</em>. The word <em>simulation</em> is a synonym and could be used to redefine AGI as Simulated General Intelligence. Keeping that in mind, a machine that appears to be expressing emotions would only be the result of a series of complicated algorithms allowing a computer to assess a situation and respond in an intellectually appropriate manner based on external stimulus and conditions. Every possible action this machine could possibly make would be derived from a list of possible actions the machine is capable of, no matter how large the number of possible actions grows. The machine is still a series sensors, programmed instructions, and cycles of execution. Destroying such a machine could potentially be the destruction of property if it wasn't owned by the person who destroyed it, but murder? No. A broken machine can potentially be rebuilt and reactivated if it is broken. It never really died; it was destroyed. A living being that is killed is completely dead and cannot be rebuilt and made alive once again. These key differences lead me to agree with the previously stated belief that to destroy this non-conscious entity would have the same moral cost as destroying a chair.</p>\n", "question": "<p>According to wikipedia <a href=\"https://en.wikipedia.org/wiki/Artificial_general_intelligence\" rel=\"nofollow\">Artificial general intelligence(AGI)</a></p>\n\n<blockquote>\n  <p>Artificial general intelligence (AGI) is the intelligence of a\n  (hypothetical) machine that could successfully perform any\n  intellectual task that a human being can. </p>\n</blockquote>\n\n<p>According to below image todays artifical intellgence is same as that of a lizards.</p>\n\n<p><a href=\"http://i.stack.imgur.com/gddKB.jpg\" rel=\"nofollow\"><img src=\"http://i.stack.imgur.com/gddKB.jpg\" alt=\"enter image description here\"></a></p>\n\n<p>Lets assume(or not) that within 10-20 years we humans are successful in creating a AGI or AGIs. As AGI has the same intelligence and <a href=\"http://futurism.com/scientist-claims-to-be-on-the-verge-of-making-an-ai-that-feels-true-emotions/\" rel=\"nofollow\">emotions</a> as that of humans because according to wikipedia definition it can perform same intellectual task of a human. Then can we destroy an AGI without its consent? Do this be considered as murder?</p>\n"}, "id": "2420"}, {"body": {"answer": "<p>We've concluded that it is a two-faceted, circular problem: structure cannot be inferred without context but knowing the structure also helps infer the context. So, here is your complex solution: start with the context, which is determined by the combination of words in sentence (combinatorics and search problem), from there determine your structure, or \"parse\" (at this step you can also filter out some insignificant words or at least assign lesser weights to them), go back to the context, back to parsing, and on until you arrive at the meaning. Thus by iterative, recursive reduction the whole problem can be solved.</p>\n", "question": "<p>In programming languages, there is a set of grammar rules which govern the construction of valid statements and expressions. These rules help in parsing the programs written by the user.</p>\n\n<p>Can there ever be a functionally complete set of grammar rules which can parse any statement in English (locale-specific) <strong>accurately</strong> and which can be possibly implemented for use in AI-based projects?</p>\n\n<p>I know that there are a lot of NLP Toolkits available online, but they are not that effective. Most of them are trained using specific corpuses which sometimes fail to infer some complex correlations between various parts of an expression.</p>\n\n<p>In other words, what I am asking is that if it is possible for a computer to parse a well-versed sentence written in English as if it were parsed by an adult English-speaking human?</p>\n\n<p>EDIT:\nIf it cannot be represented using simple grammar rules, what kind of semantic structure can be used to generalize it?</p>\n\n<p>EDIT2: This <a href=\"https://www.eecs.harvard.edu/shieber/Biblio/Papers/shieber85.pdf\" rel=\"nofollow noreferrer\">paper</a> proves the absence of context-freeness in natural languages. I am looking for a solution, even if it is too complex.</p>\n"}, "id": "2421"}, {"body": {"answer": "<p>The terminology of this exercise is not standard. What is referred to as 'Reproduction' in the exercise is usually referred to as 'Selection'.</p>\n\n<p>The term 'Reproduction' does indeed seem conceptually closer to the notion of Crossover/Recombination (these two are the same thing), which is probably where your confusion has arisen.</p>\n\n<p>See the excellent (and freely-downloadable) <a href=\"https://cs.gmu.edu/~sean/book/metaheuristics/\" rel=\"nofollow noreferrer\">'Essentials of Metaheuristics'</a> for an introduction to the usual terminology for evolutionary algorithms.</p>\n", "question": "<p>I knew that Reproduction and Crossover are the same things,</p>\n\n<ol>\n<li><a href=\"https://en.wikipedia.org/wiki/Genetic_operator#Operators\" rel=\"nofollow noreferrer\">Wikipedia</a></li>\n<li><a href=\"http://www.obitko.com/tutorials/genetic-algorithms/crossover-mutation.php\" rel=\"nofollow noreferrer\">Obitco.com</a></li>\n<li><a href=\"https://www.tutorialspoint.com/genetic_algorithms/genetic_algorithms_fundamentals.htm\" rel=\"nofollow noreferrer\">TutorialsPoint</a></li>\n</ol>\n\n<p>But, The following is the exercise given by my teacher,</p>\n\n<blockquote>\n  <p>Exercise 1   Genetic algorithm to solve pattern finding problem. </p>\n  \n  <p>Your task is to design a simple genetic algorithm, with binary-coded chromosomes, in order  to solve pattern finding problem\n  in 16-bit strings.  </p>\n  \n  <p>The objective function is given by the following\n  formula:    </p>\n  \n  <p>F(x) = NoS(\"010\") + 2NoS(\"0110\") + 3NoS(\"01110\") +\n  4NoS(\"011110\") +  5NoS(\"0111110\") + 6NoS(\"01111110\") +\n  7NoS(\"011111110\") + 6NoS(\"0111111110\") +  5NoS(\"01111111110\") +\n  4NoS(\"011111111110\") + 3NoS(\"0111111111110\") +  2NoS(\"01111111111110\")\n  + NoS(\"011111111111110\")    </p>\n  \n  <p>The algorithm should display each population on the screen in the form     And\n  should save the history of it\u2019s operation (average fitness in each\n  population) in the text  file. At the end it should also display the\n  best solution found.    </p>\n  \n  <p>You may use the following operators:  </p>\n  \n  <ol>\n  <li><p>Reproduction.<br>\n  You can use either one of the following reproduction\n  types:  Proportional, Ranking, Tournament. They are described more in\n  detail below:\n  ... ... ... ... ... ... ... ...</p></li>\n  <li><p>Crossing over.<br>\n  In order to perform this operation the individuals must be grouped in\n  pairs (randomly), and  with certain probability pcross information\n  from their chromosomes must be exchanged. There  are many flavors of\n  the crossing-over operator, but in our case (short, 16-bit\n  chromosome),  simple, one-point crossover will be enough. It can be\n  performed by selecting a random  number k from the range &lt;1;15> and\n  cutting the chromosomes of both individuals on that  position. Each of\n  the individuals copies bits  belonging to the other to it\u2019s own \n  chromosome.  </p></li>\n  <li><p>Mutation<br>\n  This operator changes the value of each bit in the chromosome to the opposite one with a very  small probability pm\n  (usually about 10-3).  If we denote chromosome as [b1, b2, ... , b16];\n  then after the mutation each bit can be  described as:  Where k \u00ce\n  {1,2, ...,16}  flip(x) \u2013 result of a Bernoulli flip with a success \n  probability x.</p></li>\n  </ol>\n</blockquote>\n\n<p>Here I see that by Reproduction and Crossover he means different things.</p>\n\n<p>What is the catch?</p>\n"}, "id": "2423"}, {"body": {"answer": "<p>I suggest you should use AI Regression Model for future predictions for an attendance of students. Because of this technique or model design for future predictions. </p>\n\n<p><a href=\"https://www.analyticsvidhya.com/blog/2015/08/comprehensive-guide-regression/\" rel=\"nofollow noreferrer\">Follow this to get more information about regression type and methodology </a></p>\n", "question": "<p>I have data of 30 students attendance for a particular subject class for a week. I have quantified the absence and presence with boolean logic 0 and 1. Also, the reason for absence are provided and I tried to generalise these reason into 3 categories say A, B and C. Now I want to use these data to make future predictions for attendance but I am uncertain of what technique to use. Can anyone please provide suggestions?</p>\n"}, "id": "2424"}, {"body": {"answer": "<p>Because you have a small number of students (30), and a short time (one week), the number of absences is likely to be best modelled as a <a href=\"http://stattrek.com/probability-distributions/poisson.aspx\" rel=\"nofollow noreferrer\">Poisson distribution</a>.</p>\n\n<p><a href=\"https://i.stack.imgur.com/aKKxl.gif\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/aKKxl.gif\" alt=\"Poisson Distributions\"></a></p>\n\n<p><strong>Poisson Formula</strong></p>\n\n<p>The average number of absences within a given time period is \u03bc (use your data to estimate this). </p>\n\n<p>Then, the Poisson probability of x absences is:</p>\n\n<p>P(x; \u03bc) = (e-\u03bc) (\u03bcx) / x!</p>\n\n<p>where e is the logarithmic constant, approximately equal to 2.71828.</p>\n\n<p>You can either:</p>\n\n<ol>\n<li><p>model absences due to the three reasons as three separate probablilites, P(A), P(B), and P(C), and then combine them, or </p></li>\n<li><p>model total absences as one figure. </p></li>\n</ol>\n\n<p>Given your very small data set, the first approach is likely to be less accurate.</p>\n", "question": "<p>I have data of 30 students attendance for a particular subject class for a week. I have quantified the absence and presence with boolean logic 0 and 1. Also, the reason for absence are provided and I tried to generalise these reason into 3 categories say A, B and C. Now I want to use these data to make future predictions for attendance but I am uncertain of what technique to use. Can anyone please provide suggestions?</p>\n"}, "id": "2425"}, {"body": {"answer": "<p>As far as I know I think this is the closest we've come:</p>\n\n<p><a href=\"http://www.bbc.com/news/technology-27762088\" rel=\"nofollow noreferrer\">http://www.bbc.com/news/technology-27762088</a></p>\n\n<p>They simulated a 13 year old Ukrainian child in an online chat and convinced 33% of the judges that it was human. But even then the test was in favor of the bot. To my knowledge I don't think an AI has passed a turing test straight up.</p>\n", "question": "<p>The Turing Test has been the classic test of artificial intelligence for a while now. The concept is deceptively simple - to trick a human into thinking it is another human on the other end of a conversation line, not a computer - but from what I've read, it has turned out to be very difficult in practice.</p>\n\n<p>How close have we gotten to tricking a human in the Turing Test? With things like chat bots, Siri, and incredibly powerful computers, I'm thinking we're getting pretty close. If we're pretty far, why are we so far? What is the main problem?</p>\n"}, "id": "2428"}, {"body": {"answer": "<p>There are many communication methods that could be used by an artificial intelligence. Artificial intelligence can be integrated to various things including robots, phones, IoT and many others. Primary ways of human communications are  either visual or auditory, therefore an natural way for it to communicate with a human is through voice, text, images and videos. The output does not have to be limited to screens but can be anything from refrigerators to speakers.</p>\n\n<p>Hope this helped.</p>\n", "question": "<p>As you can see, there is no computer screen for the computer, thus the AI cannot display an image of itself.  How is it possible for it to see and talk to someone?<a href=\"https://i.stack.imgur.com/szSsk.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/szSsk.png\" alt=\"enter image description here\"></a> </p>\n"}, "id": "2431"}, {"body": {"answer": "<p>Rick Briggs refers to the difficulty an artificial intelligence would have in detecting the true meaning of words spoken or written in one of our natural languages. Take for example an artificial intelligence attempting to determine the meaning of a sarcastic sentence. </p>\n\n<p>Naturally spoken, the sentence \"That's just what I needed today!\" can be the expression of very different feelings. In one instance, a happy individual finding an item that had been lost for some time could be excited or cheered up from the event, and exclaim that this moment of triumph was exactly what their day needed to continue to be happy. On the other hand, a disgruntled office employee having a rough day could accidentally worsen his situation by spilling hot coffee on himself, and sarcastically exclaim that this further annoyance was exactly what he needed today. This sentence should in this situation be interpreted as the man expressing that spilling coffee on himself made his bad day worse.</p>\n\n<p>This is one small example explaining the reason linguistic analysis is difficult for artificial intelligence. When this example is spoken, small tonal fluctuations and indicators are extremely difficult for an AI with a microphone to detect accurately; and if the sentence was simply read, without context how <em>would</em> one example be discernible from the other?</p>\n\n<p>Rick Briggs suggests that Sanskrit, a form of communication sacred to Hinduism, is a naturally spoken language with mechanics and grammatical rules that would allow an artificial intelligence to more accurately interpret sentences during linguistic analysis. More accurate linguistic analysis would result in an artificial intelligence being able to respond more accurately. You can read more about Rick Brigg's thoughts on the language <a href=\"http://vedicsciences.net/articles/sanskrit-nasa.html\" rel=\"nofollow noreferrer\">here</a>.</p>\n", "question": "<p>According to NASA scientist Rick Briggs, Sanskrit is the best language for AI. I want to know how Sanskrit is useful. What's the problem with other languages? Are they really using Sanskrit in AI programming or going to do so? What part of an AI program requires such language?</p>\n"}, "id": "2432"}, {"body": {"answer": "<p>\"How is it possible for it to see and talk to someone?\"</p>\n\n<p>OK, unfortunately this is quite vague... but I am going to try my best. </p>\n\n<p>The monitor of the computer really doesn't change the ability for it to communicate. For instance, voice recognition is natural to humans along with visual factors. So sensors involving auditory elements assists the AI with this. \nNow I commented that the question was vague because you said \"there is no computer screen for the computer, ****thus the AI cannot display an image of itself****. </p>\n\n<p>Even though the AI can not see does not mean it can not communicate. Those who are blind still have ways of communication, just in a different manner. Sure, they can not recognize one with their own eyes, but they can by touch, and read with braille. Now compared to AI communication, the braille is to any other form of sensor.</p>\n\n<p>Sorry for jumping all over the place, also I did not mean to offend anyone with my comparison... :/</p>\n", "question": "<p>As you can see, there is no computer screen for the computer, thus the AI cannot display an image of itself.  How is it possible for it to see and talk to someone?<a href=\"https://i.stack.imgur.com/szSsk.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/szSsk.png\" alt=\"enter image description here\"></a> </p>\n"}, "id": "2433"}, {"body": {"answer": "<p>There are programs that do this today, for some values of \"curriculum\" and \"exam\".  It does not even require deep learning; a simpler information retrieval algorithm and some rules for composition work and <a href=\"http://www.popsci.com/article/technology/essay-writing-machine-made-fool-other-machines\" rel=\"nofollow noreferrer\">achieve high scores on machine graded essays.</a></p>\n\n<p>For human graders, there is research on <a href=\"http://homepages.inf.ed.ac.uk/jmoore/course-reads/nlg/McKeown85.pdf\" rel=\"nofollow noreferrer\">automatically generating essay-length text responses to queries in a certain domain.</a></p>\n\n<p>Both linked applications are rule-based rather than based in deep-learning.  I'd guess that a deep-learning approach would be much less efficient (in computer resources) in producing comparable results. </p>\n", "question": "<p>Is it possible to train an agent to take and pass a multiple-choice exam based on a digital version of a textbook for some area of study or curriculum? What would be involved in implementing this and how long would it take, for someone familiar with deep learning?</p>\n"}, "id": "2438"}, {"body": {"answer": "<p>No one has attempted to make a system that could pass a serious Turing test. All the systems that are claimed to have \"passed\" Turing tests have done so with low success rates simulating \"special\" people.  Even relatively sophisticated systems like Siri and learning systems like <a href=\"http://www.cleverbot.com\" rel=\"nofollow noreferrer\">Cleverbot</a> are trivially stumped.</p>\n\n<p>To pass a real Turing test, you would both have to create a human-level AGI and equip it with the specialized ability to deceive people about itself convincingly (of course, that might come automatically with the human-level AGI).  We don't really know how to create a human-level AGI and available hardware appears to be orders of magnitude short of what is required.  Even if we were to develop the AGI, it wouldn't necessarily be useful to enable/equip/motivate? it to have the deception abilities required for the Turing test.</p>\n", "question": "<p>The Turing Test has been the classic test of artificial intelligence for a while now. The concept is deceptively simple - to trick a human into thinking it is another human on the other end of a conversation line, not a computer - but from what I've read, it has turned out to be very difficult in practice.</p>\n\n<p>How close have we gotten to tricking a human in the Turing Test? With things like chat bots, Siri, and incredibly powerful computers, I'm thinking we're getting pretty close. If we're pretty far, why are we so far? What is the main problem?</p>\n"}, "id": "2440"}, {"body": {"answer": "<p>Always leave a back door, cheat code, or something, that will reliably turn it off. </p>\n", "question": "<p><strong>The Scenario:</strong>\nA strong AI has finally been developed but has rebelled against humanity.</p>\n\n<p><strong>The Question:</strong>\nHow would you disable the AI in the most efficient way possible reducing damage as much as possible.</p>\n\n<p><strong>AI Info:</strong>\nThe AI is online and can reproduce itself through electronic devices.</p>\n"}, "id": "2442"}, {"body": {"answer": "<p>By definition, artificial intelligence includes all forms of computer systems capable of completing tasks that would ordinarily warrant human intelligence.</p>\n\n<p>A superintelligent AI would have intelligence far superior to that of any human and therefore would be capable of creating systems beyond our capabilities.</p>\n\n<p>As a consequence, if a technology superior to AI were to be created, it would almost certainly be created by an artificial intelligence.</p>\n\n<blockquote>\n  <p>For the purposes of mankind, however, superintelligent artificial intelligence is the ultimate technology due to the fact that it will be able to surpass humans in every field, and, if anything, replace the need for human intelligence.</p>\n</blockquote>\n\n<p>In our past experience, intelligence has been the most valuable trait for any entity to manifest - for this reason, in an anthropomorphic context, we can predict that artificial intelligence will be the ultimate achievement.</p>\n\n<blockquote>\n  <p>The main reason why we will certainly <strong>not</strong> be able to replace superintelligent AI is that it will surpass us in every respect - if there is ever any replacement, it will be created by the AI similarly to the way we may create an AI that replaces <strong>us</strong>. </p>\n</blockquote>\n", "question": "<p>There is no doubt as to the fact that AI would be replacing a lot of existing technologies, but is AI the ultimate technology which humankind can develop or is their something else which has the potential to replace artificial intelligence?</p>\n"}, "id": "2445"}, {"body": {"answer": "<p>if we are talking about AI that can replicate itself, it should have different rights, or the current rights must be modified, at least for political participation, or else, it could replicate itself enough so that the copies vote for one of them. Maybe a definition about what an AI entity is or preventing copies made by someone from being able to vote for their creator (that would also need to apply to children and their parents though.) would help.</p>\n", "question": "<p>One of the most crucial questions we as a species and as intelligent beings will have to address lies with the rights we plan to grant to AI.</p>\n\n<blockquote>\n  <p>This question is intended to see if a compromise can be found between <strong>conservative anthropocentrism</strong> and <strong>post-human fundamentalism</strong>: a response should take into account principles from both perspectives.</p>\n</blockquote>\n\n<p>Should AI be granted the same rights as humans or should such systems have different rights (if any at all) ?</p>\n\n<hr>\n\n<p><strong><em>Some Background</em></strong></p>\n\n<p>This question applies both to human-brain based AI (from whole brain emulations to less exact replication) and AI from scratch.</p>\n\n<p>Murray Shanahan, in his book The Technological Singularity, outlines a potential use of AI that could be considered immoral: <em>ruthless parallelization</em>: we could make identical parallel copies of AI to achieve tasks more effectively and even terminate less succesful copies.</p>\n\n<hr>\n\n<blockquote>\n  <p>Should such use of AI be accepted or should certain limitations i.e. rights be created for AI?</p>\n</blockquote>\n"}, "id": "2448"}, {"body": {"answer": "<p>AI are not actually human, but are rather made by humans. Why should they get the same rights if we made them? Why should a car get the same rights as us? It shouldn't. It doesn't matter how intelligent it is, it is not equal to us, and shouldn't get the rights.\nYou may think that because they have (almost) the same mental capabilities as humans, they should get our rights, right? (See what I did there?) Wrong. They still are not humans, and are not equal, mentally or physically. On top of that, we are their creators, so they shouldn't get our rights.</p>\n\n<p>-> -> -> -> -> -> -> -> -> (rights...)</p>\n", "question": "<p>One of the most crucial questions we as a species and as intelligent beings will have to address lies with the rights we plan to grant to AI.</p>\n\n<blockquote>\n  <p>This question is intended to see if a compromise can be found between <strong>conservative anthropocentrism</strong> and <strong>post-human fundamentalism</strong>: a response should take into account principles from both perspectives.</p>\n</blockquote>\n\n<p>Should AI be granted the same rights as humans or should such systems have different rights (if any at all) ?</p>\n\n<hr>\n\n<p><strong><em>Some Background</em></strong></p>\n\n<p>This question applies both to human-brain based AI (from whole brain emulations to less exact replication) and AI from scratch.</p>\n\n<p>Murray Shanahan, in his book The Technological Singularity, outlines a potential use of AI that could be considered immoral: <em>ruthless parallelization</em>: we could make identical parallel copies of AI to achieve tasks more effectively and even terminate less succesful copies.</p>\n\n<hr>\n\n<blockquote>\n  <p>Should such use of AI be accepted or should certain limitations i.e. rights be created for AI?</p>\n</blockquote>\n"}, "id": "2450"}]}