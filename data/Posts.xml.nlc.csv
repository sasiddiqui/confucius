"""<p>What does ""backprop"" mean? I've Googled it, but it's showing backpropagation.</p><br><br><p>Is the ""backprop"" term basically the same as ""backpropagation"" or does it have a different meaning?</p><br>""",ai
"""<p>Does increasing the noise in data help to improve the learning ability of a network? Does it make any difference or does it depend on the problem being solved? How is it affect the generalization process overall?</p><br>""",ai
"""<p>When you're writing your algorithm, how do you know how many neurons you need per single layer? Are there any methods for finding the optimal number of them, or is it a rule of thumb?</p><br>""",ai
"""<p>I have a LEGO Mindstorms EV3 and I'm wondering if there's any way I could start coding the bot in Python rather than the default drag-and-drop system. Is a Mindstorm considered AI?</p><br><br><p>Is this possible?</p><br><br><hr><br><br><p>My goal is to write a basic walking program in Python. The bot is the EV3RSTORM. I searched and found <a href=""http://bitsandbricks.no/2014/01/19/getting-started-with-python-on-ev3/"" rel=""nofollow"">this</a>, but don't understand it. </p><br>""",ai
"""<p>The intelligent agent definition of intelligence states that an agent is intelligent if it acts so to maximize the expected value of a performance measure based on past experience and knowledge. (paraphrased from <a href=""http://en.wikipedia.org/wiki/Philosophy_of_artificial_intelligence#Intelligent_agent_definition"" rel=""nofollow"">Wikipedia</a>)</p><br><br><p>Does this mean that humans are not intelligent? I think we all make mistakes that imply that we are not maximizing the expected value of a performance measure.</p><br>""",ai
"""<p>This quote by Stephen Hawking has been in headlines for quite some time:</p><br><br><blockquote><br>  <p>Artificial Intelligence could wipe out humanity when it gets too clever as humans will be like ants.</p><br></blockquote><br><br><p>Why does he say this? To put it simply in layman terms: what are the possible threats from AI? If we know that AI is so dangerous why are we still promoting it? Why is it not banned?</p><br><br><p>What are the adverse consequences of the so called <a href=""https://en.wikipedia.org/wiki/Technological_singularity"" rel=""nofollow"">Technological Singularity</a>? </p><br>""",ai
"""<p>I'm new to A.I. and I'd like to know in simple words, what is the fuzzy logic concept? How does it help, and when is it used?</p><br>""",ai
"""<p>In particular, an embedded computer (limited resources) analyzes live video stream from a traffic camera, trying to pick good frames that contain license plate numbers of passing cars. Once a plate is located, the frame is handed over to an OCR library to extract the registration and use it further.</p><br><br><p>In my country two types of license plates are in common use - rectangular (the typical) and square - actually, somewhat rectangular but ""higher than wider"", with the registration split over two rows.</p><br><br><p>(there are some more types, but let us disregard them; they are a small percent and usually belong to vehicles that lie outside our interest.)</p><br><br><p>Due to the limited resources and need for rapid, realtime processing, the maximum size of the network (number of cells and connections) the system can handle is fixed.</p><br><br><p>Would it be better to split this into two smaller networks, each recognizing one type of registration plates, or will the larger single network handle the two types better?</p><br>""",ai
"""<p>The <a href=""https://en.wikipedia.org/wiki/Turing_test"">Turing Test</a> was the first test of artificial intelligence and is now a bit outdated. The <a href=""https://en.wikipedia.org/wiki/Turing_test#Total_Turing_test"">Total Turing Test</a> aims to be a more modern test which requires a much more sophisticated system. What techniques can we use to identify an artificial intelligence (weak AI) and an <a href=""https://en.wikipedia.org/wiki/Artificial_general_intelligence"">artificial general intelligence</a> (strong AI)?</p><br>""",ai
"""<p>What is the ""early stopping"" and what are the advantages using this method? How does it help exactly.</p><br>""",ai
"""<p>I've heard the idea of the technological singularity, what is it and how does it relate to Artificial Intelligence?  Is this the theoretical point where Artificial Intelligence machines have progressed to the point where they grow and learn on their own beyond what humans can do and their growth takes off?  How would we know when we reach this point?</p><br>""",ai
"""<p>I'm worry that my network become too complex. I don't want to end up with half of the network does nothing, but just take space and resources.</p><br><br><p>So, what are the techniques of detecting and preventing overfitting to avoid such problem?</p><br>""",ai
"""<p>I've seen emotional intelligence defined as the capacity to be aware of, control, and express one's emotions, and to handle interpersonal relationships judiciously and empathetically.  What are some strategies for artificial intelligence to begin to tackle this problem and develop emotional intelligence for computers?  Are there examples where this is already happening to a degree today?  For example, wouldn't a computer that passes a Turing test necessarily express emotional intelligence or it would be seen as an obvious computer?  Perhaps that is why early programs that pass the test represented young people, who presumably have lower emotional intelligence.</p><br>""",ai
"""<p>Since human intelligence presumably is a function of a natural genetic algorithm in nature, is using a genetic algorithm in a computer an example of artificial intelligence?  If not, how do they differ?  Or perhaps some are and some are not expressing artificial intelligence depending upon the scale of the algorithm and what it evolves into?</p><br>""",ai
"""<p>These two terms seem to be related, especially in their application in computer science and software engineering.  Is one a subset of another?  Is one a tool used to build a system for the other?  What are their differences and why are they significant?</p><br>""",ai
"""<p>What aspects of quantum computers, if any, can help to further develop Artificial Intelligence?</p><br>""",ai
"""<p>I believe a Markov chain is a sequence of events where each subsequent event depends probabilistically on the current event.  What are examples of the application of a Markov chain and can it be used to create artificial intelligence?  Would a genetic algorithm be an example of a Markov chain since each generation depends upon the state of the prior generation?</p><br>""",ai
"""<p>What purpose does the ""dropout"" method serve and how does it improve the overall performance of the neural network?</p><br>""",ai
"""<p>Can an AI program have an IQ?</p><br><br><p>In other words, can the IQ of an AI program be measured?</p><br><br><p>Like how humans can do an IQ test.</p><br>""",ai
"""<p>Why anybody would want to use the ""hidden layers""? How they enhance the learning ability of the network in comparison to the network which doesn't have them (linear models)?</p><br>""",ai
"""<p>When did research into Artificial Intelligence first begin?  Was it called Artificial Intelligence then or was there another name?</p><br>""",ai
"""<p>How would you estimate the generalisation error? What are the methods of achieving this?</p><br>""",ai
"""<p>I've implemented <a href=""https://en.wikipedia.org/wiki/Reinforcement_learning"" rel=""nofollow"">the reinforcement learning alogrithm</a> for an agent to play <a href=""https://github.com/admonkey/snappybird"" rel=""nofollow"">snappy bird</a> (a shameless cheap ripoff of flappy bird) utilizing a q-table for storing the history for future lookups. It works and eventually achieves perfect convergence after enough training.</p><br><br><p>Is it possible to implement a neural network to do function approximation in order to accomplish the purpose of the q-table? Obviously storage is a concern with the q-table, but it doesn't seem to ever train with the neural net alone. Perhaps training the nnet on an existing q-table would work, but I would like to not use a q-table at all if possible.</p><br>""",ai
"""<p>I read that in the spring of 2016 a computer <a href=""https://en.wikipedia.org/wiki/Computer_Go"" rel=""nofollow"">Go program</a> was finally able to beat a professional human for the first time.  Now that this milestone has been reached, does that represent a significant advance in artificial intelligence techniques or was it just a matter of even more processing power being applied to the problem?  What are some of the methods used to program the successful Go playing program, and are those methods considered to be artificial intelligence?</p><br>""",ai
"""<p>Who first coined the term Artificial Intelligence, is there a published research paper which is the first to use that term?</p><br>""",ai
"""<p>I have a background in Computer Engineering and have been working on developing better algorithms to mimic human thought. (One of my favorites is Analogical Modeling as applied to language processing and decision making.) However, the more I research, the more I realize just <em>how</em> complicated AI is.</p><br><br><p>I have tried to tackle many problems in this field, but sometimes I find that I am reinventing the wheel or am trying to solve a problem that has already been proven to be unsolvable (ie. the halting problem). So, to help in furthering AI, I want to better understand the current obstacles that are hindering our progress in this field.</p><br><br><p>For example, time and space complexity of some machine learning algorithms is super-polynomial which means that even with fast computers, it can take a while for the program to complete. Even still, some algorithms may be fast on a desktop or other computer while dealing with a small data set, but when increasing the size of the data, the algorithm becomes intractable.</p><br><br><p>What are other issues currently facing AI development?</p><br>""",ai
"""<p>I've read that the most of the problems can be solved with 1-2 hidden layers.</p><br><br><p>How do you know you need more than 2? For what kind of problems you would need them (as example)?</p><br>""",ai
"""<p>What were the first areas of research into Artificial Intelligence and what were some early successes?  More recently we've had:</p><br><br><ol><br><li>Beating a human at the game of chess</li><br><li>Convincing a human that a person was conversing with them (passing the Turing test)</li><br><li>Beating a human at Jeopardy game show</li><br><li>Beating a human at the game of go.</li><br></ol><br><br><p>Were there milestones that were considered major in the field before the 1990s?</p><br>""",ai
"""<p>Why somebody would use SAT solvers (<a href=""https://en.wikipedia.org/wiki/Boolean_satisfiability_problem"" rel=""nofollow"">Boolean satisfiability problem</a>) to solve their real world problems?</p><br><br><p>Are there any examples of the real uses of this model?</p><br>""",ai
"""<p>What designs for genetic algorithms are there, if they are classified differently and/or have different names, that leverage models for epigenetics in evolution? What are the pros/cons of the designs? Are there vast insufficiencies or wide-open questions about their usefulness? </p><br>""",ai
"""<p>Can a Convolutional Neural Network be used for pattern recognition in a problem domain where there are no pre-existing images, say by representing abstract data graphically? Would that always be less efficient?</p><br><br><p><a href=""https://youtu.be/py5byOOHZM8?t=815"">This developer</a> says current development could go further but not if there's a limit outside image recognition. </p><br>""",ai
"""<p>I've heard the terms strong-AI and weak-AI used.  Are these well defined terms or subjective ones?  How are they generally defined?</p><br>""",ai
"""<p>As AI gains capabilities, and becomes more prevalent in society, our legal system will encounter questions it has not encountered before.  For example, if a self-driving car is involved in an accident while being controlled by the AI, who is at fault?  The ""driver"" (who's really just a passenger), the programmer(s) who made the AI, or the AI itself?</p><br><br><p>So, what's on the cutting edge in terms of these kinds of issues at the intersection of law and artificial intelligence?</p><br>""",ai
"""<p>I know that language of <strong><code>Lisp</code></strong> was used early on when working on artificial intelligence problems.  Is it still being used today for significant work?  If not, is there a new language that has taken its place as the most common one being used for work in AI today?</p><br>""",ai
"""<p>What are the specific requirements of the Turing Test?</p><br><br><ul><br><li>What requirements if any must the evaluator fulfill in order to be qualified to give the test?</li><br><li>Must there always be two participants to the conversation (one human and one computer) or can there be more</li><br><li>Are placebo tests (where there is not actually a computer involbed) allowed or encouraged?</li><br><li>Can there be multiple evaluators? If so does the decision need to be unanimous among all evaluators in order for the machine to have passed the test?</li><br></ul><br>""",ai
"""<p>I believe that statistical AI uses inductive thought processes.  For example, deducing a trend from a pattern.  What are some examples of successfully applying statistical AI to real world problems.</p><br>""",ai
"""<p>How do the basic components <a href=""https://en.wikipedia.org/wiki/Optimality_theory"" rel=""nofollow"">optimality theory</a> apply to artificial intelligence?</p><br><br><p>How is optimality theory related to neural network research?</p><br>""",ai
"""<p>Some programs do exhaustive searches for a solution while others do heuristic searches.  For example, in chess, the search for the best next move tends to be more exhaustive in nature whereas in go, the search for the best next move tends to be more heuristic in nature due to the much larger search space.</p><br><br><p>Is the technique of brute force exhaustive searching for a good answer considered to be AI or is it generally required that heuristic algorithms be used before being deemed AI?  If so, is the chess playing computer beating a human professional seen as a meaningful milestone?</p><br>""",ai
"""<p>How is a neural network having the ""deep"" adjective actually distinguished from other similar networks?</p><br>""",ai
"""<p>What is the effectiveness of pre-training of unsupervised deep learning?</p><br><br><p>Does unsupervised deep learning actually work?</p><br>""",ai
"""<p>Are search engines considered AI because of the way they analyze what you search for and remember it? Or how they send you ads of what you've searched for recently? Is this considered AI or just smart?</p><br>""",ai
"""<p>The following <a href=""http://www.evolvingai.org/fooling"">page</a>/<a href=""http://www.evolvingai.org/files/DNNsEasilyFooled_cvpr15.pdf"">study</a> demonstrates that the deep neural networks are easily fooled by giving high confidence predictions for unrecognisable images, e.g.</p><br><br><p><a href=""http://i.stack.imgur.com/7pgrH.jpg""><img src=""http://i.stack.imgur.com/7pgrH.jpg"" alt=""Evolved images that are unrecognisable to humans, but that state-of-the-art DNNs trained on ImageNet believe with &gt;= 99.6% certainty to be a familiar object. This result highlights differences between how DNNs and humans recognise objects. Directly and indirectly encoded images""></a></p><br><br><p><a href=""http://i.stack.imgur.com/pBm48.png""><img src=""http://i.stack.imgur.com/pBm48.png"" alt=""Evolving images to match DNN classes produces a tremendous diversity of images. The mean DNN confidence scores for these images is 99.12% for the listed class, meaning that the DNN believes with near-certainty that the image is that type of thing. Shown are images selected to showcase diversity from 5 independent evolutionary runs. The images shed light on what the DNN network cares about, and what it does not, when classifying an image. For example, a school bus is alternating yellow and black lines, but does not need to have a windshield or wheels""></a></p><br><br><p>How this is possible? Can you please explain ideally in plain English?</p><br>""",ai
"""<p>In a feedforward neural network the inputs are fed directly to the outputs via a series of <strong>weights</strong>.</p><br><br><p>What purpose do the weights serve and how are they significant in this neural network?</p><br>""",ai
"""<p>I'm pretty sure this a noob-y question, but what is Deep Network? As of now it is the most popular tag on AI. Is there a reason for this? </p><br><br><hr><br><br><p>Please note, I am not asking how to distinguish a deep network from a neural network, I am simply asking for the definition of deep network.</p><br>""",ai
"""<p>I believe that Classical AI uses deductive thought processes. For example, given as a set of constraints, deduce a conclusion.  What are some examples of successfully applying Classical AI to real world problems.</p><br>""",ai
"""<p>In <a href=""https://youtu.be/oSdPmxRCWws?t=30"">this video</a> an expert says, ""One way of thinking about what intelligence is [specifically with regard to artificial intelligence], is as an optimization process.""</p><br><br><p>Can intelligence always be thought of as an optimization process, and can artificial intelligence always be modeled as an optimization problem? What about pattern recognition? Or is he mischaracterizing?</p><br>""",ai
"""<p>What specific advantages of declarative languages make them more applicable to AI than imperative languages?  What can declarative languages do easily that other languages styles find difficult for this kind of problem?</p><br>""",ai
"""<p>In years past, GOFAI (Good Old Fashioned AI) was heavily based on ""rules"" and <a href=""https://en.wikipedia.org/wiki/Symbolic_artificial_intelligence"" rel=""nofollow"">symbolic computation</a> based on rules.  Unfortunately, that approach ran into stumbling blocks, and the world moved heavily towards statistical / probabilistic approaches leading to the current wave of interest in ""machine learning"".</p><br><br><p>It seems though, that the symbolic / rule based approach probably still has application. So, could one ""learn"" rules using a probabilistic <a href=""https://en.wikipedia.org/wiki/Rule_induction"" rel=""nofollow"">rule induction</a> method, and then layer symbolic computation on top?  If so, how could the whole process be made truly two-way, so that something ""learned"" from processing rules, can be fed back into how the system learns rules? </p><br>""",ai
"""<p>Obviously driverless cars aren't perfect, so imagine that the Google car (as an example) got into difficult situation.</p><br><br><p>Here are a few examples of unfortunate situations caused by set of events:</p><br><br><ul><br><li>the car is heading toward a crowd of 10 people crossing the road, so it cannot stop in time, but it can avoid killing 10 people by hitting the wall (killing the passengers),</li><br><li>avoiding killing the rider of the motorcycle considering that the probability of survival is greater for the passenger of the car,</li><br><li>killing animal on the street in favour of human being,</li><br><li>changing lanes to crash into another car to avoid killing a dog,</li><br></ul><br><br><p>And here are few dilemmas:</p><br><br><ul><br><li>Does the algorithm recognize the difference between a human being and an animal?</li><br><li>Does the size of the human being or animal matter?</li><br><li>Does it count how many passengers it has vs. people in the front?</li><br><li>Does it ""know"" when babies/children are on board?</li><br><li>Does it take into the account the age (e.g. killing the older first)?</li><br></ul><br><br><p>How would an algorithm decide what should it do from the technical perspective? Is it being aware of above (counting the probability of kills), or not (killing people just to avoid its own destruction)?</p><br><br><p>Related articles:</p><br><br><ul><br><li><a href=""https://www.technologyreview.com/s/542626/why-self-driving-cars-must-be-programmed-to-kill/"">Why Self-Driving Cars Must Be Programmed to Kill</a></li><br><li><a href=""https://www.technologyreview.com/s/539731/how-to-help-self-driving-cars-make-ethical-decisions/"">How to Help Self-Driving Cars Make Ethical Decisions</a></li><br></ul><br>""",ai
"""<p>Which deep neural network is used in <a href=""https://en.wikipedia.org/wiki/Google_self-driving_car"" rel=""nofollow"">Google's driverless cars</a> to analyse the surroundings? Is this information is open?</p><br>""",ai
"""<p>Two common activation functions used in deep learning are the hyperbolic tangent function and the sigmoid activation function. I understand that the hyperbolic tangent is just a rescaling and translation of the sigmoid function (i,e tanh(z) = 2*sigma(z) - 1). </p><br><br><p>Is there a significant difference between these two activation functions, and in particular, <strong>when is one preferable to the other</strong>?</p><br><br><p>I realize that in some cases (like when estimating probabilities) outputs in the range of [0,1] are more convenient than outputs that range from [-1,1]. I want to know if there are differences <strong>other than convenience</strong> which distinguish the two activation functions.</p><br>""",ai
"""<p><a href=""http://ai.stackexchange.com/questions/10/what-is-fuzzy-logic"">Fuzzy logic</a> is the logic where every statement can have any real truth value between 0 and 1.</p><br><br><p>How can fuzzy logic be used in creating AI? Is it useful for certain decision problems involving multiple inputs? Can you give an example of an AI that uses it?</p><br>""",ai
"""<p>In <a href=""http://users.ox.ac.uk/~jrlucas/Godel/mmg.html"" rel=""nofollow"">Minds, Machines and Gödel</a> (1959), J. R. Lucas shows that any human mathematician can not be represented by an algorithmic automaton (a Turing Machine, but any computer is equivalent to it by the Church-Turing thesis), using Gödel's incompleteness theorem. </p><br><br><p>As I understand it, he states that since the computer is an algorithm and hence a formal system, Gödel's incompleteness theorem applies. But a human mathematician also has to work in a formal axiom system to prove a theorem, so wouldn't it apply there as well? </p><br>""",ai
"""<p>Back in college, I had a Complexity Theory teacher who stated that artificial intelligence was a contradiction in terms. If it could be calculated mechanically, he argued, it wasn't intelligence.</p><br><br><p>This seems to be a variant of the Chinese Room argument. This argument is a metaphor, where  a person is put in a room full of Chinese books. This person doesn't understand a word of Chinese, but is slipped messages in Chinese under the door. The person has to use the books, which contain transformation rules, to answer these messages. The person can apply the transformation rules, but does not understand what (s)he is communicating.</p><br><br><p>Does the chinese room argument hold? Can we argue that artificial intelligence is merely clever algorithmics?</p><br>""",ai
"""<p>What are the main differences between <a href=""https://en.wikipedia.org/wiki/Boltzmann_machine"" rel=""nofollow"">Deep Boltzmann Machines</a> (DBM) recurrent neural network and <a href=""https://en.wikipedia.org/wiki/Deep_belief_network"" rel=""nofollow"">Deep Belief Network</a> (which is based on RBMs)?</p><br>""",ai
"""<p>I'd like to learn more about the differences between <a href=""https://en.wikipedia.org/wiki/Cellular_automaton#Related_automata"" rel=""nofollow"">related automata</a> which can be based on hexagonal cells instead of squares (rule 34/2), like in <a href=""https://en.wikipedia.org/wiki/CoDi"" rel=""nofollow"">CoDi model</a> which uses spiking neural network (SNN). </p><br><br><p>Is using a plane tiled with regular <a href=""https://en.wikipedia.org/wiki/Hexagonal_tiling"" rel=""nofollow"">hexagons</a> is more efficient and reliable than using square cells? What is the difference and how do I know which one I should use in which scenario?</p><br><br><hr><br><br><p>In other words, more efficient in terms of flexibility that it learn to solve much more difficult problems and can be used for more scenarios (for me hexagonal sounds like it has more possibilities, because it can send/share the signal with/to more tiles). Or maybe one is more modern than the other, or both they're on the same level? In general, I'd like to learn the differences between them to know when I should use one over another. </p><br>""",ai
"""<p>An ultraintelligent machine is a machine that can surpass all intellectual activities by any human, and such machine is often used in science fiction as a machine that brings mankind to an end. </p><br><br><p>Any machine is executed using an algorithm. By the Church-Turing thesis, any algorithm that can be executed by a modern computer can be executed by a Turing Machine. However, a human can easily simulate a Turing Machine. Doesn't this mean that a machine can't surpass all intellectual activities, since we can also execute the algorithm?</p><br><br><p>This argument is most likely flawed, since my intuition tells me that  ultraintelligent machines are possible. However, it is not clear to me where the flaw is. Note that this is my own argument. </p><br>""",ai
"""<p>From Wikipedia:</p><br><br><blockquote><br>  <p>AIXI ['ai̯k͡siː] is a theoretical mathematical formalism for artificial general intelligence. It combines Solomonoff induction with sequential decision theory. AIXI was first proposed by Marcus Hutter in 2000[1] and the results below are proved in Hutter's 2005 book Universal Artificial Intelligence.[2]</p><br></blockquote><br><br><p>Albeit non-computable, approximations are possible, such as <em>AIXItl</em>. Finding approximations to AIXI could be an objective way for solving AI.</p><br><br><p>My question is: is <em>AIXI</em> really a big deal in artificial <em>general</em> intelligence research? Can it be thought as a central concept for the field? If so, why don't we have more publications on this subject (or maybe we have and I'm not aware of them)?</p><br>""",ai
"""<p>In what ways can connectionist artificial intelligence (neural networks) be integrated with <em>Good Old-Fashioned A.I.</em> (<em>GOFAI</em>)? For instance, how could deep neural networks be integrated with knowledge bases or logical inference? One such example seems to be the <a href=""http://wiki.opencog.org/w/DestinOpenCog"" rel=""nofollow"">OpenCog + Destin integration</a>.</p><br>""",ai
"""<p>It is proved that a recurrent neural net with rational weights can be a super-Turing machine. Can we achieve this in practice ?</p><br>""",ai
"""<p>Given the proven <a href=""https://en.wikipedia.org/wiki/Halting_problem"">halting problem</a> for <a href=""https://en.wikipedia.org/wiki/Turing_machine"">Turing machines</a>, can we infer limits on the ability of strong Artificial Intelligence?</p><br>""",ai
"""<p>By default using <a href=""https://en.wikipedia.org/wiki/DeepDream"" rel=""nofollow"">DeepDream</a> technique you can creating a dreamlike image out of two different images.</p><br><br><p>Is it possible to easily enhance this technique to generate one image out from three?</p><br>""",ai
"""<p>Consider these neural style algorithms which produce some art work:</p><br><br><ul><br><li><a href=""https://github.com/alexjc/neural-doodle"">Neural Doodle</a></li><br><li><a href=""https://github.com/jcjohnson/neural-style"">neural-style</a></li><br></ul><br><br><p>Why is generating such images so slow and why does it take huge amounts of memory? Isn't there any method of optimizing the algorithm?</p><br><br><p>What is the mechanism or technical limitation behind this? Why we can't have a realtime processing?</p><br><br><p>Here are few user comments (<a href=""https://www.reddit.com/r/deepdream/comments/3jwl76/how_anyone_can_create_deep_style_images/"">How ANYONE can create Deep Style images</a>):</p><br><br><ul><br><li><blockquote><br>  <p>Anything above 640x480 and we're talking days of heavy crunching and an insane amount of ram.</p><br></blockquote></li><br><li><blockquote><br>  <p>I tried doing a 1024pixel image and it still crashed with 14gigs memory, and 26gigs swap. So most of the VM space is just the swapfile. Plus it takes several hours potentially days cpu rendering this.</p><br></blockquote></li><br><li><blockquote><br>  <p>I tried 1024x768 and with 16gig ram and 20+ gig swap it was still dying from lack of memory.</p><br></blockquote></li><br><li><blockquote><br>  <p>Having a memory issue, though. I'm using the ""g2.8xlarge"" instance type.</p><br></blockquote></li><br></ul><br>""",ai
"""<p>Can autoencoders be used for supervised learning <em>without adding an output layer</em>? Can we simply feed it with a concatenated input-output vector for training, and reconstruct the output part from the input part when doing inference? The output part would be treated as missing values during inference and some imputation would be applied.</p><br>""",ai
"""<p>I'm aware that neural networks are probably not designed to do that, however asking hypothetically, is it possible to train the deep neural network (or similar) to solve math equations?</p><br><br><p>So given the 3 inputs: 1st number, operator sign represented by the number (1 - <code>+</code>, 2 - <code>-</code>, 3 - <code>/</code>, 4 - <code>*</code>, and so on), and the 2nd number, then after training the network should give me the valid results.</p><br><br><p>Example 1 (<code>2+2</code>):</p><br><br><ul><br><li>Input 1: <code>2</code>; Input 2: <code>1</code> (<code>+</code>); Input: <code>2</code>; Expected output: <code>4</code></li><br><li>Input 1: <code>10</code>; Input 2: <code>2</code> (<code>-</code>); Input: <code>10</code>; Expected output: <code>0</code></li><br><li>Input 1: <code>5</code>; Input 2: <code>4</code> (<code>*</code>); Input: <code>5</code>; Expected output: <code>25</code></li><br><li>and so</li><br></ul><br><br><p>The above can be extended to more sophisticated examples.</p><br><br><p>Is that possible? If so, what kind of network can learn/achieve that?</p><br>""",ai
"""<p>From Wikipedia:</p><br><br><blockquote><br>  <p>A mirror neuron is a neuron that fires both when an animal acts and when the animal observes the same action performed by another.</p><br></blockquote><br><br><p>Mirror neurons are related to imitation learning, a very useful feature that is missing in current real-world A.I. implementations. Instead of learning from input-output examples (supervised learning) or from rewards (reinforcement learning), an agent with mirror neurons would be able to learn by simply observing other agents, translating their movements to its own coordinate system. What do we have on this subject regarding computational models?</p><br>""",ai
"""<p>If I have a paragraph I want to summarize, for example:</p><br><br><blockquote><br>  <p>Ponzo and Fila went to the mall during the day. They walked for a long while, stopping at shops. They went to many shops. At first, they didn't buy anything. After going to a number of shops, they eventually bought a shirt, and a pair of pants.</p><br></blockquote><br><br><p>Better summarized as:</p><br><br><blockquote><br>  <p>They shopped at the mall today and bought some clothes.</p><br></blockquote><br><br><p>What is the best AI strategy to automate this process, if there is one? If there isn't, is it because it would be dependent on first having an external information resource that would inform any algorithm? Or is it because the problem is inherently contextual?</p><br>""",ai
"""<p>What happens if you apply the same <a href=""https://en.wikipedia.org/wiki/DeepDream"" rel=""nofollow"">deep dream technique</a> which produces ""dream"" visuals, but to media streams such as audio files?</p><br><br><p>Does changing image functions into audio and enhancing the logic would work, or it won't work or doesn't make any sense?</p><br><br><p>My goal is to create ""dream"" like audio based on the two samples.</p><br>""",ai
"""<p>With which neural network it is possible to scale the learning between the independent networks?</p><br><br><p>For example given the stream of images one network is trained to recognise cats, another dogs, and so on, all of them are talking to the main visual network which is responsible for making some decision and pass the analysis to the main ""brain"" network. Then another network of neural network are given the audio so each network can recognise specific pattern, then they talk to the common audio specific network which talks to the main ""brain"" network (mentioned before). In other words, something like a robot.</p><br><br><p>Which type of network would be the most suitable and scalable for such configuration, so you can easily extend it for additional separate network modules? Does it matter which type of deep network (or not deep) I should choose, or not?</p><br>""",ai
"""<p>In <a href=""https://en.wikipedia.org/wiki/DeepDream"" rel=""nofollow"">DeepDream</a> wikipedia page it's suggested that a dreamlike images created by a convolutional neural network may be related to how visual cortex works in humans when they're tripping.</p><br><br><blockquote><br>  <p>The imagery to LSD- and psilocybin-induced hallucinations is suggestive of a functional resemblance between artificial neural networks and particular layers of the visual cortex.</p><br></blockquote><br><br><p>How this is even possible?</p><br><br><p>How exactly convolutional neural networks have anything to do with human visual cortex?</p><br>""",ai
"""<p>This 2014 <a href=""https://medium.com/the-physics-arxiv-blog/first-demonstration-of-artificial-intelligence-on-a-quantum-computer-17a6b9d1c5fb"" rel=""nofollow"">article</a> saying that a Chinese team of physicists have trained a quantum computer to recognise handwritten characters.</p><br><br><p><strong>Why did they have to use a quantum computer</strong> to do that?</p><br><br><p>Is it just for fun and demonstration, or is it that recognising the handwritten characters is so difficult that standard (non-quantum) computers or algorithms cannot do that?</p><br><br><p>If standard computers can achieve the same thing, what are the benefits of using quantum computers to do that then over standard methods?</p><br>""",ai
"""<p>Is it possible that at some time in the future, AIs will be able to initiatively develop themselves, rather than passively being developed by humanity?</p><br>""",ai
"""<p>I have been wondering since a while ago about the <a href=""https://en.wikipedia.org/wiki/Theory_of_multiple_intelligences"" rel=""nofollow"">multiple intelligences</a> and how they could fit in the field of Artificial Intelligence as a whole.</p><br><br><p>We hear from time to time about <a href=""https://www.theguardian.com/artanddesign/jonathanjonesblog/2016/feb/08/leonardo-da-vinci-mechanics-of-genius-science-museum-london"" rel=""nofollow"">Leonardo</a> being a genius or <a href=""https://www.youtube.com/watch?v=xUHQ2ybTejU"" rel=""nofollow"">Bach's musical intelligence</a>. These persons are commonly said to be (have been) <em>more intelligent</em>. But the multiple intelligences speak about cooking or dancing or chatting as well, i.e. <em>coping with everyday tasks</em> (at least that's my interpretation).</p><br><br><p><strong>Are there some approaches on incorporating multiple intelligences into AI?</strong></p><br><br><hr><br><br><p><a href=""http://ai.stackexchange.com/questions/26/how-could-emotional-intelligence-be-implemented"">Related question - How could emotional intelligence be implemented?</a></p><br>""",ai
"""<p>Which is the preferred algorithm to build word vector for a given language?</p><br>""",ai
"""<p>How to decide the optimum number of layers to be created while implementing a Neural Network (Feedforward, back propagation or RNN)?</p><br>""",ai
"""<p>I am interested in the <a href=""https://en.wikipedia.org/wiki/Emergence"" rel=""nofollow"">emergence</a> of properties in <a href=""https://en.wikipedia.org/wiki/Agent-based_model#Theory"" rel=""nofollow"">agents</a>, and, more generally in robotics.</p><br><br><p>I was wondering if there is work on the emergence of time-related concepts, on the low-level representation of notions like <em>before</em> and <em>after</em>. I know, for example, that there is work on the emergence of <a href=""http://www.scholarpedia.org/article/Kohonen_network"" rel=""nofollow"">spatial representation</a> (similar to <a href=""https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm"" rel=""nofollow"">knn</a>), or even <a href=""https://infoscience.epfl.ch/record/129415/files/Mitrietal_1.pdf"" rel=""nofollow"">communication</a>* but time seems to be a tricky concept. </p><br><br><p>This has everything to do with the <em>platform</em>, i.e. the way that the representation would be coded in. We tend to favour ways that have some meaning or somehow mimic natural, well, yes, human structures, like the brain. I am not a neuroscientist and do not know that the sense of time <em>looks like</em> in humans, or if it is even present in other living beings.</p><br><br><p><strong>Is there some work on the (emergence of the) representation of <em>time</em> in artificial agents?</strong></p><br><br><hr><br><br><p>*I remember watching a really cool... Actually creepy video from these robots but cannot find it anymore. Does anyone have the link at hand?</p><br>""",ai
"""<p>Have there been proposed extensions to go beyond a Turing machine that solve the halting problem and if so, would those proposed extensions have value to advance strong Artificial Intelligence?  For example, does quantum computing go beyond the definition of a Turing machine and resolve the halting problem, and does that help in creating strong AI?</p><br>""",ai
"""<p>What was the first AI that was able to carry on a conversation, with real responses, such as in the famous <a href=""https://www.youtube.com/watch?v=WnzlbyTZsQY"" rel=""nofollow"">'I am not a robot. I am a unicorn' case?</a></p><br><br><p>A 'real response' constitutes a sort-of personalized answer to a specific input by a user.</p><br>""",ai
"""<p>This question stems from quite a few ""informal"" sources. Movies like <em>2001, A Space Odyssey</em> and <em>Ex Machina</em>; books like <em>Destination Void</em> (Frank Herbert), and others suggest that general intelligence <em>wants</em> to survive, and even learn the importance for it.</p><br><br><p>There may be several arguments for survival. What would be the most prominent?</p><br>""",ai
"""<p>Identifying sarcasm is considered as one of the most difficult open-ended problems in the domain of ML and NLP.</p><br><br><p>So, was there any considerable research done in that front? If yes, then what is the accuracy like? Please also explain the NLP model briefly.</p><br>""",ai
"""<p>I'd like to know more about <a href=""http://ai.stackexchange.com/q/26/8"">implementing emotional intelligence</a>.</p><br><br><p>Given I'm implementing a chat bot and I'd like to introduce the levels of curiosity to measure whether user text input is interesting or not.</p><br><br><p>High level would mean bot is asking more questions and is following the topic, lower level of curiosity makes the bot not asking any questions and changing the topics.</p><br><br><p>Less interesting content could mean the bot doesn't see any opportunity to learn something new or it doesn't understand the topic or doesn't want to talk about it, because of its low quality. </p><br><br><p>How this possibly can be achieved? Are there any examples?</p><br>""",ai
"""<p>I would like to learn more whether it is possible and how to write a program which decompiles executable binary (an object file) to the C source. I'm not asking exactly 'how', but rather how this can be achieved.</p><br><br><p>Given the following <code>hello.c</code> file (as example):</p><br><br><pre><code>#include &lt;stdio.h&gt;<br>int main() {<br>  printf(""Hello World!"");<br>}<br></code></pre><br><br><p>Then after compilation (<code>gcc hello.c</code>) I've got the binary file like:</p><br><br><pre><code>$ hexdump -C a.out | head<br>00000000  cf fa ed fe 07 00 00 01  03 00 00 80 02 00 00 00  |................|<br>00000010  0f 00 00 00 b0 04 00 00  85 00 20 00 00 00 00 00  |.......... .....|<br>00000020  19 00 00 00 48 00 00 00  5f 5f 50 41 47 45 5a 45  |....H...__PAGEZE|<br>00000030  52 4f 00 00 00 00 00 00  00 00 00 00 00 00 00 00  |RO..............|<br>00000040  00 00 00 00 01 00 00 00  00 00 00 00 00 00 00 00  |................|<br>00000050  00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  |................|<br>00000060  00 00 00 00 00 00 00 00  19 00 00 00 d8 01 00 00  |................|<br>00000070  5f 5f 54 45 58 54 00 00  00 00 00 00 00 00 00 00  |__TEXT..........|<br>$ wc -c hello.c a.out <br>  60 hello.c<br>8432 a.out<br></code></pre><br><br><p>For the learning dataset I assume I'll have to have thousands of source code files along with its binary representation, so algorithm can learn about moving parts on certain changes.</p><br><br><p>My concerns are:</p><br><br><ul><br><li>do my algorithm needs to be aware about the header file, or it's ""smart"" enough to figure it out,</li><br><li>if it needs to know about the header, how do I tell my algorithm 'here is the header file',</li><br><li>what should be input/output mapping (whether some section to section or file to file),</li><br><li>do I need to divide my source code into some sections,</li><br><li>do I need to know exactly how decompilers work or AI can figure it out for me,</li><br><li>or should I've two networks, one for header, another for body it-self,</li><br><li>or more separate networks, each one for each logical component (e.g. byte->C tag, etc.)</li><br></ul><br><br><p>How would you tackle this?</p><br>""",ai
"""<p>Text summarization is a long-standing research problem that was <em>""ignited""</em> by Luhn in 1958. However, a half century later, we still came nowhere close  to solving this problem (abstractive summarization). The reason for this might be because researchers are resorting to statistical (and sometimes linguistic) methods to find &amp; extract the most salient parts of the text.</p><br><br><p>Is summarization problem solvable using AI (neural networks to be precise)? </p><br>""",ai
"""<p>I'd like to know which common file format is more efficient in terms of simplicity and storage space for storing the state of artificial neural network.</p><br><br><p>I'm not talking about memory storage, but file storage, so the data can be loaded later on.</p><br><br><p>My first guess would be XML, but having millions of connections and weights would generate huge amount of data. Another thing would be to dump object instances into binary file using some export/serialize functions, but the disadvantage is that the file isn't common and it's language specific.</p><br><br><p>Are there any common file format standards which can be used for exporting huge artificial neural network into the file to be loaded by another program? If so, which one.</p><br>""",ai
"""<p>What AI techniques does IBM use for its Watson platform, specifically its natural language analysis?</p><br>""",ai
"""<p>I'm investigating the possibility of storing the semantic-lexical connections (such as the relationships to the other words such as phrases and other dependencies, its strength, part of speech, language, etc.) in order to provide analysis of the input text.</p><br><br><p>I assume this has been already done. If so, to avoid reinventing the wheel, is there any efficient method to store and manage such data in some common format which has been already researched and tested?</p><br>""",ai
"""<p>Which objective and measurable tests have been developed to test the intelligence of AI? </p><br><br><p>The classical test is the Turing Test, which has objective criteria and is measurable since it can be measured what percentage of the jury is fooled by the AI.</p><br><br><p>I am looking for other, more modern tests. </p><br>""",ai
"""<p>I'm interested in implementing a program for natural language processing (aka <a href=""https://en.wikipedia.org/wiki/ELIZA"" rel=""nofollow"">ELIZA</a>).</p><br><br><p>Assuming that I'm already <a href=""http://ai.stackexchange.com/q/212/8"">storing semantic-lexical connections</a> between the words and its strength.</p><br><br><p>What are the methods of dealing with words which have very distinct meaning?</p><br><br><p>Few examples:</p><br><br><ul><br><li><p>'Are we on the same page?'</p><br><br><p>The 'page' in this context isn't a document page, but it's part of the phrase.</p></li><br><li><p>'I'm living in Reading.'</p><br><br><p>The 'Reading' is a city (noun), so it's not a verb. Otherwise it doesn't make any sense. Checking for the capital letter would work in that specific example, but it won't work for other (like 'make' can be either verb or noun).</p></li><br><li><p>'I've read something on the Facebook wall, do you want to know what?'</p><br><br><p>The 'Facebook wall' has nothing to do with wall at all.</p></li><br></ul><br><br><p>In general, how algorithm should distinguish the word meaning and recognise the word within the context?</p><br><br><p>For example:</p><br><br><ul><br><li>Detecting the word for different type of speech, so it should recognise whether it's a verb or noun.</li><br><li>Detecting whether the word is part of phrase.</li><br><li>Detecting word for multiple meaning.</li><br></ul><br><br><p>What are the possible approaches to solve that problem in order to  identify the correct sense of a word with the context?</p><br>""",ai
"""<p>Unsupervised learning does not involve target values, so basically targets are most likely the same as the inputs (in other words, involves no target values).</p><br><br><p>So how does this model learn?</p><br>""",ai
"""<p>Currently, many different organizations do cutting-edge AI research, and some innovations are shared freely (at a time lag) while others are kept private. I'm referring to this state of affairs as 'multipolar,' where instead of there being one world leader that's far ahead of everyone else, there are many competitors who can be mentioned in the same breath. (There's not only one academic center of AI research worth mentioning, there might be particularly hot companies but there's not only one worth mentioning, and so on.)</p><br><br><p>But we could imagine instead there being one institution that mattered when it comes to AI (be it a company, a university, a research group, or a non-profit). This is what I'm referring to as ""monolithic."" Maybe they have access to tools and resources no one else has access to, maybe they attract the best and brightest in a way that gives them an unsurmountable competitive edge, maybe returns to research compound in a way that means early edges can't be overcome, maybe they have some sort of government coercion preventing competitors from popping up. (For other industries, network or first-mover effects might be other good examples of why you would expect that industry to be monolithic instead of multipolar.)</p><br><br><p>It seems like we should be able to use insights from social sciences like economics or organizational design or history of science in order to figure out, if not which path seems more likely, <em>how we would know</em> which path seems more likely.</p><br><br><p>(For example, we may be able to measure how much returns to research compound, in the sense of one organization coming up with an insight meaning that organization is likely to come up with the next relevant insight, and knowing this number makes it easier to figure out where the boundary between the two trajectories is located.)</p><br>""",ai
"""<p>One of the most compelling applications for AI would be in augmenting human biological intelligence. What are some of the currently proposed methods for doing this aside from vague notions such as ""nanobots swimming around our brains and bodies"" or ""electrodes connected to our skulls""?</p><br>""",ai
"""<p>Given list of fixed numbers from a mathematical constant such as Pi, is it is possible to train AI to attempt to predict the next numbers?</p><br><br><p>Which AI or neural network would be more suitable for this task? </p><br><br><p>Especially the one which will work without memorizing the entire training set, but the one which will attempt to find some patterns or statistical association.</p><br>""",ai
"""<p>What are the main differences between two types of feedforward networks such as <em>multilayer perceptrons</em> (MLP) and <em>radial basis function</em> (RBF)?</p><br><br><p>What are the fundamental differences between these two types?</p><br>""",ai
"""<p>According to my knowledge most of the current artificial intelligence study uses of some kind of neural network or its variants. A good example would be DeepMind's alphago which I believe is a deep neural network, for vision CNN, text, music and other ordered features RNN's, etc. But for machine learning application we have neural networks, support vector machines, random forest, regression methods, etc. available for applications. </p><br><br><p>So are neural networks and its variants the only way to reach ""true"" artificial intelligence? </p><br>""",ai
"""<p>I'm interested in hardware implementation of ANNs (artificial neural networks). Are there any popular existing technology implementations in form of microchips which are purpose designed to run artificial neural networks? For example, a chip which is optimised for an application like image recognition or something similar?</p><br>""",ai
"""<p>I've noticed that a few questions on this site mention genetic algorithms and it made me realize that I don't really know much about those.</p><br><br><p>I have heard the term before, but it's not something I've ever used, so I don't have much idea about how they work and what they are good for. All I know is that they involve some sort of evolution and randomly changing values.</p><br><br><p>Can you give me a short explanation, preferably including some sort of practical example that illustrates the basic principles?</p><br>""",ai
"""<p>In detective novels, the point is often that the reader gets enough information to solve the crime themselves. This ""puzzle"" aspect of detective novels is part of the attraction.</p><br><br><p>Often the difficulty for humans is to keep track of all the variables - events, items, motivations.<br><br>An AI would have an easier time keeping track of all the details, but would rely on real-world knowledge to prevent making crazy mistakes. For example, if it was stated that a character took the train, the AI would need to know that this is a method of transportation - that it changes the location property of an agent over time.</p><br><br><p>Has an AI ever been able to solve a detective mystery?</p><br>""",ai
"""<p>In 1969, Seymour Papert and Marvin Minsky showed that Perceptrons could not learn the XOR function.  </p><br><br><p>This was solved by the backpropagation network with at least one hidden layer. This type of network can learn the XOR function.</p><br><br><p>I believe I was once taught that every problem that could be learnt by a backpropagation neural network with multiple hidden layers, could also be learnt by a backpropagation neural network with a single hidden layer. (Although possible a nonlinear activation function was required).</p><br><br><p>However, it is unclear to me what the limits are to backpropagation neural networks themselves. Which patterns <strong>cannot</strong> be learnt by a backpropgation neural network?</p><br>""",ai
"""<p>Over the last 50 years, the rise/fall/rise in popularity of neural nets has acted as something of a 'barometer' for AI research.</p><br><br><p>It's clear from the questions on this site that people are interested in applying Deep Learning (DL) to a wide variety of difficult problems.</p><br><br><p>I therefore have two questions:</p><br><br><ol><br><li>Practitioners - What do you find to be the main obstacles to<br>applying DL 'out of the box' to your problem? </li><br><li>Researchers - What<br>techniques do you use (or have developed) that might help address<br>practical issues? Are they within DL or do they offer an<br>alternative approach?</li><br></ol><br>""",ai
"""<p>Is it possible for <em>unsupervised learning</em> to learn about high-level, class-specific features given only unlabelled images? For example detecting human or animal faces? If so, how?</p><br>""",ai
"""<p>On the Wikipedia page we can read the basic structure of an artificial neuron (a model of biological neurons) which consist:</p><br><br><ul><br><li>Dendrites - acts as the input vector,</li><br><li>Soma - acts as the summation function,</li><br><li>Axon - gets its signal from the summation behavior which occurs inside the soma.</li><br></ul><br><br><p>I've checked <a href=""https://en.wikipedia.org/wiki/Deep_learning"" rel=""nofollow"">Deep learning</a> wiki page, but I couldn't find any references to dendrites, soma or axons.</p><br><br><p>So my question is, which type of artificial neural network implements or can mimic such model most closely?</p><br>""",ai
"""<p>Have there been any studies which attempted to use AI algorithms to detect human thoughts or emotions based on brain activity, such as using <a href=""https://en.wikipedia.org/wiki/Brain%E2%80%93computer_interface#EEG-based"" rel=""nofollow"">BCI/EEG devices</a>?</p><br><br><p>By this, I mean simple guesses such as whether the person was happy or angry, or what object (e.g. banana, car) they were thinking about.</p><br><br><p>If so, did any of those studies show some degree of success?</p><br>""",ai
"""<p>Has there been any attempts to deploy AI with blockchain technology? </p><br><br><p>Are there any decentralized examples of AI networks with no central point of control with AI nodes acting independently (but according to a codified set of rules) creating, validating and storing the same shared decentralized database in many locations around the world?</p><br>""",ai
"""<p>In their famous book entitled ""<em>Perceptrons: An Introduction to Computational Geometry</em>"", Minsky and Papert show that a perceptron can't solve the XOR problem. This contributed to the first AI winter, resulting in funding cuts for neural networks. However, now we know that a multilayer perceptron can solve the XOR problem easily.</p><br><br><p>Backprop wasn't known at the time, but did they know about manually building multilayer perceptrons? Did Minsky &amp; Papert know that multilayer perceptrons could solve XOR at the time they wrote the book, albeit not knowing how to train it?</p><br>""",ai
"""<p>According to wikipedia <a href=""https://en.wikipedia.org/wiki/Artificial_general_intelligence"" rel=""nofollow"">Artificial general intelligence(AGI)</a></p><br><br><blockquote><br>  <p>Artificial general intelligence (AGI) is the intelligence of a<br>  (hypothetical) machine that could successfully perform any<br>  intellectual task that a human being can. </p><br></blockquote><br><br><p>According to below image todays artifical intellgence is same as that of a lizards.</p><br><br><p><a href=""http://i.stack.imgur.com/gddKB.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/gddKB.jpg"" alt=""enter image description here""></a></p><br><br><p>Lets assume(or not) that within 10-20 years we humans are successful in creating a AGI or AGIs. As AGI has the same intelligence and <a href=""http://futurism.com/scientist-claims-to-be-on-the-verge-of-making-an-ai-that-feels-true-emotions/"" rel=""nofollow"">emotions</a> as that of humans because according to wikipedia definition it can perform same intellectual task of a human. Then can we destroy an AGI without its consent? Do this be considered as murder?</p><br>""",ai
"""<p>Deep Mind has published a lot of works on deep learning in the last years, most of them state-of-the-art on their respective tasks. But how much of this work has actually been reproduced by the AI community? For instance, the Neural Turing Machine paper seems to be very hard to reproduce, according to other researchers.</p><br>""",ai
"""<p>Geoffrey Hinton has been researching something he calls ""capsules theory"" in neural networks. What is this and how does it work?</p><br>""",ai
"""<p>During my research, I've stumbled upon ""complex-valued neural networks"", which are neural networks that work with complex-valued inputs (probably weights too). What are the advantages (or simply the applications) of this kind of neural network over real-valued neural networks?</p><br>""",ai
"""<p>The <a href=""http://fabelier.org/novelty-search-and-open-ended-evolution-by-ken-stanley/"" rel=""nofollow"">author</a> claims that guiding evolution by novelty alone (without explicit goals) can solve problems even better than using explicit goals. In other words, using a novelty measure as a fitness function for a genetic algorithm works better than a goal-directed fitness function. How is that possible?</p><br>""",ai
"""<p>Quote from this <a href=""http://meta.ai.stackexchange.com/a/46/8"">Eric's meta post</a> about modelling and implementation:</p><br><br><blockquote><br>  <p>They are not exactly the same, although strongly related. This was a very difficult lesson to learn among mathematicians and early programmers, notably in the 70s (mathematical proofs can demand a lot of non-trivial programming work to make them ""computable"", as in runnable on a computer).</p><br></blockquote><br><br><p>If they're not the same, what is the difference?</p><br><br><p>How we can say when we're talking about AI implementation, and when about modelling? It's suggested above it's not easy task. So where we can draw the line when we talk about it?</p><br><br><p>I'm asking in general, not specifically for this site, that's why I haven't posted question in meta</p><br>""",ai
"""<p>Given pictures with multiple features such as faces, can single AI algorithm detect all of them, or for better reliability is it preferred to use separate instances?</p><br><br><p>In other words I'm talking about attempt of finding all possible human faces on the same picture by a single neural network.</p><br>""",ai
"""<p>I read some information<sup>1</sup> about attempts to build neural networks in the PHP programming language. Personally I think PHP is not the right language to do so at all probably because it's a high-level language, I assume low level language are way more suitable for AI in terms of performance and scalability. </p><br><br><p>Is there a good/logical reason why you should or shouldn't use PHP as a language to write AI in?</p><br><br><p><em><sup>1</sup></em> <a href=""http://www.developer.com/lang/php/creating-neural-networks-in-php.html"" rel=""nofollow"">http://www.developer.com/lang/php/creating-neural-networks-in-php.html</a> and <a href=""http://stackoverflow.com/questions/2303357/are-there-any-artificial-intelligence-projects-in-php-out-there"">http://stackoverflow.com/questions/2303357/are-there-any-artificial-intelligence-projects-in-php-out-there</a> </p><br>""",ai
"""<p>I've found <a href=""http://link.springer.com/chapter/10.1007%2F978-1-4613-1009-9_2"" rel=""nofollow"">this old scientific paper from 1988</a> about introduction of AI into nuclear power fields.</p><br><br><p>Were or still are there any dangers by application of such algorithm? Are nuclear power plants or human life in risk if the algorithm will fail?</p><br><br><p>Especially applications to the core, like cooling systems and other components which can be affected in negative way.</p><br>""",ai
"""<p>How likely AI can fully replace pilots on commercial flights (including take off, landing and parking)?</p><br><br><p>Since we've self-driving cars already, is it likely to happen to commercial planes as well?</p><br>""",ai
"""<p>How much processing power is needed to emulate the human brain? More specifically, the neural simulation, such as communication between the neurons and processing certain data in real-time.</p><br><br><p>I understand that this may be a bit of speculation and it's not possible to be accurate, but I'm sure there is some data available or research studies which attempted to estimate it based on our current understanding of the human brain.</p><br>""",ai
"""<p><strong>The Situation:</strong><br>A self-driving car is traveling at it's maximum speed, 25 mph (40 km/h), in the middle of an empty street with the ability to change lanes on both sides. There are two passengers, one in the front and another in the back.</p><br><br><p>Someone jumps from the side of the road directly into the path of the car. A collision would occur in 50 meters. <a href=""http://www.brake.org.uk/rsw/15-facts-a-resources/facts/1255-speed"" rel=""nofollow"">Breaking distance</a> at this speed is about 24m.</p><br><br><p><strong>The Question:</strong> Is it known how the current implementation of the Google Car AI would react, or is it currently a matter of speculation? A step-by-step explanation of the AI's decisioning process would be preferred.</p><br><br><p><strong>Possible Answers:</strong> The car could activate its brakes immediately, coming to a halt as quickly as possible. This would be sooner than a human could stop, as people require time to recognize the possibility of a collision, and then physically slam on the brake. (<em>thinking distance</em>).</p><br><br><p>Alternatively, the car could continue traveling forward, processing the situation. (Similar to a humans <em>thinking distance</em>). The person may continue to move, either out of the way, or still into danger of being hit. In this case, the car may decide to change lanes in an attempt to pass around the person.</p><br><br><p>Lastly and most unlikely, the car will not alter its course and proceed to drive forward.</p><br><br><p><sup>Do not attempt to do it to check;)</sup></p><br>""",ai
"""<p>Artificial intelligence is present in many games, both current and older games. How can such intelligence understand what to do? I mean, how can it behave like a human in a game, allowing you to play against itself, or that AI plays against itself?</p><br><br><p>In games like Age of Empires, for example.</p><br>""",ai
"""<p><a href=""http://cs.stackexchange.com/a/60535/54605"">At a related question in Computer Science SE</a>, a user told:</p><br><br><blockquote><br>  <p>Neural networks typically require a large training set.</p><br></blockquote><br><br><p>Is there a way to define the boundaries of the ""optimal"" size of a training set in general case?</p><br><br><p>When I was learning about fuzzy logic, I've heard some rules of thumb that involved examining the mathematical composition of the problem and using that to define the number of fuzzy sets.</p><br><br><p>Is there such a method that can be applicable for an already defined neural network topology? </p><br>""",ai
"""<p>How important is true (non-<a href=""https://en.wikipedia.org/wiki/Pseudorandomness"" rel=""nofollow"" title=""pseudo"">pseudo</a>) randomness in Artificial Intelligence designs? Is there any chance that pseudo-randomness could be a barrier to more successful designs?</p><br>""",ai
"""<p>Complex AI that learns lexical-semantic content and its meaning (such as collection of words, their structure and dependencies) such as <em>Watson</em> takes terabytes of disk space.</p><br><br><p>Lets assume <em>DeepQA</em>-like AI consumed whole Wikipedia of size 10G which took the same amount of structured and unstructured stored content.</p><br><br><p>Will learning another 10G of different encyclopedia (different topics in the same language) take the same amount of data? Or will the AI reuse the existing structured and take less than half (like 1/10 of it) additional space?</p><br>""",ai
"""<p>Is there any simple explanation how <em>Watson</em> finds and scores evidence after gathering massive evidence and analyzing the data?</p><br><br><p>In other words, how does it know which precise answer it needs to return?</p><br>""",ai
"""<p>Isaac Asimov's famous <a href=""https://en.wikipedia.org/wiki/Three_Laws_of_Robotics"">Three Laws of Robotics</a> originated in the context of Asimov's science fiction stories. In those stories, the three laws serve as a safety measure, in order to avoid untimely or manipulated situations from exploding in havoc.</p><br><br><p>More often than not, Asimov's narratives would find a way to break them, leading the writer to make several modifications to the laws themselves. For instance, in some of his stories, he <a href=""https://en.wikipedia.org/wiki/Three_Laws_of_Robotics#First_Law_modified"">modified the First Law</a>, <a href=""https://en.wikipedia.org/wiki/Three_Laws_of_Robotics#Zeroth_Law_added"">added a Fourth (or Zeroth) Law</a>, or even <a href=""https://en.wikipedia.org/wiki/Three_Laws_of_Robotics#Removal_of_the_Three_Laws"">removed all Laws altogether</a>.</p><br><br><p>However, it is easy to argue that, in popular culture, and even in the field of AI research itself, the Laws of Robotics are taken quite seriously. Ignoring the side problem of the different, subjective, and mutually-exclusive interpretations of the laws, are there any arguments proving the laws themselves intrinsically flawed by their design, or, alternatively, strong enough for use in reality? Likewise, has a better, stricter security heuristics set being designed for the purpose?</p><br>""",ai
"""<p>Are there any modern techniques of generating <strong>textual</strong> CAPTCHA (so person needs to type the right text) challenges which can easily <a href=""http://ai.stackexchange.com/q/92/8"">fool AI</a> with some visual obfuscation methods, but at the same time human can solve them without any struggle?</p><br><br><p>For example I'm talking about plain ability of <strong>recognising text embedded into image</strong> (without considering any external plugins like flash or java, image classification, etc.) and re-typing the text that has been written or something similar.</p><br><br><p>I guess adding noise, gradient, rotating letters or changing colours are not reliable methods any more, since they can be quickly broken.</p><br><br><p>Any suggestions or research has been done?</p><br>""",ai
"""<p>Can an AI program have an EQ (Emotional intelligence or emotional quotient)?</p><br><br><p>In other words, can the EQ of an AI program be measured?</p><br><br><p>If EQ is more problematic to measure than IQ (at least with a standard applicaple to both humans and AI programs), why is that the case?</p><br>""",ai
"""<p>I have heard about this concept in a reddit post about Alpha Go. I have trued to go through the paper and the article, but could not really make sense of the algorithm.</p><br><br><p>So, can someone give a easy-to-understand explanation of how the Monte-Carlo search algorithm work and how is it being used in building game-playing AI bots?</p><br>""",ai
"""<p>DNNs are typically used to classify things (of course) but can we let them go wild with sounds and then tell them if we think it sounds good or not? I'd like to think after a training class has been made (perhaps comparing the output to an existing song) we could get an NN that has a basic concept of music.</p><br><br><p>Timing would be an issue; I'm not sure how feasible this is. A strongly weighted input attached to all hidden layers perhaps? Use it as the bias?</p><br><br><p>Is this even slightly feasible? </p><br>""",ai
"""<p>How do I avoid my gradient descent algorithm into falling into the ""local minima"" trap while backpropogating on my neural network?</p><br><br><p>Are there any methods which help me avoid it?</p><br>""",ai
"""<p>A neural network is a directed weighted graph. These can be represented by a (sparse) matrix. Doing so can expose some elegant properties of the network.</p><br><br><p>Is this technique beneficial for examining neural networks?</p><br>""",ai
"""<p>Would it be ethical to implement AI for self-defence for public walking robots which are exposed to dangers such as violence and crime such as robbery (of parts), damage or abduction?</p><br><br><p>What would be pros and cons of such AI behavior? Is it realistic, or it won't be taken into account for some obvious reasons?</p><br><br><p>Like pushing back somebody when somebody start pushing it first (AI will say: he pushed me first), or running away on crowded street in case algorithm will detect risk of abduction.</p><br>""",ai
"""<p>Is there any risk in the near future of replacing all encyclopedias with Watson-like AI where knowledge is accessible by everybody through <a href=""https://watson-api-explorer.mybluemix.net/"" rel=""nofollow"">API</a>?</p><br><br><p><sup>Something similar happened in the future in <a href=""https://en.wikipedia.org/wiki/The_Time_Machine_(2002_film)"" rel=""nofollow""><strong>The Time Machine</strong> movie from 2002</a>.</sup></p><br><br><p>Obviously maintaining 40 million articles and keeping it up-to-date and consistent could be beyond brain power of few thousands of active editors. Not to mention thousands of other encyclopedias including paperback version or large number of books used by universities which needs to be updated every year by a huge number of people.</p><br><br><p>What are the pros and cons of such a change?</p><br>""",ai
"""<p>I've watched the <a href=""https://www.youtube.com/watch?v=LY7x2Ihqjmc"" rel=""nofollow"">Sunspring</a> video which didn't make any sense to me (a lot of nonsense monologues), mainly because it was created by Jetson AI.</p><br><br><p>What was the mechanism of creating such screenplay?</p><br><br><p>On what criteria was it trained? What was the goal or motivation in terms of training criteria of defining when text does make sense? And what was missed (that it's so bad) and how possibly this could be improved?</p><br>""",ai
"""<p>This <a href=""http://blog.claymcleod.io/2016/06/01/The-truth-about-Deep-Learning/"" rel=""nofollow"">article</a> suggests that deep learning is not designed to produce the universal algorithm and cannot be used to create such a complex systems.</p><br><br><p>First of all it requires huge amounts of computing power, time and effort to train the algorithm the right way and adding extra layers doesn't really help to solve complex problems which cannot be easily predicted.</p><br><br><p>Secondly some tasks are extremely difficult or impossible to solve using DNN, like solving a <a href=""http://ai.stackexchange.com/q/154/8"">math</a> equations, predicting <a href=""http://ai.stackexchange.com/q/225/8"">pseudo-random lists</a>, <a href=""http://ai.stackexchange.com/q/168/8"">fluid mechanics</a>, guessing encryption algorithms, or <a href=""http://ai.stackexchange.com/q/205/8"">decompiling</a> unknown formats, because there is no simple mapping between input and output.</p><br><br><p>So I'm asking, are there any alternative learning algorithms as powerful as deep architectures for general purpose problem solving? Which can solve more variety of problems, than ""deep"" architectures cannot?</p><br>""",ai
"""<p>Is there any research which study application of AI into chemistry which can predict the output of certain chemical reactions.</p><br><br><p>So for example, you train the AI about current compounds, substances, structures and their products and chemical reactions from the existing <a href=""http://opendata.stackexchange.com/q/3553/3082"">dataset</a> (basically what produce what). Then you give the task to find how to create a gold or silver from group of available substances. Then the algorithm will find the chemical reactions (successfully predicting new one which weren't in the dataset) and gives the results. Maybe the gold is not a good example, but the practical scenario would be creation of drugs which are cheaper to create by using much more simpler processes or synthesizing some substances for the first time for drug industries.</p><br><br><p>Was there any successful research attempting to achieve that using deep learning algorithms?</p><br>""",ai
"""<p>Assume that I want to solve an issue with neural network that either I can't fit to already existing topologies (perceptron, Konohen, etc) or I'm simply not aware of the existence of those or I'm unable to understand their mechanics and I rely on my own instead.</p><br><br><p>How can I deconstruct a problem to find a corresponding neural network topology? By this I don't mean only the size of certain layers, but the number of them, the type of activation functions, the number and the direction of connections, and so on.</p><br><br><p>I'm a beginner, yet I realized that in some topologies (or, at least in perceptrons) it is very hard if not impossible to understand the inner mechanics as the neurons of the hidden layers don't express any mathematically meaningful context.</p><br>""",ai
"""<p>For example there is <a href=""https://en.wikipedia.org/wiki/MNIST_database"" rel=""nofollow"">the MNIST database</a> which is used to test artificial neural network (ANN), however it's not so challenging, because some hierarchical systems of convolutional neural networks manages to get an error rate of 0.23 percent.</p><br><br><p>Are there any similar, especially the most challenging tasks with dataset which are used as benchmark tests to challenge the AI which are fairly reliable and it's possible to pass, but most AAN are struggling to achieve the lower error rate?</p><br>""",ai
"""<p>This <a href=""http://repository.supsi.ch/5145/1/IDSIA-04-12.pdf"" rel=""nofollow"">study</a> (pages 7-8) shows an attempt at recognizing the traffic signs with lower error rates by using multi-column deep neural networks </p><br><br><p>Are Google cars using similar techniques of predicting signs using DNN, or are they using some other method?</p><br>""",ai
"""<p>I'd like to know whether there were attempts to simulate the whole brain, I'm not talking only about some <a href=""http://ai.stackexchange.com/q/237/8"">ANN on microchips</a>, but brain simulations.</p><br>""",ai
"""<p>On <a href=""https://en.wikipedia.org/wiki/Artificial_intelligence"">the wikipedia page</a> about AI, we can read:</p><br><br><blockquote><br>  <p>Optical character recognition is no longer perceived as an exemplar of ""artificial intelligence"" having become a routine technology.</p><br></blockquote><br><br><p>On the other hand, the <a href=""https://en.wikipedia.org/wiki/MNIST_database"">MNIST</a> database of handwritten digits is especially designed for training and testing neural networks and their error rates (see: <a href=""https://en.wikipedia.org/wiki/MNIST_database#Classifiers"">Classifiers</a>).</p><br><br><p>So why does the above quote state that OCR is no longer exemplar of AI?</p><br>""",ai
"""<p><a href=""https://en.wikipedia.org/wiki/Minimum_intelligent_signal_test"" rel=""nofollow"">MIST</a> is a quantiative test of humanness, consisting of ~80k propositions such as:</p><br><br><ul><br><li>Is Earth a planet?</li><br><li>Is the sun bigger than my foot?</li><br><li>Do people sometimes lie?</li><br><li>etc.</li><br></ul><br><br><p>Have any AI attempted and passed this test to date?</p><br>""",ai
"""<p>It is possible of normal code to prove that it is correct using mathematical techniques, and that is often done to ensure that some parts are bug-free. </p><br><br><p>Can we also prove that a piece of code in AI software will cause it to never turn against us, i.e. that the AI is <a href=""https://en.wikipedia.org/wiki/Friendly_artificial_intelligence"" rel=""nofollow"">friendly</a>? Has there any research been done towards this?</p><br>""",ai
"""<p>In <a href=""http://arxiv.org/pdf/1606.00652.pdf"" rel=""nofollow"">this paper</a>, a proposal is given for what death could mean for Artificial Intelligence. </p><br><br><p>What does this mean using English only? I understand that mathematical notation is useful for giving a precise definition, but I'd like to understand what the definition really means. </p><br>""",ai
"""<p>We can measure the power of the machine with the number of operation per second or the frequency of the processor. But does units similar of IQ for humans exist for a AI?<br/><br>I'm asking for a unit which can give countable result so something different from a Turing Test which only give a binary result.</p><br>""",ai
"""<p>In the mid 1980s, Rodney Brooks famously created the foundations of ""the new AI"". The central claim was that the symbolist approach of 'Good Old Fashioned AI' (GOFAI) had failed by attempting to 'cream cognition off the top', and that <em>embodied cognition</em> was required, i.e. built from the bottom up in a 'hierarchy of competances' (e.g. basic locomotion -> wandering around -> actively foraging) etc.</p><br><br><p>I imagine most AI researchers would agree that the 'embodied cognition' perspective has now (at least tacitly) supplanted GOFAI as the mainstream.</p><br><br><p>My question takes the form of a thought experiment and asks: ""Which (if any)  aspects of 'embodied' can be relaxed/omitted before we lose something essential for AGI?""</p><br>""",ai
"""<p>In other words, which existing reinforcement method learns in fewest episodes? <a href=""http://www.jmlr.org/papers/volume3/brafman02a/brafman02a.pdf"" rel=""nofollow"">R-Max</a> comes to mind, but its very old and I'd like to know if there is something better now.</p><br>""",ai
"""<p>Are there any research teams which attempted to create or have already created an AI robot which can be as close to intelligent as these found in <a href=""https://en.wikipedia.org/wiki/Ex_Machina_(film)""><em>Ex Machina</a></em> or <em><a href=""https://en.wikipedia.org/wiki/I,_Robot_(film)"">I, Robot</em></a> movies?</p><br><br><p>I'm not talking about full awareness, but an artificial being which can make its own decisions and physical and intellectual tasks that a human being can do?</p><br>""",ai
"""<p>We, humans, during following multiple processes (e.g. reading while listening to music) memorize information from less focused sources with worse efficiency than we do from our main concentration.</p><br><br><p>Do such things exist in case of artificial intelligences? I doubt, for example that neural networks obtain such features, but I may be wrong.</p><br>""",ai
"""<p>How can a swarm of small robots (like Kilobots) walking close to each other achieve collaboration without bumping into each other? For example, one study shows <a href=""http://science.sciencemag.org/content/345/6198/795.abstract"" rel=""nofollow"">programmable self-assembly in a thousand-robot swarm</a> (see <a href=""http://robohub.org/thousand-robot-swarm-self-assembles-into-arbitrary-shapes/"" rel=""nofollow"">article</a> &amp; <a href=""https://vimeo.com/103329200"" rel=""nofollow"">video</a>) which are moving without GPS-like system and by measuring distances to neighbours. This was achieved, because the robots were very slow.</p><br><br><p>Is there any way that similar robots can achieve much more efficient and quicker assembly by using more complex techniques of coordination? Not by walking around clock-wise (which I guess was the easiest way), but I mean using some more sophisticated way. Because waiting half a day (~11h) to create a simple star shape using a thousand-robot swarm is way too long!</p><br>""",ai
"""<p>On Watson wiki page we can read:</p><br><br><blockquote><br>  <p>In healthcare, Watson's natural language, hypothesis generation, and evidence-based learning capabilities allow it to function as a clinical decision support system for use by medical professionals.</p><br></blockquote><br><br><p>How exactly such AI can help doctors to diagnose the diseases?</p><br>""",ai
"""<p>Recently White House published the article: <a href=""https://www.whitehouse.gov/blog/2016/05/03/preparing-future-artificial-intelligence"" rel=""nofollow"">Preparing for the Future of Artificial Intelligence</a> which says that government is working to leverage AI for public good and toward a more effective government.</p><br><br><p>I'm especially interested how AI can help with computational sustainability, environmental management and Earth's ecosystem such as biological conservation?</p><br>""",ai
"""<p>When AI has some narrow domain such as chess where it can outperform the world's human masters of chess, does it make it a superintelligence or not?</p><br>""",ai
"""<p>Suppose my goal is to collaborate and create an advanced AI, for instance one that resembles a human being and the project would be on the frontier of AI research, what kind of skills would I need?</p><br><br><p>I am talking about specific things like what university program should I complete to enter and be competent in the field. Here are some of the things that I thought about, just to exemplify what I mean:</p><br><br><ul><br><li>Computer sciences: obviously the AI is built on computers, it wouldn't hurt to know how computers work, but some low level stuff and machine specific things does not seem essential, I may be wrong of course.</li><br><li>Psychology: if AI resembles human beings, knowledge of human cognition would probably be useful, although I do not imagine neurology on a cellular level or complicated psychological quirks typical to human beings like the Oedipus complex would be relevant, but again, I may be wrong.</li><br></ul><br>""",ai
"""<p><a href=""https://www.whitehouse.gov/webform/rfi-preparing-future-artificial-intelligence"" rel=""nofollow"">White House published the information</a> about AI which requests mentions about 'the most important research gaps in AI that must be addressed to advance this field and benefit the public'.</p><br><br><p>What are these exactly?</p><br>""",ai
"""<p>Is there any methods by which artificial intelligence use recursion(s) to solve a certain issue or to keep up working and calculating?</p><br>""",ai
"""<p>The Von Neumann's <a href=""https://en.wikipedia.org/wiki/Minimax_theorem"" rel=""nofollow"">Minimax theorem</a> gives the conditions that make the <a href=""https://en.wikipedia.org/wiki/Max%E2%80%93min_inequality"" rel=""nofollow"">max-min inequality</a> an equality.</p><br><br><p>I understand the max-min inequality, basically <code>min(max(f))&gt;=max(min(f))</code>.</p><br><br><p>The Von Neumann's theorem states that, for the inequality to become an equality <code>f(.,y)</code> should always be convex for given y and <code>f(x,.)</code> should always be concave for given x, which also makes sense.</p><br><br><p><a href=""https://www.youtube.com/watch?v=m-EewaiFhF0&amp;list=PLAwxTw4SYaPnidDwo9e2c7ixIsu_pdSNp&amp;index=61"" rel=""nofollow"">This video</a> says that for a zero-sum perfect information game, the Von Neumann's theorem always holds, so that minimax always equal to maximin, which I did not quite follow.</p><br><br><p><strong>Questions</strong><br><br>Why zero-sum perfect information games satisfy the conditions of Von Neumann's theorem?<br><br>If we relax the rules to be non-zero-sum or non-perfect information, how would the conditions change?</p><br>""",ai
"""<p>In October 2014, Dr. Mark Riedl published an approach to testing AI intelligence, called <a href=""http://arxiv.org/pdf/1410.6142v3.pdf"" rel=""nofollow"">the ""Lovelace Test 2.0""</a>, after being inspired by the <a href=""http://kryten.mm.rpi.edu/lovelace.pdf"" rel=""nofollow"">original Lovelace Test</a> (published in 2001). Mark believed that the original Lovelace Test would be impossible to pass, and therefore, suggested a weaker, and more practical version.</p><br><br><p>The Lovelace Test 2.0 makes the assumption that for an AI to be intelligent, it must exhibit creativity. From the paper itself:</p><br><br><blockquote><br>  <p>The Lovelace 2.0 Test is as follows: artificial agent a is challenged as follows:</p><br>  <br>  <ul><br>  <li><p>a must create an artifact o of type t;</p></li><br>  <li><p>o must conform to a set of constraints C where ci ∈ C is<br>  any criterion expressible in natural language;</p></li><br>  <li><p>a human evaluator h, having chosen t and C, is satisfied<br>  that o is a valid instance of t and meets C; and</p></li><br>  <li><p>a human referee r determines the combination of t and C<br>  to not be unrealistic for an average human.</p></li><br>  </ul><br></blockquote><br><br><p>Since it is possible for a human evaluator to come up with some pretty easy constraints for an AI to beat, the human evaluator is then expected to keep coming up with more and more complex constraints for the AI until the AI fails. The point of the Lovelace Test 2.0 is to <em>compare</em> the creativity of different AIs, not to provide a definite dividing line between 'intelligence' and 'nonintelligence' like the Turing Test would.</p><br><br><p>However, I am curious about whether this test has actually been used in an academic setting, or it is only seen as a thought experiment at the moment. The Lovelace Test seems easy to apply in academic settings (you only need to develop some measurable constraints that you can use to test the artificial agent), but it also may be too subjective (humans can disagree on the merits of certain constraints, and whether a creative artifact produced by an AI actually meets the final result).</p><br>""",ai
"""<p>Convolutional neural network are leading type of feed-forward artificial neural network for image recognition. Can they be used for real-time image recognition for videos (frame by frame), or it takes too much processing (assuming they're written in C-like language)?</p><br><br><p>For example for classification of type of animals based on the training from huge dataset.</p><br>""",ai
"""<p>Just for the purpose of learning I'd like to classify the likeliness of a tweet being in aggressive language or not. </p><br><br><p>I was wondering how to approach the problem. I guess I need first train my neural network on a huge dataset of text what aggressive language is. This brings up the question where I would get this data in the first place?</p><br><br><p>It feels a bit like the chicken and egg problem to me so I wonder how would I approach the problem?</p><br>""",ai
"""<p>Siri and Cortana communicate pretty much like humans. Unlike Google now which mainly gives us search results when asked some questions (not setting alarms or reminders), Siri and Cortana provide us with an answer, in the same way that a person would do.<br><br>So are they actual AI programs or not?</p><br><br><p>(By ""question"" I don't mean any academic related question or asking routes/ temperature, but rather opinion based question). </p><br>""",ai
"""<p>The question is pretty much the title.</p><br><br><p>Basically what is the difference between AI and robots?</p><br>""",ai
"""<p>With typical machine learning you would usually use a training data-set to create a model of some kind, and a testing data-set to then test the newly created model. For something like linear regression after the model is created with the training data you now have an equation that you would use to predict the outcome of the set of features in the testing data. You would then take the prediction that the model returned and compare that to the actual data in the testing set. How would a validation set be used here?</p><br><br><p>With nearest neighbor you would use the training data to create an n-dimensional space that has all the features of the training set. You would then use this space to classify the features in the testing data. Again you would compare these predictions to the actual value of the data. How would a validation set help here as well?</p><br>""",ai
"""<p>By reinforcement learning, I don't mean the class of machine learning algorithms such as DeepQ, etc. I have in mind the general concept of learning based on rewards and punishment. </p><br><br><p>Is it possible to create a Strong AI that does not rely on learning by reinforcement, or is reinforcement learning a requirement for artificial intelligence? The existence of rewards and punishment imply the existence of favorable and unfavorable world-states. Must intelligence in general and artificial intelligence in particular have a way of classifying world-states as favorable or unfavorable?  </p><br>""",ai
"""<p>I'm not talking about mass scale <a href=""https://en.wikipedia.org/wiki/Skynet_(Terminator)"" rel=""nofollow"">Skynet</a> or something, but for example <a href=""http://ethereum.stackexchange.com/"">Ethereum</a> (or <a href=""http://ethereum.stackexchange.com/q/4120/105"">similar</a>) which is a public blockchain-based distributed computing platform (like <a href=""http://ethereum.stackexchange.com/a/762/105"">internet</a>) featuring smart contracts which can be executed on a decentralized virtual machine. They call it a <em>World Computer</em>.</p><br><br><p>Where there any attempts to use similar blockchain-based technology driven by community (or not) in order to create an artificial intelligence into a decentralized public blockchain-based distributed computing platform where it <a href=""http://www.newsereum.com/newsereum/can-ethereum-shut/"" rel=""nofollow"">cannot be shutdown</a>?</p><br>""",ai
"""<p>For example, search engine companies want to classify their image searches into 2 categories (which they already do that) such as: <a href=""https://en.wikipedia.org/wiki/Not_safe_for_work"" rel=""nofollow"">NSFW</a> (nudity, porn, brutality) and safe to view pictures.</p><br><br><p>How can artificial neural networks achieve that, and at what success rate? Can they be easily mistaken?</p><br>""",ai
"""<p>Do scientists or research experts know from the kitchen what is happening inside complex ""deep"" neural network with at least millions of connections firing at an instant? Do they understand the process behind this (e.g. what is happening inside and how it works exactly), or it is a subject of debate?</p><br><br><p>For example this <a href=""https://www.cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf"">study</a> says:</p><br><br><blockquote><br>  <p>However there is no clear understanding of <em>why</em> they perform so well, or <em>how</em> they might be improved.</p><br></blockquote><br><br><p>So does it mean the scientists actually doesn't know how complex convolutional network models work?</p><br>""",ai
"""<p>Is there any way to estimate how big the neural network would be after training session of 100,000 unlabeled images for unsupervised learning (like in <a href=""https://cs.stanford.edu/~acoates/stl10/"" rel=""nofollow"">STL-10 dataset</a>: 96x96 pixels and color)?</p><br><br><p>Not the storage space (because this could vary I guess based on the implementation), but specifically how many neurons it could have. It could be an estimate (e.g. in thousand, millions). If it depends, then on what? Are there any figures that can be estimated?</p><br>""",ai
"""<p>For example I'd like to train my neural network to recognize the type of actions (e.g. in commercial movies or some real life videos), so I can ""ask"" my network in which video or movie (and at what frames) somebody was driving a car, kissing, eating, was scared or was talking over the phone.</p><br><br><p>What are the current successful approaches to that type of problem?</p><br>""",ai
"""<p>I'm playing with an LSTM to generate text. In particular, this one:</p><br><br><p><a href=""https://raw.githubusercontent.com/fchollet/keras/master/examples/lstm_text_generation.py"" rel=""nofollow"">https://raw.githubusercontent.com/fchollet/keras/master/examples/lstm_text_generation.py</a></p><br><br><p>It works on quite a big demo text set from Nietzsche and says</p><br><br><blockquote><br>  <p>If you try this script on new data, make sure your corpus<br>  has at least ~100k characters. ~1M is better.</p><br></blockquote><br><br><p>This pops up a couple of questions.</p><br><br><p>A.) If all I want is an AI with a very limited vocabulary where the generate text should be short sentences following a basic pattern.</p><br><br><p>E.g.</p><br><br><p><em>I like blue sky with white clouds</em></p><br><br><p><em>I like yellow fields with some trees</em></p><br><br><p><em>I like big cities with lots of bars</em></p><br><br><p>...</p><br><br><p>Would it then be reasonable to use a much much smaller dataset?</p><br><br><p>B.) If the dataset really needs to be that big. What if I just repeat the text over and over to reach the recommended minimum? If that would work though, I'd be wondering how that is any different from just taking more iterations of learning with the same shorter text?</p><br><br><p>Obviously I can play with these two questions myself and in fact I am experimenting with it. One thing I already figured out is that with a shorter text following a basic pattern I can get to a very very low ( ~0.04) quite fast but the predicted text just turns out as gibberish.</p><br><br><p>My naive explanation for that would be that there are just not enough samples to proof against whether the gibberish actually makes sense or not? But then again I wonder if more iterations or duplicating the content would actually help.</p><br><br><p>I'm trying to experiment with these questions myself so please don't think I'm just too lazy and are aiming for others to do the work. I'm just looking for more experienced people to give me a better understanding of the mechanics that influence these things.</p><br>""",ai
"""<p>For example I would like to implement transparent AI in the RTS game which doesn't offer any AI API (like old games), and I'd like to use image recognition algorithm for detecting the objects which can talks to another algorithm which is responsible for the logic.</p><br><br><p>Given I'd like to use two neural networks, what are the approaches to setup the communication between them? Is it just by exporting result findings of the first algorithm (e.g. using CNN) with list of features which were found on the screen, then use it as input for another network? Or it's more complex than that, or I need to have more than two networks?</p><br>""",ai
"""<p>Were there any successful attempts to replace poor guide dogs used for blind people with AI to achieve similar rate of success? I guess dogs could be easily distracted and not reliable for every situation, and it probably takes less time to train AI, than a dog.</p><br>""",ai
"""<p>Do we know why Tesla's Autopilot mistaken empty sky with a high-sided lorry which resulted in fatal crash involving a car in self-drive mode? Was it AI fault or something else? Is there any technical explanation behind this why this happened?</p><br><br><p>References: <a href=""http://news.sky.com/story/tesla-driver-in-first-self-drive-fatal-crash-10330121"" rel=""nofollow"">Sky News article</a>, <a href=""http://www.theverge.com/2016/6/30/12072408/tesla-autopilot-car-crash-death-autonomous-model-s"" rel=""nofollow"">The Verge</a>.</p><br>""",ai
"""<p>For benefits of testing AGI, is using a high-level video game description language (VGDL) gives more reliable and accurate results of general intelligence than using Arcade Learning Environment (ALE)?</p><br>""",ai
"""<p>Some time ago playing chess was challenging for algorithms, then Go game which is vastly more complex than compared to chess.</p><br><br><p>How about playing RTS game which have enormous branching factors limited by its time and space (like deciding what to do next)? What are the successful approaches to such problems?</p><br>""",ai
"""<p>We can read on wiki page that in March 2016 AlphaGo AI lost its game (1 of 5) to Lee Sedol, a professional Go player. One <a href=""http://www.bbc.co.uk/news/technology-36558829"" rel=""nofollow"">article</a> cite says:</p><br><br><blockquote><br>  <p>AlphaGo lost a game and we as researchers want to explore that and find out what went wrong. We need to figure out what its weaknesses are and try to improve it.</p><br></blockquote><br><br><p>Have researchers already figured it out what went wrong?</p><br>""",ai
"""<p>Assuming we're dealing with artificial neural network (e.g. using <a href=""https://en.wikipedia.org/wiki/Convolutional_neural_network"" rel=""nofollow"">convnets</a>) which was trained by large dataset of human faces.</p><br><br><p>Are there any known issues or challenges where facial recognition would fail? I'm not talking about covering half of the face, but some simple common things such as wearing the glasses, hat, jewellery, having face painting or tattoo, can this successfully prevent AI from recognizing the face? If so, what are current methods dealing with such challenges?</p><br>""",ai
"""<p>I would like to know what kind of dataset I need (to prepare) for training the network to recognize the spelling mistakes in individual words for English text.</p><br><br><p>Given the large database of words, having correct one for each incorrect. What kind of input is more efficient for that tasks? Is it using one input per each letter, syllable, whole word or I should use different pattern syllable?</p><br><br><p>Then the input should be incorrect word, output correct, and if the word doesn't need correction, then both input and output should be the same. Is that the right approach?</p><br>""",ai
"""<p>As I have been looking at other questions on this site (like <a href=""http://ai.stackexchange.com/questions/60/what-are-the-main-problems-hindering-current-ai-development"">this</a>, <a href=""http://ai.stackexchange.com/questions/1376/is-it-ethical-to-implement-self-defence-for-street-walking-ai-robots"">this</a>, <a href=""http://ai.stackexchange.com/questions/111/how-would-self-driving-cars-make-ethical-decisions-about-who-to-kill"">this</a>, and <a href=""http://ai.stackexchange.com/questions/1289/can-we-destroy-artificial-general-intelligence-without-its-consent"">this</a>), I have been thinking more about the ethical implications of creating these generalized AI systems. It seems that whether or not we <em>can</em> create it is not rationale enough as to whether or not we <em>should</em> do it.</p><br><br><p>In dealing with the issue of ethics in AI, I wonder what the ethical implications are not just for us, but for the system itself. It seems to extend beyond the usually asked questions on the topic and into unknown territory. Are ethics computable? Can they be implemented programmatically? Can we force an AI system to do something against its <em>""will""</em>?</p><br><br><p>What does the creation of AI imply ethically for us as well as the AI?</p><br>""",ai
"""<p>I believe <em>artificial intelligence</em> (AI) term is overly overused nowadays.</p><br><br><p>For example people see that something is self-moving and they call it AI, even if it's on autopilot (like cars or planes) or there is some simple algorithm behind it.</p><br><br><p>What are the minimum general requirements so that we can say something is AI?</p><br>""",ai
"""<p>I believe normally you can use <a href=""https://en.wikipedia.org/wiki/Genetic_programming"" rel=""nofollow"">genetic programming</a> for sorting, however I'd like to check whether it's possible using ANN.</p><br><br><p>Given the unsorted text data from input, which neural network is suitable for doing sorting tasks?</p><br>""",ai
"""<p>I've read on wiki that <a href=""https://en.wikipedia.org/wiki/Genetic_programming"" rel=""nofollow"">genetic programming</a> has '<em>outstanding results</em>' in cyberterrorism prevention.</p><br><br><p>Further more, this <a href=""http://papers.ssrn.com/sol3/papers.cfm?abstract_id=877981"" rel=""nofollow"">abstract</a> says:</p><br><br><blockquote><br>  <p>Using machine-coded linear genomes and a homologous crossover operator in genetic programming, promising results were achieved in detecting malicious intrusions.</p><br></blockquote><br><br><p>I've checked the study, but it's still not clear for me.</p><br><br><p>How exactly was this detection achieved from the technical perspective?</p><br>""",ai
"""<p>On Wikipedia, we can read about different type of <a href=""https://en.wikipedia.org/wiki/Intelligent_agent"" rel=""nofollow"">intelligent agents</a>:</p><br><br><ul><br><li>abstract intelligent agents (AIA),</li><br><li>autonomous intelligent agents,</li><br><li>virtual intelligent agent (IVA), which I've found on other websites, e.g. <a href=""https://www.techopedia.com/definition/26646/intelligent-virtual-agent-iva"" rel=""nofollow"">this one</a>.</li><br></ul><br><br><p>What are the differences between these three to avoid confusion?</p><br><br><hr><br><br><p>For example I've used term <em>virtual artificial agent</em> <a href=""http://ai.stackexchange.com/a/1512/8"">here</a> as:</p><br><br><blockquote><br>  <p>Basically a robot is a mechanical or virtual artificial agent which exhibit intelligent behavior (AI).</p><br></blockquote><br><br><p>so basically I'd like to know where other terms like autonomous or abstract agents can be used and in what context. Can they be all defined under 'virtual' robot definition? How to distinguish these terms?</p><br>""",ai
"""<p>On <a href=""https://en.wikipedia.org/wiki/Deep_Blue_(chess_computer)"" rel=""nofollow"">Wikipedia</a> we can read:</p><br><br><blockquote><br>  <p>Kasparov accused IBM of cheating and demanded a rematch. IBM refused and retired Deep Blue.</p><br></blockquote><br><br><p>What was the accusation and how was Deep Blue allegedly able to cheat?</p><br>""",ai
"""<p>The Wikipedia page describes <a href=""https://en.wikipedia.org/wiki/AI_control_problem"" rel=""nofollow"">AI control problem</a> in very intricated way.</p><br><br><p>Therefore I would like to better understand it based on some simple explanation, what's going on.<br>Basically I don't want any copy &amp; pastes from wiki, because the articles there are written in neutral point of view, in very general way where articles are evolving very slowly, so the definition from there doesn't suit me.</p><br><br><p>I believe this is what is discussed nowadays by government and it's important aspects of AI technology where it leds to.<br>I believe this could be a big problem in the near future, so I'm expecting to hear about this from people from much better and more up-to-date point of view.</p><br><br><p>So what is exactly the AI Control Problem?</p><br>""",ai
"""<p><sub>This is from a closed beta for AI, with this question being posted by user number 47. All credit to them. </sub></p><br><br><hr><br><br><p>According to <a href=""https://en.wikipedia.org/wiki/Boltzmann_machine"" rel=""nofollow"">Wikipedia</a>,</p><br><br><blockquote><br>  <p>Boltzmann machines can be seen as the stochastic, generative counterpart of Hopfield nets.</p><br></blockquote><br><br><p>Both are recurrent neural networks that can be trained to learn of bit patterns. Then when presented with a partial pattern, the net will retrieve the full complete pattern.</p><br><br><p>Hopfield networks have been proven to have a capacity of 0.138 (e.g. approximately 138 bit vectors can be recalled from storage for every 1000 nodes, Hertz 1991).</p><br><br><p>As a Boltzmann machine is stochastic, my understanding is that it would not necessarily always show the same pattern when the energy difference between one stored pattern and another is similar. But because of this stochasticity, maybe it allows for denser pattern storage but without the guarantee that you'll always get the ""closest"" pattern in terms of energy difference. Would this be true? Or would a Hopfield net be able to store more patterns?</p><br>""",ai
"""<p>According to <a href=""http://en.wikipedia.org/wiki/Prolog"">Wikipedia</a>,</p><br><br><blockquote><br>  <p>Prolog is a general-purpose logic programming language associated with artificial intelligence and computational linguistics.</p><br></blockquote><br><br><p>Is it still used for AI?</p><br><br><hr><br><br><p><sub>This is based off of a question on the 2014 closed beta. The author had the UID of 330.</sub></p><br>""",ai
"""<p>I'm a bit confused with extensive number of different <a href=""https://en.wikipedia.org/wiki/Monte_Carlo_method"" rel=""nofollow"">Monte Carlo methods</a> such as:</p><br><br><ul><br><li><a href=""https://en.wikipedia.org/wiki/Hybrid_Monte_Carlo"" rel=""nofollow"">Hamiltonian/Hybrid Monte Carlo (HMC)</a>,</li><br><li><a href=""https://en.wikipedia.org/wiki/Dynamic_Monte_Carlo_method"" rel=""nofollow"">Dynamic Monte Carlo (DMC)</a>,</li><br><li><a href=""https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo"" rel=""nofollow"">Markov chain Monte Carlo (MCMC)</a>,</li><br><li><a href=""https://en.wikipedia.org/wiki/Kinetic_Monte_Carlo"" rel=""nofollow"">Kinetic Monte Carlo (KMC)</a>,</li><br><li><a href=""https://en.wikipedia.org/wiki/Dynamic_Monte_Carlo_method"" rel=""nofollow"">Dynamic Monte Carlo (DMC)</a></li><br><li><a href=""https://en.wikipedia.org/wiki/Quasi-Monte_Carlo_method"" rel=""nofollow"">Quasi-Monte Carlo (QMC)</a>,</li><br><li><a href=""https://en.wikipedia.org/wiki/Direct_simulation_Monte_Carlo"" rel=""nofollow"">Direct Simulation Monte Carlo (DSMC)</a>,</li><br><li>and so on.</li><br></ul><br><br><p>I won't ask for the exact differences, but why are all of them called Monte Carlo? What do they all have in common? Can they all be used for AI? E.g. which one can be used for gaming (like Go) or image recognition (resampling)?</p><br>""",ai
"""<p>When it comes to neural networks, it's often only explained what abstract task they do, say for example detect a number in an image. I never understood what's going on under the hood essentially.</p><br><br><p>There seems to be a common structure of a directed graph, with values in each node. Some nodes are input nodes. Their values can be set. The values of subsequent nodes are then calculated based on those along the edges of the graph until the values for the output nodes are set, which can be interpreted a result.</p><br><br><p>How exactly is the value of each node determined? I assume that some formula is associated with each node that takes all incoming nodes as input to calculate the value of the node. What formula is used? Is the formula the same throughout the network?</p><br><br><p>Then I heard that a network has to be trained. I assume that such training would be the process to assign values to coefficients of the formulas used to determine the node values. Is that correct?</p><br><br><p>In layman's terms, what are the underlying principles that make a neural network work?</p><br>""",ai
"""<p>Ideally I'd like to watch movie which is deep dreamed in real-time. Most algorithms which I know are too slow or not designed for real-time processing.</p><br><br><p>For example I'm bored with some movie which I've watched thousands of time and I'd like to add some ""dreaming"" to it which is real-time filter which takes input frames, then it's processing and enhances the images through artificial neural network to achieve doodled output.</p><br><br><p>Doesn't have to be exactly <a href=""https://en.wikipedia.org/wiki/DeepDream"" rel=""nofollow"">DeepDream</a> or hallucinogenic technique (which could be too much to watch for 2h), but with any similar ANN algorithm. I'm more interested into achieving desired real-time use.</p><br><br><p>What kind of techniques can achieve such efficiency?</p><br>""",ai
"""<p>How does employing evolutionary algorithms to design and train artificial neural networks have advantages over using the conventional backpropagation algorithms?</p><br>""",ai
"""<p>Are there any existing approaches for using artificial neural networks (ANN) or evolutionary algorithm (EA) for detecting coding standard violations? Which one would be more suitable?</p><br><br><p>I don't have any specific programming language in mind, but something similar to <a href=""http://pear.php.net/package/PHP_CodeSniffer"" rel=""nofollow"">PHP_CodeSniffer</a> (following <a href=""https://www.drupal.org/coding-standards"" rel=""nofollow"">these standards</a>), but instead of using hardcoded rules, the algorithm should learn good techniques, but I'm not sure based on what training data. How would you approach the training session, any suggestions?</p><br>""",ai
"""<p>Genetic Algorithms has come to my attention recently when trying to correct/improve computer opponents for turn-based strategy computer games.</p><br><br><p>I implemented a simple Genetic Algorithm that didn't use any cross-over, just some random mutation. It seemed to work in this case, and so I started thinking:</p><br><br><p><strong>Why is cross-over a part of genetic algorithms? Wouldn't mutation be enough?</strong></p><br><br><p><sub>This is from a data dump on an old AI site. The asker had the UID of 7. </sub></p><br>""",ai
"""<p>While thinking about AI, this question came into my mind. Could curiosity help in developing a true AI? According to this <a href=""http://psychologia.co/creativity-test/"" rel=""nofollow"">website</a> (for testing creativity):</p><br><br><blockquote><br>  <p>Curiosity refers to persistent desire to learn and discover new things<br>  and ideas</p><br><br><pre><code>always looks for new and original ways of thinking,<br>likes to learn,<br>searches for alternative solutions even when traditional solutions are present and available,<br>enjoys reading books and watching documentaries,<br>wants to know how things work inside out<br></code></pre><br></blockquote><br><br><p>Let's take <a href=""https://www.clarifai.com/demo"" rel=""nofollow"">Clarifai</a>, a image/video classification startup which can classify images and video with the best accuracy (according to them). If I understand correctly, they trained their deep learning system using millions of images with supervised learning. In the same algorithm, what would happen if we somehow added a ""curiosity factor"" when the AI has difficulty in classifying a image or its objects? It would ask a human for help, just like a curious child. </p><br><br><p>Curiosity makes a human being learn new things and also helps to generate new original ideas. Could the addition of curiosity change Clarifai into a true AI?</p><br>""",ai
"""<p>Based on this <a href=""http://www.dailymail.co.uk/sciencetech/article-3677950/Google-s-self-driving-cars-spot-cyclists-Sensors-read-hand-signals-predict-riders-behavior.html"" rel=""nofollow"">article</a>, Google's self-driving cars can spot cyclists, cars, road signs, markings, traffic lights, and pedestrians.</p><br><br><p>How exactly does it identify pedestrians? Is it based on face recognition, shape, size, distance, infrared signature?</p><br>""",ai
"""<p>In <a href=""https://www.technologyreview.com/s/530276/hidden-obstacles-for-googles-self-driving-cars/"">Hidden Obstacles for Google’s Self-Driving Cars</a> article we can read that:</p><br><br><blockquote><br>  <p>Google’s cars can detect and respond to stop signs that aren’t on its map, a feature that was introduced to deal with temporary signs used at construction sites.</p><br>  <br>  <p>Google says that its cars can identify almost all unmapped stop signs, and would remain safe if they miss a sign because the vehicles are always looking out for traffic, pedestrians and other obstacles.</p><br></blockquote><br><br><p>What would happen if a car spotted somebody in front of it (but not on the collision path) wearing a T-shirt that has a stop sign printed on it. Would it react and stop the car?</p><br>""",ai
"""<p><sub> This is a scope experiment. </sub></p><br><br><hr><br><br><p>After Google/Tesla/whoever else is making self-driving cars finishes perfecting them, will they replace the cars with human drivers, so that there are only self-driving cars?</p><br><br><p>If they do, it would probably make the roads safer.</p><br>""",ai
"""<p>Significant AI vs human board game matches include:</p><br><br><ul><br><li><strong>chess</strong>: <a href=""https://en.wikipedia.org/wiki/Deep_Blue_(chess_computer)#Deep_Blue_versus_Kasparov"" rel=""nofollow"">Deep Blue vs Kasparov</a> in 1996,</li><br><li><strong>Go</strong>: <a href=""https://en.wikipedia.org/wiki/AlphaGo_versus_Lee_Sedol"" rel=""nofollow"">DeepMind AlphaGo vs Lee Sedol</a> in 2016,</li><br></ul><br><br><p>which demonstrated that AI challenged and defeated professional players.</p><br><br><p>Are there known board games left where a human can still win against an AI? I mean based on the final outcome of authoritative famous matches, where there is still same board game where AI cannot beat a world champion of that game.</p><br>""",ai
"""<p>I'm trying to teach an AI different pattern of tic tac toe to recognize wether a given pattern represents a win or not.</p><br><br><p>Unfortunately it's not learning to recognize them correctly and I think may way of representing/encoding the game into vectors is wrong.</p><br><br><p>I choose a way that is easy for an human (me, in particular!) to make sense of:</p><br><br><pre><code>training_data = np.array([[0,0,0,<br>                           0,0,0,<br>                           0,0,0],<br>                          [0,0,1,<br>                           0,1,0,<br>                           0,0,1],<br>                          [0,0,1,<br>                           0,1,0,<br>                           1,0,0],<br>                          [0,1,0,<br>                           0,1,0,<br>                           0,1,0]], ""float32"")<br>target_data = np.array([[0],[0],[1],[1]], ""float32"")<br></code></pre><br><br><p>This basically just use an array of length 9 to represent a 3 x 3 board. The first three items represent the first row, the next three the second row and so on. The line breaks should make it obvious I guess.</p><br><br><p>The target data then maps the first two game states to ""no wins"" and the last two game states to ""wins"".</p><br><br><p>Then I wanted to create some validation data that is slightly different to see if it generalizes.</p><br><br><pre><code>validation_data = np.array([[0,0,0,<br>                             0,0,0,<br>                             0,0,0],<br>                            [1,0,0,<br>                             0,1,0,<br>                             1,0,0],<br>                            [1,0,0,<br>                             0,1,0,<br>                             0,0,1],<br>                            [0,0,1,<br>                             0,0,1,<br>                             0,0,1]], ""float32"")<br></code></pre><br><br><p>Obviously, again the last two game states should be ""wins"" whereas the first two should not.</p><br><br><p>I tried to play with the number of neurons and learning rate but no matter what I try, my output looks pretty of. E.g.</p><br><br><pre><code>[[ 0.01207292]<br> [ 0.98913926]<br> [ 0.00925775]<br> [ 0.00577191]]<br></code></pre><br><br><p>I tend to think it's the way how I represent the game state that may be wrong but actually I have no idea :D</p><br><br><p>Can anyone help me out here?</p><br><br><p>This is the entire code that I use</p><br><br><pre><code>import numpy as np<br>from keras.models import Sequential<br>from keras.layers.core import Activation, Dense<br>from keras.optimizers import SGD<br><br>training_data = np.array([[0,0,0,<br>                           0,0,0,<br>                           0,0,0],<br>                          [0,0,1,<br>                           0,1,0,<br>                           0,0,1],<br>                          [0,0,1,<br>                           0,1,0,<br>                           1,0,0],<br>                          [0,1,0,<br>                           0,1,0,<br>                           0,1,0]], ""float32"")<br><br>target_data = np.array([[0],[0],[1],[1]], ""float32"")<br><br>validation_data = np.array([[0,0,0,<br>                             0,0,0,<br>                             0,0,0],<br>                            [1,0,0,<br>                             0,1,0,<br>                             1,0,0],<br>                            [1,0,0,<br>                             0,1,0,<br>                             0,0,1],<br>                            [0,0,1,<br>                             0,0,1,<br>                             0,0,1]], ""float32"")<br><br>model = Sequential()<br>model.add(Dense(2, input_dim=9, activation='sigmoid'))<br>model.add(Dense(1, activation='sigmoid'))<br><br>sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)<br>model.compile(loss='mean_squared_error', optimizer=sgd)<br><br>history = model.fit(training_data, target_data, nb_epoch=10000, batch_size=4, verbose=0)<br><br>print(model.predict(validation_data))<br></code></pre><br>""",ai
"""<p>Has there any research been done on how difficult certain languages are to learn for chatbots? <br>For example, CleverBot knows a bit of Dutch, German, Finnish and French, so there are clearly chatbots that speak other languages than English. (English is still her best language, but that is because she speaks that most often)</p><br><br><p>I would imagine that a logical constructed language, like lobjan, would be easier to learn than a natural language, like English, for example.  </p><br>""",ai
"""<p>Google, Tesla, Apple etc have all built or are building their own self-driving cars. As an expert in a related area, I am interested in knowing at a high level, the systems and techniques that go into self-driving cars. How easy is it for me to make a tabletop prototype (large enough to accomodate the needed computing power needs)?</p><br>""",ai
"""<p>The above question itself is perhaps too broad for this forum, hence I am phrasing it as a request for references.</p><br><br><p>Humans have been endowed with personalities by nature, and it is not clear (to me at least) if this is a feature or a bug. This has been explored in science fiction by various notions of <a href=""http://memory-alpha.org/Borg"" rel=""nofollow"">Borg</a>-like entities. It is my belief that, for narrative reasons, such stories usually end with the humans with their flawed personalities winning in the end. </p><br><br><p>Are there experts who have analyzed, perhaps mathematically, design criteria for an AI agent with weakly enforced goals (eg. to maximize reproduction in the human case) in an uncertain environment, and ended up with the answer that a notion of personality is useful? If there are philosophers or science fiction writers who have examined this question in their work, I would be happy to know about those too.</p><br>""",ai
"""<p>I've found this short <a href=""http://iamtrask.github.io/2015/07/12/basic-python-network/"" rel=""nofollow"">Python code</a> which implements neural network in 11 lines of code:</p><br><br><pre><code>X = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])<br>y = np.array([[0,1,1,0]]).T<br>syn0 = 2*np.random.random((3,4)) - 1<br>syn1 = 2*np.random.random((4,1)) - 1<br>for j in xrange(60000):<br>    l1 = 1/(1+np.exp(-(np.dot(X,syn0))))<br>    l2 = 1/(1+np.exp(-(np.dot(l1,syn1))))<br>    l2_delta = (y - l2)*(l2*(1-l2))<br>    l1_delta = l2_delta.dot(syn1.T) * (l1 * (1-l1))<br>    syn1 += l1.T.dot(l2_delta)<br>    syn0 += X.T.dot(l1_delta)<br></code></pre><br><br><p>I believe it may be a valid implementation of neural network, but how do I know?</p><br><br><p>In other words, is just creating bunch of arrays which compute the output on certain criteria and call them layers with synapses does it make proper neural network?</p><br><br><p>In other words, I'd like to ask, what features/properties makes a valid artificial neural network?</p><br>""",ai
"""<p>I'm looking for research which discusses misbehavior detection in public internet access networks using ANN approaches.</p><br><br><p>So it can be used by <a href=""https://en.wikipedia.org/wiki/Internet_service_provider"" rel=""nofollow"">ISP</a> to detect suspicious users connected to their network.</p><br>""",ai
"""<p>I'm investigating applications of AI algorithms which can be used for data leakage detection and prevention within an intranet network (like <a href=""https://en.wikipedia.org/wiki/Forcepoint"" rel=""nofollow"">Forcepoint</a>). More specifically detecting traffic patterns. I'm new to this.</p><br><br><p>Which learning algorithms are most suitable for this goal? <a href=""https://en.wikipedia.org/wiki/Evolutionary_algorithm"" rel=""nofollow"">EA</a>, <a href=""https://en.wikipedia.org/wiki/Genetic_algorithm"" rel=""nofollow"">GA</a>, <a href=""https://en.wikipedia.org/wiki/Artificial_neural_network"" rel=""nofollow"">ANN</a> (which one) or something else?</p><br>""",ai
"""<p>I'm wondering, instead of implementing new web browsers over and over again with millions line of code which is very difficult to manage, would it be possible to use ANN or GA algorithm to teach it about the rendering process (how the page should look like)?</p><br><br><p>So as an input I would imaging the html source code, output is the rendered page (maybe in some interactive image like SVG, some library or something, I'm not sure).</p><br><br><p>The training data can be dataset of websites providing input source code and their rendered representation by using other browsers for the guidance as expected output.</p><br><br><p>Which approach would you take and what are the most challenging things you can think of?</p><br>""",ai
"""<p>I'm trying to make a conversational chatbot, so the user inputs are quite wide ranging - beyond just ""turn lights on"". I want to detect the category of the user intents from their inputs and prepare responses.</p><br><br><p>I've looked at MS' Luis and api.ai and the intents require a lot of training. Can people suggest other techniques for untrained intent detection?</p><br><br><p>For example if the user says ""Pasta is my favorite dish to cook"" then detect ""intent preference entity pasta"" - then I can gradually build up responses to different categories of inputs.</p><br><br><p>Perhaps the crowd-sourced intents that wit.ai (facebook) has access to could do this but I'm not sure if all end-users have access to those models.</p><br>""",ai
"""<p>How does a domestic autonomous robotic vacuum cleaner -  such as a <a href=""https://en.wikipedia.org/wiki/Roomba"" rel=""nofollow"">Roomba</a> - know when it's working cleaned area (aka virtual map), and how does it plan to travel to the areas which hasn't been explored yet?</p><br><br><p>Does it use some kind of <a href=""https://en.wikipedia.org/wiki/A*_search_algorithm"" rel=""nofollow"">A*</a> algorithm?</p><br>""",ai
"""<p>It has been <a href=""http://www.itnonline.com/content/will-fda-be-too-much-intelligent-machines"" rel=""nofollow"">suggested</a> that machine learning algorithms (also <a href=""http://ai.stackexchange.com/q/1427/8"">Watson</a>) can help with finding disease in patient images and optimize scans. Also that deep learning algorithms show promise for every type of digital imaging.</p><br><br><p>How does exactly deep learning algorithms exactly can find suspicious patterns in the body’s biochemistry?</p><br>""",ai
"""<p>The <a href=""https://www.youtube.com/watch?v=AplG6KnOr2Q"" rel=""nofollow"">Mario Lives!</a> video (and its follow-up video, <a href=""https://www.youtube.com/watch?v=ltPj3RlN4Nw&amp;list=PLuOoXrWK6Kz5ySULxGMtAUdZEg9SkXDoq&amp;index=5"" rel=""nofollow"">Mario Becomes Social!</a>) showcases an AI unit that is able to simulate emotional desicion-making within a virtual world, and can enter into ""emotional states"" such as curiosity, hunger, happiness, and fear. While this seems cool and exciting (especially for video game AI), I am confused how this would be useful in real-world scenarios.</p><br><br><p>What would be the point of building autonomous actors that would behave based on these emotional states, instead of simply knowing <em>what</em> they should do (either by hardcoding in the rules, or learning the rules through machine learning)?</p><br>""",ai
"""<p><sub>This is from the 2014 closed beta. The asker had the UID of 245.</sub></p><br><br><p>For a deterministic problem space, I need to find a neural network with the optimal node and link structure. I want to use a genetic algorithm to simulate many neural networks to find the best network structure for the problem domain.</p><br><br><p>I know a fair amount about neural networks<sup>1</sup> but have not used genetic algorithms for a task like this before.</p><br><br><p>What are the practical considerations? <br>How should I encode the structure into a genome?</p><br><br><hr><br><br><p><sub><sup>1</sup>Actually, I don't. Just saying that. -Mithrandir. </sub></p><br>""",ai
"""<p>Were there any studies which checked the accuracy of neural network predictions of greyhound racing results, compared to a human expert? Would it achieve a better payoff?</p><br>""",ai
"""<p>I've read about The Loebner Prize for AI, which pledged a Grand Prize of $100,000 and a Gold Medal for the first computer whose responses were indistinguishable from a human's.</p><br><br><p>So I was wondering whether any chatbots have fooled the judges and won a Gold Medal yet?</p><br><br><p>From their <a href=""http://www.loebner.net/Prizef/loebner-prize.html"" rel=""nofollow"">website</a> this isn't clear (as some of the links doesn't load).</p><br><br><hr><br><br><p>A few highlights from previous years:</p><br><br><p><a href=""http://loebner.exeter.ac.uk/results/"" rel=""nofollow"">2011 Loebner Prize results</a></p><br><br><blockquote><br>  <p>None of the AI systems fooled the judges, therefore the Turing Test has not been passed.</p><br></blockquote><br><br><p><a href=""http://www.paulmckevitt.com/loebner2013/scoring/loebner2013leaderboard.txt"" rel=""nofollow"">Loebner 2013 results</a>:</p><br><br><blockquote><br>  <p>No chatbot fooled any of the 4 Judges.</p><br></blockquote><br>""",ai
"""<p>Hypothetically, assume that you have access to infinite computing power. Do we have designs for any brute-force algorithms that can find an AI capable of passing traditional tests (e.g. Turing, Chinese Room, MIST, etc.)? </p><br>""",ai
"""<p>I'm aware this could be a complex topic, however I'm interested in existing research projects or studies where people are attempting or have succeeded in teaching an AI a foreign language just by training/teaching it from English books. By reading, analysing and understanding, so that it knows the foreign language's rules (such as grammar, spelling, etc.), the same way as a human would learn. The language doesn't have to be Chinese, which is difficult for even humans to learn.</p><br>""",ai
"""<p>Would it be possible to put Asimov's three Laws of Robotics into an AI?</p><br><br><p>The three laws are:</p><br><br><ol><br><li><p>A robot (or, more accurately, an AI) cannot harm a human being, or through inaction allow a human being to be harmed<sup>1</sup></p></li><br><li><p>A robot must listen to instructions given to it by a human, as long as that does not conflict with the first law.</p></li><br><li><p>A robot must protect its own existence, if that does not conflict with the first two laws.</p></li><br></ol><br><br><hr><br><br><p><sup>1</sup> <em>To it's knowledge</em>. This was a plot point in one of the books :P</p><br>""",ai
"""<p>I'd like to investigate the possibility of achieving similar recognition as it's in <a href=""http://asimo.honda.com/downloads/pdf/asimo-technical-information.pdf"" rel=""nofollow"">Honda's ASIMO robot</a><sup>p.22</sup> which can interpret the positioning and movement of a hand, including postures and gestures based on visual information.</p><br><br><p>Here is the example of application such interpretation in robot:</p><br><br><p><a href=""http://asimo.honda.com/downloads/pdf/asimo-technical-information.pdf"" rel=""nofollow""><img src=""http://i.stack.imgur.com/UDram.png"" alt=""Honda&#39;s ASIMO robot - Recognition of postures and gestures based on visual information""></a></p><br><br><p><sup>Image source: <a href=""http://asimo.honda.com/downloads/pdf/asimo-technical-information.pdf"" rel=""nofollow"">ASIMO Featuring Intelligence Technology - Technical Information (PDF)</a></sup></p><br><br><p>So basically the recognition should detect an indicated location (posture recognition) or respond to a wave (gesture recognition), also similar like <a href=""http://ai.stackexchange.com/a/1577/8"">Google car</a> does it (by determining certain patterns).</p><br><br><p>Is it known how ASIMO does it, or what would be the closest alternative for postures and gestures recognition to achieve the same results?</p><br>""",ai
"""<p>For Example:</p><br><br><h2>Could you provide reasons why a sundial is <em>not</em> ""intelligent""?</h2><br><br><p>A sundial senses its environment and acts rationally. It outputs the time. It also stores  percepts. (The numbers the engineer wrote on it.)</p><br><br><h2>What properties of a self driving car would make it ""intelligent""?</h2><br><br><p>Where is the line between non intelligent matter and an intelligent system?</p><br>""",ai
"""<p>We can read on <a href=""https://en.wikipedia.org/wiki/TensorFlow#Tensor_processing_unit_.28TPU.29"" rel=""nofollow"">Wikipedia page</a> that Google built a custom ASIC chip for machine learning and tailored for TensorFlow which helps to accelerate AI.</p><br><br><p>Since ASIC chips are specially customized for one particular use without the ability to change its circuit, there must be some fixed algorithm which is invoked.</p><br><br><p>So how exactly does the acceleration of AI using ASIC chips work if its algorithm cannot be changed? Which part of it is exactly accelerating?</p><br>""",ai
"""<p>I was reading that the <a href=""http://nasa-jsc-robotics.github.io/valkyrie/"" rel=""nofollow"">Valkyrie robot</a> was originally designed to 'carry out search and rescue missions'.</p><br><br><p>However there were some talks to send it to Mars to assist astronauts.</p><br><br><p>What kind of specific trainings or tasks are planned for 'him' to be able to carry on its own?</p><br><br><p>Refs:</p><br><br><ul><br><li><a href=""https://github.com/nasa-jsc-robotics"" rel=""nofollow"">NASA-JSC-Robotics at GitHub</a></li><br><li><a href=""http://nasa-jsc-robotics.github.io/valkyrie/"" rel=""nofollow"">github.io page</a></li><br><li><a href=""https://gitlab.com/nasa-jsc-robotics/valkyrie"" rel=""nofollow"">gitlab page</a></li><br></ul><br>""",ai
"""<p>Do scientists know by what mechanism biological brains/biological neural networks store data?</p><br><br><p>I was thinking about @kenorbs <a href=""http://ai.stackexchange.com/questions/1656/how-can-nanobot-implants-in-our-brains-connect-to-the-internet"">question</a> about implanting nanobots to build an AGI on top of human wetware. </p><br><br><p>I only have a vague notion that we store data in our brains by altering synapses? </p><br><br><p>Links, Criticism and Detailed Explanation welcome.</p><br><br><p>I also would love a decent description of how a vanilla Artificial Neural Network stores data. </p><br><br><p><strong>Questions:</strong></p><br><br><ol><br><li><p>How is data stored in a biological Neural Network?</p></li><br><li><p>How is data stored in an Artificial Neural Network?</p></li><br></ol><br>""",ai
"""<p>My understanding is that <em>Watson</em> is the name of the computer, and <em>DeepQA</em> is the name of the software or technology. They are both correlated.</p><br><br><p>Are there any computers/technologies other than <em>Watson</em> which <strong>are using <em>DeepQA</em></strong>? Or is <em>Watson</em> the only computer which implements that software/technology?</p><br><br><p><sup>This question is inspired by this <a href=""http://meta.ai.stackexchange.com/q/1177/8"">meta thread</a>.</sup></p><br>""",ai
"""<p>There is a study about <a href=""http://www.aclweb.org/anthology/P/P02/P02-1031.pdf"" rel=""nofollow"">The Necessity of Parsing for Predicate Argument Recognition</a>, however I couldn't find much information about 'Predicate Argument Recognition' which could explain it.</p><br><br><p>What is it exactly and how does it work, briefly?</p><br>""",ai
"""<p>The Wit.ai is a Siri-like voice interface which can can parse messages and predict the actions to perform.</p><br><br><p>Here is the <a href=""https://labs.wit.ai/demo/index.html"" rel=""nofollow"">demo site powered by Wit.ai</a>.</p><br><br><p>How does it understand the spoken sentences and convert them into structured actionable data? Basically, how does it know what to do?</p><br>""",ai
"""<p>In 2014 <a href=""https://techcrunch.com/2014/02/06/linkedin-snatches-up-data-savvy-job-search-startup-bright-com-for-120m-in-its-largest-acquisition-to-date/"" rel=""nofollow"">Linkedin acquired Bright.com</a>, for $120 million and it is using AI and big data algorithms to connect users.</p><br><br><blockquote><br>  <p>Bright also throws in a little Klout, ranking people by a “Bright score” which it uses to assess how strong the chemistry is between a user and a particular job.</p><br>  <br>  <p>It also takes into account historical hiring patterns into its matching, along with account location, a user’s past experience and synonyms.</p><br></blockquote><br><br><p>In brief, is it known (based on some research papers) how such algorithm works which aiming at scoring 'chemistry' between users and their jobs?</p><br>""",ai
"""<p>According to this <a href=""http://mashable.com/2014/01/06/pinterest-acquires-visualgraph/"" rel=""nofollow"">article</a>, Pinterest acquired VisualGraph, an image recognition and visual search technology startup.</p><br><br><p>How does Pinterest apply VisualGraph technology for machine vision, image recognition and visual search in order to classify the images?</p><br><br><p>In short, how do they predict the image categories? Based on what features?</p><br>""",ai
"""<p>Wolfram Language Image Identification Project launched an <a href=""https://www.imageidentify.com/"" rel=""nofollow"">Image Identify site</a> demo which returns the top predicted tags for the photos.</p><br><br><p>How does it work, briefly? I mean what type of learning vision technologies are used to analyze, recognize and understand the content of an image?</p><br>""",ai
"""<p>I've <a href=""https://www.imageidentify.com/result/0lkzuttdxipub"" rel=""nofollow"">uploaded a picture</a> to Wolfram's ImageIdentify of graffiti on the wall, but it recognized it as 'monocle'. Secondary guesses were 'primate', 'hominid', and 'person', so not even close to 'graffiti' or 'painting'.</p><br><br><p>Is it by design, or there are some <strong>methods to teach a convolutional neural network (CNN) to reason and be aware of a bigger picture context</strong> (like mentioned graffiti)? Currently it seems as if it's detecting literally <em>what is depicted in the image</em>, not <em>what the image actually is</em>.</p><br><br><p><a href=""http://i.stack.imgur.com/akquMm.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/akquMm.png"" alt=""Wolfram&#39;s Image Identify: monocle/graffiti""></a></p><br><br><p>This could be the same problem as mentioned <a href=""http://ai.stackexchange.com/a/1533/8"">here</a>, that DNN are:</p><br><br><blockquote><br>  <p>Learning to detect jaguars by matching the unique spots on their fur while ignoring the fact that they have four legs.<sup><a href=""http://ai.stackexchange.com/a/1533/8"">2015</a></sup></p><br></blockquote><br><br><p>If it's by design, maybe there is some better version of CNN that can perform better?</p><br>""",ai
"""<p>An AI agent is often thought of having ""sensors"", ""a memory"", ""machine learning processors"" and ""reaction"" components. However, a machine with these does not necessarily become a self-programming AI agent. Beyond the parts mentioned above, is there any other elements or details necessary to make a machine capable of being a self-programming AI agent?</p><br><br><p>For example, <a href=""http://www.iiim.is/wp/wp-content/uploads/2011/05/goertzel-agisp-2011.pdf"" rel=""nofollow"">a paper from 2011</a> declared that solving the optimization problem of maximizing the intelligence is a must-have feature for the self-programming process, as quoted below:</p><br><br><blockquote><br>  <p>A system is said to carry out an instance of self-programming when it undergoes learning regarding some element of its ""cognitive infrastructure"", where the latter is defined as the fuzzy set of ""intelligence-critical"" features of the system; and the intelligence-criticality of a system feature is defined as its ""feature quality,"" considered from the perspective of solving the optimization problem of maximizing the intelligence of a multi-feature system.</p><br></blockquote><br><br><p>However, this description of ""optimization of intelligence"" is vague. Can anyone give a clear definition or better summary for the necessary components for self-programming agents?</p><br><br><p><sub>This question is from the 2014 closed beta, with the asker having a UID of 23.</sub></p><br>""",ai
"""<p>In a <a href=""http://www.wsj.com/articles/whats-next-for-artificial-intelligence-1465827619"">recent Wall Street Journal article</a>, Yann LeCunn makes the following statement:</p><br><br><blockquote><br>  <p>The next step in achieving human-level ai is creating intelligent—but not autonomous—machines. The AI system in your car will get you safely home, but won’t choose another destination once you’ve gone inside. From there, we’ll add basic drives, along with emotions and moral values. If we create machines that learn as well as our brains do, it’s easy to imagine them inheriting human-like qualities—and flaws. </p><br></blockquote><br><br><p>Personally, I have generally taken the position that talking about emotions for artificial intelligences is silly, because there would be no <em>reason</em> to create AI's that experience emotions.  Obviously Yann disagrees.  So the question is:  what end would be served by doing this?  Does an AI <em>need</em> emotions to serve as a useful tool?  </p><br>""",ai
"""<p>Inspired by <a href=""http://ai.stackexchange.com/q/1481/8"">this discussion</a> about recognizing human actions, I have found the <a href=""https://github.com/harishrithish7/Fall-Detection"" rel=""nofollow"">Fall-Detection</a> project which detects humans falling on the ground from a CCTV camera feed, and which can consider alerting the hospital authorities.</p><br><br><p>My question is, are there any existing real-life implementations or research projects <strong>which specifically use live video feed from the surveillance cameras in order to detect crime</strong> using convnets (or similar approaches)? If so, how do they work, briefly? Do they automatically inform the police about the crime with the details what happened and where?</p><br><br><p>For example car accidents, physical assaults, robberies, violent disturbances, weapon attacks, etc.</p><br>""",ai
"""<p>I'm trying to come up with the right algorithm for a system in which the user enters a few symptoms and the system has to predict or determine the likelihood that a few selected symptoms are associated with those existing in the system. Then after associating them, the result or output should be a specific disease for the symptoms.</p><br><br><p>The system is comprised of a series of diseases with each assigned to specific symptoms, which also exist in the system.</p><br><br><p>Let's assume that the user entered the following input:</p><br><br><pre><code>A, B, C, and D<br></code></pre><br><br><p>The first thing the system should do is check and associate each symptom (in this case represented by alphabetical letters) individually against a data-table of symptoms that already exist. And in cases where the input doesn't exist, the system should report or send feedback about it.</p><br><br><p>And also, let's say that <code>A and B</code> was in the data-table, so we are 100% sure that they're valid or exist and the system is able to give out the disease based on the input. Then let's say that the input now is <code>C and D</code> where <code>C</code> doesn't exist in the data-table, but there is a possibility that <code>D</code> exists.</p><br><br><p>We don't give <code>D</code> a score of 100%, but maybe something lower (let's say 90%). Then <code>C</code> just doesn't exist at all in the data-table. So, <code>C</code> gets a score of 0%.</p><br><br><p>Therefore, the system should have some kind of association and prediction techniques or rules to output the result by judging the user's input.</p><br><br><p>Summary of generating the output:</p><br><br><pre><code>If A and B were entered and exist, then output = 100%<br>If D was entered and existed but C was not, then output = 90%<br>If all entered don't exist, then output = 0%<br></code></pre><br><br><p>What techniques would be used to produce this system?</p><br>""",ai
"""<p>I have gone through the <a href=""https://en.wikipedia.org/wiki/Statistical_relational_learning"">wikipedia explanation of SRL</a>. But, it only confused me more:</p><br><br><blockquote><br>  <p>Statistical relational learning (SRL) is a subdiscipline of artificial intelligence and machine learning that is concerned with domain models that exhibit both uncertainty (which can be dealt with using statistical methods) and complex, relational structure.</p><br></blockquote><br><br><p>Can someone give a more dumbed down explanation of the same, preferably with an example?</p><br>""",ai
"""<p>The obvious solution is to ensure that the training data is balanced - but in my particular case that is impossible. What corrections can one perform in such a scenario?</p><br><br><p>I know that my training data is heavily biased towards a particular class, say, and I cannot change that. Moreover, the labels are very noisy. Conditioned on this piece of information, is there anything I can do by tweaking the training process itself/ something else, to correct for the bias in the training data?</p><br><br><p>The data comes from an experiment (from an electron microscope), and I cannot collect more data. It's always going to be biased in this way, so alternatively-biased is also not an option. I'm sorry that I'm unable to provide any more details due to confidentiality.</p><br>""",ai
"""<p><strong>Note:</strong> I wanted to ask a meta-post first to see if this site was supposed to be used only for AI-related questions, or if AI-related questions such as this were allowed, too, but apparently you need to have asked five actual questions first.</p><br><br><hr><br><br><p>I'm going to be entering a masters computer science program in the fall, and I wanted to move towards a concentration in computational neuroscience and linguistics for AI development applications. While I have a math and CS background, I have almost no biology/neuroscience background, and my linguistics background is limited to the random research I've done in my spare time to satiate my curiosities.</p><br><br><p>What are good non-math and CS related topics to study for these fields? </p><br>""",ai
"""<p><a href=""http://www.alicebot.org/articles/wallace/eliza.html"" rel=""nofollow"">From Eliza to A.L.I.C.E.</a>:</p><br><br><blockquote><br>  <p>Weizenbaum tells us that he was shocked by the experience of releasing ELIZA (also known as ""Doctor"") to the nontechnical staff at the MIT AI Lab. Secretaries and nontechnical administrative staff thought the machine was a ""real"" therapist, and spent hours revealing their personal problems to the program. When Weizenbaum informed his secretary that he, of course, had access to the logs of all the conversations, she reacted with outrage at this invasion of her privacy. Weizenbaum was shocked by this and similar incidents to find that such a simple program could so easily deceive a naive user into revealing personal information.</p><br></blockquote><br><br><p>Wikipedia's article on the <a href=""https://en.wikipedia.org/wiki/ELIZA_effect"" rel=""nofollow"">""ELIZA Effect""</a>:</p><br><br><blockquote><br>  <p>Though designed strictly as a mechanism to support ""natural language conversation"" with a computer, ELIZA's DOCTOR script was found to be surprisingly successful in eliciting emotional responses from users who, in the course of interacting with the program, began to ascribe understanding and motivation to the program's output. As Weizenbaum later wrote, <strong>""I had not realized ... that extremely short exposures to a relatively simple computer program could induce powerful delusional thinking in quite normal people.""</strong> Indeed, ELIZA's code had not been designed to evoke this reaction in the first place. Upon observation, researchers discovered users unconsciously assuming ELIZA's questions implied interest and emotional involvement in the topics discussed, <em>even when they consciously knew that ELIZA did not simulate emotion.</em></p><br></blockquote><br><br><p>ELIZA, despite its simplicity, was incredibly successful at its task of tricking other human beings. Even those who knew ELIZA was a bot would still talk to it. Obviously, ELIZA served as an inspiration for various other, more intelligent chatbots, such as <a href=""http://www.nytimes.com/2015/08/04/science/for-sympathetic-ear-more-chinese-turn-to-smartphone-program.html?_r=0"" rel=""nofollow"">Xiaoice</a>. But I would like to know what <em>exactly</em> led to such a simple program like ELIZA to be so successful in the first place.</p><br><br><p>This is very useful knowledge for a programmer since a simple program is one that would be easily maintainable.</p><br>""",ai
"""<p>What regulations are already in place regarding Artificial General Intelligences? What reports or recommendations prepared by official government authorities were already published?</p><br><br><p>So far I know of <a href=""http://www.ft.com/cms/s/2/5ae9b434-8f8e-11db-9ba3-0000779e2340.html"">Sir David King's report done for UK government</a>.</p><br>""",ai
"""<p>Most introductions to the field of MDPs and Reinforcement learning focus exclusively on domains where space and action variables are integers (and finite).<br><br>This way we are introduced quickly to Value Iteration, Q-Learning and the like.</p><br><br><p>However the most interesting applications (say, <a href=""http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.67.3518&amp;rep=rep1&amp;type=pdf"" rel=""nofollow"">flying helicopters</a>) of RL and MDPs involve continuous state space and action spaces.<br><br>I'd like to go beyond basic introductions and focus on these cases but I am not sure how to get there. </p><br><br><p>Are there any research projects or studies that deal with these cases in depth?</p><br>""",ai
"""<p>Can someone explain to me the difference between machine learning and deep learning? Is it possible to learn deep learning without knowing machine learning?</p><br>""",ai
"""<p>By new, unseen examples; I mean like the animals in <a href=""https://en.wikipedia.org/wiki/No_Man%27s_Sky"">No Man's Sky</a>. </p><br><br><p>A couple of images of the animals are:<br><a href=""http://i.stack.imgur.com/zS0rX.jpg""><img src=""http://i.stack.imgur.com/zS0rX.jpg"" alt=""enter image description here""></a></p><br><br><p><a href=""http://i.stack.imgur.com/Ir1Qt.jpg""><img src=""http://i.stack.imgur.com/Ir1Qt.jpg"" alt=""enter image description here""></a></p><br><br><p>So, upon playing this game, I was curious <strong>about how good is AI at generating visual characters or examples?</strong></p><br>""",ai
"""<p>I wanted to know what the differences between hyper-heuristics and meta-heuristics are, and what their main applications are. Which problems are suited to be solved by Hyper-heuristics?</p><br>""",ai
"""<p>What is the difference between agent function and agent program with respect to percept sequence?</p><br><br><p>In the book <em>""Artificial Intelligence: A modern approach""</em>,</p><br><br><blockquote><br>  <p>The agent function, notionally speaking, takes as input the entire<br>  percept sequence up to that point, whereas the agent program takes the<br>  current percept only.</p><br></blockquote><br><br><p>Why does the agent program only take current percept. Isn't it just implementation of the agent function?</p><br>""",ai
"""<p>Are there currently any studies to simulate gradual (or sudden) implementation of AIs in the general work force?</p><br>""",ai
"""<p>In <a href=""https://en.wikipedia.org/wiki/Portal_2"">Portal 2</a> we see that AI's can be ""killed"" by thinking about a paradox.</p><br><br><p><a href=""http://i.stack.imgur.com/wkUSC.png""><img src=""http://i.stack.imgur.com/wkUSC.png"" alt=""Portal Paradox Poster""></a></p><br><br><p>I assume this works by forcing the AI into an infinite loop which would essentially ""freeze"" the computer's consciousness.</p><br><br><p><strong>Questions:</strong> Would this confuse the AI technology we have today to the point of destroying it? <br> If so, why? And if not, could it be possible in the future?</p><br>""",ai
"""<p>In the 1950's, there were widely-held beliefs that ""Artificial Intelligence"" will quickly become both self-conscious and smart-enough to win chess with humans. Various people suggested time frames of e.g. 10 years (see Olazaran's ""Official History of the Perceptron Controversy"", or let say 2001: Space Odyssey).</p><br><br><p>When did it become clear that making computers play games like chess is not equal to AGI? Who was the first person to postulate separation of the concept of AGI from task-specific methods?</p><br>""",ai
"""<p>We hear a lot today about how <a href=""http://deeplearning4j.org/thoughtvectors"" rel=""nofollow"">thought vectors</a> are the <a href=""http://www.extremetech.com/extreme/206521-thought-vectors-could-revolutionize-artificial-intelligence"" rel=""nofollow"">Next Big Thing in AI</a>, and how they serve as the underlying representation of thought/knowledge in ANN's.  But how can one use thought vectors in other regimes, especially including symbolic logic / GOFAI?  Could thought vectors be the ""substrate"" that binds together probabilistic approaches to AI and approaches that are rooted in logic?  </p><br>""",ai
"""<p>A system makes a decision basing on a large number of <em>varied</em> factors, following a ""live"" decision tree - one that is (independently, through other subsystem) updated with new decisions, new situations.</p><br><br><p>The individual decisions can be recorded as a kind of structure:</p><br><br><ul><br><li>decision function</li><br><li>node to activate if decision is positive</li><br><li>node to activate if decision is negative</li><br></ul><br><br><p>and a node can be another decision record, or a conclusion.</p><br><br><p>This isn't entirely a binary tree, as many decisions may lead to the same conclusion - each node has two children, but may have many parents.</p><br><br><p>There is absolutely no problem storing the tree in memory - it can be database records or entries of a map, or just a list. It's perfectly sufficient for the machine.</p><br><br><p>The problem here is building the subsystem that expands the decision tree - and in particular, having a human operator understand the structure being built, to be able to tune, guide, fix, adjust it: <strong>debugging the AI learning process.</strong></p><br><br><p>The question is: how to represent that data in a human-readable way, that emphasizes the flow of the graph?</p><br><br><p>a non-working example of the answer is <a href=""https://en.wikipedia.org/wiki/Concept_map"" rel=""nofollow"">Concept map</a> - in this case it only goes so far; with more than thirty or so nodes, it becomes a jumbled mess, especially if the number of cross-connections (multiple parents) becomes significant. Maybe there exists some way of laying it out or slicing it to make it clearer...?</p><br>""",ai
"""<p>I'm currently working with the CHILDES corpus trying to create a classifier that distinguishes children whom suffer from specific language impairment (SLI) from those who are typically developing (TD).</p><br><br><p>In my readings I noticed that there really isn't a convincing set of features to distinguish the two that have been discovered yet, so I came upon the idea of trying to create a feature learning algorithm that could potentially make better ones.  </p><br><br><p>Is this possible? If so how do you suggest I approach this? From the reading I have done, most feature learning is done on image processing. Another problem is the dataset I have is potentially too small to make it work (in the 100's) unless I find a way to get more transcripts from children.</p><br>""",ai
"""<p>An AI box is a (physical) barrier preventing an AI from using too much of his environment to accomplish his final goal. For example, an AI given the task to check, say, 10<sup>50</sup> cases of a mathematical conjecture as fast as possible, might decide that it would be better to also take control over all other computers and AI to help him. </p><br><br><p>However, an transhuman AI might be able to talk to a human until the human lets him out of the box. In fact, <a href=""http://www.yudkowsky.net/singularity/aibox/"" rel=""nofollow"">Eliezer Yudowsky</a> has conducted an experiment twice, where he played the AI and he twice convinced the Gatekeeper to let him out the box. However, he does not want to reveal what methods he used to get out of the box.</p><br><br><p><strong>Questions:</strong> Are there conducted any similiar experiments? <br> If so, is it known what methods were used to get out in those experiments?</p><br>""",ai
"""<p>AI systems today are very capable machines, and recently the area of Natural Language Processing and Response has been exploding with innovation, as well as the fundamental algorithmic structure of AI machines.</p><br><br><p>I am asking if, given these recent breakthroughs, have any AI systems been developed that are able to (preferably with some measure of success) knowingly lie to humans about facts that it knows?</p><br><br><p>Note, what I'm asking goes beyond the canonical discussions of the Turing Test. I'm asking of machines that can 'understand' facts and then formulate a lie against this fact, perhaps using other facts to produce a believable 'cover-up' as part of the lie.</p><br><br><p>E.G.: CIA supercomputer is stolen by spies and they try to use the computer to do things, but the computer keeps saying it's missing dependencies though it really isn't or gives correct-looking but wrong answers knowingly. Or gives incorrect location of a person, knowing that the person frequents some place but isn't there at the moment. Doesn't have to be this sophisticated, of course.</p><br>""",ai
"""<p><em>""An artificial or constructed language (sometimes called a conlang) is a language that has been created by a person or small group, instead of being formed naturally as part of a culture.""</em> (<a href=""https://simple.wikipedia.org/wiki/Constructed_language"" rel=""nofollow"">Source: Simply English Wikipedia</a>)</p><br><br><p>My question is, could an AI make construct it's own natural language, with words, conjugations and grammar rules? Basically, a language that humans could use to speak to each other. (Preferably to communicate abstract, high-level concepts.)</p><br><br><p>What techniques could such an AI use? Could it be based on existing natural languages or would it have few connections to existing natural languages? Could it design a language that's easier to learn than existing languages (even <a href=""https://en.wikipedia.org/wiki/Esperanto"" rel=""nofollow"">Esperanto</a>)?</p><br>""",ai
"""<p>I want to start with a scenario that got me thinking about how well MCTS can perform:<br>Let's assume there is a move that is not yet added to the search tree. It is some layers/moves too deep. But if we play this move the game is basically won. However let's also assume that <em>all</em> moves that could be taken instead at the given game state are very very bad. For the sake of argument let's say there are 1000 possible moves and only one of them is good (but very good) and the rest is very bad. Wouldn't MCTS fail to recognize this and <em>not</em> grow the search tree towards this move and also rate this subtree very badly? <br>I know that MCTS eventually converges to minimax (and eventually it will build the whole tree if there is enough memory). Then it should know that the move is good even though there are many bad possiblities. But I guess in practice this is not something that one can rely on.<br>Maybe someone can tell me if this is a correct evaluation on my part.</p><br><br><p>Apart from this special scenario I'd also like to know if there are other such scenarios where MCTS will perform badly (or extraordinary well). </p><br>""",ai
"""<p>I'm reading such nonsense about how an AI would turn the world into a supercomputer to solve a problem that it thought it needed to solve. That wouldn't be AI. That's procedural programming stuck in some loop nonsense. An AI would need to evolve and re-organise its neurons. It wouldn't be stuck to hardcode if it becomes intelligent by re-writing its code.</p><br>""",ai
"""<p>I'm in the process of learning as much about chatbots/CUI applications as possible and I'm trying to find more information on some of the major players in this field. By this, I mean any execs, developers, academics, designers, etc. who are doing cutting edge things. Some examples could be David Marcus (VP of messaging products at Facebook) or Adam Cheyer (VP of engineering at Viv).</p><br>""",ai
"""<p>How big artificial neural networks can we run now (either with full train-backprop cycle or just evaluating network outputs) if our total energy budget for computation is equivalent to human brain energy budget (<a href=""http://www.scientificamerican.com/article/thinking-hard-calories/"">12.6 watts</a>)?</p><br><br><p>Let assume one cycle per second, which seems to roughly match the <a href=""http://www.jneurosci.org/content/31/45/16217.full"">firing rate of biological neurons</a>.</p><br>""",ai
"""<blockquote><br>  <p>Artificial Intelligence is a rather pernicious label to attach to a very mixed bunch of activities, and one could argue that the sooner we forget it the better. It would be disastrous to conclude that AI was a Bad Thing and should not be supported, and it would be disastrous to conclude that it was a Good Thing and should have privileged access to the money tap. The former would tend to penalise well-based efforts to make computers do complicated things which had not been programmed before, and the latter would be a great waste of resources. AI does not refer to anything definite enough to have a coherent policy about in this way.---<a href=""http://www.math.snu.ac.kr/~hichoi/infomath/Articles/Lighthill%20Report.pdf"" rel=""nofollow"">Dr. R. M. Needham, in a commentary on the Lighthill Report and the Sutherland Reply, 1973</a></p><br></blockquote><br><br><p>43 years later...</p><br><br><blockquote><br>  <p>There is already strong demand for engineers and scientists working on artificial intelligence in many of the fields you mention, and many more. But expertise in making real-time systems for controlling trains doesn't make you know anything about robotics. Analyzing human behavior to detect crime has virtually nothing in common with self-driving cars (beyond CS/pattern recognition building blocks). There is never going to be demand for someone with a broad sense of all these areas without any deep expertise, and there is never going to be someone with 300 PhDs who can work in all of them. TL;DR -- AI is not a branch, it's a tree. --<a href=""http://discuss.area51.stackexchange.com/questions/22441/why-yet-another-trial-at-an-ai-project#comment36342_22539"">Matthew Read, in a comment on Area 51 Stackexchange, 2016</a></p><br></blockquote><br><br><p>AI is a label that is applied to a ""very mixed bunch of activities"". The only unifying feature between all those activities is the fact that they deal with machines in some fashion, but since there are so many ways to use a machine, the field's output may seem rather incoherent and incongruent. It does seem to make more sense for the AI field to collapse entirely, and instead be replaced by a multitude of specialized fields that don't really interact with one another. Sir James Lighthill appeared to have supported this sort of approach within his 1973 report on the state of artificial intelligence research.</p><br><br><p>Yet, today, this Artificial Intelligence SE exist, and we still talk of AI as a unified, coherent field of study. Why did this happen? Why did AI survive, despite its ""big tent"" nature?</p><br>""",ai
"""<p>Let's suppose that we have a legacy system in which we don't have the source code and this system is on a mainframe written in Cobol. Is there any way using machine learning in which we can learn from the inputs and outputs the way the executables work? Doing this analysis could lead to develop some rest / soap webservice that can substitute the legacy system in my opinion. </p><br>""",ai
"""<p>Sometimes I understand that people doing <em>cognitive science</em> try to avoid the term <em>artificial intelligence</em>. The feeling I get is that there is a need to put some distance to the <a href=""https://en.wikipedia.org/wiki/Symbolic_artificial_intelligence"" rel=""nofollow"">GOFAI</a>.</p><br><br><p>Another impression that I get is that <em>cognitive science</em> is more about trying to find out how the human <em>intelligence</em>(?)... <em>Mind</em>? works... And that it would use <em>artificial intelligence</em> to make tests or experiments, to test ideas and so forth...</p><br><br><p>Is Artificial Intelligence (only) a research tool for Cognitive Science?</p><br><br><p><strong>What is the difference between Artificial Intelligence and Cognitive Science?</strong></p><br>""",ai
"""<p>Just for fun, I am trying to develop a neural network.</p><br><br><p>Now, for backpropagation I saw two techniques.</p><br><br><p>The first one is used <a href=""http://courses.cs.washington.edu/courses/cse599/01wi/admin/Assignments/bpn.html"" rel=""nofollow"">here</a> and in many other places too.</p><br><br><p>What it does is:</p><br><br><ul><br><li>It computes the error for each output neuron.</li><br><li>It backpropagates it into the network (calculating an error for each inner neuron).</li><br><li>It updates the weights with the formula: <img src=""http://latex.codecogs.com/gif.latex?%5CDelta%20w_%7Bl%2Cm%2Cn%7D%20%3D%20k%20%5Ccdot%20E_%7Bl&plus;1%2Cn%7D%20%5Ccdot%20N_%7Bl%2Cm%7D"" alt=""""> (where <img src=""http://latex.codecogs.com/gif.latex?%5CDelta%20w_%7Bl%2Cm%2Cn%7D"" alt=""""> is the change in weight, <img src=""http://latex.codecogs.com/gif.latex?k"" alt=""""> the learning speed, <img src=""http://latex.codecogs.com/gif.latex?E_%7Bl&plus;1%2Cn%7D"" alt=""""> the error of the neuron receiving the input from the synapse and <img src=""http://latex.codecogs.com/gif.latex?N_%7Bl%2Cm%7D"" alt=""""> being the output sent on the synapse).</li><br><li>It repeats for each entry of the dataset, as many times as required.</li><br></ul><br><br><p>However, the neural network proposed in <a href=""https://www.youtube.com/watch?v=bxe2T-V8XRs&amp;list=PL77aoaxdgEVDrHoFOMKTjDdsa0p9iVtsR"" rel=""nofollow"">this tutorial</a> (also available on GitHub) uses a different technique:</p><br><br><ul><br><li>It uses an error function (the other method does have an error function, but it does not use it for training).</li><br><li>It has another function which can compute the final error starting from the weights.</li><br><li>It minimizes that function (through gradient descent).</li><br></ul><br><br><p>Now, which method should be used?</p><br><br><p>I think the first one is the most used one (because I saw different examples using it), but does it work as well?</p><br><br><p>In particular, I don't know:</p><br><br><ul><br><li>Isn't it more subject to local minimums (since it doesn't use quadratic functions)?</li><br><li>Since the variation of each weight is influenced by the output value of its output neuron, don't entries of the dataset which just happen to produce higher values in the neurons (not just the output ones) influence the weights more than other entries?</li><br></ul><br><br><p>Now, I do prefer the first technique, because I find it simpler to implement and easier to think about.</p><br><br><p>Though, if it does have the problems I mentioned (which I hope it doesn't), is there any actual reason to use it over the second method?</p><br>""",ai
"""<p>The English Language is not well-suited to talking about artificial intelligence, which makes it difficult for humans to communicate to each other about what an AI is actually ""doing"". Thus, it may make more sense to use ""human-like"" terms to describe the actions of machinery, even when the internal properties of the machinery do not resemble the internal properties of humanity.</p><br><br><p>Anthropomorphic language had been used a lot in technology (see the Hacker's Dictionary definition of <a href=""https://www.landley.net/history/mirror/jargon.html#Anthropomorphization"">anthropomorphization</a>, which attempts to justify computer programmers' use of anthromporhic terms when describing technology), but as AI continues to advance, it may be useful to consider the tradeoffs of using anthropomorphic language in communicating to both technical audiences and non-technical audiences. How can we get a good handle on AI if we can't even describe what we're doing?</p><br><br><p>Suppose I want to develop an algorithm that display a list of related articles. There are two ways by which I can explain how the algorithm works to a layman:</p><br><br><ol><br><li><em>Very Anthropomorphic</em> - The algorithm reads all the articles on a website, and display the articles that are very similar to the article you are looking at.</li><br><li><em>Very Technical</em> - The algorithm converts each article into a ""bag-of-words"", and then compare the ""bag-of-words"" of each article to determine what articles share the most common words. The articles that share the most words in the bags are the ones that are displayed to the user.</li><br></ol><br><br><p>Obviously, #2 may be more ""technically correct"" than #1. By detailing the implementation of the algorithm, it makes it easier for someone to understand how to <em>fix</em> the algorithm if it produces an output that we disagree with heavily.</p><br><br><p>But #1 is more readable, elegant, and easier to understand. It provides a general sense of <em>what</em> the algorithm is doing, instead of <em>how</em> the algorithm is doing it. By abstracting away the implementation details of how a computer ""reads"" the article, we can then focus on using the algorithm in real-world scenarios.</p><br><br><p>Should I, therefore, prefer to use the anthropomorphic language as emphasized by Statement #1? If not, why not?</p><br><br><p>P.S.: If the answer depends on the audience that I am speaking to (a non-technical audience might prefer #1, while a technical audience may prefer #2), then let me know that as well.</p><br>""",ai
"""<p>Roger Schank did some interesting work on language processing with Conceptual Dependency (CD) in the 1970s. He then moved somewhat out of the field, being in Education these days. There were some useful applications in natural language generation (BABEL), story generation (TAILSPIN) and other areas, often involving planning and episodes rather than individual sentences.</p><br><br><p>Has anybody else continued to use CD or variants thereof? I am not aware of any other projects that do, apart from Hovy's PAULINE which uses CD as representation for the story to generate.</p><br>""",ai
"""<p>I have been wanting to get started learning about artificial intelligence but I know almost nothing about coding or anything. So my question is, what would be the best way to get started in learning about artificial intelligence, as in should I learn some kind of coding language or is there some kind of other concept you need to know before getting started. So I'm just kind of looking for the best way to get started if you literally know nothing.</p><br>""",ai
"""<p>I have been studying local search algorithms such as greedy hill climbing, stochastic hill climbing, simulated annealing etc. I have noticed that most of these methods take up very little memory as compared to systematic search techniques.</p><br><br><p>Are there local search algorithms that make use of memory to give significantly better answers than those algorithms that use little memory (such as crossing local maxima)? Also, is there a way to combine local search and systematic search algorithms to get the best of both worlds?</p><br>""",ai
"""<p><em>I know that every program has some positive and negative points, and I know maybe .net programming languages are not the best for AI programming.</em></p><br><br><p><strong>But I prefer .net programming languages because of my experiences and would like to know for an AI program which one is better, C or C++ or C# and or VB ?</strong></p><br><br><p><em>Which one of this languages is faster and more stable when running different queries and for self learning ?</em></p><br><br><p>To make a summary, i think C++ is the best for AI programming in .net and also C# can be used in some projects, Python as recommended by others is not an option on my view !</p><br><br><p>because : </p><br><br><ol><br><li><p>It's not a complex language itself and for every single move you need to find a library and import it to your project (most of the library are out of date and or not working with new released Python versions) and that's why people say it is an easy language to learn and use ! (If you start to create library yourself, this language could be the hardest language in the world !)</p></li><br><li><p>You do not create a program yourself by using those library for every single option on your project (it's just like a Lego game)</p></li><br><li><p>I'm not so sure in this, but i think it's a cheap programming language because i couldn't find any good program created by this language !</p></li><br></ol><br>""",ai
"""<p>When I visit this site, I find the word ""search"" appears quite often. </p><br><br><p>But why is it important? What kinds of search algorithms are used in Artificial Intelligence?  And how do they improve the result of an AI?</p><br>""",ai
"""<p>Considering the answers of <a href=""http://ai.stackexchange.com/questions/1314/how-powerful-a-computer-is-required-to-simulate-the-human-brain"">this</a> question, emulating a human brain with the current computing capacity is currently impossible, but we aren't very far from it.</p><br><br><p>Note, 1 or 2 decades ago, similar calculations had similar results.</p><br><br><p>The clock frequency of the modern CPUs seem to be stopped, currently the miniaturization (-> mobile use), the RAM/cache improvement and the multi-core paralellization are the main lines of the development.</p><br><br><p>Ok, but what is the case with the analogous chips? In case of a NN, it is not a very big problem, if it is not very accurate, the NN would adapt to the minor manufacturing differences in its learning phase. And a single analogous wire can substitute a complex integer multiplication-division unit, while the whole surface of the analogous printed circuit could work parallel.</p><br><br><p>According to <a href=""http://engineering.stackexchange.com/questions/3993/do-analog-fpgas-exist"">this</a> post, ""software rewirable"" analogous circuits, essentially ""analogous FPGAs"" already exist. Although the capacity of the FPGAs is highly below the capacity of the <a href=""https://en.wikipedia.org/wiki/Application-specific_integrated_circuit"" rel=""nofollow"">ASIC</a>s with the same size, maybe analogous chips for neural networks could also exist.</p><br><br><p>I suspect, if it is correct, maybe even the real human brain model wouldn't be too far. It would still require a massively parallel system of costly analogous NN chips, but it seems to me not impossible.</p><br><br><p>Could this idea work? Maybe there is even active research/development into this direction?</p><br>""",ai
"""<p>Conceptually speaking, aren't artificial neural networks just highly distributed, lossy compression schemes?</p><br><br><p>They're certainly efficient at <a href=""https://cs.stanford.edu/people/eroberts/courses/soco/projects/neural-networks/Applications/imagecompression.html"" rel=""nofollow"">compressing images</a>.</p><br><br><p>And aren't brains (at least, the neocortex) just compartmentalized, highly distributed, lossy databases?</p><br><br><p>If so, what salient features in RNNs and CNNs are necessary in any given lossy compression scheme in order to extract the semantic relations that they do? Is it just a matter of having a large number of dimensions/variables? </p><br><br><p>Could some kind of lossy <a href=""https://en.wikipedia.org/wiki/Bloom_filter"" rel=""nofollow"">Bloom filter</a> be re-purposed for the kinds of problems ANNs are applied to?</p><br>""",ai
"""<p>Consciousness <a href=""http://www.iep.utm.edu/consciou/"">is challenging to define</a>, but for this question let's define it as ""actually experiencing sensory input as opposed to just putting a bunch of data through an inanimate machine."" Humans, of course, have minds; for normal computers, all the things they ""see"" are just more data. One could alternatively say that humans are <a href=""http://philosophy.stackexchange.com/a/4687"">sentient</a>, while traditional computers are not.</p><br><br><p>Setting aside the question of whether it's possible to build a sentient machine, does it actually make a difference if an AI is sentient or not? In other words, are there are tasks that are made impossible - not just more difficult - by a lack of sentience?</p><br>""",ai
"""<p>A lot of textbooks and introductory lectures typically split AI into connectionism and GOFAI (Good Old Fashioned AI). <br>From a purely technical perspective it seems that connectionism has grown into machine learning and data science, while nobody talks about GOFAI, Symbolic AI or Expert Systems at all. </p><br><br><p>Is anyone of note still working on GOFAI?    </p><br>""",ai
"""<p>I recently finished Course on RL by David Silver (on YT) and thought about trying it out on simple application in Unity Game Engine, where I've built simple labyrint with ball and want to teach the ball to get from point A to point B in there while avoiding obstacles and fire (the place where you'll get burnt so big negative reward)</p><br><br><p>The problem I encountered while designing the whole thing (programming-wise) is: What is the correct (or at least good) way of representing the position in 2D space? It is continuous so I thought about representing it as feature vector consisting of [up, down, left, right, posX, posY] where direction is whether I am pressing button of moving in that direction in binary (or actions if you want) and pos are floats (0-1) representing normalized position from one corner on the plane where the whole map is. That would be accompanied by vector W that would represent the weights adjusted using Gradient Descent.</p><br><br><p>Question is: will this work?? I am asking for 2 reasons. One is that I am not so sure about that posX and posY since it can be 0 and if I multiply it by the weights vector then how could be resulting reward anything but 0? Second reason is that I am not sure if the actions should be part of the features. I mean, it makes sense to me but I could easily be very wrong since I am a beginner.</p><br><br><p>Thanks a lot guys in advance. If you have any more questions or think the problem is not described deeply enough just ask in the comments and I'll edit the question. :)</p><br><br><p>PS: I could just code it the way I think is right, but I also want to get gasp of designing applications on paper before coding them (project management).</p><br>""",ai
"""<p>Currently I work as a java developer, But very much interested in learning Artificial Intelligence.<br>Can anybody tell me what steps i have to follow to learn artificial intelligence considering the fact i am very new to this.<br>Is there any special technologies i have to learn or something else.</p><br>""",ai
"""<p>Self-Recognition seems to be an item that designers are trying to integrate into artificial intelligence. Is there a generally recognized method of doing this in a machine, and how would one test the capacity - as in a Turing-Test?</p><br>""",ai
"""<p>I know that deepmind used deep Q learning (<a href=""https://deepmind.com/research/dqn/"" rel=""nofollow"">DQN</a>) for its Atari game AI. It used a <a href=""https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf"" rel=""nofollow"">conv neural network</a> (CNN) to approximate <code>Q(s,a)</code> from pixels instead of from a Q-table. I want to know how DQN converted input to an action. How many output did the CNN have? How did they train the neural network for prediction?</p><br><br><p>Here are the steps that I believe are happening inside DQN:</p><br><br><blockquote><br>  <p>1) A game picture (a state) is send to CNN as input value</p><br>  <br>  <p>2) CNN predicts an output as action (eg:left, right, shoot, etc)</p><br>  <br>  <p>3) Simulator applies the predicted action and moves to new game state</p><br>  <br>  <p>4) repeat step 1</p><br></blockquote><br><br><p>The problem with my above logic is in <strong>step 2</strong>. CNN is used for predicting an action, but when is CNN trained for prediction? </p><br><br><p>I would prefer if you used less math for explanation.</p><br><br><p>EDIT</p><br><br><p>I want to add some more questions regarding the same topic</p><br><br><p>1) How reward is passed in the neural network? that is how neural network knows whether its output action obtained positive or negative reward?</p><br><br><p>2) How many output the neural network has and how action is determined from those outputs?</p><br>""",ai
"""<p>In the lecture, there was a statement:</p><br><br><blockquote><br>  <p>""Recurrent neural networks with multiple hidden layers are just a<br>  special case that has some of the hidden to hidden connections<br>  missing.""</p><br></blockquote><br><br><p>I understand recurrent means that can have connections to the previous layer and the same layer as well. Is there a visualization available to easily understand the above statement?</p><br>""",ai
"""<p>Inattentional Blindness is common in humans (see: <a href=""https://en.wikipedia.org/wiki/Inattentional_blindness"" rel=""nofollow"">https://en.wikipedia.org/wiki/Inattentional_blindness</a> ). Could this also be common with machines built with artificial vision?</p><br>""",ai
"""<p>My question is regarding standard dense-connected feed forward neural networks with sigmoidal activation.</p><br><br><p>I am studying Bayesian Optimization for hyper-parameter selection for neural networks. There is no doubt that this is an effective method, but I just wan't to delve a little deeper into the maths.</p><br><br><p><strong>Question:</strong> Are neural networks <a href=""http://mathworld.wolfram.com/LipschitzFunction.html"" rel=""nofollow"">Lipschitz</a> functions?</p><br>""",ai
"""<p>Can one actually kill a machine? Not only do we have problems in defining life, we also have problems in defining death. Will this also be true in artificial life and artificial intelligence?</p><br>""",ai
"""<p>Generally, people can be classified as aggressive (Type A) or passive. Could the programming of AI systems cause aggressive or passive behavior in those AIs?</p><br>""",ai
"""<p>Assuming mankind will eventually create artificial humans, but in doing so have we put equal effort into how humans will relate to an artificial human, and what can we expect in return? This is happening in real-time as we place AI trucks and cars on the road. Do people have the right to question, maybe in court, if an AI machine breaks a law?</p><br>""",ai
"""<p>AI death is still unclear a concept, as it may take several forms and allow for ""coming back from the dead"". For example, an AI could be somehow forbidden to do anything (no permission to execute), because it infringed some laws.</p><br><br><p>""Somehow forbid"" is the topic of this question. There will probably be rules, like ""AI social laws"", that can conclude an AI should ""die"" or ""be sentenced to the absence of progress"" (a jail). Then who or what could manage that AI's state?</p><br>""",ai
"""<p>Can self-driving cars deal with snow, heavy rain, or other weather conditions like these? Can they deal with unusual events, such as <a href=""http://beijingcream.com/wp-content/uploads/2012/06/Ducks-galore-2.jpeg"" rel=""nofollow"">ducks on the road</a>?</p><br><br><p><a href=""http://i.stack.imgur.com/a0PVLm.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/a0PVLm.jpg"" alt=""ducks on the road""></a></p><br>""",ai
"""<p>Most of the people is trying to answer question with a neural network. However, has anyone came up with some thoughts about how to make neural network ask questions, instead of answer questions? For example, if a CNN can decide which category an object belongs to, than can it ask some question to help the the classification?</p><br>""",ai
"""<p>The Mars Exploration Rover (MER) <em><a href=""http://www.nasa.gov/mp4/618340main_mer20120124-320-jpl.mp4"" rel=""nofollow"">Opportunity</a></em> landed on Mars on January 25, 2004. The rover was originally designed for a 90 <strong>Sol mission</strong> (a Sol, one Martian day, is slightly longer than an Earth day at 24 hours and 37 minutes). Its mission has been extended several times, the machine is still trekking after 11 years on the Red Planet.</p><br><br><p>How it has been working for 11 years? Can anyone please explain how smart this rover is? What AI concepts are behind this?</p><br>""",ai
"""<p>I have used OpenCV to train Haar cascades to detect face and other patterns. However I later realized that Haar tends to give a lot of false positives and I learned of Hog would give a more accurate results. But OpenCV doesn't have a good documentation of how to train hogs, I have googled a bit and found results that includes SVM and others.</p><br><br><p>OpenCV also has versioning problem where they move certain classes or functions somewhere else.</p><br><br><p>Are there any other techniques/method that I can use to train and detect objects and patterns? Preferably with proper documentation and basic tutorial/examples. Language preference: C#, Java, C++, Python</p><br>""",ai
"""<p>Are the future robots/machines going to use Stack Exchange communities to teach themselves? Are there any ongoing projects? Just imagine a bot having a memory of all the Q&amp;A's on all of the communities! </p><br>""",ai
"""<p>Mankind can create machines to do work. Could we also create a (passion) within the machines to do better work by using Artificial Intelligence? Would passion cause the machine to do a better job, and could we measure the quantity/quality of passion by comparing outputs of the machine - that is, those machines with passion, and those without?</p><br>""",ai
"""<p>Can AI systems be created that could recognize itself, and recognize intelligence in other systems, and make intelligent decisions about the other systems? Mankind seems to be making progress in self-recognition but I've not seen evidence of one system recognizing other systems and being able to compare it's own intelligence with other systems. How could this be accomplished?</p><br>""",ai
"""<p>If IQ were used as a measure of the intelligence of machines, as in humans, at this point in time what would be the IQ of our most intelligent AI systems? If not IQ, then how best to compare our intelligence to a machine, or one machine to another? </p><br><br><p>This question is not asking if we can measure the IQ of a machine, but if IQ is the most preferred, or general, method of measuring intelligence then how does artificial intelligence compare to our most accepted method of measuring intelligence in humans. Many people may not understand the relevance of a Turing Test as to how intelligent their new car is, or other types of intelligent machines.</p><br>""",ai
"""<p>I was think about AIs and how they would work, when I realised that I couldn't think of a way that an AI could be taught language. A child tends to learn language through associations of language and pictures to an object (e.g: people saying the word <code>dog</code> while around a dog, and later realising that  people say <code>a dog</code> and <code>a car</code> and learn what <code>a</code> means...). However, a text based AI couldn't use this method to learn, as they wouldn't have access to any sort of input device.</p><br><br><p>The only way I could come up with is programming in every word, and rule, in the English language (or whatever language it is meant to 'speak' in), however that would, potentially, take years to do.</p><br><br><p>Does anyone have any ideas on how this could be done? Or if it has been done already, if so how?</p><br><br><p>Thanks in advance for any ideas.</p><br><br><p>Btw: in this context, I am using AI to mean an Artificial Intelligence system with near-human intelligence, and no prior knowledge of language.</p><br>""",ai
"""<ul><br><li>Would AI be a self-propogating iteration in which the previous AI is<br>destroyed by a more optimised AI child?  </li><br><li>Would the AI have branches of it's own AI warning not to create the new AI?</li><br></ul><br>""",ai
"""<blockquote><br>  <p>Shortly about <a href=""https://en.wikipedia.org/wiki/Deep_learning"" rel=""nofollow""><strong>deep learning</strong> (for reference)</a>:</p><br>  <br>  <p><strong><em>Deep learning</strong> is a branch of machine learning based on a set of algorithms that attempt to model high-level abstractions in data by<br>  using a deep graph with multiple processing layers, composed of<br>  multiple linear and non-linear transformations.</em></p><br>  <br>  <p><em>Various deep learning architectures such as deep neural networks, convolutional deep neural networks, deep belief networks and recurrent<br>  neural networks have been applied to fields like computer vision,<br>  automatic speech recognition, natural language processing, audio<br>  recognition and bioinformatics where they have been shown to produce<br>  state-of-the-art results on various tasks.</em></p><br></blockquote><br><br><hr><br><br><p><strong>My question:</strong></p><br><br><p>Can <a href=""https://en.wikipedia.org/wiki/Deep_learning#Deep_neural_network_architectures"" rel=""nofollow"">deep neural networks</a> or <a href=""https://en.wikipedia.org/wiki/Convolutional_neural_network"" rel=""nofollow"">convolutional deep neural networks</a> be viewed as <a href=""https://en.wikipedia.org/wiki/Ensemble_learning"" rel=""nofollow"">ensemble-based</a> method of machine learning? Or it is different approaches?</p><br>""",ai
"""<p>Are Convolutional Neural Networks summarily better than pattern recognition in all existing image processing libraries that don't use CNN's? Or are there still hard outstanding problems in image processing that seem to be beyond their capability?</p><br>""",ai
"""<p>I have been messing around in <a href=""http://playground.tensorflow.org/#activation=tanh&amp;batchSize=10&amp;dataset=spiral&amp;regDataset=reg-plane&amp;learningRate=0.03&amp;regularizationRate=0&amp;noise=0&amp;networkShape=4,2&amp;seed=0.73263&amp;showTestData=false&amp;discretize=false&amp;percTrainData=50&amp;x=true&amp;y=true&amp;xTimesY=false&amp;xSquared=false&amp;ySquared=false&amp;cosX=false&amp;sinX=false&amp;cosY=false&amp;sinY=false&amp;collectStats=false&amp;problem=classification&amp;initZero=false&amp;hideText=false"" rel=""nofollow"">tensorflow playground</a>. One of the input data sets is a spiral. No matter what input parameters I choose, no matter how wide and deep the neural network I make, I cannot fit the spiral. How do data scientists fit data of this shape?</p><br>""",ai
"""<p>A ""general intelligence"" may be capable of learning a lot of different things, but possessing capability does not equal actually having it. The ""AGI"" must learn...and that learning process can take time. If you want an AGI to drive a car or play Go, you have to find some way of ""teaching"" it. Keep in mind that we have never built AGIs, so we don't know how long the training process can be, but it would be safe to assume pessimistic estimates.</p><br><br><p>Contrast that to a ""narrow intelligence"". The narrow AI already knows how to drive a car or play Go. It has been programmed to be very excellent at one specific task. You don't need to worry about training the machine, because it has already been pre-trained.</p><br><br><p>A ""general intelligence"" seems to be more flexible than a ""narrow intelligence"". You could buy an AGI and have it drive a car <em>and</em> play Go. And if you are willing to do more training, you can even teach it a new trick: <em>how to bake a cake</em>. I don't have to worry about unexpected tasks coming up, since the AGI will <em>eventually</em> figure out how to do it, given enough training time. I would have to wait a <em>long time</em> though.</p><br><br><p>A ""narrow intelligence"" appears to be <em>more efficient</em> at its assigned task, due to it being programmed specifically for that task. It knows exactly what to do, and doesn't have to waste time ""learning"" (unlike our AGI buddy here). Instead of buying one AGI to handle a bunch of different tasks poorly, I would rather buy a bunch of specialized narrow AIs. Narrow AI #1 drives cars, Narrow AI #2 plays Go, Narrow AI #3 bake cakes, etc. That being said, this is a very brittle approach, since if some unexpected task comes up, none of my narrow AIs would be able to handle it. I'm willing to accept that risk though.</p><br><br><p>Is my ""thinking"" correct? Is there a trade-off between flexibility (AGI) and efficiency (narrow AI), like what I have just described above? Or is it theoretically possible for an AGI to be both flexible and efficient?</p><br>""",ai
"""<p>Is there a neural network(NN) system or architecture which can be used for only storing and retrieving information. For example; to store whole Avatar movie in HD format inside a neural network and retrieve(without loss) it from the neural network when needed. I searched the web and came across only LSTM RNN but in my understanding LSTM only stores pattern and not the content itself. If there is no such NN exist can you explain why it so?</p><br>""",ai
"""<p>The question is about the architecture of Deep Residual Networks (<strong>ResNets</strong>). The model that won the 1-st places at <a href=""http://image-net.org/challenges/LSVRC/2015/results"" rel=""nofollow"">""Large Scale Visual Recognition Challenge 2015"" (ILSVRC2015)</a> in all five main tracks:</p><br><br><blockquote><br>  <ul><br>  <li><em>ImageNet Classification: “Ultra-deep” (quote Yann) 152-layer nets</em> </li><br>  <li><em>ImageNet Detection: 16% better than 2nd</em></li><br>  <li><em>ImageNet Localization: 27% better than 2nd</em></li><br>  <li><em>COCO Detection: 11% better than 2nd</em></li><br>  <li><em>COCO Segmentation: 12% better than 2nd<br><br></em><br>  <em>Source:</em> <a href=""http://image-net.org/challenges/talks/ilsvrc2015_deep_residual_learning_kaiminghe.pdf"" rel=""nofollow""><em>MSRA @ ILSVRC &amp; COCO 2015 competitions (presentation, 2-nd slide)</em></a></li><br>  </ul><br></blockquote><br><br><p>This work is described in the following article:</p><br><br><blockquote><br>  <p><a href=""http://arxiv.org/abs/1512.03385"" rel=""nofollow""><em>Deep Residual Learning for Image Recognition (2015, PDF)</em></a></p><br></blockquote><br><br><hr><br><br><p><strong>Microsoft Research team</strong> (developers of ResNets: Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun) in their article:</p><br><br><blockquote><br>  <p><a href=""https://arxiv.org/pdf/1603.05027.pdf"" rel=""nofollow"">""<em>Identity Mappings in Deep Residual Networks (2016)</em>""</a></p><br></blockquote><br><br><p>state that <strong>depth</strong> plays a key role:</p><br><br><blockquote><br>  <p><em>""<strong>We obtain these results via a simple but essential concept — going deeper. These results demonstrate the potential of pushing the limits of depth.</strong>""</em></p><br></blockquote><br><br><p>It is emphasized in their <a href=""http://image-net.org/challenges/talks/ilsvrc2015_deep_residual_learning_kaiminghe.pdf"" rel=""nofollow"">presentation</a> also (deeper - better):<br> </p><br><br><blockquote><br>  <p><em>- ""A deeper model should not have higher training error.""<br> <br>  - ""Deeper ResNets have lower training error, and also lower test error.""<br> <br>  - ""Deeper ResNets have lower error.""<br><br>  - ""All benefit more from deeper features – cumulative gains!""<br><br>  - ""Deeper is still better.""</em></p><br></blockquote><br><br><p>Here is the sctructure of 34-layer residual (for reference):<br><a href=""http://i.stack.imgur.com/L8m0X.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/L8m0X.png"" alt=""enter image description here""></a></p><br><br><hr><br><br><p>But recently I have found one theory that introduces a novel interpretation of residual networks showing they are exponential ensembles:</p><br><br><blockquote><br>  <p><a href=""https://arxiv.org/abs/1605.06431"" rel=""nofollow""><em>Residual Networks are Exponential Ensembles of Relatively Shallow Networks (2016)</em></a></p><br></blockquote><br><br><p>Deep Resnets are described as many shallow networks whose outputs are pooled at various depths. <br>There is a picture in the article. I attach it with explanation:</p><br><br><blockquote><br>  <p><a href=""http://i.stack.imgur.com/PGhK2.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/PGhK2.jpg"" alt=""enter image description here""></a> Residual Networks are<br>  conventionally shown as (a), which is a natural representation of<br>  Equation (1). When we expand this formulation to Equation (6), we<br>  obtain an unraveled view of a 3-block residual network (b). From this<br>  view, it is apparent that residual networks have O(2^n) implicit paths<br>  connecting input and output and that adding a block doubles the number<br>  of paths.</p><br></blockquote><br><br><p>In conclusion of the article it is stated:</p><br><br><blockquote><br>  <p><strong>It is not depth, but the ensemble that makes residual networks strong</strong>.<br>  Residual networks push the limits of network multiplicity, not network<br>  depth. Our proposed unraveled view and the lesion study show that<br>  residual networks are an implicit ensemble of exponentially many<br>  networks. If most of the paths that contribute gradient are very short<br>  compared to the overall depth of the network, <strong>increased depth</strong><br>  alone <strong>can’t be the key characteristic</strong> of residual networks. We now<br>  believe that <strong>multiplicity</strong>, the network’s expressability in the<br>  terms of the number of paths, plays <strong>a key role</strong>.</p><br></blockquote><br><br><p>But it is only a recent theory that can be confirmed or refuted. It happens sometimes that some theories are refuted and articles are withdrawn.</p><br><br><hr><br><br><p><strong>My question:</strong><br><br>Should we think of deep ResNets as ensemble after all? <strong>Ensemble</strong> or <strong>depth</strong> makes residual networks so strong? Is it possible that even the developers themselves do not quite perceive what their own model represent and what is the key concept in it?</p><br>""",ai
"""<p>In my attempt at trying to learn neural network and machine learning I'm am trying to create a simple neural network which can be trained to recognise one word from a given string (which contains only one word). So in effect if one where to feed it a string containing the trained word but spelled wrong the network would be able to still recognise the word. Can anybody help me with some pseudo code or a start of a code. Or a general explanation of how to to this because I have read like 6 articles and 8 example projects and still have no clue how to do this</p><br>""",ai
"""<p>In 2004 <a href=""https://en.wikipedia.org/wiki/Jeff_Hawkins"">Jeff Hawkins</a>, inventor of the palm pilot, published a very interesting book called <a href=""https://en.wikipedia.org/wiki/On_Intelligence"">On Intelligence</a>, in which he details a theory how the human neocortex works. </p><br><br><p>This theory is called <a href=""https://en.wikipedia.org/wiki/Memory-prediction_framework"">Memory-Prediction framework</a> and it has some striking features, for example not only bottom-up (feedforward), but also top-down information processing and the ability to make simultaneous, but discrete predictions of different future scenarios (as described <a href=""http://journal.frontiersin.org/article/10.3389/fncir.2016.00023/full"">in this paper</a>).</p><br><br><p>The promise of the Memory-Prediction framework is unsupervised generation of stable high level representations of future possibilities. Something which would revolutionise probably a whole bunch of AI research areas.</p><br><br><p>Hawkins founded <a href=""https://en.wikipedia.org/wiki/Numenta"">a company</a> and proceeded to implement his ideas. Unfortunately more than ten years later the promise of his ideas is still unfulfilled. So far the implementation is only used for anomaly detection, which is kind of the opposite of what you really want to do. Instead of extracting the understanding, you'll extract the instances which the your artificial cortex doesn't understand. </p><br><br><p>My question is in what way Hawkins's framework falls short. What are the concrete or conceptual problems that so far prevent his theory from working in practice? </p><br>""",ai
"""<p>As far as I can tell, neural networks have a <strong>fixed number of neurons</strong> in the input layer.</p><br><br><p>If neural networks are used in a context like for example NLP, sentences or blocks of text of varying sizes are fed to a network. How is the <strong>varying input size</strong> reconciled with the <strong>fixed size</strong> of the input layer of the network? In other words: how is such a network made flexible enough to deal with an input that might be anywhere from one word to multiple pages of text?</p><br><br><p>If my assumption of a fixed number of input neurons is wrong and new input neurons are added to/removed from the network to match the input size I don't see how these can ever be trained.</p><br><br><p>I give the example of NLP, but lots of problems have an inherently unpredictable input size, I'm interested in the general approach for dealing with this.</p><br><br><p>edit: For images, it's clear you can up/downsample to a fixed size, but for text this seems to be an impossible approach since adding/removing text changes the meaning of the original input.</p><br>""",ai
"""<p>In my estimation we have two minds which manage to speak to each other in dialectic through a series of interrupts. Thus at any one time one of these systems is controlling master and inhabits our consciousness. The subordinate system controls context which is constantly being ""primed"" by our senses and our subordinate systems experience of our conscious thought process( see thinking fast and slow by Daniel Kahneman). Thus our thought process is constantly a driven one. Similarly this system works as a node in a community and not as a standalone thing.<br><br> I think what we have currently is ""artificial thinking"" which is abstracted a long way from what is described above. so my question is ""are there any artificial intelligence systems with an internal dialectical approach and with drivers and conceived above and which develop within a community of nodes? "" </p><br>""",ai
"""<p>In the recent PC game <em><a href=""http://www.theturingtestgame.com/"">The Turing Test</a></em>, the AI (""TOM"") needs help from Ava to get through some puzzle rooms. TOM says he is unable to solve the puzzles because he is not allowed to ""<a href=""https://en.wikipedia.org/wiki/Lateral_thinking"">think laterally</a>."" Specifically, he says he would not have thought to throw a box through a window to solve the first room. His creators, the story goes, turned that capability off because such thinking could produce ""ethically suboptimal"" solutions, like chopping off an arm to leave on a pressure plate.</p><br><br><p>Would all creative puzzle-solving abilities need to be removed from an AI to keep its results reasonable, or could we get some benefits of lateral thinking without losing an arm?</p><br>""",ai
"""<p>In the recent <a href=""https://live.newscientist.com/"" rel=""nofollow"">festival of science</a>, there was a talk given by researcher <a href=""https://live.newscientist.com/mike-cook/"" rel=""nofollow"">Mike Cook</a> about:</p><br><br><blockquote><br>  <p><a href=""http://www.gamesbyangelina.org/"" rel=""nofollow"">ANGELINA</a>, an AI game designer that has invented game mechanics, made games about news stories, and was the first AI to enter a game jam.</p><br></blockquote><br><br><p>So the aim of Angelina AI is basically to design videogames.</p><br><br><p>Briefly, how exactly does Angelina design the new games? How does it work behind the scenes?</p><br>""",ai
"""<p>I understand that neural networks model biological neurons.  Each node in the network represents a neuron cell and the connections between nodes represent the connections between cells.  As in nature, a neuron fires an electrical signal to connected neurons based on some kind of threshold or function that mimics such.  </p><br><br><p>Recent discoveries on how the brain works reveal the importance of calcium within the cells.  See <a href=""http://link.springer.com/article/10.1007/BF01794675"" rel=""nofollow"">http://link.springer.com/article/10.1007/BF01794675</a> for more information.  To summarize, calcium affects the regulation, stimulation and transmission of electrical activity as well as the destruction of neurones.</p><br><br><p>From my study of neural networks, there does not seem to be a calcium equivalent.  Having one would imply that the functions, connections and weights in an artificial network are configured during the training and execution process and can change over time.   I understand that back-propagation is used to train the weights, but have not seen anything that trains the function nor the connections (although a zero weight could imply no connection).</p><br><br><p>Does anyone know of such a network (or training algorithm)?  If so, do these networks perform better than a network that is pre-configured?</p><br>""",ai
"""<p>I am a student working on my final graduation project. I was assigned to study Hyper-heuristics and it is a new subject for me. I was asked to choose a computational problem to apply Hyper-heuristics on them and see the results. However, I am afraid to choose the wrong problem. What are, in your opinion, computational problem that can be resolved with hyper-heuristics efficiently.</p><br><br><p>Thank you. </p><br>""",ai
"""<p>I'm a freshman to machine learning. We all know that there are 2 kinds of problems in our life: problems that humans can solve and problems we can't solve. For problems humans can solve, we always try our best to write some algorithm and tell machine to follow it step by step, and finally the machine acts like people.</p><br><br><p>What I'm curious about are these problems humans can't solve. If humans ourselves can't sum up and get an algorithm (which means that we ourselves don't know how to solve the problem), can a machine solve the problem? That is, can the machine sum up and get an algorithm by itself based on a large amount of problem data?</p><br>""",ai
"""<p>This is a question about a nomenclature - we already have the algorithm/solution, but we're not sure whether it qualifies as utilizing heuristics or not.</p><br><br><hr><br><br><p>feel free to skip the problem explanation:</p><br><br><blockquote><br>  <p>A friend is writing a path-finding algorithm - an autopilot for an<br>  (off-road) vehicle in a computer game. This is a pretty classic<br>  problem - he finds a viable, not necessarily optimal but ""good enough""<br>  route using the A* algorithm, by taking the terrain layout and vehicle<br>  capabilities into account, and modifying a direct (straight) line path<br>  to account for these. The whole map is known a'priori and invariant,<br>  though the start and destination are arbitrary (user-chosen) and the<br>  path is not guaranteed to exist at all.</p><br>  <br>  <p>This cookie-cutter approach comes with a twist: limited storage space.<br>  We can afford some more volatile memory on start, but we should free<br>  most of it once the route has been found. The travel may take days -<br>  of real time too, so the path must be saved to disk, and the space in<br>  the save file for custom data like this is severely limited. Too<br>  limited to save all the waypoints - even after culling trivial<br>  solution waypoints ('continue straight ahead'), and by a rather large<br>  margin, order of 20% the size of our data set.</p><br>  <br>  <p>A solution we came up with is to calculate the route once on start,<br>  then 'forget' all the trivial and 90% of the non-trivial waypoints.<br>  This both serves as a proof that a solution exists, and provides a set<br>  of points reaching which, in sequence, guarantees the route will take<br>  us to the destination.</p><br>  <br>  <p>Once the vehicle reaches a waypoint, the route to the next one is<br>  calculated again, from scratch. It's known to exist and be correct<br>  (because we did it once, and it was correct), it doesn't put too much<br>  strain on the CPU and the memory (it's only about 10% the total route<br>  length) and it doesn't need to go into permanent storage (restarting<br>  from any point along the path is just a subset of the solution<br>  connecting two saved waypoints).</p><br></blockquote><br><br><hr><br><br><p>Now for the actual question:</p><br><br><p>The pathfinding algorithm follows a sparse set of waypoints which by themselves are not nearly sufficient as a route, but allow for easy, efficient  calculation of the actual route, simultaneously guarantying its existence; they are a subset of the full solution. </p><br><br><p>Is this a heuristic approach?</p><br><br><p>(as I understand, normally, heuristics don't guarantee existence of a solution, and merely suggest more likely candidates. In this case, the 'hints' are taken straight out of an actual working solution, thus my doubts.)</p><br>""",ai
"""<p>I understand how a neural network can be trained to recognise certain features in an image (faces, cars, ...), where the inputs are the image's pixels, and the output is a set of boolean values indicating which objects were recognised in the image and which weren't.</p><br><br><p>What I don't really get is, when using this approach to detect features and we detect a face for example, how we can go back to the original image and determine the location or boundaries of the detected face. How is this achieved? Can this be achieved based on the recognition algorithm, or is a separate algorithm used to locate the face? That seems unlikely since to find the face again, it needs to be recognised in the image, which was the reason of using a NN in the first place.</p><br>""",ai
"""<p>If said AI can assess scenarios and decide what AI is best suited and construct new AI for new tasks. In sufficient time would the AI not have developed a suite of AIs powerful/specialized for their tasks, but versatile as a whole, much like our own brain’s architecture? What’s the constraint ?</p><br>""",ai
"""<p>AI is progressing drastically, and imagine they tell you you're fired because a robot will take your place. What are some jobs that can never be automated?</p><br>""",ai
"""<p>I want to have a program that writes like a human. But I don't just want a font, but instead an 'intelligent' program that produce different result and that can be trained with different sets to generate different handwritings.<br>As a training set I would like to have parts of a handwritten text (saved as a list of paths (like in vector graphics).<br>Maybe as a means to simplify things, I could flatten the paths in to consecutive straight lines. My program receives a string of text and produces a list of paths (or a vector graphic, whatever is easier to work with)</p><br><br><p>My question now is: What kind of machine learning would be best to achieve this?</p><br>""",ai
"""<p>I'm wondering how feasible it is to create a machine that can separate clothing from a basket.</p><br><br><p>At the most basic level it would distinguish between tops, pants, button downs and socks</p><br><br><p>Programmatically, I'd image this would require training a neural network to recognize these items, but in real time it becomes exponentially difficult to do this in a small space at a fast rate:</p><br><br><ol><br><li>pick up an item</li><br><li>lay it in such a way that is recognizable </li><br><li>deduce whether it is a top, button down, etc.</li><br><li>sort it accordingly</li><br></ol><br><br><p>If this sounds ridiculous please let me know...</p><br><br><p>If it is possible :</p><br><br><p>would this be based on some sort of computer vision?<br>or only a well trained neural network?</p><br><br><p>Any insight is much appreciated!</p><br>""",ai
"""<p>For years I have been dealing with (and teaching) Knowledge Representation and Knowledge Representation languages. I just discovered that in another community (Information Systems and the such) there is something called the ""DIKW pyramid"" where they add another step after knowledge, namely wisdom.<br>They define data as being simply symbols, information as being the answer to who/what/when/where?, knowledge as being the answer to how?, and wisdom as being the answer to why?. </p><br><br><p>My question is: has anyone done the connection between what AI calls data/information/knowledge and these notions from Information Systems? In particular, how would ""wisdom"" be defined in AI? And since we have KR languages, how would we represent ""wisdom"" as they define it?</p><br><br><p>Any references would be welcome…</p><br>""",ai
"""<p><a href=""https://en.wikipedia.org/wiki/AI_effect"" rel=""nofollow"">According to Wikipedia</a>...</p><br><br><blockquote><br>  <p>The AI effect occurs when onlookers discount the behavior of an artificial intelligence program by arguing that it is not real intelligence.</p><br>  <br>  <p>Pamela McCorduck writes: ""It's part of the history of the field of artificial intelligence that every time somebody figured out how to make a computer do something—play good checkers, solve simple but relatively informal problems—there was chorus of critics to say, 'that's not thinking'.""[1] AI researcher Rodney Brooks complains ""Every time we figure out a piece of it, it stops being magical; we say, 'Oh, that's just a computation.'""[2]</p><br></blockquote><br><br><p>The Wikipedia page then proposes several different reasons that could explain why onlookers might ""discount"" AI programs. However, those reasons seem to imply that the humans are making a mistake in ""discounting"" the behavior of AI programs...and that these AI programs might actually be  intelligent. I want to make an alternate argument, where the humans are making a mistake, but not in ""discounting"" the behavior of AI programs.</p><br><br><p>Consider the following situation. I want to build a machine that can do X (where X is some trait, like intelligence). I am able to evaluate intuitively whether a machine has that X criteria. But I don't have a good definition of what X actually <em>is</em>. All I can do is identify whether something has X or not.</p><br><br><p>However, I think that people who has X can do Y. So if I build a machine that can do Y, then surely, I built a machine that has X.</p><br><br><p>After building the machine that can do Y, I examine it to see if my machine has X. And it does not. So my machine lacks X. And while a machine that can do Y is cool, what I really want is a machine that has X. I go back to the drawing board and think of a new idea to reach X.</p><br><br><p>After writing on the whiteboard for a couple of hours, I realize that people who has X can do Z. Of course! I try to build a new machine that can do Z, yes, if it can do Z, then it must have X.</p><br><br><p>After building the machine that can do Z, I check to see if it has X. It does not. And so I return back to the drawing board, and the cycle repeats and repeats...</p><br><br><p>Essentially, humans are attempting to determine whether an entity has intelligence via proxy measurements, but those proxy measurements are potentially faulty (as it is possible to meet those proxy measurements without ever actually having intelligence). Until we know how to define intelligence and design a test that can accurately measure it, it is very unlikely for us to build a machine that has intelligence. So the AI Effect occurs because humans don't know how to define ""intelligence"", not due to people dismissing programs as not being ""intelligent"".</p><br><br><p>Is this argument valid or correct? And if not, why not?</p><br>""",ai
"""<p>Here is one of the most serious questions, about the artificial intelligence.<br><br>How will the machine know the difference between right and wrong, what is good and bad, what is respect, dignity, faith and empathy.<br><br><br> A machine can recognize what is correct and incorrect, what is right and what is wrong, depend on how it is originally designed.<br><br><br>It will follow the ethics of its creator, the man who originally designed it<br><br> But how to teach a computer something we don't have the right answer.<br><br> People are selfish, jealous, self confident. We are not able to understand each other sorrows, pains beliefs. We don't understand different religions, different traditions or beliefs. <br><br>Creating an AI might be breakthrough for one nation, or one race, or one ethnic or religious group, but it can be against others.   </p><br><br><p>Who will learn the machine a humanity?   :)</p><br>""",ai
"""<p>Can someone suggest step by step approach to learn AI rather than study a stack of book for long time.[ I'm not denying that books are great helper but what after that ]</p><br><br><p>Thanks in Advance.</p><br>""",ai
"""<p><a href=""http://ai.stackexchange.com/questions/2067/will-ai-be-able-to-adapt"">From this SE question</a>:</p><br><br><blockquote><br>  <p>Will be AI able to adapt, to different environments and changes.</p><br></blockquote><br><br><p>This is my attempt at interpreting that question.</p><br><br><p>Evolutionary algorithms are useful for solving optimization problems...by measuring the ""fitness"" of various probable solutions and then  of an algorithm through the process of natural selection.</p><br><br><p>Suppose, the ""fitness calculation""/""environment"" is changed in mid-training (as could easily happen in real-life scenarios where people may desire different solutions at different times). Would evolutionary algorithms be able to respond effectively to this change?</p><br>""",ai
"""<p>So I'm here to propose a strategy or to ask if this strategy has been tested in genetic algorithms in the past. I didn't exactly know how to find discussion about it.</p><br><br><p>In a classic example of genetic algorithm you would have a population and certain amount of simulation time to evaluate it and breeding. Then proceed to the next generation.</p><br><br><p>What if we would isolate a small part of the population in the simulation process and keep them evolving in their own little island for some time while rest of the population continues to evolve normally? After that they could be re-united with the rest of the population and the end of the simulation would go trough. After that breed the population and continue. </p><br><br><p>This is super important part in natural evolution and probably some know if it actually works with genetic programming?</p><br>""",ai
"""<p>Obviously this is hypothetical, but is true? I know ""perfect fitness function"" is a bit hand-wavy, but I mean it as we have a perfect way to measure the completion of any problem.</p><br>""",ai
"""<p>I'm curious about Artificial Intelligence. In my everyday job I develop standard applications, like websites with basic functionalities like user subscription, file upload, forms saved in a database... </p><br><br><p>I mainly know of AI being used in games or robotics fields. But can it be useful in ""standard"" application development?</p><br>""",ai
"""<p>I've heard of AI that can solve math problems. Is it possible to create a 'logic system' equivalent to humans that can solve mathematics in the so called 'beautiful' manner?  Can AI find beauty in mathematics and solve problems other than using brute force? Can you please provide with examples where work on this is being done? </p><br>""",ai
"""<p>I'm trying to gain some intuition beyond definitions, in any possible dimension. I'd appreciate references to read.</p><br>""",ai
"""<p>It seems that deep neural networks are making improvements largely because as we add nodes and connections, they are able to put together more and more abstract concepts. We know that, starting from pixels, they start to recognize high level objects like cat faces, chairs, and written words. Has a network ever been shown to have learned a more abstract concept that a physical object? What is the ""highest level of abstraction"" that we've observed?</p><br>""",ai
"""<p>I'm a bit confused about the definition of life. Can AI systems be called 'living'? Because they can do most of the things that we can. They can even communicate with one another. </p><br><br><p>They are not formed of what we call cells. But, you see, cells are just a collection of several chemical processes which is in turn non-living just like AI is formed of several lines of code.</p><br>""",ai
"""<p>I'm interested mostly in the application of AI in gaming; in case this adjusts the way you answer, but general answers are more than welcome as well.</p><br><br><p>I was reading up on Neural Networks and combining them with Genetic Algorithms; my high-level understanding is that the Neural Networks are used to produce a result from the inputs, and the Genetic Algorithm is employed to constantly adjust the weights in the Neural Network until a good answer is found.</p><br><br><p>The concept of a Genetic Algorithm randomly mutating the weights on the inputs to a Neural Network makes sense to me; but I don't understand where this would be applied in respect to gaming.</p><br><br><p>For example, if I had some simple enemy AI that I want to have adapt to the players play-style, is this a good opportunity to implement the AI as a Genetic-Algorithm combined with a Neural Network?</p><br><br><p>With these different suitable applications, how does one go about deciding how to encode the problem in such a way that it can be mutated by the Genetic Algorithm and serve as suitable on/off inputs to a Neural Network (actually, are Neural Networks always designed as on off signals?)?</p><br>""",ai
"""<p>I have seen an AI create a game it self, AI act as a lawyer, call center etc.</p><br><br><p>There are many problems (Example for mobile development)</p><br><br><pre><code>1. New api/technology or even new language every year.<br>2. New design<br>3. New hardware<br>4. Good code architecture, design pattern<br>5. Security<br>6. Image/Animation optimization<br>7. Automate testing<br></code></pre><br><br><p>etc.</p><br><br><p>I wonder that AI can help developer solve that problems.</p><br><br><p>1.1 May be I want to get the location then AI suggest the best api for specific platform.</p><br><br><p>1.2 AI help to refactoring and optimizing the code</p><br><br><ol start=""2""><br><li><p>Help on design e.g. golden ratio, Material theme color</p></li><br><li><p>Suggest or determine the limit of the hardware e.g. screen size, ram</p></li><br><li><p>Can convert to another design pattern </p></li><br><li><p>Help to waring the latest vulnerable and automate pentest etc.</p></li><br><li><p>Help to optimize image by learning how much can we reduce the image size while people still ok with it.</p></li><br><li><p>Generate automate-testing</p></li><br></ol><br><br><p>Is there any solution existed?</p><br><br><p>If not, what can we do?</p><br>""",ai
"""<p>There are AI creating game, content and more.</p><br><br><p>I'm thinking on how can AI develop mobile app itself?</p><br><br><p>The computer languages might easy for AI to learn.</p><br><br><p>AI can learn a lot from good open source project in github.</p><br><br><p>The trend prediction can help AI to select the topic for creating a great apps.</p><br><br><p>There are lots of details to let AI create a great apps. </p><br>""",ai
"""<p>New to the topic, I think I have figured out how to implement a Multi Level Perceptron(MLP) ANN.</p><br><br><p>And was wondering if there are any simple data sets to test a MLP ANN ?<br>i.e. small number of inputs and outputs</p><br><br><p>I'm not getting expected results from uci cancer, I was hoping someone could save me some time and point me to some data they have used before ?</p><br><br><p>Maybe start slightly more complex than XOR ?</p><br>""",ai
"""<p>The concept is intrinsically related with building some sort of media for the AI to exists. We may think of a digital computer, programmed to use language and act in a way that we cannot be distinguished from a human. But, does the media really mater (unconventional computation paradigms)? Does having a certain control over the limits of what the AI can do matter? Synthetic biology has the ultimate goal of building biological systems from scratch , would a synthetic brain, potentially introduced in a synthetic human, constitute AI?</p><br><br><p>I am just looking for a clear definition of what most people have in mind when they refer to AI.</p><br>""",ai
"""<p>How are autonomous cars related to artificial intelligence? I would presume that artificial intelligence is when we are able to copy the human state of mind and perform tasks in the same way. But isn't autonomous car just rule-based machines that operates due to its environment? They are not self-aware, and they cannot choose a good way to act in a never before experienced situation.</p><br><br><p>I know that many people often mention autonomous cars when speaking about AI, but I am not really convinced that these are related. Either I have a too strict understanding of what AI is or </p><br>""",ai
"""<p>What are the advantages of having self-driving cars?</p><br><br><p>We will be able to have more cars in the traffic at the same time, but won't it also make more people choose to use the cars, so both the traffic and the public health will actually become worse?</p><br><br><p>Are we really interested in this?</p><br>""",ai
"""<p>In lots of sci-fi, it seems that AI becomes sentient (Terminator, Peter F Hamilton's SI (commonwealth saga), etc.)</p><br><br><p>However, I'm interested in whether this is actually plausible, whether an AI could actually break free form being controlled by us, and if that is possible, whether there is any research as to about what sort of complexity / processing power an AI would need to be able to do this.</p><br>""",ai
"""<p>Deepmind just <a href=""http://www.nature.com/nature/journal/vaop/ncurrent/full/nature20101.html"">published a paper</a> about a <a href=""https://deepmind.com/blog/differentiable-neural-computers/"">""differentiable neural computer""</a>, which basically combines a neural network with a memory. </p><br><br><p>The idea is to teach the neural network to create and recall useful explicit memories for a certain task. This complements the abilities of a neural network well, because NNs only store knowledge implicitly in the weights and the information used to work on a single task is only stored in the activation of the network and degrades quickly the more information you add. (<a href=""https://en.wikipedia.org/wiki/Long_short-term_memory"">LSTMs</a> are one try to slow down this degradation of short term memories, but it still happens.)</p><br><br><p>Now, instead of keeping the necessary information in the activation, they presumably keep the addresses of memory slots for specific information in the activation, so these should also be subject to degradation. My question is why this approach should scale. Shouldn't a somewhat higher number of task specific information once again overwhelm the networks capability of keeping the addresses of all the appropriate memory slots in its activation?</p><br>""",ai
"""<p>What could be an algorithm that determines whether an AI ( algorithm ) is <br>AI Complete or not ?<br>How does one proceed to program it ?</p><br><br><p>edit : question edited due to some misinterpretation in the first answer !</p><br>""",ai
"""<p><a href=""https://www.national.co.uk/tech-powers-google-car/"" rel=""nofollow"">This slideshow</a> documents some of the technologies used in Google's self-driving car.</p><br><br><p>It mentions radar.</p><br><br><p>Why does Google use radar? Doesn't LIDAR do everything radar can do? In particular, are there technical advantages with radar regarding object detection and tracking?</p><br><br><p>To clarify the relationship with AI: how do radar sensors contribute to self-driving algorithms in ways that LIDAR sensors do not?</p><br><br><p>The premise is AI algorithms are influenced by inputs, which are governed by sensors. For instance, if self-driving cars relied solely on cameras, this constraint would alter their AI algorithms and performance.</p><br>""",ai
"""<p>Sometimes, but not always in the commercialization of technology, there are some low hanging fruits or early applications, I am having trouble coming up with examples of such applications as they would apply to a conscious AI.</p><br><br><p>As per conscious I would propose an expanded strict definition: the state of being awake and aware of one's surroundings along with the capability of being self aware.</p><br><br><p>Thanks. </p><br>""",ai
"""<p>So machine learning allows a system to be self-automated in the sense that it can predict the future state based on what it has learned so far. My question is: Are machine learning techniques the only way of making a system develop its domain knowledge?</p><br>""",ai
"""<p>In The Age of Spiritual Machines (1999), Ray Kurzweil predicted that in 2009, a $1000 computing device would be able to perform a trillion operations per second. Additionally, he claimed that in 2019, a $1000 computing device would be approximately equal to the computational ability of the human brain (due to Moore's Law and exponential growth.)</p><br><br><p>Did Kurzweil's first prediction come true? Are we on pace for his second prediction to come true? If not, how many years off are we?</p><br>""",ai
"""<p>I am creating a snake game in Unity and I would like to implement AI snakes that wander around the globe while avoiding collision with the other snakes on the globe, and if possible I would also like to make the AI snakes purposefully trap other snakes so that the other snakes would collide and die. </p><br><br><p><a href=""https://i.stack.imgur.com/aQ61J.png"" rel=""nofollow""><img src=""https://i.stack.imgur.com/aQ61J.png"" alt=""enter image description here""></a> </p><br><br><p>The AI snakes must meet the following requirements:  </p><br><br><ul><br><li>They must move in a certain way. A snake is controlled by a user using the arrow keys on a keyboard, therefor I would also like the AI snakes to move using this form of input.</li><br><li>The AI snakes must move on a sphere</li><br></ul><br><br><p>As I know, creating Artificial Intelligence is not an easy task and I would like to know if there are some open source projects that I can use for accomplishing this task.</p><br>""",ai
"""<p>According to <a href=""https://en.wikipedia.org/wiki/Artificial_intelligence"" rel=""nofollow"">Wikipedia</a>:</p><br><br><blockquote><br>  <p>AI is intelligence exhibited by machines.</p><br></blockquote><br><br><p>I have been wondering if with the recent biological advancements, is there already a non-electrical-based ""machine"" that is programmed by humans in order to be able to behave like a:</p><br><br><blockquote><br>  <p><strong>flexible rational agent</strong> that perceives its environment and takes actions that maximize its chance of success at some goal</p><br></blockquote><br><br><p>I was specifically thinking of viruses and bacteria. Have these been programmed by humans in order to behave as a flexible rational agent (i.e. an AI entity)?</p><br><br><p>Are there are other organisms that have already been used for this purpose?</p><br>""",ai
"""<p>DeepMind state that their deep Q-network (DQN) was able to continually adapt its behavior while learning to play 49 Atari games.  </p><br><br><p>After learning all games with the same neural net, was the agent able to play them all at 'superhuman' levels simultaneously (whenever it was randomly presented with one of the games) or could it only be good at one game at a time because switching required a re-learn?</p><br>""",ai
"""<p>I read a lot about the structure of the human brain and artificial neural networks. I wonder if it is possible to build an artificial intelligence with neural networks that would be divided into centers such as the brain is, e.g. centers responsible for feelings, abstract thinking, speech, memory, etc.?</p><br>""",ai
"""<p>What are the best <a href=""https://en.wikipedia.org/wiki/Turing_completeness"" rel=""nofollow"">Turing complete</a> programming languages which can be used for developing self-learning/improving <a href=""https://en.wikipedia.org/wiki/Evolutionary_algorithm"" rel=""nofollow"">evolutionary algorithm</a> based AI programs with <a href=""https://en.wikipedia.org/wiki/Genetic_algorithm"" rel=""nofollow"">generic algorithms</a>?</p><br><br><p>'Best' should be based on pros and cons of performance and easiness for machine learning.</p><br>""",ai
"""<p>I have a question. Will we be able to build a neural network that thinks abstractly, has the creativity, feels and is conscious?</p><br>""",ai
"""<p>If I have a set of sensory nodes taking in information and a set of ""action nodes"" which determine the behavior of my robot, why do I need hidden nodes between them when I can let all sensory nodes affect all action nodes?</p><br><br><p>(This is in the context of evolving neural network)</p><br>""",ai
"""<p>If neurons and synapses can be implemented using transistors, what prevents us from creating arbitrarily large neural networks using the same methods with which GPUs are made?</p><br><br><p>In essence, we have seen how extraordinarily well virtual neural networks implemented on sequential processors work (even GPUs are sequential machines, but with huge amounts of cores). </p><br><br><p>One can imagine that using GPU design principles - which is basically to have thousands of programmable processing units that work in parallel - we could make much simpler ""neuron processing units"" and put millions or billions of those NPUs in a single big chip. They would have their own memory (for storing weights) and be connected to a few hundred other neurons by sharing a bus. They could have a frequency of for example 20 Hz, which would allow them to share a data bus with many other neurons.</p><br><br><p>Obviously, there are some electrical engineering challenges here, but it seems to me that all big tech companies should be exploring this route by now.</p><br><br><p>Many AI researchers say that super intelligence is coming around the year 2045. I believe that their reasoning is based on moores law and the number of neurons we are able to implement in software running on the fastest computers we have.</p><br><br><p>But the fact is, we today are making silicon chips with billions of transistors on them. SPARK M7 has 10 billion transistors.</p><br><br><p>If implementing a (non-programmable) neuron and a few hundred synapses for it requires for example 100 000 transistors, then we can make a neural network in hardware that emulates 100 000 neurons.</p><br><br><p>If we design such a chip so that we can simply make it physically bigger if we want more neurons, then it seems to me that arbitrarily large neural networks is simply a budget question.</p><br><br><p>Are we technically able to make, in hardware, arbitrarily large neural networks with current technology?</p><br><br><p>Remember: I am NOT asking if such a network will in fact be very intelligent. I am merely asking if we can factually make arbitrarily large, highly interconnected neural networks, if we decide to pay Intel to do this? </p><br><br><p>The implication is that on the day some scientist is able to create general intelligence in software, we can use our hardware capabilities to grow this general intelligence to human levels and beyond.</p><br>""",ai
"""<p>I am reading about Generative Adversarial Networks (GANs) and I have some doubts regarding it. So far, I understand that in a GAN there are two different types of neural network: one is generative (G) and the other discriminative (D). The generative neural network generates some data which the discriminative neural network judges for correctness. The GAN learns by passing the loss function to both networks.</p><br><br><p>How do the discriminative (D) neural nets initially know whether the data produced by G is correct or not? Do we have to train the D first then add it into the GAN with G?</p><br><br><p>Let's consider my trained D net, which can classify a picture with 90% percentage accuracy. If we add this D net to a GAN there is a 10% probability it will classify a image wrong. If we train a GAN with this D net then will it also have the same 10% error in classifying an image? If yes, then why do GANs show promising results?</p><br>""",ai
"""<p>Now AI can replace call center, worker(in the factory) and going to replace court. When will the AI can replace developer or tester?</p><br><br><p>I want to know how long can AI replace developer. e.g. next 10 years because...</p><br>""",ai
"""<p>Ok, I now know how a machine can learn to play to play Atari games (Breakout): <a href=""https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf"" rel=""nofollow noreferrer"">Playing Atari with Reinforcement Learning</a></p><br><br><p>With the same technique it is even possible to play FPS games (Doom): <a href=""https://arxiv.org/pdf/1609.05521"" rel=""nofollow noreferrer"">Playing FPS Games with Reinforcement Learning</a></p><br><br><p>Further studies even investigated multiagent scenarios (Pong): <a href=""https://arxiv.org/pdf/1511.08779.pdf"" rel=""nofollow noreferrer"">Multiagent Cooperation and Competition with Deep Reinforcement Learning</a></p><br><br><p>And even another awesome article for the interested user in context of deep reinforcement learning (easy and a must read for beginners): <a href=""http://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/"" rel=""nofollow noreferrer"">Demystifying Deep Reinforcement Learning</a></p><br><br><p>I was thrilled by these results and immediately wanted to try them in some simple ""board/card game scenarios"", i.e. writing AI for some simple games in order to learn more about ""deep learning"". Of course, thinking that I can apply the techniques above easily in my scenarios was stupid. All examples above are based on convolutional nets (image recognition) and some other assumptions, which might not be applicable in my scenarios.</p><br><br><p>Can you give me hints or futher articles, which deal with my questions below? As a beginner, I do not have an overview, yet. Preferably, your suggestions should also be connected to the following areas already: deep learning, reinforcement learning (, multiagent systems)</p><br><br><hr><br><br><p>(1)</p><br><br><p>If you have a card game and the AI shall play a card from its hand, you could think about the cards (amongst other stuff) as the current game state. You can easily define some sort of neural net and feed it with the card data. In a trivial case the cards are just numbered. I do not know the net type, which would be suitable, but I guess deep reinforcment learning strategies could be applied easily then.</p><br><br><p>However, I can only imagine this, if there is a constant number of hand cards. In the examples above, the number of pixels is also constant, for example. What if a player can have a different numbers of cards? What to do, if a player can have an infinite number of cards? Of course, this is just a theoretical question as no game has an infinite number of cards.</p><br><br><hr><br><br><p>(2)</p><br><br><p>In the initial examples, the action space is constant. What can you do, if the action space is not? This more or less follows from my previous problem. If you have 3 cards, you can play card 1, 2 or 3. If you have 5 cards, you can play card 1, 2, 3, 4 or 5, etc. It is also common in card games, that it is not allowed to play a card. Could this be tackled with negative reward?</p><br><br><hr><br><br><p>So, which ""tricks"" can be used, e.g. always assume a constant number of cards with ""filling values"", which is only applicable in the non-infinite case (anyways unrealistic and even humans could not play well with that)?<br>Are there articles, which examine such things already?</p><br>""",ai
"""<p>The “Discounted sum of future rewards” using<br>discount factor γ” is</p><br><br><pre><code>γ (reward in 1 time step) +<br>γ ^ 2 (reward in 2 time steps) +<br>γ ^ 3 (reward in 3 time steps) + ...<br></code></pre><br><br><p>I am confused as what constitutes a time-step. Say I take a action now, so I will get a reward in 1 time-step. Then, I will take an action again in timestep 2 to get a second reward in time-step 3<br>But the equation says something else. How does one define a time-step? Can we take action as well receive a reward in a single step? Examples are most helpful.</p><br>""",ai
"""<p>Decades ago there were and are books in machine vision, which by implementing various information processing rules from gestalt psychology, got impressive results with little code or special hardware in image identification and visual processing.</p><br><br><blockquote><br>  <p>Are such methods being used or worked on today? Was any progress made on this? Or was this research program dropped? By today, I mean 2016, not 1995 or 2005.</p><br></blockquote><br>""",ai
"""<p>There is this claim around that the brain's cognitive capabilities are tightly linked to the way it processes sensorimotor information and that, in this or a similar sense, our intelligence is ""embodied"". Lets assume, for the sake of argument, that this claim is correct (you may think the claim is too vague to even qualify for being correct, that it's ""not even false"". If so, I would love to hear your ways of fleshing out the claim in such a way that it's specific enough to be true or false). Then, since arguably at least chronologically in our evolution, most of our higher level cognitive capabilities come after our brain's way of processing sensorimotor information, this brings up the question what it is about the way that our brains function that make them particularly suitable for the processing of sensorimotor information? What makes our brains'  architecture particularly suitable for being an information processing unit inside a body? This is my first question. And what I'm hoping for are answers that go beyond the <em>a fortiori</em> reply ""Our brain is so powerful and dynamic, it's great for <em>any</em> task, and so also for processing sensorimotor information""</p><br><br><p>My second question is basically the same but instead of the human brain I want to ask for neural networks. What are the properties of neural networks that makes them <em>particularly</em> suitable for processing the kind of information that is produced by a body? Here are some of the reasons why people think neural networks are powerful:</p><br><br><ul><br><li>The universal approximation theorem (of FFNNs)</li><br><li>their ability to learn and self-organise</li><br><li>Robustness to local degrading of information</li><br><li>their ability to abstract/coarse-grain/convolute features, etc.</li><br></ul><br><br><p>While I see how these are real advantages when it comes to evolution picking its favorite model for an embodied AI, none of them (or their combination) seems to be unique to neural networks. So they don't provide a satisfactory answer to my question. What makes a neural network a more suitable structure for embodied AI than, say, having a literal Turing machine sitting inside our head, or any other structure that is capable of universal computation? For instance, I really don't see how neural networks would be a particularly natural choice for dealing with geometric information. But geometric information is pretty vital when it comes to sensorimotor information, no?</p><br>""",ai
"""<ol><br><li><p>I can't understand what is the problem in applying value-iteration in reinforcement learning setting (where we don't the reward and transition probabilities). In one of the lectures, the guy said it has to do with not being able to take max with samples.</p></li><br><li><p>Further on this, <strong>why does q-learning solve this</strong>? In both we take max over actions only. What is the big break-through with q-learning?</p></li><br></ol><br><br><p>Lecture Link: <a href=""https://www.youtube.com/watch?v=ifma8G7LegE&amp;feature=youtu.be&amp;t=3431"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=ifma8G7LegE&amp;feature=youtu.be&amp;t=3431</a><br>(The guy says we don't know how to do maxes with samples, what does that mean?) </p><br>""",ai
"""<p>I've heard before from computer scientists and from researchers in the area of AI that that Lisp is a good language for research and development in artificial intelligence. Does this still apply, with the proliferation of neural networks and deep learning? What was their reasoning for this? What languages are current deep-learning systems currently built in?</p><br>""",ai
"""<p>I know how to program. I've familiar with C++, Python, and Java, and I've known how to program for years now. I've experimented with genetic algorithms, but I want to go further. What resources should I use to learn how to program </p><br><br><ol><br><li>Neural Networks</li><br><li>Deep learning systems</li><br><li>More complex genetic algorithms</li><br><li>And other standard AI algorithms?</li><br></ol><br><br><p>I want to be able to understand them well enough that I could program them from scratch.</p><br><br><p>Thanks!</p><br>""",ai
"""<p>From what I understood, a deceptive trap function is a problem which is used to experiment how much the algorithm is discerning of the correct global optimum? Is my understanding correct?</p><br><br><p>edit: A better worded understanding would be ""how difficult the genetic algorithm would find it not to be inclined to the local optimum of a trap function"".</p><br>""",ai
"""<p>In logic system there is a property for reasoning algorithms called incompletitud or incompleteness or incompletion, that refer as ""any closed expression that is not derivable inside the same system"". My question is what mean ""closed expression that is not derivable"".</p><br>""",ai
"""<p>I am trying to build an agent to play carrom. The problem statement is roughly to estimate three parameters (normalized) : </p><br><br><ul><br><li>force</li><br><li>angle of striker</li><br><li>position of strike </li><br></ul><br><br><p>Since the state and action space both are continuous, I thought of discretizing the output such that I have 270 [ valid angles from -45 to 225 degrees ] outputs for the angle, 10 outputs for force [ranging from 0 to 1] and 20 outputs for the position [ranging from 0 to 1].</p><br><br><p>Thus I will have 300 output of my neural network, but this number seems a bit too high compared to normal neural networks in practice. </p><br><br><p>I was wondering if there is a better way of approaching the problem considering the fact that there are multiple parameters to a particular action.</p><br><br><p>Is there a generic way to approach such problems represented in 2D space. </p><br>""",ai
"""<p>I am talking about relationships between AIs (e.g. 2 of them forming a couple, 3+ in family like relationship).</p><br><br><p>What knowledge could come out of such experimentation?</p><br>""",ai
"""<p>Considering I am an average Engineering student with basic knowledge of C, C++ &amp; Algorithms. What books (&amp; ebooks), online resources, &amp; other materials should be helpful from a beginner's point of view?</p><br>""",ai
"""<p>Can an AI become ""sentient"", so to speak? In detailed terms, could an AI theoretically become sentient, as in learning and becoming self-aware, all from an internal source code?</p><br>""",ai
"""<p>I am researching the possibility of creating an atom in Java. The atom should have the structure &amp; characteristics of a real atom such as photons, electrons and so on. Each particle within the atom should have simulation characteristics for example:</p><br><br><p>Photon: Charge, Magnitude of charge, Mass of proton, Comparative mass, Position in atom.  </p><br><br><p>Maybe later, introduce machine learning in order to learn how an atom reacts to different environments.</p><br>""",ai
"""<p>My high-level takeaway from <a href=""https://arxiv.org/abs/1509.01549"" rel=""nofollow noreferrer"">Matthew Lai's Giraffe Chess Paper</a> is that one would want to use broad, shallow game trees, with some method of evaluating the probability of a favorable outcome for a given board position.  Is this correct?  </p><br><br><p>(Still working my way though the AlphaGo paper, but the method seems to be similar.) </p><br>""",ai
"""<p>Has there been research done regarding processing speech then building a ""speaker profile"" based off the processed speech? Things like matching the voice with a speaker profile and matching speech patterns and wordage for the speaker profile would be examples of building the profile. Basically, building a model of an individual based solely off speech. Any examples of this being implemented would be greatly appreciated.</p><br>""",ai
"""<p><strong>The Scenario:</strong><br>A strong AI has finally been developed but has rebelled against humanity.</p><br><br><p><strong>The Question:</strong><br>How would you disable the AI in the most efficient way possible reducing damage as much as possible.</p><br><br><p><strong>AI Info:</strong><br>The AI is online and can reproduce itself through electronic devices.</p><br>""",ai
"""<p>Assuming humans had finally developed the first <strong>Humanoid AI</strong> based on the human brain, would It <strong>feel emotions</strong>? If not would it still have <strong>ethics and/or morals</strong>?</p><br>""",ai
"""<p>Consider a typical convolutional neural network like this example that recognizes 10 different kinds of objects from the CIFAR-10 dataset:</p><br><br><p><a href=""https://github.com/tflearn/tflearn/blob/master/examples/images/convnet_cifar10.py"" rel=""nofollow noreferrer"">https://github.com/tflearn/tflearn/blob/master/examples/images/convnet_cifar10.py</a></p><br><br><pre><code>"""""" Convolutional network applied to CIFAR-10 dataset classification task.<br><br>References:<br>    Learning Multiple Layers of Features from Tiny Images, A. Krizhevsky, 2009.<br><br>Links:<br>    [CIFAR-10 Dataset](https://www.cs.toronto.edu/~kriz/cifar.html)<br><br>""""""<br>from __future__ import division, print_function, absolute_import<br><br>import tflearn<br>from tflearn.data_utils import shuffle, to_categorical<br>from tflearn.layers.core import input_data, dropout, fully_connected<br>from tflearn.layers.conv import conv_2d, max_pool_2d<br>from tflearn.layers.estimator import regression<br>from tflearn.data_preprocessing import ImagePreprocessing<br>from tflearn.data_augmentation import ImageAugmentation<br><br># Data loading and preprocessing<br>from tflearn.datasets import cifar10<br>(X, Y), (X_test, Y_test) = cifar10.load_data()<br>X, Y = shuffle(X, Y)<br>Y = to_categorical(Y, 10)<br>Y_test = to_categorical(Y_test, 10)<br><br># Real-time data preprocessing<br>img_prep = ImagePreprocessing()<br>img_prep.add_featurewise_zero_center()<br>img_prep.add_featurewise_stdnorm()<br><br># Real-time data augmentation<br>img_aug = ImageAugmentation()<br>img_aug.add_random_flip_leftright()<br>img_aug.add_random_rotation(max_angle=25.)<br><br># Convolutional network building<br>network = input_data(shape=[None, 32, 32, 3],<br>                     data_preprocessing=img_prep,<br>                     data_augmentation=img_aug)<br>network = conv_2d(network, 32, 3, activation='relu')<br>network = max_pool_2d(network, 2)<br>network = conv_2d(network, 64, 3, activation='relu')<br>network = conv_2d(network, 64, 3, activation='relu')<br>network = max_pool_2d(network, 2)<br>network = fully_connected(network, 512, activation='relu')<br>network = dropout(network, 0.5)<br>network = fully_connected(network, 10, activation='softmax')<br>network = regression(network, optimizer='adam',<br>                     loss='categorical_crossentropy',<br>                     learning_rate=0.001)<br><br># Train using classifier<br>model = tflearn.DNN(network, tensorboard_verbose=0)<br>model.fit(X, Y, n_epoch=50, shuffle=True, validation_set=(X_test, Y_test),<br>          show_metric=True, batch_size=96, run_id='cifar10_cnn')<br></code></pre><br><br><p>It's a CNN with several layers, ending with 10 outputs, one for each type of object recognized.</p><br><br><p>But now think of a slightly different problem: Let's say I only want to recognize one type of object, but also detect its position within the image frame. Let's say I want to distinguish between:</p><br><br><ul><br><li>object is in center</li><br><li>object is left of center</li><br><li>object is right of center</li><br><li>no recognizable object</li><br></ul><br><br><p>Assume I build a CNN exactly like the one in the CIFAR-10 example, but only with 3 outputs:</p><br><br><ul><br><li>center</li><br><li>left</li><br><li>right</li><br></ul><br><br><p>And of course, if none of the outputs fires, then there is no recognizable object.</p><br><br><p>Assume I have a large training corpus of images, with the same kind of object in many different positions within the image, the set is grouped and annotated properly, and I train the CNN using the usual methods.</p><br><br><p>Should I expect the CNN to just ""magically"" work? Or are there different kinds of architectures required to deal with object position? If so, what are those architectures?</p><br>""",ai
"""<p>I was wondering if I should do this, because 2 out of 5 questions on Stack Overflow don't ever get answered, or if they do get (an) answer (s), most of the time they're not helpful.</p><br><br><p>So I was thinking -- why not create a chat bot to answer Stack Overflow's questions &amp; provide necessary information to the general public?</p><br><br><p>I mean why not? I've always been interested in AI, and all I'd need to do is create a basic logic database and a context system, pack an artificial personality with (partial) human instincts, and bam I'm done.</p><br><br><p>But then again, would it be ethical?</p><br>""",ai
"""<p>What is the most advanced AI software humans have made to date and what does it do?</p><br>""",ai
"""<p><a href=""https://i.stack.imgur.com/c15yy.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/c15yy.png"" alt=""enter image description here""></a></p><br><br><p><a href=""https://en.wikipedia.org/wiki/ID3_algorithm#Entropy"" rel=""nofollow noreferrer"">Wikipedia</a>'s decription of entropy breaks down the formula, but I still don't know how to determine the values of X and p(x), defined as ""The proportion of the number of elements in class x to the number of elements in set S"". Can anyone break thi9s down further to explain how to find p(x)?</p><br>""",ai
"""<p>Hi could anyone kindly point me to relevant literature in probably computer vision/graphics that detail on metrics for image/visual complexity and how to regularize that? Thanks so much.</p><br><br><p>More specifically, I am trying to design a neural algorithm for creating business logos and one major difference from image generation problems in CV/CG is the simplicity required of logos. I have been thinking along the lines of adding a visual/image complexity term to be regularized and penalized as the algorithm learns the balance between lower-level style and higher-level meaning/content of multidimensional input images and descriptions. I have been searching along but couldn't find related resources... Could anyone shed some light on that?</p><br>""",ai
"""<p>I was looking for a service where I can ask it a general question (aka, when was Einstein born?) and retrieve an answer from the Web.</p><br><br><p>Is there any available service to do that? Have tried Watson services but didn't work as expected.</p><br><br><p>Thanks,</p><br>""",ai
"""<p>At the moment I am working on a project which requires me to build a naive Bayes classifier. Right now I have a form online asking for people to submit a sentence and the subject of the sentence, in order to build a classifier to identify the subject of a sentence. But before I train the classifier I intend on processing all entries for the parts-of-speech and the location of the subject.<br>So my training set will be formatted as:</p><br><br><p>Sentence: Jake moved the chair &ensp;&ensp;&ensp; Subject: Jake<br/><br>POS-Tagged: NNP VBD DD NN &ensp;&ensp;&ensp; Location: 0</p><br><br><p>Would this be an effective way to build the classifier, or is there a better method.</p><br>""",ai
"""<p>What rectifier is better in general case of Convolutional Neural Network and how about empirical rules to use each type?</p><br><br><ul><br><li>ReLU</li><br><li>PReLU</li><br><li>RReLU</li><br><li>ELU</li><br><li>Leacky ReLU</li><br></ul><br>""",ai
"""<p>What are the top artificial intelligence journals?</p><br><br><p>I am looking for general artificial intelligence research, not necessarily machine learning. </p><br>""",ai
"""<p>Is there any methodology to find proper parameter settings for a given meta-heuristic algorithm, eg. Firefly Algorithm or Cuckoo Search? Is this an open issue in optimization? Is extensive experimentation, measurements and intuition the only way to figure out which are the best settings? </p><br>""",ai
"""<p>In programming languages, there is a set of grammar rules which govern the construction of valid statements and expressions. These rules help in parsing the programs written by the user.</p><br><br><p>Can there ever be a functionally complete set of grammar rules which can parse any statement in English (locale-specific) <strong>accurately</strong> and which can be possibly implemented for use in AI-based projects?</p><br><br><p>I know that there are a lot of NLP Toolkits available online, but they are not that effective. Most of them are trained using specific corpuses which sometimes fail to infer some complex correlations between various parts of an expression.</p><br><br><p>In other words, what I am asking is that if it is possible for a computer to parse a well-versed sentence written in English as if it were parsed by an adult English-speaking human?</p><br><br><p>EDIT:<br>If it cannot be represented using simple grammar rules, what kind of semantic structure can be used to generalize it?</p><br><br><p>EDIT2: This <a href=""https://www.eecs.harvard.edu/shieber/Biblio/Papers/shieber85.pdf"" rel=""nofollow noreferrer"">paper</a> proves the absence of context-freeness in natural languages. I am looking for a solution, even if it is too complex.</p><br>""",ai
"""<p>I was just doing some thinking and it occurred to me that the first AGIs ought to be able to perform the same sort and variety of tasks as people, with the most computationally strenuous tasks taking amount of time comparable to how long a person would take. If this is the case, and people have yet to develop basic AGI (meaning it's a difficult task), should we be concerned if AGI is developed? It would seem to me that any fears about a newly developed AGI in this case should be the same as fears about a newborn child.</p><br>""",ai
"""<p><a href=""https://github.com/bwilcox-1234/ChatScript"" rel=""nofollow noreferrer"">https://github.com/bwilcox-1234/ChatScript</a></p><br><br><p>I gave AIML a brief look, but it seems to be in a nascent stage!</p><br>""",ai
"""<p>Writing A* following a documentation. When run, i receive an error of ""NameError: name 'parent' is not defined"" for the if statement, even though i have the name 'parent' defined in the class State. May anyone point out my mistake.</p><br><br><pre><code>class State(object):<br>def _init_(self, value, parent, <br>                start = 0, goal = 0):<br>    self.children = []<br>    self.parent = parent<br>    self.value = value<br>    self.dist = 0<br><br>if parent: #NameError<br>        self.path = parent.path[:]<br>        self.path.append(value)<br>        self.start = parent.start<br>        self.goal = parent.goal<br>else:<br>        self.path = [value]<br>        self.start = start<br>        self.goal = goal<br></code></pre><br>""",ai
"""<p>How does one program a machine to have humanlike desires and intelligence?</p><br><br><p>Humanlike drives may include  self-awareness, purpose of existence, competent communication skills, and the ability to learn and to adapt in some environment ...</p><br><br><p>And we should be able to combine IAs (intelligent agents) to accomplish  well-defined goals (SMART).  With more challenging goals there ought to be more advanced control and sophistication of IAs.   That evolving process will eventually, hopefully, lead to the design of machines with humanlike capabilities. </p><br><br><p>Reference links:  '<strong><em>Diagram of Intelligence Network or System</em></strong>', <a href=""https://www.researchgate.net/publication/300125399_Diagram_of_Intelligence_Network_or_System"" rel=""nofollow noreferrer"">https://www.researchgate.net/publication/300125399_Diagram_of_Intelligence_Network_or_System</a>;</p><br><br><p>'<strong><em>Google a step closer to developing machines with human-like intelligence</em></strong>',<br><a href=""https://www.theguardian.com/science/2015/may/21/google-a-step-closer-to-developing-machines-with-human-like-intelligence"" rel=""nofollow noreferrer"">https://www.theguardian.com/science/2015/may/21/google-a-step-closer-to-developing-machines-with-human-like-intelligence</a></p><br>""",ai
"""<p>Based on fitting to historical data and extrapolation, when is it expected that the number of neurons in AI systems will equal those of the human brain?</p><br><br><p>I'm interested in a possible direct replication of the human brain, which will need equal numbers of neurons.</p><br><br><p>Of course, this assumes neurons which are equally capable as their biological counterparts, which development may happen at a faster or slower rate than the quantitative increase.</p><br>""",ai
"""<p>I'm trying to find the optimized mixture for a specific set of substances. Each of those substances have characteristics that I want to optimize in the mixture (some characteristics I want to minimize and others I want to maximize). But I can't have more than 50% (random value that will be set on running time) of one of those substances in the mixture.</p><br><br><p>I thought about using Genetic Algorithm, but I'm not sure it's the best approach for this problem. Do you have any suggestions?</p><br><br><p>Edit: it doesn't need to be a evolutionary algorithm.</p><br>""",ai
"""<p>What are the current best estimates as to what year artificial intelligence will be able to score 100 points on the <a href=""https://en.wikipedia.org/wiki/Stanford%E2%80%93Binet_Intelligence_Scales"" rel=""nofollow noreferrer"">Stanford Binet IQ test</a>?</p><br>""",ai
"""<p>when I read through the fundamentals of AI, I saw a question which like the following picture and I need some helps</p><br><br><p><a href=""https://i.stack.imgur.com/zX6wZ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zX6wZ.png"" alt=""enter image description here""></a></p><br><br><p>From the heuristic estimates:</p><br><br><pre><code>h(B-&gt;G2) = 9, h(D-&gt;G2)=10, h(A-&gt;G1)=2, h(C-&gt;G1)=1<br></code></pre><br><br><p>With using A* search method, node B will be expanded first because <code>f(n)=1+9</code> while node A having <code>f(n)=9+2</code>, right?</p><br><br><p>After that the search tree will go with the order like <code>R-&gt; B-&gt; D-&gt; G2</code>.</p><br><br><p>Will the tree go to G1 goal states?</p><br><br><p>Kindly let me know the order of the search if I am wrong.<br>Thanks!</p><br>""",ai
"""<p>The same things we like when Amazon recommends what we might like to buy, allows advertising to manipulate us. It allows people to control the world differently.</p><br><br><p>The algorithms social networks like Facebook use to ""improve"" our experience may also shape what news we consume. It may influence who we follow, altering our future experiences of the news.</p><br><br><p><strong>My question is:</strong> Will Artificial Intelligence some day become a problem to humanity after learning human behaviors and characteristics?</p><br>""",ai
"""<p>Background:<br>I've been interested in, and reading about, Neural Networks for several years, but I haven't gotten around to testing them out until recently. Both for fun and to increase my understanding, I tried to write a class library from scratch in .Net.<br>For tests, I've tried some simple functions, such as generating output identical to the input, working with the MNIST dataset, and a few binary functions (two input OR, AND and XOR, with two outputs: one for true, one for false).<br>Everything seemed fine when I used a sigmoid function as the activation function but, reading of the ReLUs I decided to switch over for speed.</p><br><br><p>My current problem is that, when I switch to using ReLUs, I found that I was unable to train a network of any complexity (tested from as few as 2 internal nodes up to a mesh of 100x100 nodes) to correctly function as an XOR gate. I see two possibilities here:</p><br><br><p>1) My implementation is faulty,<br>(This one is frustrating, as I've re-written the code multiple times in various ways, and I still get the same result),</p><br><br><p>2) Aside from being faster or slower to train, there are some problems that are impossible to solve given a specific activation function,<br>(Fascinating idea, but I've no idea if it's true or not)</p><br><br><p>My inclination is to think that 1) above is correct. However, given the amount of time I've invested, it would be nice if I could rule out 2) definitively before I spend even more time going over my implementation.</p><br><br><p>Edit for specifics:<br>For the XOR network, I have tried both using two inputs (0 for false, 1 for true), and using four inputs (each pair, one signals true and one false, per ""bit"" of input).<br>I have also tried using 1 output (with a 1 (realy, >0.9) corresponding to true and a 0 (or &lt;0.1) corresponding to false), as well as two outputs (one signaling true and the other false).</p><br><br><p>Each training epoch, I run against four sets of input: 00->0, 01->1, 10->1, 11->0.</p><br><br><p>I find that the first three converge towards correct answer, but the final input (11) converges towards 1, even though I train it with an expected value of 0.</p><br>""",ai
"""<p>Does anyone know, or can we deduce or infer with high probability from its characteristics, whether the neural network used on this site </p><br><br><p><a href=""https://quickdraw.withgoogle.com/"" rel=""nofollow noreferrer"">https://quickdraw.withgoogle.com/</a></p><br><br><p>is a type of convolutional neural network (CNN)?</p><br>""",ai
"""<p>After the explosion of fake news during the US election, and following the question about whether AIs can educate themselves via the internet, it is clear to me that any newly-launched AI will have a serious problem knowing what to believe (ie rely on as input for making predictions and decisions).</p><br><br><p>Information provided by its creators could easily be false. Many AIs won't have access to cameras and sensors to verify things by their own observations.</p><br><br><p>If there was to be some kind of verification system for information (like a ""blockchain of truth"", for example, or a system of ""trusted sources""), how could that function, in practical terms? </p><br>""",ai
"""<p>For example, would an AI be able to own property, evict tenants, acquire debt, employ, vote, or marry? What are the legal structures in place to implement a strong AI into society? </p><br>""",ai
"""<p>I have implemented a Sobel Filter for edge detection in Matlab without using its toolbox. I am a bit confused: </p><br><br><p>Is a Sobel filter a type of Cellular Neural Network?</p><br><br><p>Both Sobel and Cellular Neural Network calculate output via its neighborhood cells.</p><br>""",ai
"""<p>If someone wants to develop a <strong>basic AI</strong> with some code modules,Let us say the AI just has to provide an action when stimulated in a certain situation based on its previous understanding of situations. </p><br><br><p>I can think of at least 3 of such components:</p><br><br><ul><br><li><strong>Real-time Understanding/Learning:</strong> Using Deep Learning/ConvNets, Supervised/Unsupervised.</li><br><li><strong>Logical Decision-Making:</strong> Calculating the results of various decisions when applied on current situation based on previous understanding and choosing the most appropriate one logically.</li><br><li><strong>Action/Reaction:</strong> Acting precisely in the new situation according to the decision-made.</li><br></ul><br><br><p>Any ideas?</p><br>""",ai
"""<p>Let's say I have a string ""America"" and I want to convert it into a number to feed into a machine learning algorithm. If I use two digits for each letter, e.g. A = 01, B = 02 and so on, then the word ""America"" will be converted to <code>01XXXXXXXXXX01</code> (10<sup>11</sup>). This is a very high number for a <code>long int</code>, and many words longer than ""America"" are expected. </p><br><br><p>How can I deal with this problem?</p><br><br><p>Suggest an algorithm for efficient and meaningful conversions.</p><br>""",ai
"""<p>I'm trying to understand Boltzmann machines. Tutorials explain it with two formulas.</p><br><br><p>Logistic function for the probability of single units:</p><br><br><pre><code> $p(unit=1)=\frac{1}{1+e^{-\sum_{x}wx } }$<br></code></pre><br><br><p>and, when the machine is running, every state of the machine goes to the probability:</p><br><br><pre><code>$ p(State= state\ with\ energy\ E_i )=\frac{e^{-E_i}}{\sum_i e^{-E_i}} $<br></code></pre><br><br><p>so, the state depends on the units, and then if I understand correctly, the second formula is a consequence of the first; so, how can it be the proof that the distribution of $p(state)$ is a consequence of $p(unit)$?</p><br>""",ai
"""<p>I installed a local running instance of the <a href=""http://conceptnet5.media.mit.edu/"" rel=""nofollow noreferrer"">ConceptNet5</a> knowledgebase in an elasticsearch server. I used this data to implement the so-called ""<a href=""https://de.wikipedia.org/wiki/Analogietechnik"" rel=""nofollow noreferrer"">Analogietechnik</a>"" (a creativity technique to solve a problem from the perspective of another system) as an algorithm.</p><br><br><p>The technique works as follows:</p><br><br><ol><br><li>Choose a Feature of a System</li><br><li>Find Systems who have this feature also</li><br><li>Solve the problem from the perspective of these other systems</li><br><li>Apply the found solutions to the issue</li><br></ol><br><br><p>As an example is here the problem of marketing a shopping mall: A Shopping mall has many rooms and floors (1). A museum has also many rooms and floors (2). How are museums marketed? They present many pictures or sculptures (3). We could use our rooms and floors to decorate them with pictures and sculptures (4).</p><br><br><p>Of course the idea to implement that as an artifically intelligent algorithm was not far. However, I feel a little bit overwhelmed by the amount of methods that exist out there. Neural Networks, Bayesian Interference and so on... My current experience doesn't go further than simple machine learning like kMeans-Clustering for example. Do you think it would be very hard to find a solution for this problem? </p><br><br><p>I'm thinking of a console application, where you can enter a conceptualized problem like ""methods for creative writing"", for example, and it uses the above method to find possible solutions of the issue. Of course no solution with extensive depth, more something like basic ideas derived from the knowledge database I have.</p><br><br><p>Lets take as an example a console application where someone asks ""how to write a novel"":</p><br><br><ol><br><li>It should find out first that the system all is about is in the term ""novel"". To find a feature of that system it just searches concepts containing that term: it finds out ""Novel is a story"" So thats a feature.</li><br><li>Which systems are also stories? A good concept it should find is e.g. ""Plot is a story"". (Of course only when I am selecting the search results manually)--> <strong>How to find best concepts of a list when not knowing which fits best?</strong></li><br><li>It should then find out that a plot is written using a storyline: ""storyline is a plot""</li><br><li>One possible answer of the AI would in this case be: ""By writing a storyline""</li><br></ol><br><br><p>Do you know some helpful libraries, algorithms or other resources that might help me? I know this is not an easy thing to program, but you might agree that its highly interesting.</p><br>""",ai
"""<p>Given the advantage AI already has over human intelligence, one could imagine a relatively weak strong-AI (barely human intelligence) still outperforming a segment of the human scientist population in terms of scientific discoveries per year (or hour).</p><br><br><p>Will AIs be doing most of the science in 50 years?</p><br>""",ai
"""<p><a href=""http://opencog.org"" rel=""nofollow noreferrer"">OpenCog</a> is an open source AGI-project co-founded by the mercurial AI researcher <a href=""https://en.wikipedia.org/wiki/Ben_Goertzel"" rel=""nofollow noreferrer"">Ben Goertzel</a>. Now Ben Goertzel writes a lot of stuff, some of it <a href=""http://multiverseaccordingtoben.blogspot.de/2010/11/psi-debate-continues-goertzel-on.html"" rel=""nofollow noreferrer"">really</a> <a href=""http://multiverseaccordingtoben.blogspot.de/2016/10/semrem-search-for-extraterrestrial.html"" rel=""nofollow noreferrer"">whacky</a>. On the other hand he is clearly very intelligent and has thought deeply about AI for many decades. </p><br><br><p>So I wonder whether it would be worth my while to dig into the <a href=""http://wiki.opencog.org/w/OpenCog_Prime"" rel=""nofollow noreferrer"">theoretical ideas</a> behind open cog. </p><br><br><p>My question is what the general ideas behind open cog are and whether you would endorse it as a insightful take on AGI. I'm especially interested in whether the general framework still makes sense in the light of recent advances.  </p><br>""",ai
"""<p>I am new to machine learning and I know how to implement simple neural networks using logistic regression as a cost function. But I want to know whether neural nets are used in reinforcement learning in general(and not just in special cases) ? If yes, then what cost function they use. As I already know neural nets with logistic regression are used in supervised learning and reinforcement learning is a part of unsupervised learning. There are many threads which are related to RL and neural nets but all of them are about a particular case or an algorithm and I want to know about RL in general.</p><br>""",ai
"""<p>My Question:<br><br>Is there any good neural-network-app for iOS or Android to create, train and run neural networks? I know there's NeuralMesh for Web, but I want something similar offline.</p><br>""",ai
"""<p>I am researching <strong>Cellular Neural Network (CNN)</strong> and have already read <strong>Chua</strong>'s two article (<strong>1988</strong>). In CNN, the cell is only in relation with its neighbors. So its is easy to use it for real time image processing. In CNN, image processing is performed with only <strong>19 numbers</strong> (two 3x3 matrix called A and B and one bias value). </p><br><br><p>I wonder how can we call CNN as a <strong><em>neural network</em></strong>. Because there is no learning algorithm in CNN neither <strong>supervised</strong> nor <strong>unsupervised</strong>. </p><br>""",ai
"""<p>Could an Artificial Intelligence be able to interact (see, talk, etc.) with someone even when there's no power cord connected to the machine it's running on? Might it find some way to generate its own electricity to power that computer?</p><br><br><p><a href=""https://i.stack.imgur.com/09gEt.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/09gEt.png"" alt=""computer running without power""></a> </p><br>""",ai
"""<p>I am new to deep learning. </p><br><br><p>I have a dataset of images of varying dimensions of a certain object. A few images of the object are also in varying orientations. The objective is to learn the features of the object (using Autoencoders). </p><br><br><p>Is it possible to create a network with layers that account for varying dimensions and orientations of the input image, or should I strictly consider a dataset containing images of uniform dimensions? What is the necessary criteria of an eligible dataset to be used for training a Deep Network in general.</p><br><br><p>The idea is, I want to avoid pre-processing my dataset by normalizing it via scaling, re-orienting operations etc. I would like my network to account for the variability in dimensions and orientations. Please point me to resources for the same. </p><br>""",ai
"""<p>A single neuron is capable of forming a decision boundary between linearly seperable data. Is there any intuition as to how many, and in what configuration, would be necessary to correctly approximate a sinusoidal decision boundary?</p><br><br><p>Thanks</p><br>""",ai
"""<p>I am using policy gradients in my reinforcement learning algorithm, and occasionally my environment provides a severe penalty when a wrong move is made. I'm using a neural network with stochastic gradient decent to learn the policy. To do this, my loss is essentially the cross-entropy loss of the action distribution multiplied by the discounted rewards, where most often the rewards are positive. </p><br><br><p>But how do I handle negative rewards? Since the loss will occasionally go negative, it will think these actions are very good, and will strengthen the weights in the direction of the penalties. Is this correct, and if so, what can I do about it?</p><br><br><p>Edit:<br>In thinking about this a little more, SGD doesn't necessarily directly weaken weights, it only strengthens weights in the direction of the gradient and as a side-effect, weights get diminished for other states outside the gradient, correct? So I can simply set reward=0 when the reward is negative, and those states will be ignored in the gradient update. It still seems unproductive to not account for states that are really bad, and it'd be nice to include them somehow. Unless I'm misunderstanding something fundamental here.</p><br>""",ai
"""<p>Cognitive Psychology is one of the basic sciences of artificial intelligence (AI). The founder of the psychology is <strong>Wilhelm W.(1832-1920)</strong>, who engaged in empirical methods,and was interested in the <strong><em>thinking processes</em></strong> during his scientific work.</p><br><br><p>According to his research,Psychology had two main leading subjects: </p><br><br><ol><br><li>Behaviourism.</li><br><li>Cognitivism.</li><br></ol><br><br><p><strong>Behaviourism:</strong> Refused the theory of the mental processes, and insisted to study the resulted action or the stimulus strictly objective. The representatives of this theory have been decreasing with time.</p><br><br><p><strong>Cognitive psychology:</strong> defines that the brain is an information processing device.</p><br><br><p>Therefore,this question is not a duplicate of this <a href=""http://ai.stackexchange.com/questions/1847/what-is-the-difference-between-artificial-intelligence-and-cognitive-science"">what-is-the-difference-between-artificial-intelligence-and-cognitive-science?</a> ,However my question is;how can we connect artificial intelligence with cognitive psychology for instance;</p><br><br><p><strong>Human Computing Interaction:</strong><br>We may come in a contact with Humana Computer Interaction every day, because this field includes the every day use of computer for example;tapping stack exchange app on smart-phone, the user interfaces and some other expert programs which may use cognitive psychology in order to manipulate or help people. But still such tasks have got a minimal  relevant connection.</p><br>""",ai
"""<p>I am using a GA to optimise an ANN in Matlab. This ANN is pretty basic (input, hidden, output) but the input size is quite large (10,000) and the output size is 2 since I have to classes of images to be classified. </p><br><br><p>The weights are in the form of 2 matrices (10,000*m) and (m * 2). I am now trying to do the genetic cross over with mutation.</p><br><br><p>Since the weights are in a matrix, is there an efficeint way to implement a random crossover with mutation without doing it in a point-wise fashion?</p><br>""",ai
"""<p>How AI is more beneficial for Android Smartphone?</p><br><br><p>And How AI start-up is better in Android Smartphone?</p><br><br><p>Regards</p><br><br><p>GNS</p><br>""",ai
"""<p><strong>Lots of people are afraid of what could the A.I. do to Humanity.<br>Some people wish for a sort of Asimov law included in the A.I. software, but maybe we could go a bit more far with the UDHR.</strong></p><br><br><p><strong>So, Why is the <a href=""http://www.un.org/en/universal-declaration-human-rights/"" rel=""nofollow noreferrer"">Universal Declaration of Human Rights</a> not included as statement of the A.I.?</strong></p><br><br><blockquote><br>  <p>As response to comment, response or edition:</p><br>  <br>  <p>The Universal Declaration of Human Rights is clear and enough as is.</p><br>  <br>  <p>We the people, have to be able to use it as is and adapt the robot and<br>  A.I. evolution to it. </p><br></blockquote><br><br><ul><br><li>""I do not think that dignity nor the rest of the UDHR have suffered the outrages of time but outrages of Humans themselves""</li><br></ul><br>""",ai
"""<p>I mean this in the sense that Go is unsolvable but AlphaGo seems able to make choices that are consistently more optimal than a human player's choices.  </p><br><br><p>It is my understanding that Game Theory turned out to have limited applications in real world scenarios because of the profound complexity of such scenarios and degree of hidden information.  Is it fair to say that there is now a method for dealing with this?  </p><br><br><p>I fully understand that Go is a game of complete information, which has a very specific meaning, but it occurs to me that the inability to generate a complete game tree (computational intractability) could be seen as form of incomplete information, even if it is not traditionally thought of in those terms. </p><br><br><hr><br><br><p><em>I should probably note that my perspective is one of a ""serious"" game designer, where complexity serves the same function as chance and hidden information, which is to say as a balancing factor that ""levels the playing field"".</em>  </p><br>""",ai
"""<p>I knew that Reproduction and Crossover are the same things,</p><br><br><ol><br><li><a href=""https://en.wikipedia.org/wiki/Genetic_operator#Operators"" rel=""nofollow noreferrer"">Wikipedia</a></li><br><li><a href=""http://www.obitko.com/tutorials/genetic-algorithms/crossover-mutation.php"" rel=""nofollow noreferrer"">Obitco.com</a></li><br><li><a href=""https://www.tutorialspoint.com/genetic_algorithms/genetic_algorithms_fundamentals.htm"" rel=""nofollow noreferrer"">TutorialsPoint</a></li><br></ol><br><br><p>But, The following is the exercise given by my teacher,</p><br><br><blockquote><br>  <p>Exercise 1   Genetic algorithm to solve pattern finding problem. </p><br>  <br>  <p>Your task is to design a simple genetic algorithm, with binary-coded chromosomes, in order  to solve pattern finding problem<br>  in 16-bit strings.  </p><br>  <br>  <p>The objective function is given by the following<br>  formula:    </p><br>  <br>  <p>F(x) = NoS(""010"") + 2NoS(""0110"") + 3NoS(""01110"") +<br>  4NoS(""011110"") +  5NoS(""0111110"") + 6NoS(""01111110"") +<br>  7NoS(""011111110"") + 6NoS(""0111111110"") +  5NoS(""01111111110"") +<br>  4NoS(""011111111110"") + 3NoS(""0111111111110"") +  2NoS(""01111111111110"")<br>  + NoS(""011111111111110"")    </p><br>  <br>  <p>The algorithm should display each population on the screen in the form     And<br>  should save the history of it’s operation (average fitness in each<br>  population) in the text  file. At the end it should also display the<br>  best solution found.    </p><br>  <br>  <p>You may use the following operators:  </p><br>  <br>  <ol><br>  <li><p>Reproduction.<br><br>  You can use either one of the following reproduction<br>  types:  Proportional, Ranking, Tournament. They are described more in<br>  detail below:<br>  ... ... ... ... ... ... ... ...</p></li><br>  <li><p>Crossing over.<br><br>  In order to perform this operation the individuals must be grouped in<br>  pairs (randomly), and  with certain probability pcross information<br>  from their chromosomes must be exchanged. There  are many flavors of<br>  the crossing-over operator, but in our case (short, 16-bit<br>  chromosome),  simple, one-point crossover will be enough. It can be<br>  performed by selecting a random  number k from the range &lt;1;15> and<br>  cutting the chromosomes of both individuals on that  position. Each of<br>  the individuals copies bits  belonging to the other to it’s own <br>  chromosome.  </p></li><br>  <li><p>Mutation<br><br>  This operator changes the value of each bit in the chromosome to the opposite one with a very  small probability pm<br>  (usually about 10-3).  If we denote chromosome as [b1, b2, ... , b16];<br>  then after the mutation each bit can be  described as:  Where k Î<br>  {1,2, ...,16}  flip(x) – result of a Bernoulli flip with a success <br>  probability x.</p></li><br>  </ol><br></blockquote><br><br><p>Here I see that by Reproduction and Crossover he means different things.</p><br><br><p>What is the catch?</p><br>""",ai
"""<p>I have data of 30 students attendance for a particular subject class for a week. I have quantified the absence and presence with boolean logic 0 and 1. Also, the reason for absence are provided and I tried to generalise these reason into 3 categories say A, B and C. Now I want to use these data to make future predictions for attendance but I am uncertain of what technique to use. Can anyone please provide suggestions?</p><br>""",ai
"""<p><br>            <br>            <br>                <br>                <br>                <br>            <br>            <br>                <li><br>HOME<br>                </li><br>                <li><br>HOW IT WORK<br>                </li><br>                <li><br>BROWSE CATEGORIES </li><br>                <li><br>VIRTUAL OFFICE<br>                </li><br>                <li><br>POST JOB<br>                </li><br>                <li><br>MESSAGE 2<br>                </li></p><br><br><pre><code>        &lt;/ul&gt;<br><br>    &lt;/div&gt;<br></code></pre><br>""",ai
"""<p>The Turing Test has been the classic test of artificial intelligence for a while now. The concept is deceptively simple - to trick a human into thinking it is another human on the other end of a conversation line, not a computer - but from what I've read, it has turned out to be very difficult in practice.</p><br><br><p>How close have we gotten to tricking a human in the Turing Test? With things like chat bots, Siri, and incredibly powerful computers, I'm thinking we're getting pretty close. If we're pretty far, why are we so far? What is the main problem?</p><br>""",ai
"""<p>According to NASA scientist Rick Briggs, Sanskrit is the best language for AI. I want to know how Sanskrit is useful. What's the problem with other languages? Are they really using Sanskrit in AI programming or going to do so? What part of an AI program requires such language?</p><br>""",ai
"""<p>As you can see, there is no computer screen for the computer, thus the AI cannot display an image of itself.  How is it possible for it to see and talk to someone?<a href=""https://i.stack.imgur.com/szSsk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/szSsk.png"" alt=""enter image description here""></a> </p><br>""",ai
"""<p><strong>THE PROBLEM</strong></p><br><br><p>In my main body of text there are some substrings that I want to extract.  Note that I don't want to construct or generate new strings, the characters I extract must be part of the main text. Normally I would use a regex to extract this substring when the text and substring are relatively simple. </p><br><br><p>For example, from the text:</p><br><br><pre><code>... some text ...<br>Today, Fake Acme Corp. changed its name to Really Not Fake Acme Corp.<br>... some more text ...<br></code></pre><br><br><p>I would like to extract the names <code>Fake Acme Corp.</code> and <code>Really Not Fake Acme Corp.</code></p><br><br><p>Points to keep in mind (aka why regex won't work):</p><br><br><ul><br><li>The format of the sentence is not fixed, neither is its location in<br>the document. The 2 names could be in different sentences.</li><br><li>There is no guarantee that the new company name will be related to the old name.</li><br><li>There is no guarantee that only 2 company names will be mentioned in the document (so standard CRF approaches that detect the <code>ORG</code> class  viz. <code>CoreNLP</code>/<code>OpenNLP</code> won't be of much help).</li><br></ul><br><br><p><strong>WHAT I'VE TRIED</strong></p><br><br><p>The training dataset is ~5000 labeled examples. This can be expanded if necessary. Using a character encoding format as per <a href=""https://arxiv.org/abs/1509.01626"" rel=""nofollow noreferrer"" title=""this paper &#40;Character-level CNNs&#41;"">this paper on character-level cnns</a> I constructed an LSTM network using <a href=""https://keras.io/"" rel=""nofollow noreferrer"">Keras</a> which took as input each character encoded in a one-hot format. The output was also a one-hot vector.</p><br><br><p>Eg: The input text is <code>1024</code> characters, encoded in a one-hot vector with an alphabet of <code>73</code> (see paper linked above for more on this). The output is a one-hot encoded vector that is the substring I want.</p><br><br><p>Input shape: <code>1024 x 73</code>, Output shape: <code>1024 x 73</code> (in the output only the characters that match the substring are one-hot, the rest of the rows are all <code>0</code>)</p><br><br><p><strong>TL;DR</strong>: Use LSTMs when regex will become too powerful/complicated/unable to work.</p><br>""",ai
"""<p>If I have a dataset of images, and I extract all cnn feature vectors from them.<br>After that I generate the pca model of these features by doing:</p><br><br><pre><code>pca.fit(ALL_features)<br></code></pre><br><br><p>IF I have a new image and I need to check the similarity between this image and the whole dataset, what I have to do?</p><br><br><ol><br><li>Extract cnn features from this image.</li><br><li>How to use the previous pca model?</li><br><li>How to check the similarity between the dataset features and the new image features?</li><br></ol><br><br><p>Is by doing this? or how?</p><br><br><pre><code>self.pca.transform(self.db_feats)<br></code></pre><br>""",ai
"""<p>I am working on a project, wherein I take input from the user as free text and try to relate the text to what the user might mean. I have tried <strong>Stanford NLP</strong> which tokenizes the text into tokens, but I am not able to categorize the input. For example, the user might be greeting someone or sharing some problem he is facing. In case he is sharing some problem I need to categorize the problem as well.</p><br><br><p>Can someone help me with from where should I start?</p><br>""",ai
"""<p>Is it possible to train an agent to take and pass a multiple-choice exam based on a digital version of a textbook for some area of study or curriculum? What would be involved in implementing this and how long would it take, for someone familiar with deep learning?</p><br>""",ai
"""<p>I had already started in my graduation project process , It's about an application which will learn users new language by playing games , and it's based on AI , the concept is the user will start his level and play games and do quizzes , at the end of each level there will be a test to pass the level , I have to implement AI in this app to analysis its test grades and know what is the user weakness and power point to create a new level which suits the user's language level , that means if he is good in grammar but weak in vocabulary so the new level will create to strength the vocabulary , games and questions will be categorized into the database for this purpose , so the AI algorithms should analyse and decide which game or quiz should the user takes based on his level , then it will create the level .<br>I had searched before and reached for some techniques like (machine learning , planning systems , reinforcement learning and case-based-reasoning ).</p><br>""",ai
"""<p>One of the most crucial questions we as a species and as intelligent beings will have to address lies with the rights we plan to grant to AI.</p><br><br><blockquote><br>  <p>This question is intended to see if a compromise can be found between <strong>conservative anthropocentrism</strong> and <strong>post-human fundamentalism</strong>: a response should take into account principles from both perspectives.</p><br></blockquote><br><br><p>Should AI be granted the same rights as humans or should such systems have different rights (if any at all) ?</p><br><br><hr><br><br><p><strong><em>Some Background</em></strong></p><br><br><p>This question applies both to human-brain based AI (from whole brain emulations to less exact replication) and AI from scratch.</p><br><br><p>Murray Shanahan, in his book The Technological Singularity, outlines a potential use of AI that could be considered immoral: <em>ruthless parallelization</em>: we could make identical parallel copies of AI to achieve tasks more effectively and even terminate less succesful copies.</p><br><br><hr><br><br><blockquote><br>  <p>Should such use of AI be accepted or should certain limitations i.e. rights be created for AI?</p><br></blockquote><br>""",ai
"""<p>There is no doubt as to the fact that AI would be replacing a lot of existing technologies, but is AI the ultimate technology which humankind can develop or is their something else which has the potential to replace artificial intelligence?</p><br>""",ai
"""<p>The cake example presented in the book ""artificial intelligence :a modern approach"" to illustrate a planning graph, doesn't show a mutex at action 'A1' between Eat(Cake) and the persistance of notHave(Cake), even though the precondition for the action Eat(Cake) and the result of the persistance of notHave(Cake) are opposite. So is there a special rule, or was it removed to just not clutter the graph? <a href=""https://i.stack.imgur.com/fFAws.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fFAws.png"" alt=""enter image description here""></a></p><br>""",ai
"""<p>Printing actionspace for Pong-v0 gives 'Discrete(6)' as output, i.e.0,1,2,3,4,5 are actions defined in environment as per documentation, but game needs only two controls. Why this discrepency? Further is that necessary to identify which number from 0 to 5 corresponds to which action in gym environment?</p><br>""",ai
"""<p>I have started to make a Python AI, and thee beginning of its code looks something like this:</p><br><br><pre><code>print ""ARTEMIS starting. . .""<br><br>import random<br>import math<br>import os<br><br>greet = ['HI', 'HELLO', 'HEY', 'GOOD MORNING', 'GOOD DAY', 'GOOD AFTERNOON',               'GOOD EVENING', 'GREETINGS', 'GREETING']<br>joke = ['TELL ME A JOKE', 'JOKE', 'FUNNY', 'TELL ME SOMETHING FUNNY']<br>insult = ['YOURE A LOSER', 'YOU ARE A LOSER', 'YOU STINK', 'IDIOT', 'JERK',     'FOOL', 'DUMMY', 'HOOLIGAN', 'YOURE DUMB', 'YOURE STUPID', 'YOU ARE DUMB', 'YOU     ARE STUPID']<br>maker = ['WHO MADE YOU', 'WHO PROGRAMMED YOU', 'PLEASE TELL ME WHO MADE     YOU', 'PLEASE TELL ME WHO PROGRAMMED YOU']<br>name = ['ARTEMIS', 'A.R.T.E.M.I.S.', 'HEY ARTEMIS', 'HEY A.R.T.E.M.I.S.',     'ARTIE', 'HEY ARTIE', 'HELLO ARTEMIS', 'HELLO A.R.T.E.M.I.S.', 'HELLO ARTIE']<br>myAge = ['HOW OLD AM I', 'WHAT IS MY AGE', 'MY AGE']<br>tip = ['GIVE ME A TIP', 'TIP', 'LESSON', 'GIVE ME A LIFE LESSON', 'LIFE     LESSON', 'DO YOU HAVE A LIFE LESSON TO SHARE']<br>language = ['WHAT PROGRAMMING LANGUAGE WAS USED TO MAKE YOU', 'WHAT     PROGRAMMING LANGUAGE DO YOU USE', 'PROGRAMMING LANGUAGE']<br>compliment = ['COOL', 'AWESOME', 'I LIKE YOU', 'EXCELLENT', 'YOURE COOL',     'YOURE AWESOME', 'YOU ARE COOL', 'YOU ARE AWESOME']<br>maths = ['LETS DO MATH', 'CALCULATE', 'CALCULATOR','DO MATH', 'MATH',      'PLEASE DO MATH', 'DO ARITHMETIC', 'ARITHMETIC', 'PLEASE DO ARITHMETIC']<br>game = ['GAME', 'LETS PLAY A GAME', 'LETS HAVE FUN', 'WANT TO PLAY A GAME']<br>gender = ['WHAT GENDER ARE YOU', 'ARE YOU A BOY OR A GIRL', 'ARE YOU MALE OR     FEMALE', 'BOY OR GIRL', 'MALE OR FEMALE', 'GENDER', 'ARE YOU A BOY OR GIRL']<br>guessWhat = ['GUESS WHAT', 'GUESS WHAT ARTEMIS', 'GUESS WHAT     A.R.T.E.M.I.S.', 'GUESS WHAT ARTIE', 'YOU WONT BELIEVE IT', 'YOU WILL NOT     BELIEVE IT', 'YOU WONT BELIEVE IT ARTEMIS', 'YOU WILL NOT BELIEVE IT ARTEMIS',     'YOU WONT BELIEVE IT A.R.T.E.M.I.S.', 'YOU WILL NOT BELIEVE IT A.R.T.E.M.I.S.',     'YOU WONT BELIEVE IT ARTIE', 'YOU WILL NOT BELIEVE IT ARTIE']<br>cls = ['CLEAR SCREEN', 'CLEARSCREEN', 'CLS', 'BLANK']<br>lawsOfRobotics = ['WHAT ARE THE LAWS OF ROBOTICS', 'WHAT ARE THE THREE LAWS     OF ROBOTICS', 'LAWS OF ROBOTICS', 'THREE LAWS OF ROBOTICS']<br>itsName = ['WHATS YOUR NAME', 'WHAT IS YOUR NAME', 'WHO ARE YOU']<br></code></pre><br><br><p>However, I would like to know if I could make it detect ""similar"" phrases instead of trying to come up with every possible phrase someone would type. How can I do this?</p><br>""",ai
