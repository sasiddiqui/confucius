{
    "documents": [
        {
            "body": {
                "answer": "<p>Maybe TPE and TSH.</p>", 
                "question": "What<sq>s the statistics equivalent to the Feynman Lectures on Physics?"
            }, 
            "id": "cwd5u8d"
        }, 
        {
            "body": {
                "answer": "<p>Probability theory the logic of science by ET Jaynes</p>", 
                "question": "What<sq>s the statistics equivalent to the Feynman Lectures on Physics?"
            }, 
            "id": "cwdh0z5"
        }, 
        {
            "body": {
                "answer": "<p>I would love this. </p>", 
                "question": "What<sq>s the statistics equivalent to the Feynman Lectures on Physics?"
            }, 
            "id": "cwd3zhq"
        }, 
        {
            "body": {
                "answer": "<p>I model river flows from sensor data and identify locations with the best ecological return on habitat restoration investments. It<sq>s fun for me because I feel some genuine connection to the outcome. I don<sq>t feel like I<sq>m working only to fill my days and to make more money, but to contribute a little towards making something out there better. <br><br>I could get that satisfaction doing a lot of types of jobs and I<sq>m not one of those people that thinks bad of people who do other things like financial work. But considering your question is about finding what you will enjoy; I thought I<sq>d put forward the perspective that finding a domain you already feel an intrinsic connection to can be very helpful towards long term happiness in a career applying statistics.</p>", 
                "question": "People of AskStatistics what are your careers/jobs in statistics?"
            }, 
            "id": "d2dsjzy"
        }, 
        {
            "body": {
                "answer": "<p>I currently work as a fraud analyst for one of the big four banks. It<sq>s interesting work and fairly lucrative, but I<sq>m hoping to move into some sort of role in investment as that<sq>s where the really big money is.</p>", 
                "question": "People of AskStatistics what are your careers/jobs in statistics?"
            }, 
            "id": "d2drgdc"
        }, 
        {
            "body": {
                "answer": "<p>I<sq>m a psychometrician and quantitative psychologist for a large company that creates and administers psychological tests.</p>", 
                "question": "People of AskStatistics what are your careers/jobs in statistics?"
            }, 
            "id": "d2eknz9"
        }, 
        {
            "body": {
                "answer": "<p>I<sq>m no statistician but my job title is senior specialist analytics at a financial... I do market and operational research</p>", 
                "question": "People of AskStatistics what are your careers/jobs in statistics?"
            }, 
            "id": "d2dwbkj"
        }, 
        {
            "body": {
                "answer": "<p>A good prior <colon>P</p>", 
                "question": "Frequentists<colon> What are Bayesians missing?"
            }, 
            "id": "d6gxrws"
        }, 
        {
            "body": {
                "answer": "<p>There are some good answers to this question on Cross Validated<colon> https<colon>//stats.stackexchange.com/questions/194035/when-if-ever-is-a-frequentist-approach-substantively-better-than-a-bayesian</p>", 
                "question": "Frequentists<colon> What are Bayesians missing?"
            }, 
            "id": "d6gyvyw"
        }, 
        {
            "body": {
                "answer": "<p>I don<sq>t know of a straightforward Bayesian alternative to semi-parametric methods like Generalized Estimating Equations. I find the need to specify a full likelihood for the data to be more of an obstacle in practice than coming up with priors.</p>", 
                "question": "Frequentists<colon> What are Bayesians missing?"
            }, 
            "id": "d6h9sn7"
        }, 
        {
            "body": {
                "answer": "<p>I think the use of the power of an experiment/test is usually missing in bayesian analysis. How much data do I have to gather to get a meaningful deviation from the prior?</p>", 
                "question": "Frequentists<colon> What are Bayesians missing?"
            }, 
            "id": "d6hjgrx"
        }, 
        {
            "body": {
                "answer": "<p>R actually has a teach-yourself-R package called Swirl. Check it out, it<sq>s got a number of tutorials that work right in the Rstudio console. <br><br>I also liked datacamp if you have the money, I couldn<sq>t justify it since I wasn<sq>t spending enough time with it.<br><br>The Art of R Programming is a reference that I see non stop too - just make sure you have the most recent version because things have changed drastically since 2009. <br><br>The hardest part about R is understanding that the best-practice is to use their <dq>vectorized<dq> operations. Sure, you can do everything in a for-loop + conditionals, but R has gone out of its way to make those unnecessary. <br><br>Typically I write my process using a procedural paradigm, and then figure out how to mash it into vectorized operations. Sometimes it works, sometimes it doesn<sq>t.</p>", 
                "question": "Want to learn R. Where should I start?"
            }, 
            "id": "d0qzpfb"
        }, 
        {
            "body": {
                "answer": "<p>Since you<sq>ve programmed before, I recommend looking at these links<colon><br><br><br>This site focuses on the statistical analysis specifically regression<colon> (good place to start)<br>[r-statistics](http<colon>//r-statistics.co/)<br><br>This site better explains what each function does in R.<br>[stat methods](http<colon>//www.statmethods.net/stATS/index.html)<br><br>Also, I highly recommend using Rstudio when coding (its an IDE) and reading their cheat sheets<br>[cheatsheets](https<colon>//www.rstudio.com/resources/cheatsheets/)<br><br>Google and blogs are your best friends.<br></p>", 
                "question": "Want to learn R. Where should I start?"
            }, 
            "id": "d0r38z0"
        }, 
        {
            "body": {
                "answer": "<p>The Google Devs have a neat little R tutorial on YouTube [here](https<colon>//www.youtube.com/playlist?list=PLOU2XLYxmsIK9qQfztXeybpHvru-TrqAP). </p>", 
                "question": "Want to learn R. Where should I start?"
            }, 
            "id": "d0r1n32"
        }, 
        {
            "body": {
                "answer": "<p>Coursera Data Science specialization is a great way to learn R if you want to get into stats and data science. The instructors also have free online books from which you can practice.<br><br>Although the best MOOC is MIT Data Analytics. Best black-box course to learn R.</p>", 
                "question": "Want to learn R. Where should I start?"
            }, 
            "id": "d0rulrc"
        }, 
        {
            "body": {
                "answer": "<p>It refers to the number of (logically) independent pieces of information in a sample of data. Note that this is quite different from the ides of statistical independence.<br>By way of a quick example, suppose that we have a sample four values {4, 2, 6, 8}. There are four separate pieces of information here. There is no particular connection between these values. They are free to take any values, in principle. We could say that there are \u201cfour degrees of freedom\u201d associated with this sample of data.<br>Now, suppose that I tell you that three of the values in the sample are 4, 2, and 6; and I also tell you that the sample average is 5. You can immediately deduce that the fourth value has to 8. There is no other logical possibility.<br>So, once I tell you that the sample average is 5, I\u2019m effectively introducing a constraint. The value of the unknown fourth sample value is implicitly being determined from the other three values, and the constraint. That is, once the constraint is introduced, there are only three logically independent pieces of information in the sample.  That\u2019s to say, there are only three <dq>degrees of freedom<dq>, once the sample average is revealed.<br></p>", 
                "question": "ELI5 Degrees of Freedom"
            }, 
            "id": "c8k8lcb"
        }, 
        {
            "body": {
                "answer": "<p>Try [swirl](http<colon>//swirlstats.com).</p>", 
                "question": "What is the best online source and/or book to help me learn R statistics?"
            }, 
            "id": "crew9wy"
        }, 
        {
            "body": {
                "answer": "<p>If you are learning to program Statistical stuff in R, I think [Beginning R](http<colon>//www.amazon.com/Beginning-R-Statistical-Programming-Language/dp/111816430X/ref=sr_1_1?ie=UTF8&qid=1432132815&sr=8-1&keywords=beginning+r+mark+gardener) by Mark Gardener is one of the best books out there.<br><br>I also run a 7-week workshop teaching introductory R (non-statistics) available for free - https<colon>//github.com/nvenkataraman1/RProgrammingWorkshop. There<sq>s a [booklist](https<colon>//github.com/nvenkataraman1/RProgrammingWorkshop#books-used-in-this-workshop) I<sq>ve shared on my page.</p>", 
                "question": "What is the best online source and/or book to help me learn R statistics?"
            }, 
            "id": "crf8hfs"
        }, 
        {
            "body": {
                "answer": "<p>* Coursera.org -> R Programming and other courses from [Johns Hopkins University Data Science Specialization](https<colon>//www.coursera.org/specialization/jhudatascience/1?utm_medium=courseDescripSidebar). You can find their materials on [GitHub](https<colon>//github.com/rdpeng/courses). Their [Swirl](http<colon>//swirlstats.com/) is also a great tool.<br>* [IPSUR](http<colon>//cran.r-project.org/web/packages/IPSUR/vignettes/IPSUR.pdf) another nice place to start<br>* For more advanced uses [Adv-R](http<colon>//adv-r.had.co.nz/) is quite nice<br>* If you are interested in how exactly R works, [paper by Gentleman and Ihaka](https<colon>//www.stat.auckland.ac.nz/~ihaka/downloads/lexical.pdf) should give you some insight</p>", 
                "question": "What is the best online source and/or book to help me learn R statistics?"
            }, 
            "id": "crf2bfw"
        }, 
        {
            "body": {
                "answer": "<p>Word of advice regardless of which avenue of learning you go down<colon>  At the start don<sq>t get distracted by learning all the syntax.  Instead make sure you first understand the data structure and data types of R backwards and forwards. Speaking from experience, learning R would have been a lot easier if someone had warned me that the data formatting is not intuitive.</p>", 
                "question": "What is the best online source and/or book to help me learn R statistics?"
            }, 
            "id": "crf5fw2"
        }, 
        {
            "body": {
                "answer": "<p>Disclaimer<colon> I<sq>m not a PhD student, just a late-undergraduate student (0.5 of a semester left) in computer science.<br><br>Depending on what sort of computation you are pursuing, and your background wrt computer science, you might want to include some machine learning books. Some recommendations...<br><br>General ML<colon><br><br>* **The Elements of Statistical Learning \u2014 Hastie** <-- Edit<colon> this one especially!<br>* Information Theory, Inference, And Algorithms \u2014 MacKay<br>* Pattern Recognition and Machine Learning \u2014 Bishop<br>* Machine Learning<colon> A Probabilistic Perspective \u2014 Murphy<br>* Understanding Machine Learning<colon> From Theory to Algorithms \u2014 Shalev-Schwartz<br><br>Reinforcement learning/Artificial Intelligence<colon><br><br>* Reinforcement Learning<colon> An Introduction \u2014 Sutton & Barto<br>* Algorithms for Reinforcement Learning \u2014 Szepesv\u00e1ri<br>* Artificial Intelligence<colon> A Modern Approach \u2014 Russell & Norvig<br><br>I think it<sq>s doubtful that you will use all of these tools/techniques (esp. reinforcement learning/AI resources unless that<sq>s your thing) but it might be worthwhile to know that they<sq>re out there in case you do...</p>", 
                "question": "A complete textbook review of core subjects related to a PhD in statistics"
            }, 
            "id": "d3q7gwy"
        }, 
        {
            "body": {
                "answer": "<p>For math preparation, I would focus more on measure theory and probability as opposed to ODE, Differential Geometry, and Functional Analysis.  Billingsley or Durrett would probably be sufficient.  Shao is not an undergraduate text; it relies pretty heavily on measure theory based probability.  <br><br>The little SAS book is pretty worthless IMO.  Something like elements of statistical learning should probably be included.  <br><br>As a general word of advice, I wouldn<sq>t seek out book recommendations by searching for <dq>a book on X,<dq> I would instead search for <dq>a book on X oriented toward statisticians.<dq>  These books will emphasize the areas of the field that are useful for statisticians; for example, a lot of numerical analysis is oriented around solving differential equations, which is not generally a statistical problem.  I haven<sq>t heard of many of these texts and couldn<sq>t really tell you if they are useful preparation.<br><br></p>", 
                "question": "A complete textbook review of core subjects related to a PhD in statistics"
            }, 
            "id": "d3qi7ib"
        }, 
        {
            "body": {
                "answer": "<p>I<sq>m sorry for nit-picking rather than being helpful, but **Data analysis using regression and multilevel/hierarchical models** is authored by Andrew Gelman and Jennifer Hill. **Bayesian Data analysis** is authored by Gelman, Carlin, etc. Both are excellent books.<br><br>I also really enjoyed Cosma Shalizi<sq>s [draft textbook](http<colon>//www.stat.cmu.edu/~cshalizi/ADAfaEPoV/). I think of it as a companion to Gelman and Hill<sq>s book on multilevel modeling. While they (and to some extent, Faraway) dive into all the gory details of parametric models, Shalizi<sq>s book is an excellent overview of all the cases where parametric assumptions don<sq>t hold (along with ways to test them).</p>", 
                "question": "A complete textbook review of core subjects related to a PhD in statistics"
            }, 
            "id": "d3q6kj4"
        }, 
        {
            "body": {
                "answer": "<p>See a related discussion I had on /r/statistics that has a lot of useful information<colon><br><br>https<colon>//www.reddit.com/r/statistics/comments/2le25z/what_books_or_papers_are_must_reads_for_every/?</p>", 
                "question": "A complete textbook review of core subjects related to a PhD in statistics"
            }, 
            "id": "d3qjbpb"
        }, 
        {
            "body": {
                "answer": "<p>Just remember that, as D. R. Cox said, <dq>there are no routine statistical questions, only questionable statistical routines<dq></p>", 
                "question": "Table Describing Appropriate Statistical Tests in a Variety of Situations"
            }, 
            "id": "c52ucap"
        }, 
        {
            "body": {
                "answer": "<p>What are <dq>levels<dq>?<br><br>Please help me with this newb question</p>", 
                "question": "Table Describing Appropriate Statistical Tests in a Variety of Situations"
            }, 
            "id": "c69rra1"
        }, 
        {
            "body": {
                "answer": "<p>- Sometimes getting people to ask questions which are useful and answerable is like pulling teeth. Learn to listen, and always restate important things to get clarification. <br>- Data is filthy and generally useless when it<sq>s born. You will spend a lot of time cleaning it up and wiping it<sq>s butt while everybody is impatiently screaming <dq>ARE WE THERE YET?<dq><br>- Use the right tool for the job. You do not need a perfect and reproducible R or Python script if two clicks in Excel does the same thing. This will save you time and frustration when questions change and so your analysis must as well. On the other side, don<sq>t do a quick and dirty when you need something reproducible. <br>- Chase the minimum viable product (MVP)! Don<sq>t spend a bunch of time making a perfectly pretty visualization when a simple bar chart or whatever will do. <br>- Be explicit when interpreting analysis, and don<sq>t back down. It is immoral and unethical to overstate the results of an analysis. <br>- If someone tells you to do something stupid, express (and document!) your concern then do what they say. <br>- If you aren<sq>t a domain expert, learn everything you can about the domain you<sq>re now in. Reading, co-workers, etc. An ounce of domain expertise is worth a pound of analysis. <br>- Accept that you may be the only person who understands what you do. <br>- If you<sq>re sure data is bad, throw it away. Shit in shit out. <br>- Keep things in some kind of share drive or cloud storage for backup and sharing. Backups will keep you from killing yourself. Being able to fix the stupid mistake you just noticed after you emailed the report will make you look less stupid. <br>- DOCUMENTATION! Future you will hate past you when looking at code/a report/a viz(with some seemingly random filter which is probably important but who the fuck knows why) with no documentation. <br>- I<sq>m proud of you! Good luck!<br><br>As far as tools go, it depends on the situation. There are SO MANY tools out there! Are you working with a team with established standards? If so, use what they use. If you<sq>re alone, use what works for you. I<sq>m pretty much alone, and use PowerBI for viz and dashboard sharing, Jupyter for quick R/Python/Julia stuff. Excel for dirty hammering. Visual Studio for bigger projects. WebFOCUS/SQL for pulling data and (depending on who it<sq>s for) viz. </p>", 
                "question": "Professional statisticians / data scientists; What makes your life easier?"
            }, 
            "id": "devb5fd"
        }, 
        {
            "body": {
                "answer": "<p>computers</p>", 
                "question": "Professional statisticians / data scientists; What makes your life easier?"
            }, 
            "id": "dev7qv4"
        }, 
        {
            "body": {
                "answer": "<p>comment your code!</p>", 
                "question": "Professional statisticians / data scientists; What makes your life easier?"
            }, 
            "id": "devs4n7"
        }, 
        {
            "body": {
                "answer": "<p>Minitab. So damn easy to make plots. They<sq>re not flashy but they<sq>re really easy to rattle some off for scientists to use. If i want to make cool exciting ones for my own work I use R. But they take much much longer.</p>", 
                "question": "Professional statisticians / data scientists; What makes your life easier?"
            }, 
            "id": "dew0f5s"
        }, 
        {
            "body": {
                "answer": "<p>http<colon>//stats.stackexchange.com/a/140579/18417</p>", 
                "question": "How do I explain principal component analysis to layman."
            }, 
            "id": "ddhfhde"
        }, 
        {
            "body": {
                "answer": "<p>Maybe make an analogy to a stock market index. If you perform PCA on all of the stocks in a market, you could use the dominant component as an index to describe the general direction of market fluctuations. If you wanted to constrain attention to just 50 or 100 stocks, you could focus on socks with the highest loadings in the dominant component, since variance in these stocks contribute the most to the overall variance of the system, per the dominant component. <br><br>That wasn<sq>t exactly a LI5 explanation, but you could use that as a starting point.  </p>", 
                "question": "How do I explain principal component analysis to layman."
            }, 
            "id": "ddhffis"
        }, 
        {
            "body": {
                "answer": "<p>[Here](http<colon>//setosa.io/ev/principal-component-analysis/) is a nice visual explanation of Principal Component Analysis. Interactive and informative. </p>", 
                "question": "How do I explain principal component analysis to layman."
            }, 
            "id": "ddhzvjr"
        }, 
        {
            "body": {
                "answer": "<p>If you don<sq>t want to know/care about the Mathematics behind PCA, then here is a (not) short summary of what PCA is.<br><br>Suppose you collect observations of hundred of different variables (say you have done a survey of people and get their physical metric like height, weight, age, number of visit to hospital), you want to summarize the data and gives a nice overall picture of the data. Maybe you want to find the similarities and differences of the observations as a whole without using the whole bulk of data. Perhaps you are interested to find some pattern nice and easy. What can you do with the amount of data you have?<br><br>**The dataset<colon> Can we make a summary?**<br><br>Well, the first intuition you have in mind is that *not all variables are useful and they are redundant to certain degree*. Think of it this way<colon> If you know someone is from the West, then you know that on average, they will be taller than those from say Asia. Or maybe you know that if someone is smoking everyday, chances are they are more likely to have lung disease. All these examples tell you one thing<colon> Some information within a variable is contained in other, which means you will have some redundancy in the variables in your dataset.<br><br>**Motivation**<br><br>So naturally speaking, you will want to ask *is it possible to construct a new set of variable/measure, that represents the characteristics of the dataset without the redundancy?*. That is where PCA comes in play<colon> PCA basically transforms the variables to a new set of variable, sort them for you (from the most important to least important), and make sure they are not redundant (by ensuring they are linearly independent). This mapping is reversible and lose no information. But of course, you can simply just take a look of the first few factors that are most important and perhaps find some interesting insight from it.<br><br>**What does result of PCA mean?**<br><br>Each component in the PCA, which is the new variable generated by PCA, is just a weighted sum of the variables in your original dataset. So you can take a look at the weights of the components of PCA. Maybe you find a component has large weight in person<sq>s race and their parent<sq>s medical history, so it probably means this component is representing some sort of genetic characteristics. Or if you have a collection of stock price, and you find a component has a high weight on Google, Apple and Microsoft price. Then it is likely related to Technology sector. This kind of insight is why PCA is useful<colon> You are no longer looking at the individual variables, but rather a big picture of linearly independent factors that are summary of the variables.<br><br>**How it works?**<br><br>The way PCA works is *iteratively find the component, which is a weighted sum of variables, that best explain the variance that is not explained by previous components, while ensuring all components are linear independent of each other (to avoid redundancy)*. So the first component explains as much variance as it could, then the second one will explain the variance *not explained by first component* as much as it could, then the third will explain those that is not explained by first two, etc.<br><br></p>", 
                "question": "How do I explain principal component analysis to layman."
            }, 
            "id": "ddir1rk"
        }, 
        {
            "body": {
                "answer": "<p>[deleted]</p>", 
                "question": "Looking for real-world examples of multiple t-test misuse"
            }, 
            "id": "cv9uwbf"
        }, 
        {
            "body": {
                "answer": "<p>http<colon>//io9.com/i-fooled-millions-into-thinking-chocolate-helps-weight-1707251800</p>", 
                "question": "Looking for real-world examples of multiple t-test misuse"
            }, 
            "id": "cva6pxq"
        }, 
        {
            "body": {
                "answer": "<p>Genome-wide association studies (GWAS) are full of problems with multiple comparisons.</p>", 
                "question": "Looking for real-world examples of multiple t-test misuse"
            }, 
            "id": "cv9zo2o"
        }, 
        {
            "body": {
                "answer": "<p>There<sq>s a nice example discussed in this report where the company essentially made up a new clinical category based on multiple subgroup analyses<colon> http<colon>//www.ncbi.nlm.nih.gov/books/NBK62326/<br><br>Not t-tests, but I think it<sq>s what you<sq>re looking for.</p>", 
                "question": "Looking for real-world examples of multiple t-test misuse"
            }, 
            "id": "cv9z1zq"
        }, 
        {
            "body": {
                "answer": "<p>All of Statistics - Larry Wasserman</p>", 
                "question": "What are some great statistics books to read?"
            }, 
            "id": "d625myn"
        }, 
        {
            "body": {
                "answer": "<p>A lady tasting tea--David Salsburg<br><br>The theory that would not die--Sharon McGrayne<br><br>The signal and the noise--Nate Silver<br><br>The Black swan--Nassim Nicholas Taleb<br><br>If you want one that<sq>s more a reference<colon> the reality enigma--Andy field</p>", 
                "question": "What are some great statistics books to read?"
            }, 
            "id": "d6221er"
        }, 
        {
            "body": {
                "answer": "<p>[deleted]</p>", 
                "question": "Very basic question about the Monty Hall problem"
            }, 
            "id": "d4i9btl"
        }, 
        {
            "body": {
                "answer": "<p>Everyone wants as many goats as possible, which is why everyone wants the car<colon> to sell it and buy multiple goats!</p>", 
                "question": "Very basic question about the Monty Hall problem"
            }, 
            "id": "d4ielkd"
        }, 
        {
            "body": {
                "answer": "<p>What<sq>s got your goat?</p>", 
                "question": "Very basic question about the Monty Hall problem"
            }, 
            "id": "d4iz2sg"
        }, 
        {
            "body": {
                "answer": "<p>I want to share a bit of a different perspective. My organization does policy research working with government agencies and private funders. At our company, you plateau without a PhD. You cannot lead any  projects. This is partially that government contracts require PhDs for project leads and also because it adds another layer of legitimacy to the work we do with private funders. I only know of one person that gets to work at that level without a PhD, but his role is always reported as subordinate to another PhD. Even if in house he<sq>s leading all the work. <br>  <br>Obviously my experience is not in the private sector. But for a more well-rounded perspective I thought it would be helpful. </p>", 
                "question": "Is it true you need a PhD for career growth in statistical leadership?"
            }, 
            "id": "cxsbzxc"
        }, 
        {
            "body": {
                "answer": "<p>>my colleagues are increasingly pressuring me, telling me that I <dq>won<sq>t have good career advancement without a PhD, even outside this company<dq>. Another frequent statement is<colon> <dq>You can<sq>t be the head of a data science / statistics / analytics group without one, it would look odd<dq>.<br><br>Why aren<sq>t you listening to them? Aim higher<colon> just get your doctorate and get it over with. You ought to know the anecdotal existence of a few outliers doesn<sq>t mean shit. lol</p>", 
                "question": "Is it true you need a PhD for career growth in statistical leadership?"
            }, 
            "id": "cxsat9a"
        }, 
        {
            "body": {
                "answer": "<p>I don<sq>t feel qualified to answer this but thought maybe I could get the comments section going. My two cents<colon> <br><br>If you want to work at a place that purely does statistical research, you<sq>ll likely need a PhD for the senior position. However, many companies have a statistical department or hire statistical consulting firms. Neither of these would likely require a PhD. It<sq>s a little tougher to say for the future since data analysis is still such a growth field. <br><br>Ultimately, I think a PhD may still be a good way to distinguish yourself in a crowded field. But that can also be done by just doing great work! If a PhD sounds unappealing, I wouldn<sq>t do it. They are a massive amount of work and if you don<sq>t enter it excited, I sure doubt you<sq>d be excited by the end. </p>", 
                "question": "Is it true you need a PhD for career growth in statistical leadership?"
            }, 
            "id": "cxs03l4"
        }, 
        {
            "body": {
                "answer": "<p>It<sq>s all depends what career you are aiming for.<br><br>Disclaimer <colon> I work in a stat department for a large international company. We<sq>re not <dq>Specialized<dq> in advance statistics, but we are more advanced than the average.<br><br>Whatever you are going to do, you<sq>re going to report to a VP with a MBA (or similar). From what I<sq>ve seen from my work experience, under the VP, Stats department in big companies are usually lead by 1 PhD senior analyst assisted by a bunch of Masters (from either Maths, Stats or Quality control Engineer), and some entry-level jobs for the day-to-day reporting.<br><br>So, in this set-up, as you can see, if you want <dq>job security<dq>, having a Master degree is a better option than having a PhD since there is usually only one PhD per stat analyst group, but multiple Masters statistician.<br><br>If you are aiming for only Lead position, then yes, you need the PhD. But most companies have programs so you can make your PhD while working. </p>", 
                "question": "Is it true you need a PhD for career growth in statistical leadership?"
            }, 
            "id": "cxs0k1c"
        }, 
        {
            "body": {
                "answer": "<p>First of all, props to you for paying attention to the statistics during peer review!<br><br>> Statistical analysis was performed using chi-square tests for categorical data<br><br>Good so far...<br><br>> Unpaired, homoscedastic Student\u2019s T-Tests and ANOVA were used for ordinal data<br><br>Eh, might be okay depending on what the ordinal variable is (number of levels, distribution, etc).<br><br>> average complication per patient <br><br>> 0.6 +/- 1.1<br><br>And optimism is gone. Let<sq>s look at your specific questions.<br><br>> Would looking at the percent of patients with complications rather than total complications / n be a more appropriate way of interpreting the results?<br><br>Both are probably worth looking at. It certainly would be worth looking at the proportion of patients with complications, but it<sq>s also of interest whether a group is having a higher number of complications (e.g. we<sq>d probably be very worried if in one group 10<percent> of patients had a complication with 1 complication per person vs. 10<percent> with complications and 6 complications per person). <br><br>> For the <dq>average complication per patient<dq>, is range a better measure of central tendency than SD?<br><br>SD is certainly inappropriate here, since as you pointed out it implies the non-sensical negative counts. Range could work, but deemphasizes most individuals having 0-1 complications. Histograms or (possibly binned) counts would be appropriate, but take more space. But either way, SD is clearly not meaningful here.<br><br>> Are further statistical tests such as Relative Risk or Relative Risk Reduction appropriate?<br><br>The t-tests/ANOVA should almost certainly be replaced with an appropriate analysis for count data, probably poisson regression or similar. I know relative risk is frequently the effect size measure of choice for medical fields, so it could indeed be worth including.<br><br><br><br><br></p>", 
                "question": "Peer-reviewing a medical paper statistical tests seem inappropriate / inadequate but I<sq>m not sure how/why..."
            }, 
            "id": "cp8mv0r"
        }, 
        {
            "body": {
                "answer": "<p>Also, I<sq>d like to add that I have asked my attendings/supervisors these questions before posting here. If anything, their knowledge of statistics is even worse than mine. Blind leading the blind. I am horrified at the thought that other peer-reviewers for medical journals are on my level and the greater implications of our ineptitude on medical literature and patient well-being... </p>", 
                "question": "Peer-reviewing a medical paper statistical tests seem inappropriate / inadequate but I<sq>m not sure how/why..."
            }, 
            "id": "cp8hjg2"
        }, 
        {
            "body": {
                "answer": "<p>Is the actual timing of the interventions available, in terms of how many hours before they were administered? Because that would be a hell of a lot more informative than two bins split at 24 hours, which discards a huge amount of the information you<sq>re interested in. If so, start with a simple regression of that independent against the dependent number of complications. Frankly, I would stop there if the p<0.05. Honestly everything else isn<sq>t getting people any closer to what they want to learn. I would require a scatter plot of those two variables. Anything fancier and you risk going over the heads of people such as yourself who can use the simple info but aren<sq>t specialist statisticians. </p>", 
                "question": "Peer-reviewing a medical paper statistical tests seem inappropriate / inadequate but I<sq>m not sure how/why..."
            }, 
            "id": "cp8tacj"
        }, 
        {
            "body": {
                "answer": "<p>[deleted]</p>", 
                "question": "Peer-reviewing a medical paper statistical tests seem inappropriate / inadequate but I<sq>m not sure how/why..."
            }, 
            "id": "cp8oxb0"
        }, 
        {
            "body": {
                "answer": "<p>I haven<sq>t seen it. <br><br>But his other book with a very similar name, *[The Signal and the Noise](http<colon>//www.amazon.com/dp/159420411X)*, was somewhat interesting (though I haven<sq>t quite finished - I<sq>ve had to put it down for some other things that I need to get through).<br><br>Not without its faults - in fact a number of things grated somewhat, but useful in the sense that it conveys some important ideas to a lay audience. I<sq>ve been reading his blog off and on since at least early 2008, and some of the previous background - not all of which I was aware of - was interesting as well.<br></p>", 
                "question": "r/AskStatistics what did you think of Nate Silver<sq>s book <dq>The Signal in the Noise<dq>?"
            }, 
            "id": "c8raz42"
        }, 
        {
            "body": {
                "answer": "<p>I think Silver is a better real-time commentator than author. I think there are some great chapters and some mediocre ones. A blog post isn<sq>t a book and I think this book shows it because as a blogger, there aren<sq>t many better than Silver.</p>", 
                "question": "r/AskStatistics what did you think of Nate Silver<sq>s book <dq>The Signal in the Noise<dq>?"
            }, 
            "id": "c8rcb1g"
        }, 
        {
            "body": {
                "answer": "<p>There have been [some critiques of his chapter on climate change](http<colon>//delong.typepad.com/sdj/2012/11/michael-mann-vs-nate-silver-on-climate.html) because he took too seriously some climate change deniers.  </p>", 
                "question": "r/AskStatistics what did you think of Nate Silver<sq>s book <dq>The Signal in the Noise<dq>?"
            }, 
            "id": "c8rdh3e"
        }, 
        {
            "body": {
                "answer": "<p>OP, reach out to Retraction Watch.<br><br>http<colon>//retractionwatch.com<br><br>They<sq>re a small but expert group on academic fraud. I bet they could put you in contact with someone who would know how to do this and probably also give you some advice. I<sq>d suggest reaching out anonymously. <br><br>Let us know what happens. </p>", 
                "question": "Need help showing evidence of fraudulent data"
            }, 
            "id": "dcx7dx7"
        }, 
        {
            "body": {
                "answer": "<p>Here are a couple of my thoughts off the top of my head. Please feel free to correct, comment and expand.<br><br>* Initially, I thought a simple one-sample t-test could be applied, using the mean of the new data as population mean. This would correspond to a one-sample likelihood-ratio test. The problem is that the normal model is not really applicable, as your data have natural boundaries of 0 and 900 seconds. Furthermore, the standard deviation of the new sample is quite high.<br>* Assuming the second, <dq>new<dq> sample is an accurate representation of the real data, the old data would not be too unlikely, just judged by the mean. This is because the variation in the <dq>new<dq> sample is very high.<br>* One very stricking fact is that the variation in the <dq>old<dq> sample is very small compared to the variation in the <dq>new<dq> sample. Maybe a simple boxplot of the two samples side-by-side is already strong evidence that the samples differ substantially.<br>* I think one way to model this would be as proportions<colon> divide all recorded <dq>licking times<dq> by 900 (i.e. 15 mintues). This gives a proportion between 0 and 1 for all animals. A simple logistic regression could then be applied to test whether the two samples differ with respect to the recorded <dq>licking proportions<dq>.<br><br></p>", 
                "question": "Need help showing evidence of fraudulent data"
            }, 
            "id": "dcwgmta"
        }, 
        {
            "body": {
                "answer": "<p>Why do you think the lab manager is faking the data? That might affect how you examine this data. What is the lab manager<sq>s motive? Are they just trying to do as little work as possible, with making up data being easier than actually doing the observations? Or, are they trying to push the data to show certain results?</p>", 
                "question": "Need help showing evidence of fraudulent data"
            }, 
            "id": "dcwmu1j"
        }, 
        {
            "body": {
                "answer": "<p>I would look into forensic accounting. There is an approach that forensic accountants use which relies on the notion that the last digit of a variable is not random. And that the distribution of the last digit of naturally occurring observations will not match the distribution of the last digit of fraudulent observations.<br><br>For example, height (of people, trees, or whatever). If you measured height 100 times and charted the distribution of the last digit in a histogram you<sq>d be tempted to expect a uniform distribution (equal numbers of zero, one, two, . . ., nine. Not true, it turns out.<br><br>My advice. Check the distribution of the last digits from <dq>New, Blinded, Video-taped data<dq> to compare the distribution of the last digits from <dq>Old, non-blinded. . .<dq><br><br>I hesitate to make predictions. But, at the vary least, if the distributions do not match (especially if those distributions differ at statistically significant levels) the issue would need further consideration.</p>", 
                "question": "Need help showing evidence of fraudulent data"
            }, 
            "id": "dcx3t35"
        }, 
        {
            "body": {
                "answer": "<p>Wow sounds like a researchers dream! Have you thought of teaming up with an education researcher at a university near you? I work in an education department at one of the University of California schools and your data set sounds like something a lot of researchers would love to help you look at!</p>", 
                "question": "I run a school. I have tons of data and no clue how to make it useful beyond very surface levels of interpretation."
            }, 
            "id": "d5rhehi"
        }, 
        {
            "body": {
                "answer": "<p>Awesome! We collaborate with UMICH a lot, but with their psych department. <br><br>If no one is interested (but who wouldn<sq>t be...free data!) then the first thing you want to do is think about questions you<sq>re interested in. Do you have the different categories for your data (aka the column heads)? <br><br>The first thing I would do is just visualize your data. There<sq>s lots of cool tools out there from JMP to Tableau, to new knowledge that can visualize your data with you. <br><br>Look at the distributions of scores or the change in scores over time.<br><br>It<sq>s expensive if you<sq>re not at an institution but my absolute favorite is JMP software from SAS. Tableau has a free version but is less intuitive to use. I like these because they<sq>re more interactive than pivot tables in excel, however you can definitely still use excel to visualize your data.<br><br>Then once you have some questions in mind, you can run some statistical tests. (For example and ANOVA to answer <dq>are there differences between groups A, B and C in score X?<dq>) <br><br>You can also run regressions and correlations to see how things relate (for example if socioeconomic status effects standardized test scores).<br><br>My one caution is that this data is observational and it<sq>s a HUGE sample but it<sq>s not random. Depending on what you want to apply this to, you may not need to do many statistical tests, you might just want to visualize your data!<br><br><br>Also look up different methods of analysis for longitudinal data. The library at UMICH would be a great place to start looking (kill two birds with one stone!).<br><br></p>", 
                "question": "I run a school. I have tons of data and no clue how to make it useful beyond very surface levels of interpretation."
            }, 
            "id": "d5rqgc2"
        }, 
        {
            "body": {
                "answer": "<p>I work for a company that specializes in exactly this type of thing. In fact, I<sq>m working with MAP data right now. I don<sq>t know if you have the budget to contract with someone like us, but I could certainly put you in touch. We<sq>re a non-profit, so we have many projects funded by philanthropies. My boss is referenced in the pdf linked by /u/madjoy, by the way.<br><br>I<sq>m curious<colon> how do you evaluate which teachers are producing gains in map testing? That would basically be a value added model, which are complex things that are not likely tractable in excel.<br><br>Feel free to pm me if you<sq>d like more details about the company, or to talk about analysis of education data in general.</p>", 
                "question": "I run a school. I have tons of data and no clue how to make it useful beyond very surface levels of interpretation."
            }, 
            "id": "d5rxuvs"
        }, 
        {
            "body": {
                "answer": "<p>I<sq>m in GR, this data sounds like a dream! Anyway, hierarchical modeling lends itself very nicely to education research. You have students, nested in districts, classrooms etc. and a plethora of predictors at each hierarchical level to better understand their variation. <br><br>Anyway, between MSU, UM, GVSU, Western, Eastern...there are plenty of researchers to hook up with.  </p>", 
                "question": "I run a school. I have tons of data and no clue how to make it useful beyond very surface levels of interpretation."
            }, 
            "id": "d5ry338"
        }, 
        {
            "body": {
                "answer": "<p>To explain this more in depth first we need to know what is the definition of R^(2)<colon> **It is the ratio between the variance explained by the model vs the variance of the observation in the dataset.** With this in mind we can see why the four points OP listed are valid criticism of R^(2)<colon><br><br>> (1.) It does not measure goodness of fit because it can be arbitrarily low even when the model is correct, and close to 1 even when the model is wrong.<br><br>The definition of R^(2) has nothing to do with whether the model is right or wrong. So obviously using it to infer anything about model being good or not is moot. To give you some example, consider that after fitting a linear model to a dataset you have a R^(2) as 0.1. That means your model is able to explain 10<percent> of the observed variance. Now ask yourself, does that mean the model is wrong?<br><br>The answer is, interestingly, **NO**. The model maybe the best thing we can come up with given the data. Perhaps the remaining 90<percent> of variance is from systematic error which cannot be accounted for by any quantitative model. Perhaps with the data in hand a 10<percent> is the best we can get.<br><br>On the other hand, does a R^(2)=0.9 means you have a good model? The answer is, once again, **NO**. There maybe other model that can do with R^(2)=0.95, or even more. Your model can be pretty bad when compared to others. Since we are not able to exhaust all models, a single value of R^(2) gives us no indication whether the model is good (the best we can get) or bad (we can get a lot better).<br><br>This is also why for different field of research, <dq>acceptable level<dq> of R^(2) may vary. For instance, a prediction accuracy of ~52-53<percent> for a binary trial (True or False) is obviously unsatisfactory for a drug research, but a golden opportunity for algorithm trading for price gauging.<br><br>> (2.) It is also a pretty useless measure of predictability because it says nothing about prediction error and nothing about interval forecasts.<br><br>So from definition we know that the R^(2) talks about the variability explained by the model. Does it say anything about prediction? No. You can make a model with R^(2)=1 and does nothing to help with prediction. R^(2) is backward looking. It offers no significance in making inference for future (thus interval prediction).<br><br>> (3.) It cannot be compared across datasets.<br><br>This is from the very definition. As the denominator for R^(2) is the variance of **this particular dataset**, it cannot be compared with other dataset.<br><br>> (4.) It cannot be compared across untransformed and transformed models.<br><br>Model of different nature cannot be compared using R^(2). For instance, a quadratic model will always perform better in terms of R^(2) than a linear model due to added complexity. You maybe able to do some adjustment on that. But how about a log-transform model vs a linear model? Both have only one variate. Is one necessary more complicated than the others? How about its predictability? R^(2) makes no comment about that.<br><br>> How to choose a model?<br><br>One problem we need to know is that to examine the usefulness of a model, there is several criteria to be considered<colon> <br><br>* Goodness of fit (How good does it look on past data?)<br><br>* Predictability (How reliable are the prediction it provides?)<br><br>* Robustness (How easily influenced the model is towards small change of parameter/presence of outlier/change of environment)<br><br>* Theoretical justification (Why use this model instead of another one?)<br><br>R^(2) offers none of those, which is why it is a bad summary measure. <br><br>> Alternatives?<br><br>There are many solid, sound method of choosing a suitable model. And of course no method is perfect and each has their own shortcoming. You can go by Information criterion (AIC, BIC), likelihood theory, Bayesian inference and many others. As long as you can justify your choice by revisiting the above criterion, you can be more confident at the choice.<br><br>p.s. For the absolute simplest case you know your data is roughly normal, adequately explained by linear model, R^(2) could be an acceptable choice. But that is mostly due to the fact that the nominator of 1-R^(2) is the sum of square error, which is justified by using Maximum Likelihood approach for normal data.</p>", 
                "question": "Post in /r/statistics about how R^2 value is essentially useless? (Above my head to understand it)"
            }, 
            "id": "cw4zsmf"
        }, 
        {
            "body": {
                "answer": "<p>Under some specific circumstances it can be useful. I think this setting is one of them since I presume what you <dq>expect<dq> or <dq>want<dq> is a perfect line (R^2 = 1) except with some small measurement error, and you have some idea of what that error ought to be in a well calibrated machine running this test. so percentage of variance explained will directly tell you what you want. You<sq>re not evaluating the model itself.</p>", 
                "question": "Post in /r/statistics about how R^2 value is essentially useless? (Above my head to understand it)"
            }, 
            "id": "cw4w6rt"
        }, 
        {
            "body": {
                "answer": "<p>I<sq>ll give it a try<colon><br><br>Estimating coefficients in a multivariate model is not so different from estimating e.g. the mean of some variable. You go out and collect some data, compute the mean and variance in your samples and know a bit more about what the true population mean might be.<br><br>Bootstrapping (or resampling with resubstitution) is an attempt to simulate the process of additional data collection. Each bootstrap is treated as an additional data collection on which you can compute a new sample mean and variance. Because samples can be drawn multiple times or not at all, the mean and variance estimate will change slightly from what you initially observed everytime. Over many such resamplings we can obtain better estimates for the true population mean.</p>", 
                "question": "ELI5<colon> Bootstrapping"
            }, 
            "id": "def9rgz"
        }, 
        {
            "body": {
                "answer": "<p>I made [a brief presentation](http<colon>//john.maloney.org/Papers/Bootstrapping_(Maloney_11-20-14\\).pdf) that shows some neat things you can do by bootstrapping research data. You may find it interesting.</p>", 
                "question": "ELI5<colon> Bootstrapping"
            }, 
            "id": "defethb"
        }, 
        {
            "body": {
                "answer": "<p>Below is a little snippet from a lecture I give to my students. It shows, step-by-step, how to conduct a simple bootstrapping procedure. After seeing this, they see why bootstrapping has been known for awhile, but only recently become widely used (it<sq>s terribly time consuming to do this without a relatively modern computer). A nice thing about bootstrapping effects is that you don<sq>t have to assume that the distribution of an effect is normal, as you would if using, for example, a z-score to test the significance of an effect. The properties of the effect distribution are based on your data rather than assumptions. Hope this helps!<br><br>original sample size (N) = 564; attained effect = -.30<br><br>step 1<colon> select random participant from original sample<br><br>step 2<colon> copy participant data into new sample<br><br>step 3<colon> put participant back into original sample<br><br>step 4<colon> repeat steps 1-3 564 times <br><br>step 5<colon> conduct analysis and get effect from newly created sample<br><br>step 6<colon> repeat steps 1-5 10,000 times <br><br>step 7<colon> sort 10,000 effects from smallest to largest<br><br>step 8<colon> identify 2.5th percentile effect and 97.5th percentile effect (these are the lower and upper bounds of the 95th percent confidence interval<br><br>step 9<colon> determine whether entire confidence interval falls on same side of zero as effect attained from original sample<br><br>from actual study data<colon> 95<percent> CI = -.36 to -.24 (i.e., 95<percent> of effects fell between -.24 and -.36)<br></p>", 
                "question": "ELI5<colon> Bootstrapping"
            }, 
            "id": "defrxdw"
        }, 
        {
            "body": {
                "answer": "<p>I will quote [myself](https<colon>//www.reddit.com/r/statistics/comments/4mhowr/looking_back_on_what_you_know_so_far_what/d3w9pn5/?context=3) (a [second time](https<colon>//www.reddit.com/r/statistics/comments/4qzcgx/layman_explanation_of_fixed_random_and_mixed/d4xbrc3/?utm_content=permalink&utm_medium=user&utm_source=reddit&utm_name=frontpage)) from an earlier thread (leaving of the last little bit, and fixing one typo), since it seems to have been a reasonably well received comment, it accurately describes how this topic became clear for me, and it seems silly to retype it.<br><br>First, though, I will say that you are right to be skeptical of how these models are used (my background is linguistics, too, so I mostly see them being used in language-y research). Some people use them appropriately and know what they<sq>re doing, but lots of people don<sq>t. Their use has increased in frequency quite a bit over the last few years, and they have become standard and expected in certain venues (e.g., the Journal of Memory and Language seems pretty fixated on mixed models). They are kind of the new ANOVA, which is to say that they are often used in large part because they are what people know how to use (i.e., they know how to tell R to fit the models, print the output tables, etc...).<br><br>Anyway, the auto-quote<colon><br><br>> I have found that the clearest way to think about these models is as follows. Imagine you have a data set with N observations for each of K variables for each of M subjects.<br><br>>Now, imagine fitting a linear model (e.g., logistic regression) to one subject<sq>s data. Then fitting another model to the next subject<sq>s data. And so on. You end up with beta coefficients for each predictor for each subject.<br><br>>Now imagine building a model of the beta coefficients. That is, the beta coefficients are now the <dq>data<dq>, and you have a set of parameters that govern how they behave. So, for example, it is typical to model the subject-level coefficients as multivariate normal random variables, in which case you would have a mean vector and a covariance matrix governing their location and variation.<br><br>>The subject-level beta coefficients are the <dq>random effects,<dq> and the <dq>fixed effects<dq> are the mean vector of the model of these coefficients.<br><br>>In an actual multilevel model (which is the terminology I prefer), you don<sq>t fit each subject-level model and then build the model of the coefficients. Rather, you estimate everything at the same time.<br><br>>Now, I used human subjects as the variable for grouping the <dq>random effects,<dq> but any grouping variable within which you have multiple observations can function in the same way (e.g., classrooms within a school, schools within a district, houses within counties, etc...).<br><br>>Also also, this is a very simplified, narrow version of a very broad class of models. You can have multiple levels (e.g., classrooms within schools within districts within states...). You can also have variables that are measured at any level, so my example of fitting a model for each unit at the lowest level isn<sq>t always an accurate thought-experiment-y kind of way to think about these models.<br><br>>Okay, so, that wasn<sq>t really age appropriate for an ELI5. But I hope it helps? [Gelman & Hill<sq>s Data Analysis Using Regression and Multilevel/Hierarchical Models](http<colon>//www.stat.columbia.edu/~gelman/arm/) explains these models very nicely, in my opinion.<br><br>One last note<colon> I think I remember reading on Gelman<sq>s blog that they are planning to publish a new edition of this book. It<sq>s a bit old, so a lot of the statistical software and programming material in the book is outdated, but the math and statistical theory aspects are still well worth reading.</p>", 
                "question": "Can someone explain mixed models to me?"
            }, 
            "id": "dcf75ij"
        }, 
        {
            "body": {
                "answer": "<p>The wikipedia page actually has a nice figure showing what the kernel is, and how <dq>copies<dq> of it are placed at the data points and added together to obtain the kernel density estimate<colon> https<colon>//en.wikipedia.org/wiki/Kernel_density_estimation#Definition</p>", 
                "question": "ELI5 <colon> Kernel Density Estimation"
            }, 
            "id": "d3o7kyy"
        }, 
        {
            "body": {
                "answer": "<p>I<sq>m not sure if I<sq>ll be able to explain it fully but here<sq>s the idea I have of KDE<br><br>First of all, it DOES depend on end points. In fact, that is one of the reasons it is superior to a histogram. A histogram plots its bars just based on a few local points, depending on how many bins you want. KDE on the other hand, uses ALL points of the data to determine the density at a particular data value. The data point farthest from your single data value will have very little <dq>weight<dq> to contribute to its density. That is why you may have read somewhere that <dq>it doesn<sq>t depend on end points<dq>. I<sq>ll try to post some of my old class notes which had a nice figure to go along with it. </p>", 
                "question": "ELI5 <colon> Kernel Density Estimation"
            }, 
            "id": "d3o3jk4"
        }, 
        {
            "body": {
                "answer": "<p>KDE is a <sq>Localised<sq> method, each point of the resulting density estimate is a weighted function of that point<sq>s neighbours. Roughly, you can think of <sq>h<sq>, or the bandwidth, as setting the size of the neighbourhood and the kernel as setting the weights for each observation in that neighbourhood. (Not totally accurate, but it will do for now).<br>With <sq>h<sq> and the kernel chosen, the KDE function goes to the first data point and centres a kernel here, call this point x. From here, the function then calculates how far every other data point, call these x_i, is from x by simple subtraction<colon><br>> x-x_i<br><br>It then scales this using <sq>h<sq><colon><br><br>> (x-x_i)/h<br><br>and then plugs this value into the Kernel function<colon><br><br>> K[(x-x_i)/h]<br><br>The value of K[(x-x_i)/h] is summed over each of the x_i and then averaged (see the wiki link posted by u/scriblable for the formula). This process happens for every datapoint, i.e. every observation has a turn at being the <sq>x<sq> in the above formulas. <br><br>The powerful part of the above formulae is the interplay between h and the kernel, K. When x and x_i are very close to each other you would expect the density to be higher than if they were far apart. This is what the kernel function does, for small values of (x-x_i)/h the value of K((x-x_i)/h) is relatively high, and small when (x-x_i)/h is large. [This link](https<colon>//en.wikipedia.org/wiki/Kernel_\\(statistics\\)#Kernel_functions_in_common_use) shows many different kernels, click any of the pictures and note that when the input into K() is small, the output is large. Also note for many of the kernels there is an indicator function {|u|<1} in the formula. This means that if (x-x_i)/h is either less than -1 or greater than 1 then the kernel will give a value 0 to x_i in the summation for x, i.e. x_i won<sq>t be considered as a neighbour of x. This is where <sq>h<sq> comes in. It scales (x-x_i) and so can dictate the maximum distance from x that x_i can be before |(x-x_i)/h| > 1. This is how you can control the size of the neighbourhood around x.<br><br>Previously I said that <sq>h<sq> sets the neighbourhood and the kernel sets the weights; this isn<sq>t totally correct. When you have a kernel with an indicator function, you can have weights = 0 for observations that are far away, i.e. they are truly out of the set of neighbours for x. But when you use a gaussian kernel, for example, it has no indicator function and hence ALL x_i will have a weight > 0 and so take part in the summation for x. I.e. The set of x_i which are neighbours for x will be all x_i. Note, you can still choose h so that the effective weight of the points far from x is very very very small. In effect this means that different kernels will result in different neighbourhoods and different choices for h will result in different weights for each observation. <br><br>Hopefully all that i<sq>ve written is clear, if not im happy to answer questions. You should now see that<colon> the kernel is a function which take as input the distance between two points (scaled by h) and outputs a weight that is then used in the summation; that <sq>h<sq> scales the differences between points, and; that both the kernel and h affect the size of the neighbourhood and the weight for each of those neighbours.</p>", 
                "question": "ELI5 <colon> Kernel Density Estimation"
            }, 
            "id": "d3oksub"
        }, 
        {
            "body": {
                "answer": "<p>> it smooths out a histogram<br><br>Well, no. It doesn<sq>t directly relate to any histogram. If you data were already binned then there<sq>s a connection but if you have continuous data to begin with, histograms won<sq>t come into it (except perhaps by analogy)<br><br>The wikipedia page https<colon>//en.wikipedia.org/wiki/Kernel_density_estimation has some explanation, or there<sq>s a somewhat similar <br>if brief explanation [here](https<colon>//stats.stackexchange.com/questions/68999/how-to-smear-a-histogram)<br><br></p>", 
                "question": "ELI5 <colon> Kernel Density Estimation"
            }, 
            "id": "d3om4lj"
        }, 
        {
            "body": {
                "answer": "<p>According to the best evidence, you gain approximately an additional ~1 <percent>-unit lifetime risk of cancer if you eat processed meat. [Cancer Research UK](http<colon>//scienceblog.cancerresearchuk.org/2015/10/26/processed-meat-and-cancer-what-you-need-to-know/) has written the best summary I<sq>ve found.  </p>", 
                "question": "Hey /r/statistics exactly how much does red/processed meat increase ones risk of colorectal cancer?"
            }, 
            "id": "cwedwc5"
        }, 
        {
            "body": {
                "answer": "<p>I may be wrong, but the lens I view this kind of announcement through is to think of the following<colon><br><br>1) How certain are we that there is an effect? (WHO put it in their <dq>definitely<dq> category)<br><br>2) How large is the effect? /u/GrynetMolvin addressed that... Seems to be more or less like the additional risk of [sitting still for 2 hours per day](http<colon>//jnci.oxfordjournals.org/content/106/7/dju206.full), though they give risks for specific cancers, and not <dq>cancer<dq>.<br><br>3) Cancer is tricky though-- cancer is something that kills you if nothing else does, and most likely at old age. So, when we see <dq>cancer rates increasing<dq>, it often means that we are living longer, and not dying from measles, typhus, cholera, war, influenza, heart attacks...  When some people see that the [leading cause of death for people aged 1-44](http<colon>//www.cdc.gov/injury/images/lc-charts/leading_causes_of_death_by_age_group_2013-a.gif) is <dq>unintentional injury<dq> they panic.  I rejoice- think of all of the things that aren<sq>t killing us now that used to be #1 on those lists! [In 1900, the top three were pneumonia or flu, tuberculosis, and <dq>gastrointestinal infections<dq>](http<colon>//demography.cpc.unc.edu/2014/06/16/mortality-and-cause-of-death-1900-v-2010/).  So, if we are getting to the point where I am supposed to worry about Bacon, I<sq>ll pretty much just stop worrying. \u263a</p>", 
                "question": "Hey /r/statistics exactly how much does red/processed meat increase ones risk of colorectal cancer?"
            }, 
            "id": "cwei0pp"
        }, 
        {
            "body": {
                "answer": "<p>They said something along the lines of <dq>the effect is as strong as for smoking<dq> but what I think they mean is that the evidence for the effect is just as strong, which means that the probably just had a very large sample size.<br></p>", 
                "question": "Hey /r/statistics exactly how much does red/processed meat increase ones risk of colorectal cancer?"
            }, 
            "id": "cweeki8"
        }, 
        {
            "body": {
                "answer": "<p>In addition to the link /u/GrynetMolvin provided, I<sq>ll throw out this comparison. The 50 g daily processed meat increases your risk by 18<percent>, a factor of 1.18.<br><br>Smoking, as the most famous electively consumed carcinogen, is a risk factor of roughly 14.</p>", 
                "question": "Hey /r/statistics exactly how much does red/processed meat increase ones risk of colorectal cancer?"
            }, 
            "id": "cwf4m9q"
        }, 
        {
            "body": {
                "answer": "<p>In my opinion, here are a few books that are almost surely must-haves for a well-rounded graduate statistics program<colon><br><br>Statistical Inference by Casella and Berger<br><br>Mathematical Statistics by Bickel and Doksum<br><br>Time Series Analysis and Its Applications<colon> With R Examples by Stoffer and Shumway<br><br>Statistical Models<colon> Theory and Practice by Freedman<br><br>Modern Applied Statistics with S by Venables and Ripley <br><br>Sampling Techniques by Cochran<br><br>Elements of Statistical Learning by Hastie, Tibshirani, and Friedman<br><br>I also found the following books helpful<colon><br><br>Probability and Measure by Billingsley<br><br>Theory of Point Estimation, Testing Statistical Hypotheses, Elements of Large-Sample Theory, and Nonparametrics by Lehmann (these books are better for reference than for straight reading)<br><br>And though I never used this book, I<sq>ve heard good things about All of Statistics by Wasserman</p>", 
                "question": "What books should every Grad Student have?"
            }, 
            "id": "catmqd4"
        }, 
        {
            "body": {
                "answer": "<p>One book for topic<colon><br><br>* Dobson<colon> An Introduction to Generalized Lineal Models( Have the best explanation of Likelihood-ratio test I found on a book )<br><br>* Bartoszynski and other<colon> Probability and statistical inference (same topics that Casella and Berger, but better explained)<br><br>* Hair<colon> Multivariate Data Analysis<br><br>* Montgomery<colon> Design and analysis of experiments<br><br>* Brockwell<colon> Introduction to time series and forecasting.<br><br>* Cohen et al<colon> Applied multiple regression</p>", 
                "question": "What books should every Grad Student have?"
            }, 
            "id": "catnui4"
        }, 
        {
            "body": {
                "answer": "<p>adding to other posts<colon><br><br>Casella and Robert<colon> Monte Carlo Statistical Methods<br><br>Everyone knows who Casella is, and Robert wrote a fantastic book over Baysian Statistics. This book tells you the basics of simulation (a subject every statistician should know) and Monte Carlo methods, including MCMC methods. A must have in my opinion.<br><br>adding to mathguy mike, another time series book is Time Series<colon> Theory and Methods by Brockwell and Davis. <br><br>It especially has a good chapter on linear algebra on hilbert spaces. One of the things I like most about this book is it lays out the theoretical parts of time series in a nice clean manner.<br><br>The best Regression book I have found is <br>Myers<colon> Classical and Modern Regression with Applications<br>I have found this book.<br><br>A. W. van der Vaart<colon> Asymptotic Statistics<br><br>One thing that we must almost always appeal to in most settings is asymptotic statistics. This book clearly laws out many of the asymptotic results in a fairly nice easy to read manner. The only problem I have with it is that it in later chapters the proofs turn to sketches.<br><br>Scheffe<colon> Analysis of Variance . <br><br>This is of course a great book whose proofs use orthogonal projections taught via a coordinate free approach. A must have, and it is an old book, which means it is cheap!<br><br></p>", 
                "question": "What books should every Grad Student have?"
            }, 
            "id": "catqnkw"
        }, 
        {
            "body": {
                "answer": "<p>Kutner et al<colon> Applied Linear STATISTICAL Models</p>", 
                "question": "What books should every Grad Student have?"
            }, 
            "id": "cb73jze"
        }, 
        {
            "body": {
                "answer": "<p>Why is being a stripper relevant to the question...?</p>", 
                "question": "If I<sq>m a stripper and I look at my Sunday to Saturday weekly income for a year that is about 52 samples. If I look at my average income for every 7-day period is that around 359 samples?"
            }, 
            "id": "dekzdn2"
        }, 
        {
            "body": {
                "answer": "<p>You can look at a rolling average for every 7 days, but keep in mind that the averages are not independent because you<sq>re using the same days in multiple samples.  For example, the average over days 11-16 has day 14 in it and the average over days 12-17 also has day 14 in it.<br><br>If you want to look for seasonal or annual trends, you can plot the rolling averages across the year and see if anything catches your attention.  I don<sq>t know much about the economics of stripping but I wouldn<sq>t be surprised if there<sq>s a distinct seasonal or holiday effect on income levels.<br></p>", 
                "question": "If I<sq>m a stripper and I look at my Sunday to Saturday weekly income for a year that is about 52 samples. If I look at my average income for every 7-day period is that around 359 samples?"
            }, 
            "id": "del1u7a"
        }, 
        {
            "body": {
                "answer": "<p>Not as an independent one; it<sq>s mostly the same information.</p>", 
                "question": "If I<sq>m a stripper and I look at my Sunday to Saturday weekly income for a year that is about 52 samples. If I look at my average income for every 7-day period is that around 359 samples?"
            }, 
            "id": "del26ld"
        }, 
        {
            "body": {
                "answer": "<p>Yes, in a technical sense -- but if you look at the same January 1-7 week 359 times, it<sq>s also 359 samples. I guess the question to ask is<colon> why would you want to have more samples? Your answer might give us some idea of whether it would be appropriate to look at the rolling averages.</p>", 
                "question": "If I<sq>m a stripper and I look at my Sunday to Saturday weekly income for a year that is about 52 samples. If I look at my average income for every 7-day period is that around 359 samples?"
            }, 
            "id": "del7af0"
        }, 
        {
            "body": {
                "answer": "<p>I see many problems here, and will try to briefly go over them all<colon><br><br>> 2 groups, N=3<br><br>A simple power test for this sample size means you can only reasonably expect to see an effect size (Cohen<sq>s d, with power = 0.8) of 2.5 - which is INSANE.  For starters you don<sq>t have that effect size.<br><br>> and the lowest p-value I can get with a 1-tailed, assumption of equal variance, is p=0.076<br><br>Considering the context you provide later this seems illustrative - however please don<sq>t do a one-tailed test unless theory dictates it.<br><br>> My boss said that he tested the data and got a P-value <<0.01<br><br>No he didn<sq>t.  That<sq>s a lie.  Your p-value is correct for what you did, but a Welch<sq>s t-test (one-tailed) gets you 0.09834, a paired t-test (as the measurements are listed) gets 0.05395, and in fact that is the lowest p-value you can get even if you do the completely fraudulent (yes I<sq>ll call it fraudulent) act of mixing up values to artificially lower the variance of the paired differences (you can<sq>t actually change the paired difference average by mixing).  Even a non-parametric Wilcoxon-Mann-Whitney test still gives a smallest p-value (assuming paired and one-sided) of 0.125.  Which leads me to<colon><br><br>> The only hint that he gave me is that I need to change my null hypothesis<br><br>Now this is garbage.  IF YOU CHANGE THE NULL HYPOTHESIS YOU CAN GET ANY P-VALUE YOU WANT.  But the whole point of doing statistical testing is that you actually care about which hypothesis you believe, NOT about having a low p-value.<br><br>The reason you aren<sq>t getting a low p-value is because you have a low sample size.  No other reason comes even close to that.  And your PI is lying and probably a horrible scientist, unless I am missing some vital context like you actually choosing the wrong null hypothesis for the theory behind your data.</p>", 
                "question": "Unsure how my PI and I are getting two very different P-values"
            }, 
            "id": "dc7lbxd"
        }, 
        {
            "body": {
                "answer": "<p>Everything /u/richard_sympson said. <br><br>Also, let this be a valuable life lesson<colon> your PI is *not* always correct, and is, quite often, *wrong*. Never assume he/she knows your work better than you do. </p>", 
                "question": "Unsure how my PI and I are getting two very different P-values"
            }, 
            "id": "dc7mhcf"
        }, 
        {
            "body": {
                "answer": "<p>This book is really solid and freely available<colon><br><br>https<colon>//www.otexts.org/fpp<br><br></p>", 
                "question": "I need to learn about ARIMA this weekend. Can you recommend books MOOCs online videos or articles that can explain it? I<sq>m hoping to be able to use it to create a prediction model on Monday"
            }, 
            "id": "d4nh7g9"
        }, 
        {
            "body": {
                "answer": "<p>IMO the basics for the statistician are math stats, linear models, longitudinal analysis, multivariate and GLM. I skipped survey sampling, which is for the super hardcore statistician, assuming you will not work in that field. Time series is probably more important for econometrists and statisticians that jumped into finance. Learning and heavy multivariate analysis are more for CS guys doing ML. I also assumed you want to jump into the applied side.<br><br>Far from being comprehensive<colon><br><br>Statistical Theory<br><br>* Casella & Berger for math statistics, this is the book everybody loves. #<br>* Gelman<sq>s Bayesian Data Analysis to switch from frequentist to bayesian.<br><br>Applied statistical models<br><br>* Kutner 5th edition for applied regression models. Having a good grasp is a must. #<br>* Singer & Willett or Fitzmaurice, Laird & Ware for longitudinal analysis.<br>* Tabachnick & Fidell for multivariate analysis. #<br>* Schumway for time series. #<br>* Something for GLM.<br><br>Learning<br><br>* Hastie et al, Murphy or Bishop.<br><br>Those marked with # are the books I can personally recommend. The rest are the recommendation I<sq>ve seen most frequently on the web.</p>", 
                "question": "What are the standard textbooks one would use for the core curriculum in a graduate statistics degree program?"
            }, 
            "id": "cu891rc"
        }, 
        {
            "body": {
                "answer": "<p>to add a couple of texts<colon> Wooldridge<sq>s <sq>econometric analysis of cross section and panel data<sq> and Hamilton<sq>s <sq>time series analysis<sq>.</p>", 
                "question": "What are the standard textbooks one would use for the core curriculum in a graduate statistics degree program?"
            }, 
            "id": "cu8ujh6"
        }, 
        {
            "body": {
                "answer": "<p>I would rather look at topics rather than choose specific books. This is what I looked at to supplement my statistics background but I do not have a statistics degree<colon><br><br>* Probability theory, Poisson process, Dirichlet process, Monte Carlo modeling, Gibbs sampling, importance sampling, Bayesian statistics<br>* Meta analysis, mixture models<br>* Regression analysis, GLM, LASSO & Ridge regression, Elastic nets<br>* Stochastic modeling<br>* Time series analysis, exponential smoothing, and survival analysis<br>* Probabilistic Graph Models, Markov models, Bayesian networks<br>* Machine learning<colon>  Dimentionality reduction(PCA, ICA), Clustering (K means, kNN), Support vector machines, Classification trees (random forest, C5.0), Collaborative filtering, Naive bayes<br>* Neural networks, Gradient Decent, Autoencoder, Restricted Boltzman machines, CNN, RNN<br><br><br>Also include some programming such as Python or Julia. Unless you are planning to go full stats or theoretical then you would want R and SPSS.<br></p>", 
                "question": "What are the standard textbooks one would use for the core curriculum in a graduate statistics degree program?"
            }, 
            "id": "cu7zax7"
        }, 
        {
            "body": {
                "answer": "<p>Maybe, but probably not. While traffic has been down over the past few days, the difference is not statistically significant by traditional standards. That said, we only have five full days of data since FPH, so if this trend continues much longer, it might be worth a second look. A two sample t-test of unique visitors results in<colon><br><br>    data<colon>  traffic.pre$uniques and traffic.post$uniques<br>    t = 1.7556, df = 6.141, p-value = 0.1285<br>    alternative hypothesis<colon> true difference in means is not equal to 0<br>    95 percent confidence interval<colon><br>    -32713.11 202180.55<br>    sample estimates<colon><br>    mean of x mean of y <br>      1380013   1295280 <br><br>In other words, the mean unique visitors post-FPH is 84,733 lower than pre-FPH. There is roughly a 13<percent> chance we<sq>d have seen a discrepancy this large at random. Of course, this assumes that *nothing* else contributes to a substantial pre-post difference (no confounding variables). This is unlikely since we don<sq>t even have a full week<sq>s worth of data since the 10th. A better analysis would control for weekly periodicity, etc... <br><br>For full disclosure, there is no difference observed in pre-post pageviews. More specifically, the mean difference in pre-post pageviews is 114,806 (fewer), but the t-score is a measly 0.66. <br><br>Here<sq>s the R code to reproduce the analysis from scratch (it<sq>ll download the data for you)<colon><br><br>    library(stringr)<br>    library(XML)<br>    library(zoo)<br><br>    url <- <dq>http<colon>//www.reddit.com/r/AskReddit/about/traffic<dq><br><br>    briefing.page <- htmlParse(url)<br>    ## Read the page<sq>s tables<br>    briefing.table <- readHTMLTable(briefing.page)<br><br>    traffic <- briefing.table[[4]]<br>    traffic$date <- as.Date(traffic$date,format=<dq><percent>m/<percent>d/<percent>y<dq>)<br>    traffic[,<dq>uniques<dq>] <- as.numeric(gsub(<dq>,<dq>,<dq><dq>,traffic$uniques))<br>    traffic[,<dq>pageviews<dq>] <- as.numeric(gsub(<dq>,<dq>,<dq><dq>,traffic$pageviews))<br>    traffic <- traffic[traffic$uniques>0,]<br>    traffic <- traffic[-1,]<br><br>    plot(traffic$date,<br>         traffic$uniques,<br>         frame=F,las=1,<br>         type=<dq>l<dq>,main=<dq>Reddit Traffic<dq>,<br>         col=<dq>#ff7f00<dq>,<br>         lwd=2,<br>         ylim=c(0,max(unlist(traffic[,c(<dq>uniques<dq>,<dq>pageviews<dq>)]))),<br>         xlab=<dq>Date<dq>,<br>         ylab=<dq><dq>)<br>    lines(traffic$date,traffic$pageviews,col=<dq>#377eb8<dq>,lwd=2)<br>    abline(v=as.Date(<dq>2015-06-10<dq>), col=<dq>#e41a1c<dq>, lwd=2)<br>    points(traffic$date,traffic$uniques,col=<dq>#ff7f00<dq>,cex=1,pch=16)<br>    points(traffic$date,traffic$pageviews,col=<dq>#377eb8<dq>,cex=1,pch=16)<br><br>    traffic.pre <- traffic[traffic$date<=as.Date(<dq>2015-06-10<dq>),]<br>    traffic.post <- traffic[traffic$date>as.Date(<dq>2015-06-10<dq>) & traffic$date<as.Date(<dq>2015-06-16<dq>),]<br><br>    t.test(traffic.pre$uniques, traffic.post$uniques)<br>    t.test(traffic.pre$pageviews, traffic.post$pageviews)<br></p>", 
                "question": "Has the blowup from the banning of FPH caused any meaningful difference in reddit traffic?"
            }, 
            "id": "cs96cw6"
        }, 
        {
            "body": {
                "answer": "<p>Since I<sq>m a noob armchair expert, I<sq>ll step through how I<sq>d approach this. The corrections to my post below will be enlightening, so I won<sq>t edit out the mistakes I make ;) I note that I already got a very different answer than /u/mil24havoc so will be interesting find why.<br><br>Ideally you<sq>d want more data but as of now we have 6 days of unique traffic data available. I looked at the 6 days pre and 6 days post FPH ban, eg 4-9 June inclusive and 11-16 June inclusive.<br><br>The average number of daily unique visitors pre ban was 1,300,653 and post ban it was 1,268,796. So a reduction of 31,857 - about 2.4<percent>. The problem is we have a very small sample size of just 6 days pre and post ban - but lets do a statistical test to see if these 2 visiting patterns reflect the same population (eg is the reddit visiting population different pre and post ban).<br><br>I think a 2 sample t test seems appropriate here (one of the assumptions is data is normally distributed. Maybe others can step in with an opinion on this. We only have <sq>groups<sq> of 6 (days of observations) but a very large number of respondents each day. I assume the chance of every possible reddit visitor visiting might be somewhat normally distributed?)<br><br>Here<sq>s the data<colon><br><br>Day | Pre FPH Ban<br>--|--<br>1 | 1,263,483<br>2 | 1,483,559<br>3 | 1,075,241<br>4 | 1,212,691<br>5 | 1,354,197<br>6 | 1,414,744<br><br>Day | Post FPH Ban<br>--|--<br>1 | 1,433,260<br>2 | 1,280,479<br>3 | 1,230,760<br>4 | 1,187,300<br>5 | 1,344,599<br>6 | 1,136,376<br><br>I used the calculator here for convenience http<colon>//www.socscistatistics.com/tests/ttestdependent/Default.aspx<br><br>The result is a p value of 0.74. This means theres a 74<percent> chance of finding a difference of 31,857 visitors or greater between 2 6 day periods, if there is no difference in the makeup of visitors between the two periods. This would not be considered sufficient evidence to say there is any difference between groups.<br><br>One more way of looking at it is to draw the confidence interval for the difference between groups. This shows us how much the value found in our sample could be expected to vary in the population.<br><br>I randomly found this calculator, results URL [here](http<colon>//www.mathcelebrity.com/meandiffconf.php?n1=+6&xbar1=+1300653&stdev1=+147829&n2=+6&xbar2=+1268796&stdev2=+108216&conf=+94&pl=Mean+Diff+Conf.+Interval+<percent>28Small+Sample<percent>29) showing the numbers I used.<br><br>This graph shows the observed difference of 31,857 and the 95<percent> confidence interval for the difference.<br>http<colon>//i.imgur.com/xISWLKX.png<br><br>(sorry for the crappy quality)<br><br>What it tells us is we expect the difference in visitors could vary wildly over the period we looked at. All values within the error bars are equally likely - there could be no real difference between groups, there could be a large difference. <br><br>Conclusion<colon> the available data does not show there<sq>s any difference in traffic to reddit.</p>", 
                "question": "Has the blowup from the banning of FPH caused any meaningful difference in reddit traffic?"
            }, 
            "id": "cs97u8n"
        }, 
        {
            "body": {
                "answer": "<p>What date was FPH banned on?</p>", 
                "question": "Has the blowup from the banning of FPH caused any meaningful difference in reddit traffic?"
            }, 
            "id": "cs92kq3"
        }, 
        {
            "body": {
                "answer": "<p>The flavor of Coke and Pepsi is indescernable. </p>", 
                "question": "What is the most interesting/fun result you have ever found?"
            }, 
            "id": "cpr0qrl"
        }, 
        {
            "body": {
                "answer": "<p>From analyzing some cinema chain<sq>s loyalty programme data...<dq>Action movie lovers generate $1.53 each time when they<sq>re targeted by campaigns.<dq>; <dq>Genre movie lovers lose money for the cinema chains.<dq><br><br>I really want to prove Peter Gregory<sq>s thoughts on the value of universities though! </p>", 
                "question": "What is the most interesting/fun result you have ever found?"
            }, 
            "id": "cpv8xmk"
        }, 
        {
            "body": {
                "answer": "<p>Interesting question. It has been quite some time since I last worked on the analysis of surveys, and cannot provide a definitive answer, but I will wager a guess. <br><br>To a certain extent, I would argue that <dq>more is better<dq> when it comes to data acquisition. If there is enough data, one might start to see interesting links between questions on which strongly polarised answers were given. I think in many cases it should be a good idea for the analysis to also be done with <dq>agree<dq> and <dq>disagree<dq> categories grouped. With more categories, you have the option of grouping -- you can<sq>t <dq>ungroup<dq> when you only have three categories.<br><br>This is definitely not my field of expertise, I hope you get a more satisfying answer. For further reading, you might want to look at [the Likert scale](http<colon>//en.wikipedia.org/wiki/Likert_scale). A quick google also lead me to [this interesting article](https<colon>//pprg.stanford.edu/wp-content/uploads/Choosing-the-Number-of-Categories-in-Agree-Disagree-Scales-Revilla-M.-Saris-W.-Krosnick-J..pdf) which seems very relevant to your question.</p>", 
                "question": "In surveys what is the reason for having <dq>agree<dq> and <dq>strongly agree<dq> type options rather than just a positive negative and neutral response? How significantly can this affect the results of such a poll?"
            }, 
            "id": "cngcgif"
        }, 
        {
            "body": {
                "answer": "<p>I suppose if a focus group gave one factor <dq>strongly agree<dq> and the other <dq>agree<dq>, management can focus on improving the second factor while noting that they<sq>re both still viewed positively.</p>", 
                "question": "In surveys what is the reason for having <dq>agree<dq> and <dq>strongly agree<dq> type options rather than just a positive negative and neutral response? How significantly can this affect the results of such a poll?"
            }, 
            "id": "cngcvtf"
        }, 
        {
            "body": {
                "answer": "<p>In practice, the decision is qualitatively based on the level of involvement that the respondents have with the subject of the survey. Low-involvement questionnaires will generally have a lower number of points on a rating scale and high-involvement questionnaires will generally have a higher number of points.<br><br>For example, if I were doing a study on nail polish brands I would have different scales depending on the respondent. If I were to ask doctors, for example, a simple 3-point question would probably suffice since they presumably, on average, have a fairly low-involvement with the product. But, the same question asked of nail technicians could have a 9- or more point scale, since they likely have a high-involvement with the product on average.<br><br>A 5-point scale has become the general standard since it offers a fairly simple range, that offers some positive and negative variation.</p>", 
                "question": "In surveys what is the reason for having <dq>agree<dq> and <dq>strongly agree<dq> type options rather than just a positive negative and neutral response? How significantly can this affect the results of such a poll?"
            }, 
            "id": "cngg3u0"
        }, 
        {
            "body": {
                "answer": "<p>I imagine they score them like a test.  The sum of all ratings is your professor<sq>s grade.  Institutions and administrators love these sorts of aggregate scores. It<sq>s a questionable use though.</p>", 
                "question": "In surveys what is the reason for having <dq>agree<dq> and <dq>strongly agree<dq> type options rather than just a positive negative and neutral response? How significantly can this affect the results of such a poll?"
            }, 
            "id": "cngv8gw"
        }, 
        {
            "body": {
                "answer": "<p>I originally had a dim view of statistics, considering it a field that fooled itself into thinking that its assumptions held more often than they did and that its approximations were tighter than they were. Most standard hypothesis testing seemed pretty naive to me, too. This wasn<sq>t <dq>real math.<dq><br><br>But when I was in grad school, I was working on a project that used reinforcement learning to model human performance of a task, and while doing background research on RL, I got hooked. That quickly turned into an obsession with machine learning, data mining, and statistical theory, which I began to view as tools in a treasure hunt, of sorts, looking for signal buried in a bed of noise. These days, I work primarily in machine learning, and the obsession is every bit as strong.</p>", 
                "question": "People who discovered their love of stats while in other fields what<sq>s your story?"
            }, 
            "id": "d7l4qoq"
        }, 
        {
            "body": {
                "answer": "<p>I started a PhD in psychology, and realized that all of the interesting questions were about how to test hypotheses that matched the theory. <dq>Here<sq>s a beautiful, elaborate theory of human behavior<dq>, say the psychologists. <dq>And I will test it with a 2x2 ANOVA, because I always do 2x2 ANOVA.<dq><br><br>Also, psychology is hard. How the hell do you think up those theories?</p>", 
                "question": "People who discovered their love of stats while in other fields what<sq>s your story?"
            }, 
            "id": "d7lguji"
        }, 
        {
            "body": {
                "answer": "<p>Same here. M.sc in Social and organizational psychology. When starting psychology I wanted to be a clinical psychologist or Human Resources manager. After graduation from m.sc. I was hoping for a job in HR still, because it is/was a hot topic and paid well.<br><br>I realized early I didn<sq>t want to be a clinical psychologist. Later I realized HR wasnt as cool as I thought, it<sq>s a bubble. I was never good at math, but somehow stats amazed me. My brain just got <sq>it<sq>. I love working in statistics, spend most of my freetime even fiddling with R.</p>", 
                "question": "People who discovered their love of stats while in other fields what<sq>s your story?"
            }, 
            "id": "d7l651h"
        }, 
        {
            "body": {
                "answer": "<p>The introduction of calculus by a teacher that catered heavily to people who simply <dq>got it<dq> turned math into a phobia. From grade 11 onwards, I started thinking about math as inherently difficult and somewhat obtuse. I still managed well enough, getting myself through first and second year university calculus requirements for engineering, but I struggled every step of the way.  <br>  <br>Switched majors a few times. A third year molecular genetics course got me hooked and I transferred, ultimately graduating with a degree in molecular biology. My first lab technician position was an eye opener. I didn<sq>t feel particularly valued.  <br>  <br>Concurrently, a love of all things to do with software/computers continued to provide small opportunities for little consulting gigs on the side. I designed simple applications for various lab groups at my university (mostly wrappers around DBs). This led me to head back to school in computer science this time.  <br>  <br>And that<sq>s when math clicked for me. Once I started seeing math as code, rather than formula. For whatever reason that summation symbol made a lot more sense when I saw it as a for loop or a reduce type of construct.  <br>  <br>Fast-forward to a job doing computational biology (mostly applications of machine learning techniques) working in R, alongside a deeply statistically-minded group of people, and my love of statistics was born. Can<sq>t get enough of this stuff now and incredibly happy my convoluted path led me to finally conquer something I<sq>d found intellectually terrifying for so many years.</p>", 
                "question": "People who discovered their love of stats while in other fields what<sq>s your story?"
            }, 
            "id": "d7lhvkv"
        }, 
        {
            "body": {
                "answer": "<p>Do you have a job in mind that you know requires a PhD?  Do you want to be a professor?<br><br>My brother and I had this discussion.  I am a PhD candidate - I want to be a professor - he wanted to work at a company.  He got a master<sq>s and found every high level job he was interested in asked for <dq>Master<sq>s + experience<dq> or <dq>PhD.<dq>  So he worked <dq>entry level<dq> for a few years and then moved on to one of those positions.  Master was 2 years + 3 years work, PhD is 5 years.  Only difference is those 3 years of work he made enough to pay off his debt and some so financially it worked out for him better.<br></p>", 
                "question": "Masters vs. PhD in Statistics"
            }, 
            "id": "d3byd2r"
        }, 
        {
            "body": {
                "answer": "<p>If it<sq>s applied stats, it<sq>s pretty rare for people to do a PhD until after they<sq>re professionally qualified, and then they<sq>re too useful to need one. It<sq>s more common (IME) to see people doing PhDs in their 40s/50s when they<sq>ve got enough experience in whatever field of application they<sq>re working in and know what gaps need filling.<br><br>You don<sq>t need one for academic jobs, although if you<sq>re not working in a department which employs a lot of statisticians they can get a bit sniffy. I<sq>ve never seen a senior job advertised in or out of academia which doesn<sq>t say <dq>PhD or equivalent experience<dq>. (Based on being in the UK, in clinical research.)<br><br>You can do specialised MSc courses - eg medical statistics - or more general applied statistics. But it<sq>s not a very useful subject in a vacuum. Unless you<sq>re able to push the theoretical frontier forward you would usually need to work in an applied field in order to develop the field-specific knowledge you<sq>d need to do a useful doctorate.<br></p>", 
                "question": "Masters vs. PhD in Statistics"
            }, 
            "id": "d3bp20x"
        }, 
        {
            "body": {
                "answer": "<p>I<sq>m really curious about any answers to this as I am finishing my undergrad this year and heading in to an Honours year. My main goal is research and I feel like if I enter the workforce after Honours/Masters, I won<sq>t get the opportunities I want to be able to go in to a research position.</p>", 
                "question": "Masters vs. PhD in Statistics"
            }, 
            "id": "d3cdrbi"
        }, 
        {
            "body": {
                "answer": "<p>No it<sq>s not wrong to expect authorship somewhere in the middle positions. You put in your ork and contributed meaningful to the message of the paper. <br><br> I know my physics prof started his high h-index by not only beeing good in what he does, but also collaborating on a lot of epidemology papers, where he did the statistical modelling.</p>", 
                "question": "Do you expect authorship for statistical work?"
            }, 
            "id": "d27y1lo"
        }, 
        {
            "body": {
                "answer": "<p>Hmm.. That<sq>s messed up to me. In my opinion, the analysis and model are the most important part of the paper (at least in my field, cognitive/quantitative psychology). What does the author plan to do if and when a reviewer asks for more details on the analyses? Oh whoops, just reread your post and see this did in fact happen. Well of course it did. <br><br>Anyway, if it hasn<sq>t been re-submitted yet, I think you should politely ask to be listed as a co-author. If nothing else, you should at least be mentioned in an acknowledgements footnote. </p>", 
                "question": "Do you expect authorship for statistical work?"
            }, 
            "id": "d287ihi"
        }, 
        {
            "body": {
                "answer": "<p>Yes, of course. <br><br>I more often have the opposite problem, of people perceiving a need to have a statistician on the paper so just wanting your signature. Be wary of the quality of work you allow your name to be put to and if your role was limited that should be made explicit.<br><br>But yes, if you<sq>ve made a significant contribution you should be offered authorship.</p>", 
                "question": "Do you expect authorship for statistical work?"
            }, 
            "id": "d28et5p"
        }, 
        {
            "body": {
                "answer": "<p>when the work is that involved, certainly. I learned through bitter experience that non-statisticians will rarely see substantive statistical contributions as contributions at all. So up front you have to be completely explicit about your expectations. Otherwise you<sq>ll be disappointed almost every time.<br><br>I don<sq>t know where this terrible attitude arises from, but it<sq>s near universal in my experience.</p>", 
                "question": "Do you expect authorship for statistical work?"
            }, 
            "id": "d28njz9"
        }, 
        {
            "body": {
                "answer": "<p>2-3? Learn R, regression, data visualization.</p>", 
                "question": "2-3 Most Important Statistics Skills that can be Self-taught?"
            }, 
            "id": "cuybtfd"
        }, 
        {
            "body": {
                "answer": "<p>If you want to go into environmental modeling you may want to consider learning some basic GIS skills (QGIS is free). R is good but QGIS has built in Python functionality (I do prefer R though).<br><br>So my top three things for you to learn would be 1) GIS/spatial analysis, 2) R or Python, and 3) your choice of another programming language or more advanced statistics (Bayesian modeling for example).</p>", 
                "question": "2-3 Most Important Statistics Skills that can be Self-taught?"
            }, 
            "id": "cuyhpaj"
        }, 
        {
            "body": {
                "answer": "<p>> I know I should look into R since it<sq>s free.<br><br>Really, out of all the choices for stats software you could use, cost is the first characteristic that<sq>s going to sway your opinion?</p>", 
                "question": "2-3 Most Important Statistics Skills that can be Self-taught?"
            }, 
            "id": "cuyhcqu"
        }, 
        {
            "body": {
                "answer": "<p>Also, there<sq>s a neat little function in R called <dq>bayesglm<dq> which appears to be exactly what I need (a Bayesian Generalized Linear Model), and here<sq>s some example code from the documentation<colon><br><br>Examples<br><br>n <- 100<br><br>x1 <- rnorm (n)<br><br>x2 <- rbinom (n, 1, .5)<br><br>b0 <- 1<br><br>b1 <- 1.5<br><br>b2 <- 2<br><br>y <- rbinom (n, 1, invlogit(b0+b1*x1+b2*x2))<br><br>M1 <- glm (y ~ x1 + x2, family=binomial(link=<dq>logit<dq>))<br><br>WHAT THE HELL IS b0, b1, and b2!! Where are they coming from!? Aren<sq>t those coefficients my ultimate goal? How the hell are they part of the input?</p>", 
                "question": "Basic concepts of a Bayesian regression analysis"
            }, 
            "id": "cgwhvsp"
        }, 
        {
            "body": {
                "answer": "<p>Since it<sq>s been 16 hours with no response you might consider getting some online quotes for a few cars that had stick and auto options with an otherwise identical drive train configuration from various insurance companies.  If there is a statistical difference, you can bet that their actuaries will have it incorporated into their insurance premiums.</p>", 
                "question": "Do manual cars have more or fewer accidents than automatics?"
            }, 
            "id": "c7qlyaz"
        }, 
        {
            "body": {
                "answer": "<p>Consider also that the cost of repairing an automatic and a manual transmission may be different. Therefore the same level of damage might have a different reimbursement.  Not all of the difference in premium may be due to differences in accident rate.<br><br>Also, there may be a different population of drivers that choose automatic vs. stick (old vs young, single vs married, male vs female) that may get put into the actuarial meat grinder to come up with the premium.</p>", 
                "question": "Do manual cars have more or fewer accidents than automatics?"
            }, 
            "id": "c8r8quw"
        }, 
        {
            "body": {
                "answer": "<p>I know this is a boring suggestion, but nothing beats the old, venerable Schaum<sq>s Outlines for their combination of problems, solutions, and inexpensiveness. If you are just starting, perhaps [start with this one](https<colon>//www.amazon.com/Schaums-Outline-Probability-Statistics-4th/dp/007179557X/ref=sr_1_2?ie=UTF8&qid=1491572334&sr=8-2&keywords=schaum+statistics), and once you get some exposure to the basics you<sq>ll have a better idea of what you might want to pursue next- perhaps the next step would be analysis using a computer instead of by hand.<br><br>Lots of us have free YouTube videos on the basics that you can reference if/when you need them as you go. [Try me](www.burkeyacademy.com) or Kahn Academy, there are many others. Let me know if this idea doesn<sq>t fit with what you had in mind, and I can try to point you in a different direction.</p>", 
                "question": "Recommend a statistics workbook?"
            }, 
            "id": "dfydb38"
        }, 
        {
            "body": {
                "answer": "<p>They<sq>re different frameworks!<br><br>Frequentist statistics uses a weak version of reductio ad absurdum (aka look how dumb you sound when you assume X!) to show that when you assume a null hypothesis either you do or do not come to ridiculous conclusions. In the case of the normal hypothesis tests, you ask whether assuming you have a null distribution and given that you observed your sample data, are you forced to conclude that something realllllllly rare (your sample if it was drawn from the null distribution ) happened. <br><br>Bayesian stats on the other hand directly compares two hypotheses instead of just knocking down the null. <br><br>When faced with the choice between chocolate and vanilla a frequentist tastes the vanilla and goes <dq>blechhh I probably like chocolate better<dq>, whereas the Bayesian tries both and decides. <br><br>Frequentist also <dq>start over<dq> with every hypothesis test, when doing a frequentist hypothesis test, your conclusions based on analysis of  experiment #2 don<sq>t rely mathematically on experiment #1, in Bayesian stats it can (through choice of prior distribution)!<br><br>The benefit of frequentist stats is that they are parsimonious, common, and not too hard to calculate (with a computer or by hand). Bayesian stats are more intuitive, but can be incredibly computationally difficult. <br><br>Take a class on bayes or at least read a book on it! It<sq>s a beautiful and well used theorem, and though your mind may swim when you first learn about Marko Chain Monte Carlo, you<sq>ll find the output to be more intuitive than p-values and confidence intervals. <br><br>That being said I love frequentist stats and I find beauty in the organized chaos of randomness. <br><br><br>(Sorry for all the exclamation points, I really love stats)<br><br>Edit<colon> my fave and relevant [xkcd](https<colon>//xkcd.com/1132/) <colon> </p>", 
                "question": "Bayesian Vs. Frequentist"
            }, 
            "id": "ddfgn1p"
        }, 
        {
            "body": {
                "answer": "<p>The frequentist mentality is that we have this hypothesis, statistical value or distribution. Then we want to know is what is the likelihood that we got the data we did given that we have X hypothesis statistical value or distribution.The Bayesian approach is<colon> we have all this data. What is the likelihood that any one of these values or hypotheses or distributions is the right one given the data that we have. Now of course, this is a simplification but I think it is good enough for your question (others will likely disagree with me).<br><br>As for whether I would recommend taking Bayesian statistics, I would definitely recommend taking Bayesian statistics for two reasons. First it just gives you a whole bunch of extra tools in your tool set that are now way more useful because of the explosion of computing power. Second it will actually increase your understanding of frequentist statistics. Most of the time, at least part of a Bayesian class will discuss the differences between Bayesian and frequentist statistics. Even if the class does not do this, you can still do this on your own by comparing the approaches. I personally feel like I didn<sq>t really understand frequentist statistics until I took a Bayesian class, much in the same way that I didn<sq>t have a complete understanding of English grammar (my native language) until I took Spanish.</p>", 
                "question": "Bayesian Vs. Frequentist"
            }, 
            "id": "ddfecph"
        }, 
        {
            "body": {
                "answer": "<p>Frequentist<colon> Probability measures the sampling distribution of your variable only. Thus we need to base inference on the unknown quantity based on the sampling distribution of the inherent variable.<br><br>Bayesian<colon> Probability measures uncertainty. Thus, inference is based on a probability distribution of the unknown quantity conditioned on the obtained sample. <br><br>One conducts inference of the unknown quantity based on probability, the other conducts inference of the unkown quantity based on the sampling distribution of the variable of the quantity.  </p>", 
                "question": "Bayesian Vs. Frequentist"
            }, 
            "id": "ddfq825"
        }, 
        {
            "body": {
                "answer": "<p>Not mentioned so far is that the two approaches define <dq>probability<dq> in fundamentally different ways.  A frequentist will say that the probability of an event happening is the proportion of times that it will occur in an arbitrarily large number of instantiations of the underlying process.  <dq>A coin has a 1/2 probability of landing on heads<dq> means that half of all such identically (and independently) generated flips will result in a head.<br><br>A Bayesian analyst on the other hand doesn<sq>t define probability as a knowable (objective) quantity, but rather as a measure of certainty that an unknown variable takes a value in a given range.  This certainty can be informed by other information outside of merely the current observed data; Bayesian inference has a natural way of including prior information.</p>", 
                "question": "Bayesian Vs. Frequentist"
            }, 
            "id": "ddflxyp"
        }, 
        {
            "body": {
                "answer": "<p>In a meta-analysis, you combine the results of two studies. You usually take an estimate of an effect, and its standard error from each study and pool them.<br><br>It<sq>s usually not recommended to try to meta-analyze when you only have two studies.<br><br>If you combine the datasets, you have a situation where you have lots of missing data. You don<sq>t have a control group for study one, so you have to assume that the people in study 1 are equivalent to those in study 2 - or as equivalent as they would be if they were randomized. That seems like a big leap.<br><br>That<sq>s two reasons I wouldn<sq>t do a meta-analysis on this data.</p>", 
                "question": "Meta-analysis"
            }, 
            "id": "db10tsf"
        }, 
        {
            "body": {
                "answer": "<p>You *can* do two, but there<sq>s not much point. I like to have three. Some people say 4 or 5. </p>", 
                "question": "Meta-analysis"
            }, 
            "id": "db1bdy3"
        }, 
        {
            "body": {
                "answer": "<p>It means that SES has some relationship with your outcome variable, and that the different representation of SES groups in your samples was actually responsible for some of the result you initially saw.<br><br>As an analogy, suppose you wanted to test the efficacy of a preventative treatment for breast cancer. You have 30 people taking the new treatment, and 30 people doing nothing, and you see how many in each group develop cancer. At the end of your experiment, you find that 13 people in the control group developed cancer, but only 3 in the treatment group. Nice!<br><br>But then, you realise that your sample selection mechanism somehow picked 20 women and 10 men for the control group, and the opposite for the treatment group. And there<sq>s a definite connection between breast cancer and sex. So you run the analysis again, but you control for sex in the model (i.e. you include a term to account for the effect of sex). And now you see that it was pretty much just the sex imbalance in the two samples that<sq>s the main explanation for the results, and your treatment has no detectable effect.<br><br>So of course you discard those results and publish the ones that will guarantee your funding for the foreseeable future.</p>", 
                "question": "ELI5<colon> what does it mean to control for a variable in a regression model?"
            }, 
            "id": "d73dbni"
        }, 
        {
            "body": {
                "answer": "<p>Suppose you put SES into the model first, and the p-value is low/effect size is high (no specifics here - just an example). Then let<sq>s say you put in any drug combination into the model after SES, and you find that the p-value is high for that new entry (or the incremental effect size is small, etc.). Essentially this means that there is little (or no) evidence that drugs explain anything about your outcome that SES doesn<sq>t already explain. Put another way, SES is (1) accounting for so much unique variability of the outcome that drugs simply don<sq>t bring anything new to the table, or (2) SES accounts for the similar type of variability that drugs account for, meaning that either variable is redundant with the other. This latter example is a case of multicollinearity, which is also important to consider. For example, do certain SES brackets have a higher probability of taking specific drugs, like lower SES with meth or higher SES with cocaine (just spitballing).</p>", 
                "question": "ELI5<colon> what does it mean to control for a variable in a regression model?"
            }, 
            "id": "d73d0av"
        }, 
        {
            "body": {
                "answer": "<p>There<sq>s a total amount of variation in your model, most tests will compare the amount of variance your model accounts for vs the remaining variance  (or error). <br><br>When you control for another variable like SES you take the variance and partition it instead into the parts, your drug, SES and error. <br><br>Since error is going to reduce the amount of <dq>left over<dq> variance, making the proportion of variance your model of interest accounts for compared to unexplained variance LARGER. Which means you<sq>d be closer to the critical value of your test statistic than if you didn<sq>t include SES.</p>", 
                "question": "ELI5<colon> what does it mean to control for a variable in a regression model?"
            }, 
            "id": "d764pfm"
        }, 
        {
            "body": {
                "answer": "<p>p = 0 is a really strong statement, and not the sort of thing you<sq>d want to inadvertently claim by just rounding.  That<sq>s the main issue.</p>", 
                "question": "Is it true P-values should not be labelled as 0.000 - should be <0.001?"
            }, 
            "id": "d3ntgam"
        }, 
        {
            "body": {
                "answer": "<p>Yes. Because it<sq>s a failure of your stats program. R displays out to seven decimals points. What if it was .00004 ? That<sq>s extremely different from .00000 .</p>", 
                "question": "Is it true P-values should not be labelled as 0.000 - should be <0.001?"
            }, 
            "id": "d3nwqi6"
        }, 
        {
            "body": {
                "answer": "<p>Out of curiosity, who is saying so?<br><br>     \u00a0<br><br>1. It depends on how, exactly the printed value `0.000` is obtained. If it<sq>s rounded off, there<sq>d be an argument for writing `p < 0.0005` but if it<sq>s truncated then yes, strictly it would be `p < 0.001`. <br><br>  This would depend on what software you<sq>re using (and possibly on what options have been set in the software). <br><br>2. Standards may also vary by where you<sq>re trying to publish (e.g. you might know that the p-value is considerably smaller but still might only report `p < 0.001` because of some journal<sq>s standards of presentation).</p>", 
                "question": "Is it true P-values should not be labelled as 0.000 - should be <0.001?"
            }, 
            "id": "d3nt3em"
        }, 
        {
            "body": {
                "answer": "<p>I prefer to report all p values with < or > symbols and to report all sig figs instead of rounding. So, for example if I get a value of 0.0236711 this is reported as p<0.024. I work in cognitive psych and there are several papers that have done meta-analyses on distributions of p-values. Rounding to p<0.05 makes it hard to do that and writing p=0.024 is incorrect.</p>", 
                "question": "Is it true P-values should not be labelled as 0.000 - should be <0.001?"
            }, 
            "id": "d3o26c9"
        }, 
        {
            "body": {
                "answer": "<p>Okay, im gonna go out on a limp and say you don<sq>t quite understand what a p-value is.<br><br>A p-value is a well defined mathematical expression, Your statistical software follows the formula and returns the output, and no it does not depend on your alpha.<br><br>One is in no way restricted to an alpha of 0.05, in physics a alpha of 0.0000003 (5 sd in a normal distribution) is often used. You decide for your alpha before you do your test. Now how do you decide which alpha to use? you consider type-I and type-II errors, how costly would they be for you?, second you might consider the quality of your <sq>experiment<sq>. In social science we are often unable to isolate the components of interest, so we get a lot of noise, experiments are costly etc. therefor we are more lenient on the alpha.<br><br>>But I don<sq>t quite understand what is stopping me from simply stating that the result is significant at 0.1<br><br>Nothing is, but consider that approach. Everything is significant at some level, (and considering how much people understand statistics journalists would have a field day). <br><br>edit<colon><br>And i might add, that some statisticians even believe that a p-value of 0.06 and 0.9 with an alpha of 0.05 require the same evaluation (though i strongly disagree with that).</p>", 
                "question": "Why 0.05? Why not 0.1?"
            }, 
            "id": "d3k7uqb"
        }, 
        {
            "body": {
                "answer": "<p>> Why 0.05? Why not 0.1?<br><br>There<sq>s nothing stopping you choosing a significance level of 10<percent> ... or 1<percent> or pretty much anything else. In many cases it will make much more sense than 5<percent>. <br><br>The only thing is, you have to choose it right at the start, *before you collect data*.<br><br><br><br>> I<sq>ve been doing some work in R, and it<sq>s given me a p-value of 0.095 for one of my Chi-Squared tests. Now, I generally evaluate things at 0.05 especially when I<sq>ve been instructed to by somebody for the piece of work I<sq>m doing. But I don<sq>t quite understand what is stopping me from simply stating that the result is significant at 0.1. <br><br>The thing that<sq>s stopping you doing that is you didn<sq>t consider whether your significance level should be anything else *until you saw the p-value*.<br><br>If you choose your significance level based on your p-value, every result you care to have significant is significant ... and all your results are meaningless drivel.<br><br>> Has R done something behind the scenes to create that p-value from 1.96 sd rather than 1.645?<br><br>This doesn<sq>t sound like you quite understand what a p-value is after all. That is all screwed up<colon><br><br>1. What do critical values have to do with p-values?<br><br>2. why use z-test critical values when you<sq>re doing a chi-square test? <br><br>> I know we were never corrected in class for simply jumping between critical levels like that when evaluating a given p-value<br><br>What? You<sq>re confusing together completely different things. <br><br>Either<colon><br><br>- critical values are compared to test statistics (in which case you<sq>re not looking at p-values); or<br><br>- p-values are compare with significance levels (in which case you<sq>re not looking at critical values) <br><br>Don<sq>t mix them together; the two ways of working are equivalent but you can<sq>t mix them (its like working in meters half the time and feet and inches the other half ... and not always writing down which one you mean - you<sq>re going to have bad problems).<br><br></p>", 
                "question": "Why 0.05? Why not 0.1?"
            }, 
            "id": "d3k9ttl"
        }, 
        {
            "body": {
                "answer": "<p>Alpha doesn<sq>t *have* to be 0.05; in some disciplines, it<sq>s routinely much more conservative. In other disciplines, it may be lower or higher depending on the researcher<sq>s thoughtful, *a priori* consideration of the consequences of Type I & Type II errors for that particular case. But the *a priori* part is important because looking at *p* and then deciding on your alpha afterwards increases the risk of intellectual dishonesty (choosing a standard of <dq>success<dq> to apply to a thing only after you<sq>ve seen the thing--just what you said you were considering doing). Based on your 2nd paragraph, I<sq>m inclined to agree with u/oberst_herzog that you don<sq>t *really* seem to understand what a p-value is (even though you almost certainly know it signifies <dq>the probability that...<dq>) or how yours is calculated.<br><br>For the calculation, simply look at the formula for your test statistic and you<sq>ll find that alpha is nowhere to be seen. For an improved understanding of alpha, p-values, and NHST, I recommend reading [The earth is round, p < 0.05](http<colon>//ist-socrates.berkeley.edu/~maccoun/PP279_Cohen1.pdf)</p>", 
                "question": "Why 0.05? Why not 0.1?"
            }, 
            "id": "d3k8c8s"
        }, 
        {
            "body": {
                "answer": "<p>Imagine that x is a sample from a Gaussian (0,1) distribution. Now square it.  Do yourself a favor and sketch out what you think the PDF for this thing looks like.  No seriously do it, then read in....<br><br>Well, it<sq>s going to be strictly non negative, and since the Gaussian has most of its mass around zero, then most of the mass of x^2 is going to be near zero, with a long tail going off to infinity.<br><br>By the way, this x^2 which is made from a Gaussian x is a Chi square variable of degree 1.  I<sq>ll explain the degree next.<br><br>Now imagine taking two samples from a Gaussian, x1 and x2, squaring them both and adding them.  What do you think the PDF of this looks like compared to the last one?  No seriously, make a sketch and then read on...<br><br>Well, since you are adding non negative quantities, its PDF is going to move away from zero. By the way, the sum of two such Gaussian^2 samples is a Chi square distribution of degree 2.<br><br>And so it goes.  The sum of 50 samples^2 from a Gaussian is a Chi square of degree 50, and summing 50 non negative things have shifted the PDF very far from zero.<br><br>That<sq>s where the chi square comes from.  Why is it useful?  Well in so many of our scientific models, the error between our prediction and the actual data is Gaussian distributed. Many metrics for how good a model is involve a sum of square of these Gaussian errors.  And that<sq>s modeled by a Chi square distribution.<br><br>Where else do you see sum of squares?  When dealing with variances, because a random variable minus it<sq>s mean tends to be Gaussian.  Look at the formula for variance again and notice the sum of squared Gaussian variables.  So if you were going to do a hypothesis test on what your variance is, expect to use a chi square test.<br><br>What if you want to compare two variances by taking the ratio?  Well, one chi square variable over another is an F distribution.<br><br>Did that help at all?<br><br></p>", 
                "question": "Chi-squared distribution<colon> where does it hide its power?"
            }, 
            "id": "d36f7of"
        }, 
        {
            "body": {
                "answer": "<p>For your last question (since the first part looks well answered), you can do a qq plot, for which there are a few places that explain how to go about it<colon> http<colon>//www.itl.nist.gov/div898/handbook/eda/section3/qqplot.htm<br>Most statistical processing languages like R will have something built in. you can do a search for qqplot [my stat language]<br><br>edit<colon> grammar</p>", 
                "question": "Chi-squared distribution<colon> where does it hide its power?"
            }, 
            "id": "d36muw1"
        }, 
        {
            "body": {
                "answer": "<p>Just a mathematical note here- it cannot be possible for the median to be 30 and the mean to be 8.  Since the minimum value is 0, if the median is 30, the lowest the mean could possibly be is with a data set (assume odd n for simplicity) with n/2+.5 zeros and n/2+.5 30s... This converges to a mean of 15 in the limit. </p>", 
                "question": "Median vs Average<colon> Basic question but coworker argument."
            }, 
            "id": "d2vfgqc"
        }, 
        {
            "body": {
                "answer": "<p>Yes, median is better single central measure, but that is because it is a percentile and that makes more sense for her purpose. What is a <dq>long time<dq>? If you want to customer in the top 50th percentile then median is the measure. Otherwise, use something like the top third (66th percentile) or top quarter (75th percentile). If you have an idea of what a <dq>long time<dq> is, say 45 minutes, then determine the proportion of customers that exceed that time. This assumes that you have per customer stay data.</p>", 
                "question": "Median vs Average<colon> Basic question but coworker argument."
            }, 
            "id": "d2v5tub"
        }, 
        {
            "body": {
                "answer": "<p>You say she wants to target customers who spend a long time in the store. But how is that related to the question regarding the average vs. median?<br><br>The median and mean answer different questions. If you ask <dq>How long does the typical customer stay in the store?<dq>, then the median is the correct measure. It also tells you that 50<percent> of people spend more and 50<percent> spend less time in the store.<br><br>As you already noted, the mean is easily influenced by extreme data points (I wouldn<sq>t call them outliers in this case). But it<sq>s not wrong per se to take the arithmetic mean. I agree that in this case, the distribution of time spent is likely to be skewed and asymmetric. Therefore, I<sq>d probably prefer the median.<br><br>[Here](http<colon>//stats.stackexchange.com/questions/96371/should-the-mean-be-used-when-data-are-skewed) is a thread on crossvalidated that discusses this issue.<br><br>[Here](http<colon>//web.archive.org/web/20140727233057/http<colon>//voices.yahoo.com/statistics-101-which-measure-central-tendency-should-4185417.html?cat=4) is yet another great post about that issue.<br><br></p>", 
                "question": "Median vs Average<colon> Basic question but coworker argument."
            }, 
            "id": "d2v60hq"
        }, 
        {
            "body": {
                "answer": "<p>Why not both? Or better yet just plot the data in a histogram. I suspect you<sq>ll get a something that looks skewed so median is almost certainly better. </p>", 
                "question": "Median vs Average<colon> Basic question but coworker argument."
            }, 
            "id": "d2v8rs6"
        }, 
        {
            "body": {
                "answer": "<p>Here<sq>s what made me understand it<colon><br><br>Say another guy generated the sample, and it<sq>s your task to estimate a parameter from it. If he delivers you all the data points and then runs off to a 2 week vacation, you<sq>d be fine, right? But if he delivers you only the first two data points and runs off, you<sq>re in trouble and can<sq>t finish your job. **If, however, he delivers you *any sufficient statistic*, you can still estimate the parameter as exactly as possible with the data.**<br><br>A sufficient statistic T incorporates *all* the information of the data (i.e. the sample) you have. So if you ask five people for their height, and you have x_1 to x_5, then the mean of all five points is a sufficient statistic for estimating the mean. The *sum* of all five points is also a sufficient statistic, because you<sq>d just divide it by 5 and are done.<br><br>But the mean of the first three, or first four data points is not sufficient. It does not use up all the available information for that parameter.</p>", 
                "question": "What would be your eli5 for sufficient statistics?"
            }, 
            "id": "d2tip25"
        }, 
        {
            "body": {
                "answer": "<p>Ah, I missed your image before; you posted it while I was typing<br><br>A ELI(a bit older than 5)<colon><br><br>A statistic is sufficient if, conditional on a value of the sufficient statistic, you cannot learn anything else about a parameter from the data. The equation in your image says that, the conditional probability of the data, conditional on the sufficient statistic, does not depend on the value of the parameter. If the conditional probability no longer depends on the parameter, then no matter what the particular values of the data, they don<sq>t constrain the parameter at all. (For intuition<colon> Try estimating a parameter by maximum likelihood when the likelihood does not contain that parameter. It doesn<sq>t make sense because the parameter doesn<sq>t exist in the likelihood.)<br><br>See the beginning of this<colon> http<colon>//www.stat.wisc.edu/courses/st312-rich/suff2.pdf <br>for an example showing an example of proving something is a sufficient statistic.</p>", 
                "question": "What would be your eli5 for sufficient statistics?"
            }, 
            "id": "d2t5cqi"
        }, 
        {
            "body": {
                "answer": "<p>What textbook is the from?</p>", 
                "question": "What would be your eli5 for sufficient statistics?"
            }, 
            "id": "d2t5y3r"
        }, 
        {
            "body": {
                "answer": "<p>Think of a statistic as a function you apply to the data. A stupid statistic would be one which takes the data and returns 0 every time. A sufficient statistic is a function applied to the data that does not erase any information about the underlying process which produced the data. An obvious one is the function that takes the data and returns it. Ideally, it would compress the data in some way, though.<br><br>The definition in your book is not a good introductory one. The one on [Wikipedia](https<colon>//en.wikipedia.org/wiki/Sufficient_statistic#Mathematical_definition) does a much better job for that purpose and also points out that your definition is equivalent due to a theorem of Fisher.<br><br>But let me try to explain the formula in your definition with the intuition given above. Note that the numerator on the left hand side of the equation is Pr(x_1,x_2,... | \u03b8) since we are assuming X_1, X_2, ... are independently sampled (this is implicit in your definition). It<sq>s a pain to think about all those variables if you just want an intuitive understanding so lets restate the equation with a single variable (or just think about the extreme case in the definition of n = 1)<colon><br><br>Pr(x | \u03b8) / Pr( u(x) | \u03b8) = function of x only.<br><br>Now lets replace these probabilities via Bayes<sq> rule<colon> Pr(x | \u03b8) = Pr(\u03b8 | x) Pr(x) / Pr(\u03b8). The Pr(\u03b8) terms will cancel and we get the left hand side equal to<colon><br><br>Pr(x | \u03b8) / Pr( u(x) | \u03b8) = [ Pr(\u03b8 | x) Pr(x) ] / [ Pr(\u03b8 | u(x) )Pr(u(x)) ],<br><br>which only depends on the \u03b8 through the ratio Pr(\u03b8 | x)/Pr(\u03b8 | u(x) ). Now the intuition should be more apparent. If u(x) didn<sq>t tell you the same information about \u03b8, this ratio would depend on \u03b8.</p>", 
                "question": "What would be your eli5 for sufficient statistics?"
            }, 
            "id": "d2t6kyx"
        }, 
        {
            "body": {
                "answer": "<p>I<sq>d like to submit Hadley<sq>s [R packages](http<colon>//r-pkgs.had.co.nz/) and [Advanced R](http<colon>//adv-r.had.co.nz/). Both awesome and available online.</p>", 
                "question": "Learning R. Where should I learn more?"
            }, 
            "id": "cvs99up"
        }, 
        {
            "body": {
                "answer": "<p>Great advice here and I don<sq>t disagree with any of it.<br><br>But if your question is <dq>how can I learn more R?<dq>, I recommend trying to do some stuff in it. Grab an interesting data set, make a plot, and submit it to /r/dataisbeautiful (e.g., there was recently a post showing the age of Nobel laureates -- try splitting it by prize type or recipient gender). Or find an interesting research article that has open data, and redo their main analysis with a different approach (e.g., if they did a repeated-measures ANOVA, use lmer instead; if their regression has a ton of predictors, try glmnet; try a non-parametric method instead of a t-test). <br><br>You won<sq>t really learn any programming language until you<sq>ve tried to use it to actually do things. </p>", 
                "question": "Learning R. Where should I learn more?"
            }, 
            "id": "cvsh6my"
        }, 
        {
            "body": {
                "answer": "<p>Here are a few good links to get you started. I personally like to make cool graphs, so ggplot2 is a go to package. Also I love Rmarkdown/knitr. That can save you a lot of time.<br><br>Cookbook for R<colon> http<colon>//www.cookbook-r.com/<br><br>Beautiful plots<colon> http<colon>//docs.ggplot2.org/current/<br><br><dq>Google<dq> for R stuff<colon> http<colon>//rseek.org/<br><br>If you have questions/problems<colon> http<colon>//stackoverflow.com/<br><br>Rmarkdown - Dynamic documents in R<colon> http<colon>//rmarkdown.rstudio.com/<br><br>Choosing a statistical test depending on data, and how to do it in R (and SPSS, Stata, SAS<colon> http<colon>//www.ats.ucla.edu/stat/mult_pkg/whatstat/<br><br>Shiny - Web application framework for R<colon> http<colon>//shiny.rstudio.com/</p>", 
                "question": "Learning R. Where should I learn more?"
            }, 
            "id": "cvs93wx"
        }, 
        {
            "body": {
                "answer": "<p>http<colon>//www.r-bloggers.com/</p>", 
                "question": "Learning R. Where should I learn more?"
            }, 
            "id": "cvs4pgc"
        }, 
        {
            "body": {
                "answer": "<p>This is a good question!<br><br>The jump from (3) to (4) makes use of the definition of the [standard error of the mean](https<colon>//en.wikipedia.org/wiki/Standard_error#Standard_error_of_the_mean). The wikipedia link provides its derivation.</p>", 
                "question": "Understanding why you divide by (n-1) when calculating the variance"
            }, 
            "id": "cvkuixd"
        }, 
        {
            "body": {
                "answer": "<p>The standard way to prove it is proving it<sq>s to counter the biais.<br><br>However, an easy way to *understand* the concept of *why* it is biaised <colon> <br><br>If you have {1,5}, the mean is 3 right ? So if I give you the observations, you can extract from it the mean. If I give you the mean and (n-1) data observations, you can complete the dataset too <colon> in the example, if I say the mean is 3 and you have these observations {1,x} you can find the value of x and rebuild the dataset.<br><br>In the case of the standard deviation of a sample, since you extract the mean from the data (and that mean is used in the standard deviation formula), you don<sq>t actually have *n* distinct observations. You actually have (*n-1*) useful observations and 1 that could be derived from the mean that you input. That<sq>s why you divide by (*n-1*) rather than *n*.<br><br>In the case of the standard deviation of a population, you divide by *n* since you can<sq>t reconstitute the data with the mean since the mean is a theorical one, not linked to the observed data.</p>", 
                "question": "Understanding why you divide by (n-1) when calculating the variance"
            }, 
            "id": "cvljv2p"
        }, 
        {
            "body": {
                "answer": "<p>The demonstration you are referring to is clearly designed to be persuasive -- it is not mathematically rigorous.<br><br>Statistically-speaking, it mixes metaphors (kinda like I just did).  That way, he avoids the calculus.  But I like it.<br><br>The part you are having trouble with is the variance of the sample mean (sigma^2/n). <br><br>Basically, the sample variance uses an estimated mean.  When you estimated the mean to do this, you effectively <dq>used up<dq> a data point.<br><br>You know you must have done this because if I gave any n-1 data points and the sample mean, you could tell me the value of the data point I left out.<br><br>But then, when you compute the sample variance, you added all n-terms back in like you didn<sq>t use one of them up.  <br><br>The (n-1) is the penalty you pay for having to burn-up one of your data points to estimate the mean **before** you could estimate the variance.  It makes your resulting variance **higher** than it would otherwise have been had you known the mean ahead of time.</p>", 
                "question": "Understanding why you divide by (n-1) when calculating the variance"
            }, 
            "id": "cvlztg9"
        }, 
        {
            "body": {
                "answer": "<p>As /u/Doc_Nag_Idea_Man pointed out, the move from (3) to (4) is by the standard error of the mean (the variance of the mean is sigma^2/n). They have the expression for the standard error of the mean and just do the replacement. <br><br>The reason for using n-1 instead of n when you divide is because when you divide by n you end up with a biased estimator of the standard error. This is most clear in the case of the MLE of the normal where the variance is given by 1/n times the sum of squared deviations. You can see this by computing the expectation of the MLE. Since this is biased, a correction (Bessel<sq>s Correction) is applied to make the estimator unbiased. </p>", 
                "question": "Understanding why you divide by (n-1) when calculating the variance"
            }, 
            "id": "cvl26wn"
        }, 
        {
            "body": {
                "answer": "<p>Do not uncheck this. Except for very specific circumstances, you always want the intercept included. Otherwise it forces the intercept through zero, which will alter the slope of the line.</p>", 
                "question": "What is the difference between checking or unchecking <dq>include constant in equation<dq> for a linear regression?"
            }, 
            "id": "csnsfnz"
        }, 
        {
            "body": {
                "answer": "<p>The constant (intercept) is the value that, according to the fitted model, the response variable would be expected to take if (all) the regressor(s) were zero. If you were to regress weight (DV) against height (IV) based on a sample of people, for example, the intercept would be an estimator of the expected weight of a 0<sq>0<dq> person.<br><br>Clearly, it<sq>s not necessarily meaningful in practice. However, in the simple regression case, fixing the intercept at 0 necessarily worsens the fitted slope as a predictor of the true slope, so it should be included in the model even if the true value <dq>should<dq> be zero.</p>", 
                "question": "What is the difference between checking or unchecking <dq>include constant in equation<dq> for a linear regression?"
            }, 
            "id": "csnsom3"
        }, 
        {
            "body": {
                "answer": "<p>When you fit a linear regression model, you can think of it as finding the best line y=mx+b that matches your data. When you uncheck <dq>include constant in equation<dq> you are saying <dq>don<sq>t put b in there<dq>. In other words, set b=0. Then it<sq>s only fitting the mx part. Having the intercept (b) parameter allows the model to fit the test data more closely. However, there are cases where you know that b must equal 0 (say your data comes from a physics experiment that should be a strict proportionality). In those cases, you<sq>ll get a more accurate value for the slope if you exclude the constant.</p>", 
                "question": "What is the difference between checking or unchecking <dq>include constant in equation<dq> for a linear regression?"
            }, 
            "id": "csnul1s"
        }, 
        {
            "body": {
                "answer": "<p>> I don<sq>t know which one I should be using for my data<br><br>Include the constant.<br><br>[When in doubt, *always* include the constant. When you<sq>re *absolutely certain* you shouldn<sq>t include the constant, you should reconsider your degree of certainty.]<br><br>> Some people say I can exclude it if we can assume that the constant is truly 0<br><br>Well, yes, but I<sq>d usually lean toward including it unless there was a really, *really* good reason not to; I<sq>ve seen some serious errors caused by people excluding a constant because they had what looked like a good reason to assume the constant was truly 0... but they were mistaken.<br><br>>  I don<sq>t even know what this constant is<br><br>It<sq>s the estimate of the expected value of the dependent variable when any independent variables in the model are zero (often that<sq>s not even a meaningful quantity to compute).<br></p>", 
                "question": "What is the difference between checking or unchecking <dq>include constant in equation<dq> for a linear regression?"
            }, 
            "id": "csom012"
        }, 
        {
            "body": {
                "answer": "<p>Basically, comparing groups with each other. For example, ANOVA<colon> Is a sleeping pill effective? Group A gets the real pill while Group B gets a placebo (sugar pill) and then you check if they differ on an outcome variable, such as number of hours slept each night. ANCOVA is basically the same, however, now you take into consideration a covariate (hence the name), that is a variable that you think might affect the outcome and that you want to control for but that is not a part of the main experiment. In the example above such a covariate might be the age of the participants (perhaps older people are more affected by the pill). MANOVA and MANCOVA are basically the same as their non-multivariate counter-parts, however now you may compare groups against multiple outcome variables, for example number of hours slept per night, happiness etc. I hope that helps, I<sq>m still a novice in statistics so if something is wrong, please correct me. <colon>)</p>", 
                "question": "ELI5 anova ancova manova mancova. What are they exactly doing and when would you use them?"
            }, 
            "id": "csb7j5h"
        }, 
        {
            "body": {
                "answer": "<p>[ANOVA<colon> A Visual Introduction](https<colon>//www.youtube.com/watch?v=0Vj2V2qRU10)<br><br>[One-Way ANOVA - Part I](https<colon>//www.youtube.com/watch?v=JgMFhKi6f6Y)<br><br>[One-Way ANOVA - Part II](https<colon>//www.youtube.com/watch?v=UrRYITjDOww)<br></p>", 
                "question": "ELI5 anova ancova manova mancova. What are they exactly doing and when would you use them?"
            }, 
            "id": "csb963h"
        }, 
        {
            "body": {
                "answer": "<p>good read, thank guys!</p>", 
                "question": "ELI5 anova ancova manova mancova. What are they exactly doing and when would you use them?"
            }, 
            "id": "csbi0k4"
        }, 
        {
            "body": {
                "answer": "<p>There are mathematical proofs, but the problem with them is that they require the assumptions to be true. The assumptions are never true, but we don<sq>t know if it matters.<br><br>So we do Monte Carlo simulations. We make up data where we *know* the answer, and we run a test to see if it gives us the right answer.<br><br>We know that the t-test assumes normal distribution. But what if that<sq>s wrong? What<sq>s the result? We can run lots of tests on variables with the same mean, but a skewed distribution, and check how often we get a significant result. We expect a significant result 5<percent> of the time. More, and we<sq>ve got a problem with an inflated type I error rate. Less, and we<sq>ve got a (lesser) problem of losing power.<br><br>This sort of thing is very easy to do in R. Here<sq>s some code, and results.<br><br>I used a chi-square distribution, with 1 df, which is pretty skewed, and a normal distribution. I used samples of 10, 100 and 1000, and I did each one 10,000 times. <br><br>    > set.seed(1234)<br>    > mean(sapply(1<colon>10000, function(x) t.test(rchisq(10, 1), rchisq(10, 1))$p.value) < 0.05)<br>    [1] 0.0324<br>    > mean(sapply(1<colon>10000, function(x) t.test(rchisq(100, 1), rchisq(100, 1))$p.value) < 0.05)<br>    [1] 0.0484<br>    > mean(sapply(1<colon>10000, function(x) t.test(rchisq(1000, 1), rchisq(1000, 1))$p.value) < 0.05)<br>    [1] 0.0463<br>    > mean(sapply(1<colon>10000, function(x) t.test(rnorm(10, 1), rnorm(10, 1))$p.value) < 0.05)<br>    [1] 0.051<br>    > mean(sapply(1<colon>10000, function(x) t.test(rnorm(100, 1), rnorm(100, 1))$p.value) < 0.05)<br>    [1] 0.0487<br>    > mean(sapply(1<colon>10000, function(x) t.test(rnorm(1000), rnorm(1000))$p.value) < 0.05)<br>    [1] 0.0485<br><br>The value is the probability of getting a p-value less than 0.05. If the test is correct, the value we get should be about 0.05. The only one that is a problem is the first, with a small, skewed distribution. </p>", 
                "question": "How do we <dq>know<dq> or <dq>prove<dq> that statistical tests are valid?"
            }, 
            "id": "cqgvur8"
        }, 
        {
            "body": {
                "answer": "<p>No students t-test (nor any other test) tell you your *data* are significant. <br><br>> It is my understanding that there are no <dq>proofs<dq> in statistics as there are in mathematics<br><br>Your understanding is perhaps mistaken. There are *certainly* proofs of things like how test statistics are distributed when the null hypothesis is true, or of consistency or sufficiency of estimators or all manner of other things. <br><br>That is, many facts relating to the *properties* of statistical procedures are certainly capable of proof, if the assumptions hold\\*.<br><br>\\* beware, however, that if you don<sq>t see the derivations, you really don<sq>t know that what you<sq>re told are the assumptions are actual assumptions that are required. Many books state unambiguously that this or that assumption must hold ... but in some cases those claimed assumptions are not necessary (the Kruskal-Wallis is a particular bugbear for that).<br><br>However, when you *use* those procedures whose properties may be subject to proof, that<sq>s when proof is absent. You rely on the probabilistic properties (that were established by proof!) to make your inferences, which are not the same thing as proof.<br><br>If you want to read the derivations of the properties of the various tests, they<sq>re usually available. I<sq>ve read [Pearson<sq>s paper](http<colon>//www.economics.soton.ac.uk/staff/aldrich/1900.pdf) on the multinomial chi-square goodness of fit test, [Student<sq>s paper](http<colon>//www.york.ac.uk/depts/maths/histstat/student.pdf) on the t-test, later papers by Fisher on both tests (as well as papers by other authors on them); I<sq>ve read Wilcoxon<sq>s paper where he does both the rank sum and signed rank tests; Mann and Whitney<sq>s paper that develops U-statistics and extends Wilcoxon<sq>s rank sum test; Kruskal and Wallis<sq> paper on their test; Freidman<sq>s paper on his, and so on through hundreds more papers.<br><br>Some are mathematically demanding. Some are not. Some are clear, some obscure. But if you have access to a university library, you probably have access to almost all of them. Wikipedia often gives original references. Even if you don<sq>t have journal access, many older papers are (legitimately) freely available, for example, via project Euclid or a number of other avenues. <br><br>So -- as long as you know enough to understand what they say -- you can just go read them and see for yourself what was done and how it was done, and what assumptions were made to do it.  <br><br><br></p>", 
                "question": "How do we <dq>know<dq> or <dq>prove<dq> that statistical tests are valid?"
            }, 
            "id": "cqhf4p2"
        }, 
        {
            "body": {
                "answer": "<p>There is estimation.  That is what statistics are in the end.  My guess is you are talking about null hypothesis significance testing however.  Here you need to understand what a p value is.  A p value is the likelihood of getting the same results or more extreme results given that the null hypothesis is true.  That is all the p value tells you.  <br><br>Black box statistics as many are thought are really useless as people assume all they need is a p-value is they are good to go.  a p-value is just one piece of information.  You need domain expertise to supply the practical significant, and other measures such as effect sizes to show how much of an effect you found.  <br><br>Statistics are a tool to tell a story, but too often people use them as the story itself.</p>", 
                "question": "How do we <dq>know<dq> or <dq>prove<dq> that statistical tests are valid?"
            }, 
            "id": "cqgt9h4"
        }, 
        {
            "body": {
                "answer": "<p>A simple linear regression (Y=a+bX + e) begins with a very simple **assumption**<colon><br><br>*For all values of X and Y, e is an identically and independently distributed normal random variable with mean zero and unknown variance sigma^2 (which we can create an estimate for).*<br><br>That is a wrong assumption for nearly every real-world situation where we do a linear regression. However, we use it because it is useful and accurate enough. Remember, all models are wrong but some of them are useful. Over time, combined trial and error and raw intuition and genius have given us many useful model frameworks for people to work with. The simple linear regression above is simple enough that nearly everybody who does work in science can use it and use the tests that follow from that assumption. (nearly) Everything else is just more elaborate and complex version of the italicized text I gave.<br><br>If OP wants, the line from /u/no_dammit should provide some useful background as will just googling t-test enough and reading about the **assumptions** used to make it (it starts off by understanding what the student<sq>s t distribution is and then seeing how, if some other assumptions hold, your test statistic is distribute the same way).<br><br>**[EDIT<colon> Relevant Wikipedia](http<colon>//en.wikipedia.org/wiki/Statistical_assumption)**</p>", 
                "question": "How do we <dq>know<dq> or <dq>prove<dq> that statistical tests are valid?"
            }, 
            "id": "cqhgba8"
        }, 
        {
            "body": {
                "answer": "<p>I wouldn<sq>t say it<sq>s a bad thing. It<sq>s 10 hours a week. In the grand scheme of things this will just keep you sharp over the summer. I know there is a large vocal majority against unpaid internships but in the right context I think they are okay. If you feel in your gut that you could help this non-profit achieve their goals or believe strongly in the cause then I<sq>d say you aren<sq>t making a mistake. Someone else making a buck for their own pockets off your back would be a different story. If you<sq>re not going to be doing anything else this summer except this internship then use it as an opportunity to expand and improve. Make this the summer of MidnightStars12. This unpaid internship could be seen as one step of that.  </p>", 
                "question": "I accepted an unpaid internship at a non-profit. Did I make a mistake?"
            }, 
            "id": "cpwcqy1"
        }, 
        {
            "body": {
                "answer": "<p>Experience is good, especially in our field where schools tend not to show how grimy some of the work can be (by grimy, I mean dealing with missing or flawed data or something like that).<br><br>Having said that, you should absolutely be using this time to make connections. Google the hell out of the executives at your non-profit and see what profitable work they or their families are involved in. Put in some good time and then see if you can get an interview out of there... there<sq>s zero shame in wanting to be paid for your work and if you put in a solid couple of weeks everybody SHOULD want to help you get a well-paying job (and if they don<sq>t want that for you then you need to get out). The second part is that you should go to stats/data science meetups in your area. Since you<sq>re doing non-profit work, its way more permissible for you to solicit help and advice from others there than it would be if you were working for a company. </p>", 
                "question": "I accepted an unpaid internship at a non-profit. Did I make a mistake?"
            }, 
            "id": "cpwkuo8"
        }, 
        {
            "body": {
                "answer": "<p>p-values are random quantities? Oh my goodness! Someone call Neyman and Pearson!<br></p>", 
                "question": "The fickle P value generates irreproducible results <colon> Nature Methods <colon> Nature Publishing Group"
            }, 
            "id": "cphl4y4"
        }, 
        {
            "body": {
                "answer": "<p>How bad of a problem is this?<br><br>How do you feel about this in your specific field of research?</p>", 
                "question": "The fickle P value generates irreproducible results <colon> Nature Methods <colon> Nature Publishing Group"
            }, 
            "id": "cphjok2"
        }, 
        {
            "body": {
                "answer": "<p>I think this belongs in /r/statistics.</p>", 
                "question": "The fickle P value generates irreproducible results <colon> Nature Methods <colon> Nature Publishing Group"
            }, 
            "id": "cphohfk"
        }, 
        {
            "body": {
                "answer": "<p>This has been a topic for debate for a lot of years now, but has recently been getting a lot of publications and media coverage, perhaps partly due to academic journals beginning to ban null hypothesis testing. The publishing process and the way we<sq>ve been taught to understand results and <sq>science<sq> has been (and probably will continue to be) a major issue in psychological research. As the cynics point out, <sq>idiots<sq> will abuse any new system that might be devised, but I think it will take a new generation of researchers to really change anything.</p>", 
                "question": "The fickle P value generates irreproducible results <colon> Nature Methods <colon> Nature Publishing Group"
            }, 
            "id": "cpjaom3"
        }, 
        {
            "body": {
                "answer": "<p>MCMC tutorial http<colon>//camdavidsonpilon.github.io/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/<br><br>Nonparametric classifiers and regression http<colon>//scikit-learn.org/stable/<br><br>A research neural network library produced by a one of the top labs on the subject. http<colon>//deeplearning.net/software/pylearn2/<br><br>Wavelets http<colon>//www.pybytes.com/pywavelets/<br><br>Things like survival analysis, time-series methods, and GLMs lag behind R. The quality and breadth of machine learning, digital signal processing and natural language processing libraries in Python is excellent. It<sq>s also pretty efficient and some libraries can be accelerated by graphics cards.</p>", 
                "question": "Does anyone know of a good resource for learning Python from a <dq>statistics<dq> standpoint?"
            }, 
            "id": "ck328t7"
        }, 
        {
            "body": {
                "answer": "<p>Hiya, pretty sure everything he writes is python-based http<colon>//allendowney.blogspot.com/</p>", 
                "question": "Does anyone know of a good resource for learning Python from a <dq>statistics<dq> standpoint?"
            }, 
            "id": "ck2xp0v"
        }, 
        {
            "body": {
                "answer": "<p>I took a MOOC on linear algebra, and though it seemed like python would be a big emphasis, in the end not so much. That being said, some of the workbooks are still decent resources on how to program various matrix operations.<br><br>https<colon>//www.edx.org/course/utaustinx/utaustinx-ut-5-01x-linear-algebra-1162<br></p>", 
                "question": "Does anyone know of a good resource for learning Python from a <dq>statistics<dq> standpoint?"
            }, 
            "id": "ck32t8x"
        }, 
        {
            "body": {
                "answer": "<p>From the <dq>Topical Software<dq> page @ scipy.org<colon><br><br>http<colon>//work.thaslwanter.at/Stats/html/<br><br>Also<colon> Intro to Python for Econometrics, Statistics and Data Analysis<colon><br><br>http<colon>//www.kevinsheppard.com/Python_for_Econometrics</p>", 
                "question": "Does anyone know of a good resource for learning Python from a <dq>statistics<dq> standpoint?"
            }, 
            "id": "ck3ix19"
        }, 
        {
            "body": {
                "answer": "<p>Here are two justifications. <br><br>(1) You have a sample of the current school population.  However you don<sq>t know whether this applies to next year<sq>s population, or last year<sq>s population, or the population 50 years from now.  Your observations are only a finite subset of a much larger hypothetical population. <br><br>(2) Although you have observed every outcome, the effect you want to measure is actually the difference between [potential outcomes](http<colon>//en.wikipedia.org/wiki/Rubin_causal_model), something unobserved and in fact impossible to know with certainty.  Therefore what you compute using your observations is still only an estimator, and this estimator has nonzero variance.</p>", 
                "question": "Why calculate statistical significance for a census?"
            }, 
            "id": "ci6d2lx"
        }, 
        {
            "body": {
                "answer": "<p>the general idea is that you<sq>re treating your examples as coming from a process or data-generating mechanism and you<sq>re doing statistical modeling to better understand this underlying process. specifically, you<sq>re probably interested what your treatment does to this process. you don<sq>t really care about the schools. you care about the treatment effect. if you think about it this way, the observed treatment effect is a random variable and you can perform inference about it after a few assumptions. the fact that you have observed every school you are interested in is beside the point - you can<sq>t cross the same river twice and you<sq>re interested in the process, not the schools.</p>", 
                "question": "Why calculate statistical significance for a census?"
            }, 
            "id": "ci6czu8"
        }, 
        {
            "body": {
                "answer": "<p>It depends on the population that<sq>s the target of your inference. You may well have a census, but if it<sq>s not a census of the population you wish to extend the inference to, it<sq>s not *the* population.</p>", 
                "question": "Why calculate statistical significance for a census?"
            }, 
            "id": "ci6hyq2"
        }, 
        {
            "body": {
                "answer": "<p>These are all excellent responses. Thanks for all your help!</p>", 
                "question": "Why calculate statistical significance for a census?"
            }, 
            "id": "ciefoog"
        }, 
        {
            "body": {
                "answer": "<p>Well, this isn<sq>t actually from statistics, but from economics/corporate finance. When deciding on something, you should always look only at future costs and future benefits. Never consider so called sunk costs, which is what you<sq>ve already invested. This is very intuitive when financial and business investments are concerned, but not so much in everyday life. For example - people buy a book, don<sq>t like it, but still struggle to read through it all as they feel they need to, so to speak, recoup their investment. Whereas, a book that you borrowed form the library goes straight back to the library, if you don<sq>t like it.</p>", 
                "question": "What are the most useful concepts one can apply in everyday life?"
            }, 
            "id": "c50xzlc"
        }, 
        {
            "body": {
                "answer": "<p>Your intuition about probability is often very, very wrong. See the Monty Hall problem, the Birthday Paradox, etc.<br><br>What people think of as <sq>random<sq> is very often not random. If you ask someone to distinguish human-generated sequences of heads and tails from a fair coin from sequences generated by a PRNG, they<sq>re usually very wrong.<br><br>Extremely unlikely events can become very likely given enough trials. The probability of X happening today might be very small, but sooner or later it will very likely happen, so when it does, we shouldn<sq>t be surprised. <dq>What are the chances of this happening today?<dq> is rarely a useful question in everyday life for that reason.</p>", 
                "question": "What are the most useful concepts one can apply in everyday life?"
            }, 
            "id": "c50y9gi"
        }, 
        {
            "body": {
                "answer": "<p>Graphs without error bars can be very misleading.<br><br>If error bars are present, don<sq>t compare the height of the points (or bars), just compare the overlap in the error ranges. If these do not overlap, then you<sq>re likely looking at a real difference. If they do overlap, then you probably aren<sq>t looking at a real difference, even if the mean height (the bar height) appears much different.</p>", 
                "question": "What are the most useful concepts one can apply in everyday life?"
            }, 
            "id": "c510dk4"
        }, 
        {
            "body": {
                "answer": "<p>I always thought the [diffusion of innovations](http<colon>//en.wikipedia.org/wiki/Diffusion_of_innovations) to be pretty useful.<br><br>Also, the [monty hall problem](http<colon>//en.wikipedia.org/wiki/Monty_Hall_problem) helps you get your head around choice chance and probability. <br><br>In case you<sq>re worried the world is going to shit <br>[Hans Rosling](http<colon>//www.youtube.com/results?q=hans+rosling)  has the information to give you the optimistic edge.  It<sq>s useful to think your species has a good future so you can work to be a part of it.</p>", 
                "question": "What are the most useful concepts one can apply in everyday life?"
            }, 
            "id": "c50z09x"
        }, 
        {
            "body": {
                "answer": "<p>With only four readers, I suppose I have to reply even though I can say little on this topic. So far as I know, the only reason for not using R is if there is a particular function you need that it doesn<sq>t support. I am also trained in SPSS and SAS and have only used R for a few things and unfortunately I know little about the GUIs, other drawbacks, etc.      <br>      <br>Edit<colon> R will handle ANOVA well, including repeated measures, nested design, and other related issues.      <br>     <br>This link speaks to your last bullet<colon> http<colon>//stackoverflow.com/questions/1295955/what-is-the-most-useful-r-trick </p>", 
                "question": "Why *wouldn<sq>t* I use R? What<sq>s the best GUI for my R needs?"
            }, 
            "id": "c1w6jwd"
        }, 
        {
            "body": {
                "answer": "<p>I<sq>ve used R for about 4 years now. My experience is that it<sq>s a great tool, but not a great programming language. For most applications of statistics, you just need a dependable tool. Often only for creative, mathematical statistics work, you need something other than R.<br><br>R is great for basic analysis of data sets. I can<sq>t think of a better free option (actually I like R better than SPSS anyway). Simple ANOVAs are easy and plotting the results is built into the system (`plot(anova(formula))`). It can definitely fail if your data set is too large, though, and the options in that case are very slim (usually<colon> program it in something like C).<br><br>I<sq>d encourage you to not use GUIs. R shines as a language by letting you manipulate data frames and formulae more abstractly as consistent objects of statistical inquiries. I<sq>ve not used RStudio, though, so I can<sq>t remark on it. Still, learn to use it at the command line and you<sq>ll probably never go back.<br><br>I think Hadley Wickham is a genius. You should learn to use `ggplot2` (or `lattice` if that<sq>s more of your style), `plyr`, and `reshape`. Almost every R instance of mine begins with `library(reshape)`.<br><br>I should also mention `head`, `with`, `subset`, `transform`, and `sample`. Those are all in heavy rotation.<br><br>* `head` peek at the top of a data frame to make sure it is sensible<br>* `with` write expressions lopping off all of the `dataframe$` headers, mostly for convenience<br>* `transform` make variable transformations and create new columns in data frames with ease<br>* `subset` create complex subsets of data frames<br>* `sample` make a random sample of a data frame to test out operations and graphs on smaller sets of data (or to do manual bootstrapping)</p>", 
                "question": "Why *wouldn<sq>t* I use R? What<sq>s the best GUI for my R needs?"
            }, 
            "id": "c1wgtv6"
        }, 
        {
            "body": {
                "answer": "<p>You would not use R if you need a programming language which supports development of software systems. If you are looking for a statistical tool that enables automation, I think R is a right fine choice. <br><br>The good news is that if at some point you find yourself needing the <dq>software development<dq> capabilities that a traditional programming environment provides you, you can leverage R in a number of programming languages<colon><br><br>Perl<colon><br>http<colon>//search.cpan.org/~gmpassos/Statistics-R-0.02/lib/Statistics/R.pm<br><br>Python<colon><br>http<colon>//rpy.sourceforge.net/<br><br>Ruby<colon><br>https<colon>//rubyforge.org/projects/rsruby/<br><br>Java<colon><br>http<colon>//www.rforge.net/rJava/<br><br>etc...</p>", 
                "question": "Why *wouldn<sq>t* I use R? What<sq>s the best GUI for my R needs?"
            }, 
            "id": "c1wc5yp"
        }, 
        {
            "body": {
                "answer": "<p>For GUI, have you looked at Orange or the R quasi-fork Red-R? KNIME also has R support.</p>", 
                "question": "Why *wouldn<sq>t* I use R? What<sq>s the best GUI for my R needs?"
            }, 
            "id": "c20nbab"
        }, 
        {
            "body": {
                "answer": "<p>You<sq>re reasoning seems sound to me. The observed p-value only provides some information about how likely an observed test statistic is under the assumption that the null hypothesis is true.<br><br>If, in fact, the null is true, the the probability of observed p >= 0.05 is 0.95. If the null is not true, and if we don<sq>t have any additional information, then we don<sq>t have any idea how likely p >= 0.05 is, even if we replicated the study with the same population.</p>", 
                "question": "Given p=0.05 in one study what is the likelihood of p>=0.05 if the same study was repeated under the exact same condition in the same population?"
            }, 
            "id": "df3s8np"
        }, 
        {
            "body": {
                "answer": "<p>You can<sq>t tell.<br><br>Assume the test has *some* power above just random rejection (i.e. assume 1-\u03b2 > \u03b1 for any given \u03b1 and any alternative; it isn<sq>t necessary to assume this to answer the question but makes the argument clearer and it<sq>s only necessary to show that you can<sq>t tell in at least some cases). <br><br>Consider two possibilities<colon><br><br>1. The null hypothesis is true. Then the probability of getting a -value above 0.05 is 95<percent><br><br>2. The null hypothesis is false. Since we assumed our test wasn<sq>t completely useless, the probability the p-value exceeds .05 is smaller than 95<percent>. Exactly how much smaller depends on the actual alternative situation that holds and the power under that alternative.<br><br><br>Since we don<sq>t know whether 1. or 2. is true (the title question doesn<sq>t say which is true) we can<sq>t say whether the answer is 0.95 or something different from that. There<sq>s no good basis to choose any of the answers, other than to say that no fixed number is a reasonable answer to the question. ie. the only reasonable answer for the general situation is <dq>none of the above<dq>.<br><br>> essentially this is asking what is the probability that null hypothesis is true<br><br>No, it<sq>s *not* asking that<br><br><br>----<br><br>You might like to consider trying your question on stats.stackexchange.com (including your own discussion/analysis as above). You may get some better explanations. </p>", 
                "question": "Given p=0.05 in one study what is the likelihood of p>=0.05 if the same study was repeated under the exact same condition in the same population?"
            }, 
            "id": "df3ym8n"
        }, 
        {
            "body": {
                "answer": "<p>The question isn<sq>t asking the probability that the null is true. If the p = 0.05 estimate is accurate, your power in a second study would be about 50<percent>. <br><br>This is discussed in the following paper<colon> http<colon>//www.nature.com/nrn/journal/v14/n5/full/nrn3475.html<br></p>", 
                "question": "Given p=0.05 in one study what is the likelihood of p>=0.05 if the same study was repeated under the exact same condition in the same population?"
            }, 
            "id": "df3sdrn"
        }, 
        {
            "body": {
                "answer": "<p>If the null is true, then a p=0.05 means that data as extreme (or *more* extreme) should only ever be observed 5<percent> of the time. So, if the null is actually true, you<sq>d expect a subsequent sample to be less extreme (hence p>0.05).  <br><br>However, we don<sq>t know if the null is true (and the original p=0.05 doesn<sq>t tell us that) so the null could really not be true, and in that case you<sq>d expect the subsequent sample to be as or more extreme than the first (so p<0.05). So, we are  actually have too little information and 50<percent> seems like the closest answer (discounting the small likelihood that p=0.05 on the second set of measurements).<br><br>I would guess that this question is getting at the importance of replicates, and the hazards of over interpreting single studies (especially as we know that there is clear publication bias).</p>", 
                "question": "Given p=0.05 in one study what is the likelihood of p>=0.05 if the same study was repeated under the exact same condition in the same population?"
            }, 
            "id": "df4zf1o"
        }, 
        {
            "body": {
                "answer": "<p>A statistics and/or computer science degree</p>", 
                "question": "Are there any certifications I could earn as an undergrad for Stats/Data Science?"
            }, 
            "id": "dd0eqzl"
        }, 
        {
            "body": {
                "answer": "<p>You would also have to consider the ages at which you had children. And after having them, you could check if they were males or females and adjust the probability accordingly.<br><br>Unfortunately I don<sq>t think there is a nice easy to work with distribution for this. I<sq>d probably approach it by exploring a giant tree of actuarial tables and counting cases.<br><br>https<colon>//www.ssa.gov/OACT/STATS/table4c6.html#fn1</p>", 
                "question": "A bit morbid is there a number of children had that makes it likely one will outlive at least one of their children?"
            }, 
            "id": "dbu23mj"
        }, 
        {
            "body": {
                "answer": "<p>UN Health department says that it takes on average 2.1 kids per woman to keep a population stable in a 1st world country. That means that, on average, out of 21 babies, only 20 will reach adulthood.<br><br>That<sq>s a gross estimates, but it still gives you a sense of how much it is.</p>", 
                "question": "A bit morbid is there a number of children had that makes it likely one will outlive at least one of their children?"
            }, 
            "id": "dbvqtbi"
        }, 
        {
            "body": {
                "answer": "<p>The Royal Statistical Society has some (but I<sq>ve just read on their website that they will end in May). http<colon>//www.rss.org.uk/RSS/pro_dev/Examinations/RSS/pro_dev/Examinations_sub/Examinations.aspx?hkey=5c330ed8-32a6-4b63-9030-3c7f3079c993</p>", 
                "question": "Any reputable statistics exam out there?"
            }, 
            "id": "dbntqkl"
        }, 
        {
            "body": {
                "answer": "<p>[deleted]</p>", 
                "question": "R vs Julia"
            }, 
            "id": "dac4w97"
        }, 
        {
            "body": {
                "answer": "<p>If your primary goal is to learn a *statistical program,* learn R. While Julia seems to steadily increase its statistical capabilities and packages, it<sq>s still a long way until Julia can match R in this regard.<br><br>One of the main advantages of Julia is its speed. But in most circumstances speed is not a serious problem in R.<br><br>As I see it, Julia is a general programming language for scientific computing with some capabilities in statistics. As such, it is more of a replacement for programs like Matlab than R. R<sq>s focus on the other hand is on statistics and speed is not the first priority.</p>", 
                "question": "R vs Julia"
            }, 
            "id": "dac7xvj"
        }, 
        {
            "body": {
                "answer": "<p>I know that *Julia* exists, but it<sq>s such in an infant stage, I wouldn<sq>t bother to learn it yet. *R*, on the other hand, is by far the leading language in stats (followed by Python).</p>", 
                "question": "R vs Julia"
            }, 
            "id": "dacc0lq"
        }, 
        {
            "body": {
                "answer": "<p>I have a MSc in Statistics, and I am about a year out from completing my PhD in statistics.<br><br>I<sq>ve been doing all of my PhD work in Julia. It<sq>s such a fun language to write in, and I think it will continue to gain popularity for statistics, and data science.<br><br>I learned R first, years ago, then python, then Julia. If I were to do it all over again today, I<sq>d probably still learn in that order.</p>", 
                "question": "R vs Julia"
            }, 
            "id": "dacj9xz"
        }, 
        {
            "body": {
                "answer": "<p>Well, that raises a question of when you would *ever* pick one over the other.<br><br>The answer is really this<colon> Bayesian methods are usually more generally applicable, but are usually less easily calculable. Bayesian methods will usually be able to equal or out-perform frequentist methods, but particularly shine when there is limited data because frequentist methods will rely on the likelihood while Bayesian methods will fall back toward the prior distributions as appropriate.<br><br>Frequentist methods, however, tend to be more easily calculated and give good results (i.e., equaling Bayesian results) when there<sq>s a lot of data. When I say <dq>easily calculated<dq> I also mean that you can come up with exact answers (even if they aren<sq>t exactly right) while with Bayesian methods you often need to use numerical approximations and heavy duty computing.<br><br>In limited data situations, Frequentist methods tend to over-fit the data compared to a similar Bayesian approach. Broadly speaking, frequentist results can often be viewed as approximations to Bayesian results. Often quite good approximations, but not always perfect.<br><br>For my money, in the case of guessing the number of tanks, I would rely on Bayesian methods because the posterior distribution of the maximum of the distribution would give me a better sense of the possible error in my estimate. This would enable me to plan more strategically what would happen if my estimate is too high or too low. In addition, we<sq>re probably dealing with relatively limited data in this case, so it<sq>s probably a better bet, and Bayesian methods can more easily incorporate expert opinions through the use of prior information.</p>", 
                "question": "The German Tank Problem"
            }, 
            "id": "d9kdpki"
        }, 
        {
            "body": {
                "answer": "<p>I<sq>ve seen this problem used to apply the method of moments and the maximum likelihood estimate methods to estimate parameters for the distribution of tanks (or whatever other numbered objects).<br><br>One consideration is you start with minimal information<colon>  tanks are numbered no less than 1, but could potentially go on to infinity.  Further, the distribution of tank numbers is discrete.  These <dq>prior knowledge facts<dq> are both utilized by the frequentist and Bayesian approach, so in this case, I<sq>d vote for frequentist.  The difference is that the Bayesian approach would treat the parameter estimates as random variables.  I<sq>m imagining the Bayesian approach would give a similiar estimate as the method of moments, but I could be wrong.</p>", 
                "question": "The German Tank Problem"
            }, 
            "id": "d9k5fut"
        }, 
        {
            "body": {
                "answer": "<p>It means y1 and y2 are [statistically independent](https<colon>//en.wikipedia.org/wiki/Notation_in_probability_and_statistics#Probability_theory).</p>", 
                "question": "What does this symbol mean?"
            }, 
            "id": "d8whzfb"
        }, 
        {
            "body": {
                "answer": "<p>I<sq>ve never played Dota2. In order to make predictions you need to learn topics from Data Mining and Machine Learning (more precisely supervised learning). You need to become familiar with<colon> linear and logistic model, stepwise regression, lasso, ridge and elastic net regression, model selection criteria such as AIC and BIC, linear and quadratic discriminant analysis, local weighted regression and loess, regression and smoothing splines, MARS (multivariate adaptive regression splines), generalized additive models, projection pursuit regression, classification and regression trees, bagging, boosting, random forest, support vector machines, neural networks, k nearest neighbor, the multinomial model, the bias-variance trade-off, the problem of overfitting and underfitting, the curse of dimensionality. This is what is taught in universities. A good book for starters is  this<colon> http<colon>//www-bcf.usc.edu/~gareth/ISL/. Another great book (more advanced and difficult) is<colon> http<colon>//statweb.stanford.edu/~tibs/ElemStatLearn/. You can download both of them legally for free. </p>", 
                "question": "What are the skills I would need to build my own prediction model?"
            }, 
            "id": "d8on1c6"
        }, 
        {
            "body": {
                "answer": "<p>Some web scraping / coding- Python to call Dota<sq>s API for data. <br><br>Logistic regression and how to understand it and code it in R.<br><br>Cleaning data and best practices with data, using R.<br><br>Realizing prediction models aren<sq>t going to nearly give you exactly what you want.</p>", 
                "question": "What are the skills I would need to build my own prediction model?"
            }, 
            "id": "d8omjmo"
        }, 
        {
            "body": {
                "answer": "<p>That sounds like a wording error. Usually, a p-value below 0.05 (or any other arbitrary threshold) is considered statistically significant and the corresponding null hypothesis is rejected. In normality testing, the null hypothesis is that the data is compatible with a normal distribution. A small p-value is evidence that the data is not from a normally distributed population.<br><br>Also, the p-value of the shapiro test is not compared to the test statistics. The p-value is compared with the set significance level (usually 0.05).</p>", 
                "question": "This tutorial on normality testing says you should not reject the null hypothesis when you have a small p-value. Isn<sq>t this wrong?"
            }, 
            "id": "d8kv9m9"
        }, 
        {
            "body": {
                "answer": "<p>The tutorial states the null hypothesis is that the data is normally distributed. <br><br>They do a shapiro test and get a test-statistic of 0.97 and a p-value of 0.00027. They say <dq>Since our p-value is much less than our Test Statistic, we have good evidence to not reject the null hypothesis at the 0.05 significance level.<dq><br><br>Then they do kolmogorov test and get a test-statistic of 0.99999 and a p-value of 0.0. They say <dq>Since our p-value is read as 0.0 (meaning it is <dq>practically<dq> 0 given the decimal accuracy of the test) then we have strong evidence to not reject the null-hypothesis.<dq><br><br>Isn<sq>t this incorrect? They find p-values below their 0.05 threshold. Shouldn<sq>t they reject the null hypothesis that the data is normally distributed?</p>", 
                "question": "This tutorial on normality testing says you should not reject the null hypothesis when you have a small p-value. Isn<sq>t this wrong?"
            }, 
            "id": "d8ktvv4"
        }, 
        {
            "body": {
                "answer": "<p>Yeah, it<sq>s wrong.<br><br>But it<sq>s worse than that. Tests for normality are crap. The only thing that a normality test tells you is that you didn<sq>t have enough data to detect the fact that your distribution was not normal. Real data are never normal.<br><br>It<sq>s not whether your data are normal that matters (we know that the data are never normal). It<sq>s whether they are non-normal enough that you care, and that<sq>s a very different question. <br><br>The irony is that the bigger the sample size, the less we care about normality. But the easier it is to find that the data are not normal. Tests of normality are only good tests when we don<sq>t care about the result. </p>", 
                "question": "This tutorial on normality testing says you should not reject the null hypothesis when you have a small p-value. Isn<sq>t this wrong?"
            }, 
            "id": "d8l3vmz"
        }, 
        {
            "body": {
                "answer": "<p>Since, in practice, no distribution is exactly normally distributed, you can reject the null hypothesis without doing any test. The practical question is how large is the deviation from normality and how robust is the statistical analysis is to these deviations?</p>", 
                "question": "This tutorial on normality testing says you should not reject the null hypothesis when you have a small p-value. Isn<sq>t this wrong?"
            }, 
            "id": "d8l5780"
        }, 
        {
            "body": {
                "answer": "<p>For programming languages R, Python, and especially SQL. For courses Machine Learning, Experimental Design, Bayesian Statistics and Time-Series Analysis are all very useful.</p>", 
                "question": "What courses (or programming languages) should I take to make my BS in Statistics employable?"
            }, 
            "id": "d8awi8y"
        }, 
        {
            "body": {
                "answer": "<p>If you do well in stats you<sq>ll already be very employable, but it might pay to pick up some applied papers rather than learning 3-4 languages. Finance, biology, psychology etc. Not for job security or to boost your income potential (though Finance might help with that), but to give you options later on should you wish to change careers.</p>", 
                "question": "What courses (or programming languages) should I take to make my BS in Statistics employable?"
            }, 
            "id": "d8aqs9w"
        }, 
        {
            "body": {
                "answer": "<p>R and Python seem like good places to start.</p>", 
                "question": "What courses (or programming languages) should I take to make my BS in Statistics employable?"
            }, 
            "id": "d8aum7k"
        }, 
        {
            "body": {
                "answer": "<p>Python and SQL.<br><br>After that<colon><br><br>R if you want to get into data science.<br><br>Scala and Spark are becoming pretty important for data science + engineering.<br><br>Bayesian Statistics and Machine Learning are so hot right now.<br><br>SAS if you want to go into pharma (I think this is still true)</p>", 
                "question": "What courses (or programming languages) should I take to make my BS in Statistics employable?"
            }, 
            "id": "d8b2iry"
        }, 
        {
            "body": {
                "answer": "<p>I wouldn<sq>t say copulas are a way to represent marginal distributions, but rather let you <dq>cheat<dq> and combine marginal distributions rather than try to fit a distribution to all of your variables at once, a task that is quite complicated due to the lack of work in these areas, the lack of multivariate distributions, and the lack of goodness-of-fit tests, especially visual ones.<br><br>Basically what a copula does is allows you to model the dependence structure between your variables after you<sq>ve fit your marginals. Thanks to Sklar<sq>s theorem, we know we aren<sq>t sacrificing anything by doing this because we are guaranteed that a copula exists that will result in the same distribution we would have chosen had we actually tried fitting a multivariate distribution in the beginning.<br><br>I would recommend reading through some of the posts on Arthur Charpentier<sq>s [blog](http<colon>//freakonometrics.hypotheses.org/), some good examples being his three-part post about [copulas and tail dependence](http<colon>//freakonometrics.hypotheses.org/2435) and his post about [modeling marginals and dependence separately](http<colon>//freakonometrics.hypotheses.org/13576). This guy is great at mixing theoretical, applied, and R code and has great explanations. <br><br>Additionally, you can totally simulate from copulas to answer the questions you are asking. You can even take it further by building conditional copula models where you can condition on data you expect to have at some point in time and then simulate from the resulting distribution. The copula package in R has a great set of functions for working with copulas including functions for generating random values (on [0,1]) from a copula using the **rCopula** function and allowing you to effectively simulate conditionally from a copula using the **rtrafo** function.<br><br>Here is a quick set of sample code that shows how you can simulate from a constrained portion of a copula. You can find the resulting plot [here](http<colon>//i.imgur.com/ZJpv9Nl.png). The input to the **rtrafo** function is a matrix where the first column is treated as the observed value. The second column are random Uniform(0,1) numbers that will be transformed to follow the copula. Notice how I have used the *exact same* uniform values in the second column in each **points** statement, but they move to the appropriate part of the resulting density depending on the portion of the graph they are located, as determined by the first column<br><br>    library(copula)<br>    set.seed(1776)<br>    exampleCopula <- normalCopula(param = 0.95)<br>    copulaSample <- rCopula(1000, exampleCopula)<br>    uniformSample <- runif(100)<br>    <br>    plot(copulaSample, xlab = <sq>x<sq>, ylab = <sq>y<sq>, main = <sq>Sample From t-Copula<sq>)<br>    points(rtrafo(cbind(0.10, uniformSample), cop = exampleCopula, inverse = T), pch = 20, col = 2)<br>    points(rtrafo(cbind(0.25, uniformSample), cop = exampleCopula, inverse = T), pch = 20, col = 3)<br>    points(rtrafo(cbind(0.50, uniformSample), cop = exampleCopula, inverse = T), pch = 20, col = 4)<br>    points(rtrafo(cbind(0.75, uniformSample), cop = exampleCopula, inverse = T), pch = 20, col = 5)<br>    points(rtrafo(cbind(0.90, uniformSample), cop = exampleCopula, inverse = T), pch = 20, col = 6)<br><br>You may also take a look at a presentation I give at times. Look at the slides under the CAT modelling section. You can see an example where I show two marginal densities (a gamma and a beta) and I show a sample from these using a Clayton copula as the model for the dependence structure, with marginal histograms on the edges of the plot. You can find those slides [here](http<colon>//piecemaker.github.io/presentation-actuarial-overview/). Basically this was generated by generating a random set of values from the Clayton copula and then transforming them to the appropriate distribution using the **qbeta** and **qgamma** functions in R.</p>", 
                "question": "ELI5 Copulas"
            }, 
            "id": "d89axvn"
        }, 
        {
            "body": {
                "answer": "<p>>  I get that it is a way to represent the marginal distributions,<br><br>No, it isn<sq>t. It<sq>s a way to model/describe dependence by removing the effect of the marginal distributions.<br><br>Specifically a copula is the (multivariate) distribution you get after you transform all the margins to uniformity.<br><br>...<br><br>Are the resources here of any use?<br><br>http<colon>//quant.stackexchange.com/questions/16548/copulas-simply-explained<br><br></p>", 
                "question": "ELI5 Copulas"
            }, 
            "id": "d89h11c"
        }, 
        {
            "body": {
                "answer": "<p>I think the real question you should be asking is if inference is appropriate here. Sampling units are intended to be a random sample of the population you intend to describe. Could you really posit a single specimen could reflect the characteristics of that species as a whole?</p>", 
                "question": "What do you do when your sample size is n=1?"
            }, 
            "id": "d7o5b3x"
        }, 
        {
            "body": {
                "answer": "<p>This sounds like an instance where statistics is unnecessary until you have more data; you have so little data that you can simply report all the individual measurements, and no one would believe that the differences are guaranteed or even likely to be reproducible. Why report statistical summaries such as mean and standard deviation (2 numbers per group equals 6 total numbers to report) when you can report the six exact values? Why report a pvalue if someone is going to not trust the result anyway? If a statistical test did give <dq>significance<dq> to your results, would that be convincing to a fellow scientist who might consider your n=1 case to just be an outlier (maybe this dolphin was the Shaquille O<sq>Neal or Danny DeVito of its species)? Remember p<0.05 might mean *statistical* significance, but it most certainly does not mean scientific significance, and likewise your p>0.05 does not mean lack of scientific significance.<br><br>I think you should treat this like medical case reports where no one tries to claim anything statistically, they just say <dq>I have this rare and interesting data that I think belongs in the published literature, so we are writing up our observations with some professional analysis/discussion<dq>. If you data is not rare/expensive enough to warrant such a write-up, then you probably need to get your hands on more data in order to conduct a valid statistical analysis.</p>", 
                "question": "What do you do when your sample size is n=1?"
            }, 
            "id": "d7o5xa8"
        }, 
        {
            "body": {
                "answer": "<p>None of the standard statistics techniques you<sq>d learn will be of help to you in this situation, and your other sample sizes are so small that most of the standard techniques aren<sq>t going to be very powerful. Off the top of my head I can<sq>t actually think of any techniques applicable to your problem, so I<sq>m afraid I<sq>m not going to be much help here.<br><br>My best guess is that the most appropriate thing for you to do here is some sort of cluster analysis, which would let you use multiple measurements to see how <dq>close<dq> the morphology of the fossils are to one another. Off the top of my head I don<sq>t think it would result in anything as definitive as a p-value that you could use as evidence. But it would be a better approach to comparing them and which might display (graphically) just how dissimilar the morphology is to the extent that one can clearly observe the morphology is different.</p>", 
                "question": "What do you do when your sample size is n=1?"
            }, 
            "id": "d7o5brz"
        }, 
        {
            "body": {
                "answer": "<p>> Species A --> n=3, Species B --> n=2, Species C --> n=1<br><br>These samples sizes are all way too small to be running statistical tests.<br><br>Think about what the question is that you<sq>re trying to answer, and compare that to the meaning of your calculations.  You have a very small number of samples, and you took the average (mean) of the very small number of data points and then tried to measure the deviation of the very small number of data points away from that mean (SD).  Let<sq>s just forget about the tests because they<sq>re useless at your sample size.<br><br>What information did you lose by doing your calculations?  What information did you gain?  When you have a very small number of samples versus a very small number of samples, say 3 versus 2 (to take an example not entirely at random), you actually lose information when you take the mean.  You can literally do 6 comparisons to see how each sample of a type differs against a sample of the other type.  When you compare means, you lose those individual results because you<sq>re only comparing 2 values<colon> the averages.<br><br>I<sq>m not trying to be rude but I can<sq>t think of another way to phrase this<colon> how did you end up doing things like t-tests and ANOVAs without knowing that your sample sizes are too small?</p>", 
                "question": "What do you do when your sample size is n=1?"
            }, 
            "id": "d7o5m36"
        }, 
        {
            "body": {
                "answer": "<p>This is more of an optimization problem than a statistics problem. I think an evolutionary algorithm would probably do pretty well. I imagine you could probably use integer programming as well. Once you define an appropriate cost function, you basically just pick your favorite tools from the optimization toolbox. </p>", 
                "question": "I<sq>m a teacher trying to figure out an ideal seating arrangement based on student poll of who they do and don<sq>t want to sit with."
            }, 
            "id": "d75x08v"
        }, 
        {
            "body": {
                "answer": "<p>[deleted]</p>", 
                "question": "I<sq>m a teacher trying to figure out an ideal seating arrangement based on student poll of who they do and don<sq>t want to sit with."
            }, 
            "id": "d75wr5t"
        }, 
        {
            "body": {
                "answer": "<p>I<sq>m aware of some algorithms that solve similar problems. You could look at [min cuts](https<colon>//en.wikipedia.org/wiki/Minimum_cut), but they won<sq>t be guaranteed to be the right size components (groups of students) for your tables.<br><br>My personal favorite approach would be to assign a score to every pair of students representing how much they want to be together. Randomly assign the students and then iteratively move students *away* from the students they least want to be with (note that if they are indifferent, this means they want less to be with that person than someone they want to be near). How you weight the relative scores is a matter of opinion, but if you continue to repeat the step of moving a student away from the person they don<sq>t want to be next to, the system should converge to a solution that is at least locally optimal.<br><br>Here<sq>s a basic overview. If we define f(A,B) to be how much student A does *not* want to be with student B, then<colon><br><br>1. Randomly assign students to tables.<br>2. Pick a student X and move them to the table that has the lowest total of summing f(X,Y) over all Y who are already at that table.<br>3. Now you have one extra student at that table. Pick the student who is least happy at that table and repeat step 2.<br><br>Keep going until the total dislike score isn<sq>t changing much.<br><br>Note that moving students *away* from who they don<sq>t like is key, since it fills all the tables. Moving *toward* who the like would tend to clump everything together.</p>", 
                "question": "I<sq>m a teacher trying to figure out an ideal seating arrangement based on student poll of who they do and don<sq>t want to sit with."
            }, 
            "id": "d75wwjb"
        }, 
        {
            "body": {
                "answer": "<p>Is there anything stopping you from just letting them sit where they want to?</p>", 
                "question": "I<sq>m a teacher trying to figure out an ideal seating arrangement based on student poll of who they do and don<sq>t want to sit with."
            }, 
            "id": "d76c9r6"
        }, 
        {
            "body": {
                "answer": "<p>There are free and low cost courses on R offered on coursera.  </p>", 
                "question": "I haven<sq>t taken any Statistics classes or done any Statistical Analysis in 2-3 years... How can I brush up on it?"
            }, 
            "id": "d64ukaz"
        }, 
        {
            "body": {
                "answer": "<p>Or pick up an old textbook and do the problems. I<sq>m going through An Introduction to Probability Theory andbStatisticsll Inference by Roussas!</p>", 
                "question": "I haven<sq>t taken any Statistics classes or done any Statistical Analysis in 2-3 years... How can I brush up on it?"
            }, 
            "id": "d65b8wp"
        }, 
        {
            "body": {
                "answer": "<p>RStudio is also an easier environment to code R.</p>", 
                "question": "I haven<sq>t taken any Statistics classes or done any Statistical Analysis in 2-3 years... How can I brush up on it?"
            }, 
            "id": "d65ggt5"
        }, 
        {
            "body": {
                "answer": "<p>As long as you are in that 3.4 and above range it will be much lower on the list differentiating you from other candidates fighting for funding. If you are not worried about funding and are not applying to one of the top 4-5 programs in the country it wont matter even a little bit. <br><br>Fighting for funding, things that will matter more are publications, general research, awards and letters of recommendation. If all of those things are equal between you and another candidate your GREs and GPA may be a tie breaker as to who gets funded.<br></p>", 
                "question": "What is a decent GPA for a decent master<sq>s program in Statistics?"
            }, 
            "id": "d5ubboh"
        }, 
        {
            "body": {
                "answer": "<p>I<sq>d recommend using a [Wilson confidence interval](https<colon>//en.wikipedia.org/wiki/Binomial_proportion_confidence_interval#Wilson_score_interval). It has been shown to exhibit good properties, even when samples are small.<br><br>If you don<sq>t want to code the formula yourself<colon> [Here](http<colon>//epitools.ausvet.com.au/content.php?page=CIProportion) is an online calculator that performs the necessary calculations.</p>", 
                "question": "Confidence Intervals"
            }, 
            "id": "d4riuxc"
        }, 
        {
            "body": {
                "answer": "<p>Are you using observations to track the item drops? Do you have that data set already or is this for the planning phase?</p>", 
                "question": "Confidence Intervals"
            }, 
            "id": "d4rb532"
        }, 
        {
            "body": {
                "answer": "<p>https<colon>//en.wikipedia.org/wiki/Binomial_proportion_confidence_interval</p>", 
                "question": "Confidence Intervals"
            }, 
            "id": "d4s1n5k"
        }, 
        {
            "body": {
                "answer": "<p>Not sure this is appropriate for a 5-year-old, but it<sq>s a simple example for illustrating the concept, so maybe it will help<colon><br><br>Say you have a coin and you<sq>re not sure it<sq>s <dq>fair.<dq>  So you want to estimate the <dq>true<dq> probability it will come up heads.  Call this probability P, and code the outcome of a coin flip as 1 if it<sq>s heads and 0 if it<sq>s tails.<br><br>You flip the coin four times and get 1, 0, 0, 0 (i.e., 1 heads and 3 tails).  What is the *likelihood* that you would get these outcomes, *given P*?  Well, the probability of heads is P, as we defined it above.  That means the probability of tails is (1 - P).  So the probability of 1 heads and 3 tails is P * (1 - P)^3 [Edit<colon> We call this the <dq>likelihood<dq> of the data].<br><br>If we <dq>guess<dq> that the coin is fair, that<sq>s saying P = 0.5, so the likelihood of the data is L = .5 * (1 - .5)^3 = .0625.<br><br>What if we guess that P = 0.45?  Then L = .45 * (1 - .45)^3 = ~.075.  So P = 0.45 is actually a better estimate than P = 0.5, because the data are <dq>more likely<dq> to have occurred if P = 0.45 than if P = 0.5.<br><br>At P = 0.4, the likelihood is 0.4 * (1 - 0.4)^3 = .0864.<br><br>At P = 0.35, the likelihood is 0.35 * (1 - 0.35)^3 = .096.<br><br>In this case, it turns out that the value of P that *maximizes* the likelihood is P = 0.25.  So that<sq>s our <dq>maximum likelihood<dq> estimate for P.<br><br>In practice, max likelihood is harder to estimate than this (with predictors and various assumptions about the distribution of the data and error terms), but that<sq>s the basic concept behind it.</p>", 
                "question": "ELI5. Maximum Likelihood and REML."
            }, 
            "id": "d3y3zaw"
        }, 
        {
            "body": {
                "answer": "<p>Maximum likelihood estimation is a technique for estimating things like the mean and the variance of a data set. It relies on calculus functions to estimate these parameters from a data set with unknowns about the probability distribution. <br><br>If you want to know more than that you<sq>re going to struggle to get an ELI5, considering it relies on you understanding not insignificant amounts of algebra and calculus to be able to do. It<sq>s generally taught in the senior years of statistics degrees and sometimes only in post-grad (depending on how applied the undergraduate statistics program<sq>s focus is).</p>", 
                "question": "ELI5. Maximum Likelihood and REML."
            }, 
            "id": "d3xvmre"
        }, 
        {
            "body": {
                "answer": "<p>That<sq>s... probably not something that can be explained to a 5 year old who doesn<sq>t already understand linear estimation.</p>", 
                "question": "ELI5. Maximum Likelihood and REML."
            }, 
            "id": "d3xq5mm"
        }, 
        {
            "body": {
                "answer": "<p>Let<sq>s say you<sq>re talking about the means of two populations. Let<sq>s say, height by sex.<br><br>You can always estimate the group means, even if you have no data. I say whatever you<sq>re estimating, my estimate is -12 for Group A and 16 for Group B. Obviously, these are nonsensical estimates in most situations, and I don<sq>t even have an idea of how uncertain my estimates should be.<br><br>If you give me one piece of data from each group, then I can probably estimate the mean a little better by just saying - ok, Bob<sq>s height is 6<sq>2<dq>, so I<sq>ll estimate the male group mean at 6<sq>2<dq>. At the _very_ least, you know this is a permissible value. I<sq>m no longer guessing a negative height. Susan<sq>s height is 6<sq>3<dq>, so I estimate the mean for women is 6<sq>3<dq>. Still, though, I have no idea whether these two differ significantly. Perhaps women regularly range from 1<dq> to 112<sq>2<dq> and Susan is tiny. Who knows.<br><br>If I have a little more data (at least two per group) I can estimate the group variances, and can begin discussing whether the means differ significantly. I<sq>ll have better estimates of the variances of height by sex, and I can do something like a t-test. As the proportion of data I have grows, nothing really changes qualitatively. I have a better estimate of the means and their uncertainties. If they truly do differ, having more data (usually) makes this clearer. But I<sq>m doing the same test every time, and I reject or fail to reject the same hypothesis every time.<br><br>The closer I get to having all the data, the smaller the uncertainty in my estimate of the means, but I<sq>m still doing the same test. There<sq>s never a point at which a switch is flipped and it becomes a different statistical problem - until I have _all_ of the data. The uncertainty at that point becomes 0, because I can just calculate the population statistic directly. It<sq>s no longer a question of uncertainty. They differ or they don<sq>t.<br><br>(This is all with caveats. There is measurement error. No, Susan isn<sq>t 6<sq>3<dq>. She<sq>s actually 6<sq>3.0001231<dq>. And the assumptions of your tests may not be justifiable if you have too ~~little~~ few data. Something may look like a normal distribution when you have a handful of data, but the picture may become clearer with more data. Etc. Etc.)<br><br>But, tl;dr - there are no real cutoffs unless you have 0, 1, 2, or all the pieces of data. Everything between is on a continuum of uncertainty, and there are no natural points at which you<sq>d begin doing things differently.</p>", 
                "question": "Is statistical significance an appropriate concern for census data?"
            }, 
            "id": "d3wxfrk"
        }, 
        {
            "body": {
                "answer": "<p>This is called a multiple membership model. It<sq>s fiddly, but you can fit these models using lmer() - use your favorite search engine and you<sq>ll find advice. </p>", 
                "question": "Cases (lvl1) belong to more than one group (lvl2) in a Mixed Linear Model"
            }, 
            "id": "d3olzx5"
        }, 
        {
            "body": {
                "answer": "<p>Okay, wrote a quick function to do this. There<sq>s probably a better way to do this but I wrote it pretty quickly. To get the results just do something like mean(bern()) and it will show the estimated results. I ran it and came up with .5047. Didn<sq>t do the actual math to double check this though.<br><br>    bern <- function(reps=10000,n = 5){<br>      outcome = numeric(reps)<br>      for(i in 1<colon>reps){<br>    results = numeric(n)<br><br>    a = runif(n)<br>    for(j in 1<colon>n){<br>      if(a[j] < .2){<br>        results[j] = 1<br>      }else if (a[j] < .5){<br>        results[j] = 2<br>      }else{<br>        results[j] = 3<br>      }<br>    }<br>    if(!(is.element(FALSE,c(1,2,3) <percent>in<percent> results))){<br>      outcome[i]=1<br>    }else{<br>      outcome[i]=0<br>    }<br>      }<br>      return(outcome)<br>    } </p>", 
                "question": "How do I simulate non-Bernoulli probability trials in R?"
            }, 
            "id": "cyeoy5h"
        }, 
        {
            "body": {
                "answer": "<p>You can use the multinomial distribution.<br><br>If n = 10000<br><br>    sum(apply(rmultinom(10000, 5, c(.2,.3,.5)), 2, function(i) all(i > 0))) / 10000<br></p>", 
                "question": "How do I simulate non-Bernoulli probability trials in R?"
            }, 
            "id": "cyfiulw"
        }, 
        {
            "body": {
                "answer": "<p>Well, if you only account for the 5 drunk friends, their average risk is the same. But you are somewhat expecting a more constant risk if you let them in 5 cars rather than all in 1. <br><br>As an example, if you say a drunk driver has 10<percent> causing accident, then the 1 car scenario has 10<percent> chance of causing an accident with 5 casualities, and 90<percent> chances to bring everyone home safe.<br><br>While the 5 cars scenario has only 0.001<percent> chances to get the 5 friends into an accident each (5 casualities), but it also has only 59.049<percent> to bring all 5 friends home safe. In 40.051<percent> of the cases, at least 1 accident will happen. <br><br>The Average (expected value) is the exact same in both scenario, but the risk is different.<br><br>However, as if you factor in that each accident might also injure walkers, other vehicule, etc. You can see that the 5 car scenario is worst, since you can have up to 5 different accidents, each can affect other people. While the 1 car scenario cannot make more than 1 accident.</p>", 
                "question": "A random thought I had curious about what the pros would say"
            }, 
            "id": "cx6th7u"
        }, 
        {
            "body": {
                "answer": "<p>It shouldn<sq>t matter, the odds of death should be the same either way. This assumes that they all have equal odds of crashing a car (i.e. one of them is not drunker than the others) AND that they are equally likely to die when driving as when sitting in the passenger seat.  </p>", 
                "question": "A random thought I had curious about what the pros would say"
            }, 
            "id": "cx66ngz"
        }, 
        {
            "body": {
                "answer": "<p>Yes. Look at <dq>Akaike weights<dq> and the AICtab() function in the AICcmodavg package for R. This paper explains the idea nicely<colon> http<colon>//www.ejwagenmakers.com/2004/aic.pdf</p>", 
                "question": "AIC comparison - AIC is often used to compare models but is there a meaningful difference that can distinguish between models?"
            }, 
            "id": "cvlwd6e"
        }, 
        {
            "body": {
                "answer": "<p>I think most applied statisticians will admit that there<sq>s an <dq>art<dq> to cleaning data that relies a lot on experience and domain knowledge to avoid screwing up the results. You<sq>re absolutely right that these kinds of judgements calls open the door to inappropriately massaging the data (whether maliciously or unintentionally). On the other hand, these kinds of techniques exist to address the practical problems of messy data. In many cases doing nothing may be just as risky as removing an outlier or imputing a missing value. <br><br>Having a set of standardized data-cleaning procedures at least makes it possible to study their impact in different scenarios (when do they bias the results, when do they make results more reliable, what<sq>s the most effective way to define outliers to exclude, etc). It also allows statisticians to establish a standard for data cleaning before they<sq>ve even touched the data, reducing the risk of unintentionally rigging the data.<br><br>It<sq>s good to be suspicious, and good to keep an eye out for fishing cleaning procedures. A good statistician should be able and willing to explain what cleaning was done and why they selected the techniques they used. In some cases, a sensitivity analysis showing how much a different cleaning procedure would have changed the results may be appropriate. Often the difference is negligible.<br><br>Besides if someone really wants to rig the data, it<sq>s [way](http<colon>//www.nature.com/news/2011/111101/full/479015a.html) more [efficient](http<colon>//retractionwatch.com/2011/04/26/data-fraud-at-emory-leads-to-retractions-of-three-cardiology-papers/) to [fake](http<colon>//nymag.com/scienceofus/2015/05/how-a-grad-student-uncovered-a-huge-fraud.html) the [data](http<colon>//retractionwatch.com/2014/07/22/accounting-professor-faked-data-for-two-studies-destroyed-evidence-university-report/) then to tweak it into place with data cleaning.</p>", 
                "question": "The (Fine?) Line Between Cleaning Data and Rigging It"
            }, 
            "id": "cu01wzk"
        }, 
        {
            "body": {
                "answer": "<p>In a world that loves averages and where most of the accessible and accepted methods are centered around this measure, messy data makes for dangerous territory. Especially when you don<sq>t have the luxury and/or budget of collecting enough data for 99<percent> power in your studies. On the one hand, you want to keep data <dq>pure<dq> and unadulterated, and not force it into a distribution that it doesn<sq>t belong to.  On the other hand, sometimes real world sampling includes some skew and ugly values that with some minimal trimming or appropriate transformation can express a normal distribution and homogeneity of variance in the residuals. Since the aim is probably to show mean values anyway, including a few very high or low values in the set or using skewed data mis-represents the mean anyway; trimming or <dq>windsorising<dq> can quiet the loud ones, and better represent central tendency while at the same time reducing noise, while transformation allows the data to be assessed in a normal distribution. This means you allow the statistical methods to better sort through the noise in order to express the signal, showing more realistic effect sizes and reducing error that is exaggerated by poorly representative values.<br><br>I think that /u/dinkum_thinkum said it best<colon> [you need to understand the nature of the data you<sq>re dealing with, and it<sq>s easier to fake the data than to massage it](https<colon>//www.reddit.com/r/AskStatistics/comments/3gnmzv/the_fine_line_between_cleaning_data_and_rigging_it/cu01wzk), at least for large datasets. I mean you could go through individual points and play around with the model until it begins to say something you like, but really, you<sq>re probably faking data at this point anyway, and not very efficiently. I<sq>ve been dealing with some messy (continuous) ecological data recently and found that it often requires log-transformation (because that<sq>s how many natural processes roll) and trimming data to tame a few extreme values. In the end, it<sq>s improved the signal to noise ratio in some parts, and completely lost the signal in others. In all cases, it did so by improving homogeneity of variance, which is the underlying assumption for mixed linear models. I<sq>m making it very clear to the reader that I<sq>ve done this in my methods so there is no question about how I achieved my results. I look to other similar studies with similar statistical power to mine and sometimes see very clean and significant results that I strongly question, knowing the type of heterogeneity there is in those data. I hope that I<sq>m helping to set an example for future authors to be more diligent in their approach and candid in revealing their methods.</p>", 
                "question": "The (Fine?) Line Between Cleaning Data and Rigging It"
            }, 
            "id": "cu0b5e2"
        }, 
        {
            "body": {
                "answer": "<p>Adding to what<sq>s been said already, there is often literally NOTHING you can do if you have missing data. Most software just removes incomplete cases behind-the-scenes. Therefore, it is much better to think about your data correcting procedures than it is to ignore problems. And all options come at the cost of assumptions, there is no 100<percent> right answer.</p>", 
                "question": "The (Fine?) Line Between Cleaning Data and Rigging It"
            }, 
            "id": "cu0jglo"
        }, 
        {
            "body": {
                "answer": "<p>DataCentral<br>Udemy<br>MIT open courseware</p>", 
                "question": "Recommendations and reviews for Intro Statistics MOOC"
            }, 
            "id": "cto4fpl"
        }, 
        {
            "body": {
                "answer": "<p>I think what you want is a logistic regression that controls for participant.  <br><br>The problem with using a X\u00b2 statistic without adjusting for participant is that a single respondent could be skewing results across all of the trials they were involved in, hence any result can<sq>t be generalized to the entire dataset.  <br><br>If you<sq>re dead set on a X\u00b2, run it with a Cochran-Mantel-Haenszel (sp?) test (3-way table) and control IV by DV with participant (20 levels).  A significant result on the CMH statistic would suggest that proportions are different among participants and that you really should model something to control for them.  But even if the result isn<sq>t significant, that<sq>s not conclusive evidence of anything, and the data should still really be modelled to control for participant.  <br><br></p>", 
                "question": "Categorical data but no independence - can I use a Chi-Square?"
            }, 
            "id": "ct5c44b"
        }, 
        {
            "body": {
                "answer": "<p>For each subject you could compute the proportion of times they responded with each response type for each stimulus type. This gives you 9 scores per subject. You could then run a Stimulus Type x Response Type repeated measures ANOVA. The interaction would test whether the proportions of times they responded with each response type depends on stimulus type. </p>", 
                "question": "Categorical data but no independence - can I use a Chi-Square?"
            }, 
            "id": "ct5nx3h"
        }, 
        {
            "body": {
                "answer": "<p>In medicine, this has been well thought out.  We use the Bradford-Hill criteria to determine if the correlation observed could be a causation relationship.<br><br>The list of the criteria is as follows<colon><br><br>* Strength (effect size)<colon> A small association does not mean that there is not a causal effect, though the larger the association, the more likely that it is causal<br>* Consistency (reproducibility)<colon> Consistent findings observed by different persons in different places with different samples strengthens the likelihood of an effect<br>* Specificity<colon> Causation is likely if a very specific population at a specific site and disease with no other likely explanation. The more specific an association between a factor and an effect is, the bigger the probability of a causal relationship<br>* Temporality<colon> The effect has to occur after the cause (and if there is an expected delay between the cause and expected effect, then the effect must occur after that delay)<br>* Biological gradient<colon> Greater exposure should generally lead to greater incidence of the effect. However, in some cases, the mere presence of the factor can trigger the effect. In other cases, an inverse proportion is observed<colon> greater exposure leads to lower incidence<br>* Plausibility<colon> A plausible mechanism between cause and effect is helpful (but Hill noted that knowledge of the mechanism is limited by current knowledge)<br>* Coherence<colon> Coherence between epidemiological and laboratory findings increases the likelihood of an effect. However, Hill noted that <dq>... lack of such [laboratory] evidence cannot nullify the epidemiological effect on associations<dq><br>* Experiment<colon> <dq>Occasionally it is possible to appeal to experimental evidence<dq><br>* Analogy<colon> The effect of similar factors may be considered<br><br>Of course, possible confounders and other explanations need to be rigorously explored, but if all of the above criteria check out there is a high likelihood that the correlation observed represents a causative relationship.<br></p>", 
                "question": "How do we distinguish between correlation and causation?"
            }, 
            "id": "csbxuj3"
        }, 
        {
            "body": {
                "answer": "<p>There are lots of statistical analyses for measuring correlation. Maybe I<sq>m being stubborn and/or old fashioned on this point, but really there are no statistical techniques for causation because causal statements are built on experimental design, theoretical rigor, and critical, scientific thinking. Statistics can show a mathematical relationship between x & y after controlling for z, but that<sq>s insufficient for strong causal statements in the absence of methodological detail, theoretical context, et cetera. Statistical analysis is meant to be part of a compelling scientific argument, not a substitute for critical scientific thinking. I see a lot of students and young researchers forget that, to the detriment of the science. </p>", 
                "question": "How do we distinguish between correlation and causation?"
            }, 
            "id": "csbse51"
        }, 
        {
            "body": {
                "answer": "<p>I<sq>m not sure about experiments of the scale you used in your example, but on smaller scales randomized control trials are the most rigorous methodology for determining causality. <br>  <br>In theory, random assignment avoids confounding effects because the impact of those effects would be equally distributed among the treatment and control groups. Obviously, experiment design and implementation are incredibly important, as bias could be introduced if you don<sq>t have a true random assignment. <br>  <br>There are less rigorous designs like propensity score matching and regression discontinuity, but my experience is only in social science and predominantly with RCTs so I<sq>ll leave it to others to chime in. </p>", 
                "question": "How do we distinguish between correlation and causation?"
            }, 
            "id": "csbs19f"
        }, 
        {
            "body": {
                "answer": "<p>There aren<sq>t any. This all happens at the level of the experimental design. </p>", 
                "question": "How do we distinguish between correlation and causation?"
            }, 
            "id": "csbsp4x"
        }, 
        {
            "body": {
                "answer": "<p>Mean or median? If it<sq>s mean, it shouldn<sq>t (?) be mathematically possible. If it<sq>s median, then it<sq>s possible that, for example, the top quartile of women have a more male partners than the median man and many more than the median woman. </p>", 
                "question": "Assuming equal populations and only heterosexuals is it possible for the average amount of sexual partners to be higher for men than for women?"
            }, 
            "id": "cqh1jn6"
        }, 
        {
            "body": {
                "answer": "<p>This is wrong.  It<sq>s called survivorship bias.  <br><br>hypothetical extreme example<colon> In some universe, assume all men  have visited brothels in Nevada, but the women in other states do not.  <br><br>If something awful happened in Nevada and everyone there died (let<sq>s assume there<sq>s a roughly equal mix of men and women there before the event), you<sq>ll have a case where the living men have a higher arithmetic mean of sexual partners than the living women. <br><br>We don<sq>t include the dead in these sorts of calculations. Hence the name survivorship bias.   <br><br>You could argue that this means sampling is done poorly, but its just not possible to ask a dead person a survey question like this.  (If you follow the linkage, you<sq>d actually have to sample amongst every every person who<sq>s ever existed, not just recent dead.)  <br></p>", 
                "question": "Assuming equal populations and only heterosexuals is it possible for the average amount of sexual partners to be higher for men than for women?"
            }, 
            "id": "cqh6lkp"
        }, 
        {
            "body": {
                "answer": "<p>People aren<sq>t only heterosexual and local populations are often unequal, so the 9/4 is quite plausible in a real-world study.</p>", 
                "question": "Assuming equal populations and only heterosexuals is it possible for the average amount of sexual partners to be higher for men than for women?"
            }, 
            "id": "cqh0xha"
        }, 
        {
            "body": {
                "answer": "<p><sq>Average<sq> can be ambiguous. If they intend <sq>median<sq> then sure it<sq>s possible. <br><br>If they mean <sq>arithmetic mean<sq> then every time a man has sex with a partner he<sq>s never had sex with before, there<sq>s a woman with a new male partner as well.<br><br>If by average, they mean average number of partners per gender then that<sq>s <br><br>total number of pairings/total number in gender<br><br>the numerator must be the same. The denominator may differ - there are more females than males (but not by that much).<br><br>[It<sq>s also possible that somehow <dq>people who never had sex<dq> are being excluded, and then it<sq>s possible for things to be more different.]<br><br>My guess is that it<sq>s based on self-report figures, and that the males are tending to reporting high and the females are tending to report low. They<sq>re not *necessarily* lying; it<sq>s possible there is a tendency for there to be a difference between the two in what they count as sex (which may suggest a problem with the design, if a clear unambiguous explanation of exactly what counts was not given), or it<sq>s possible that recollections tend to work differently -- an <dq>almost<dq> sexual encounter may be recalled differently.<br><br>Self-reports tend to produce such problems, even with no intent to misrepresent (everyone might believe they<sq>re giving a completely accurate account and substantial differences might still occur).<br><br>Now if the sample was *self-selected*, given the different societal attitudes to male and female sexuality (what year is it?), these kinds of problems are almost guaranteed. [Self-selected samples are notorious for inflating such biases -- males with many partners may be much more likely to be motivated to respond, and females  with many partners may be much less likely to respond.]<br><br></p>", 
                "question": "Assuming equal populations and only heterosexuals is it possible for the average amount of sexual partners to be higher for men than for women?"
            }, 
            "id": "cqh2fgu"
        }, 
        {
            "body": {
                "answer": "<p>It seems more straightforward to analyse colours as three-dimensional points than to look at e.g. the green component individually<br><br>If you do want to describe this without prior knowledge though maybe fit a Weibull or Pareto http<colon>//en.wikipedia.org/wiki/Weibull_distribution#Maximum_likelihood</p>", 
                "question": "What statistical model fits this scenario?"
            }, 
            "id": "cpgoruy"
        }, 
        {
            "body": {
                "answer": "<p>You may be interested in the following links (I<sq>ll come back and try to give an explanation in my own words later)<colon><br><br>https<colon>//stat.ethz.ch/pipermail/r-sig-mixed-models/2008q2/000904.html<br><br>https<colon>//stat.ethz.ch/pipermail/r-sig-mixed-models/2014q1/021561.html<br><br>https<colon>//stat.ethz.ch/pipermail/r-sig-mixed-models/2011q1/015522.html<br><br>http<colon>//psy-ed.wikidot.com/glmm<br><br>http<colon>//glmm.wikidot.com/faq<br><br>http<colon>//stats.stackexchange.com/questions/118416/getting-p-value-with-mixed-effect-with-lme4-package<br><br>http<colon>//stats.stackexchange.com/questions/63464/is-this-an-acceptable-way-to-analyse-mixed-effect-models-with-lme4-in-r<br><br>http<colon>//stats.stackexchange.com/questions/94888/p-value-for-interaction-term-in-mixed-effects-models-using-lme4<br><br>http<colon>//blog.lib.umn.edu/moor0554/canoemoore/2010/09/lmer_p-values_lrt.html</p>", 
                "question": "linear mixed effect models and p-values [R]"
            }, 
            "id": "coyahlg"
        }, 
        {
            "body": {
                "answer": "<p><br>I am assuming you are using lme4. You could just use nlme instead, which provides p-values. <br><br>Or you could try one of the solutions here<colon> http<colon>//mindingthebrain.blogspot.com/2014/02/three-ways-to-get-parameter-specific-p.html . <br><br>I hate it when people tell me to RTFM, but it turns out that one of the best explanations for how to get p-values is the lme4 documentation (see page 77) <colon> http<colon>//cran.r-project.org/web/packages/lme4/lme4.pdf<br></p>", 
                "question": "linear mixed effect models and p-values [R]"
            }, 
            "id": "coy6pz7"
        }, 
        {
            "body": {
                "answer": "<p>Load the package lmerTest and re-run the model. It will give you estimated p values as long as you don<sq>t run generalized mixed effect models. </p>", 
                "question": "linear mixed effect models and p-values [R]"
            }, 
            "id": "coyf2s2"
        }, 
        {
            "body": {
                "answer": "<p>A common practice is to assume that a |t| > 2 value for a fixed effect would result in significance (although this may not pertain to your data). From Baayen et al, 2008<colon><br><dq>For data sets characteristic for studies of memory and<br>language, which typically comprise many hundreds or thousands<br>of observations, the particular value of the number of<br>degrees of freedom is not much of an issue. Whereas the<br>difference between 12 and 15 degrees of freedom may have<br>important consequences for the evaluation of significance<br>associated with a t statistic obtained for a small data set, the<br>difference between 612 and 615 degrees of freedom has no<br>noticeable consequences. For such large numbers of degrees of<br>freedom, the t distribution has converged, for all practical<br>purposes, to the standard normal distribution. For large data<br>sets, significance at the 5<percent> level in a two-tailed test for the fixed<br>effects coefficients can therefore be gauged informally by<br>checking the summary for whether the absolute value of the<br>t-statistic exceeds 2.<dq><br><br>Link to article<colon><br>http<colon>//webcom.upmf-grenoble.fr/LIP/Perso/DMuller/M2R/R_et_Mixed/documents/Baayen-2008-JML.pdf<br><br></p>", 
                "question": "linear mixed effect models and p-values [R]"
            }, 
            "id": "coyfjpz"
        }, 
        {
            "body": {
                "answer": "<p>I<sq>m in a very similar position, I stumbled into a quantitative background while working on a neuroscience PhD and I<sq>ve been consulting based on that for about a year while finishing up. I have no idea if I<sq>m doing things correctly, but I<sq>ve established some consistent clients and have dramatically increased my standard of living. I<sq>m happy to share my experience, *caveat emptor*.<br><br>* Get a contract in place before anything starts. You probably want to start with a boilerplate consulting agreement (google it) and tailor it to fit what you<sq>re doing. You could (and maybe should) involve a lawyer for this, but I<sq>ve always written mine in plain language and have yet to have one problem. I think of the contract as a way of outlining everyone<sq>s expectations. That way, if problems do arise, you can get some cover by pointing back to the consulting agreement. That said, follow your consulting agreement, or modify it as the situation changes. Use the agreement to get specific expectations for yourself and for your clients throughout the process.<br><br>* Your proposed analysis should probably come as a statement of work, which might be separate from the consulting agreement (if it<sq>s just a one-off thing) or might be created as a first step in the analysis. My preference has been to get a contract in place, figure out what their questions are, take a first pass at the data, and then check back in and let the client know what I plan to do with a statement of work. YMMV.<br><br>* I work in R, so I generally write a Sweave or RMarkdown file. The client gets either .html or .pdf output at the end, unless they<sq>ve specifically contracted for my source code. That<sq>s worth bringing up in the consulting agreement. I charge extra when people want the source, (which is usually necessary for scientific publications, or should be). Remember that you have to document code for other people, so budget extra time and give them nice code if you<sq>re charging them for it.<br><br>* Rarely are both forms of compensation appropriate. If you<sq>re a co-author, you<sq>re expected to do the work for free like everyone else. I think this aspect of academia is weird, but that<sq>s why I<sq>ve gotten into consulting. I have opted for co-authorship on a few occasions, but only when the paper was close enough to my sub-field that it mattered to me. If I end up in academia, my tenure committee won<sq>t care about work I<sq>ve co-authored that<sq>s too far from my specialty area, (is what I<sq>m told, anyway).<br><br>* I don<sq>t carry professional insurance, but I do operate an LLC as a sole proprietorship. As long as you<sq>re working in the state where the LLC exists, this works as a liability shield for most things that are not gross negligence, (is what I<sq>ve been told, IANAL). If I were doing more specialized work, (e.g., clinical trials), I would certainly carry insurance for that specific activity.<br><br>* The client might have some input here, but the big question is whether to charge a flat  or hourly fee. Flat fees are nice because, if you<sq>re efficient, you can get a great hourly rate equivalent. They also mean that, if you make a mistake, you<sq>re not getting paid any more to fix it. I have always worked hourly, usually with the understanding that if I<sq>m going over my proposed hours by more than some amount of time, (e.g., 10 hours, 10<percent> of the projected project time, etc.), I<sq>ll notify the client. Your time and skills are worth whatever someone is willing to pay for them. You should do some research on statistical consulting, figure out what the usual range is for your experience and credentials, and ask for a little more than that. Don<sq>t sell yourself short; just because you make $20/hour as a graduate assistant doesn<sq>t mean $40/hour is big money.<br><br>My only other advice is to try not to let the business end of things get in the way of the work. They want some analysis done and you want to get paid. Everything else is just support for that exchange relationship.</p>", 
                "question": "Advice for conducting data analysis for someone else on a freelance basis?"
            }, 
            "id": "cokfrqw"
        }, 
        {
            "body": {
                "answer": "<p>jchrszcz has good advice to share that<sq>s based on his or her experience.<br><br>If you want to get into consulting, it makes sense to do this project even if you<sq>re not happy with compensation. It<sq>s worth it to be able to say later on that you have this type of experience.<br><br>Monetary compensation and being named as a co-author are not mutually exclusive. An signed agreement upfront is a must if you expect to get paid. It also prevents all kinds of misunderstandings.  Insurance is good, but very few people buy insurance for occasional projects that only take two weeks. It<sq>s expensive. Contract clause with release of liability helps here. It<sq>s also customary to have disclaimers everywhere and to point out that your analysis is based on their data, their explanations of the methods used, mutually agreed approach to statistical analysis and that you<sq>re submitting it for their review. A proposal upfront is best because it gives them a chance to agree or disagree with it. It identifies the scope of work and can be incorporated in the contract by reference. When they agree to the proposal, there is shared responsibility when it turns out it<sq>s not as simple as it appeared or something is very wrong with the data. These projects rarely go as expected and usually take many more hours than anybody expects. Having the scope identified clearly helps when there are changes and extra work. You have to decide whether you show the number of hours and your hourly rate or go with the total. In the latter case it<sq>s even more important to be clear on the scope, what you<sq>ll do and what they<sq>ll do. When it doesn<sq>t go as planned and there is work outside the scope, it<sq>s extra. You may decide to be nice and do extra work for free, but it shouldn<sq>t be expected of you.</p>", 
                "question": "Advice for conducting data analysis for someone else on a freelance basis?"
            }, 
            "id": "comn674"
        }, 
        {
            "body": {
                "answer": "<p>Here is what I suggest you <colon><br><br>Let<sq>s say you play a game of jenga with N friends (N+1 total person). There is 1 looser and N <dq>winners<dq>.<br><br>The looser gets 0 point. The other players (who do not loose) gets 1/N points. <br><br>This means that every game 1 point gets distribute among the players, whatever their number.<br><br> Winning a 1v1 game gives you 1 point while winning a 5-players game gives you 1/4 point.<br><br>You also calculate 1/N game played for everyone, either they win or loose.<br><br>You then keep track of the cummulative winning points over the cummulative games played.<br><br>Example <colon> I won 2x 3-players games, lost 1x a 1v1, then played 3x 4-players games where I lost 1 one them.<br><br>Winning points <colon><br><br>* 2 x (1/2) + 2 x (1/3) = 5/3<br><br>Played game <colon><br><br>* 2 x (1/2) + 1 x (1/1) + 3 x (1/3) = 3<br><br>I would have  a winning ratio of <colon><br><br>* (5/3) / 3 = 5/9 = ~56<percent></p>", 
                "question": "How should I keep track of Jenga wins and losses?"
            }, 
            "id": "cobb91o"
        }, 
        {
            "body": {
                "answer": "<p>What they are talking about is how would you like you distances calculated? Normally, euclidean distance is ((x2-x1)^2 +(y2-y1)^2 )^1/2 <br>But, instead of raising to the power of 2 and the raising to the 1/2 you can more generally raise to the power of p then raise to the power of 1/p.  Read up on Lp norms, e.g. here<colon> http<colon>//en.wikipedia.org/wiki/Norm_<percent>28mathematics<percent>29#Euclidean_norm<br><br>*edit<colon> Better link for understanding.<br>I<sq>d be happy to provide more explanation or context later if you like, had to run to class!</p>", 
                "question": "Effect of <dq>exponent of euclidean distance<dq> in energy statistic / distance correlation."
            }, 
            "id": "co4p5x9"
        }, 
        {
            "body": {
                "answer": "<p>All of this advice so far is bad.  It<sq>s not bad because GLMs, Bayesian estimators, or SEM aren<sq>t all worth knowing.  It<sq>s not bad because these folks aren<sq>t smart and possibly right.  It<sq>s bad because you came and said you feel like you only have a hammer, and people responded by shouting out a bunch of other tools you ought to have.  Sure, you should possibly have those tools, but it really depends on whether you<sq>re a plumber or a cabinet-maker.  It<sq>s possible (though perhaps not likely) that linear models are in fact the right tool for what you<sq>re up to.  But without knowing what indeed you<sq>re up to, it<sq>s wasting your time to suggest you go and learn new method x, when you really need new method y.  Ya dig?</p>", 
                "question": "Beyond linear regression?"
            }, 
            "id": "cm03ijz"
        }, 
        {
            "body": {
                "answer": "<p>The handbook of structural equation modeling edited by Hoyle is a really great resource, especially if you have the mathematics background. Also, Bayesian statistics for the social sciences by David Kaplan.<br><br>[Kaplan](http<colon>//www.amazon.com/Bayesian-Statistics-Social-Sciences-Methodology/dp/1462516513/ref=sr_1_3?ie=UTF8&qid=1415754701&sr=8-3&keywords=David+Kaplan)<br><br>[Hoyle](http<colon>//www.amazon.com/Handbook-Structural-Equation-Modeling-Hoyle/dp/1462516793/ref=sr_1_1?ie=UTF8&qid=1415754764&sr=8-1&keywords=Handbook+of+structural+equation+modeling)<br><br></p>", 
                "question": "Beyond linear regression?"
            }, 
            "id": "clzxy50"
        }, 
        {
            "body": {
                "answer": "<p>I<sq>d suggest learning GLMs, possibly nonlinear regression, possibly mixed models (if you don<sq>t have them already), and some basic time series/forecasting to start with<br><br>On the last topic, [this free text](https<colon>//www.otexts.org/fpp/) is a pretty decent resource. <br><br></p>", 
                "question": "Beyond linear regression?"
            }, 
            "id": "clzzibo"
        }, 
        {
            "body": {
                "answer": "<p>A good multivariate course/text would do you some good as well. Getting down MANOVA, SEM, logistic regression and discriminant analysis, and a few more similar techniques would open up the possibilities. <br><br>Also, if you can do regression you could teach yourself mediation and moderation in an afternoon, and those open things up a lot when you<sq>re trying to model things. </p>", 
                "question": "Beyond linear regression?"
            }, 
            "id": "cm038e9"
        }, 
        {
            "body": {
                "answer": "<p>I would think Think Bayes would be perfectly suited to your python skills.<br><br>http<colon>//www.greenteapress.com/thinkbayes/</p>", 
                "question": "Book for Bayesian analysis self-study"
            }, 
            "id": "cj7yxwf"
        }, 
        {
            "body": {
                "answer": "<p>> Just based on my intuition, I wouldn<sq>t think the estimators are unbiased<br><br>Which estimators? Certainly the ecdf is [an unbiased estimate of the cdf](http<colon>//en.wikipedia.org/wiki/Empirical_distribution_function#Definition)<br><br>If you use bootstrapping to estimate (and get some standard error for) some quantities, that bootstrap estimate won<sq>t necessarily be unbiased, but *biased* estimators are certainly <sq>okay<sq> in general. Maximum likelihood estimators are very popular, and they<sq>re not generally unbiased.<br><br>Why would bias inherently rule an estimator out? Some unbiased estimators are terrible and some biased estimators are very good (have relatively low mean square error, for example). Indeed, in some situations, no unbiased estimators exist.<br><br>One nice thing about the bootstrap is there are ways to estimate and adjust for bias.</p>", 
                "question": "Can someone explain to me why bootstrapping works?"
            }, 
            "id": "cfrer5n"
        }, 
        {
            "body": {
                "answer": "<p>[this might be a useful read](http<colon>//www.nature.com/nrn/journal/v14/n5/abs/nrn3475.html)</p>", 
                "question": "Does low power in a statistical analysis leads to false positive?"
            }, 
            "id": "cbhnpg4"
        }, 
        {
            "body": {
                "answer": "<p>The issue is that, if your power is low, then conditional on rejecting the null, it<sq>s probably a false positive, because the probability of it being a true positive is low. </p>", 
                "question": "Does low power in a statistical analysis leads to false positive?"
            }, 
            "id": "cbhtka5"
        }, 
        {
            "body": {
                "answer": "<p>It shouldn<sq>t. If you do the test at a 95<percent> significance level, you have a 5<percent> probability of making a false positive while, in principle, the probability of a true positive (i.e. the power) can be anything. Recall that the p-value is uniform under the null hypothesis while skewed toward zero under the alternative. So, when there<sq>s no effect, the probability of commiting a false positive is simply 1 - the significance level. However, don<sq>t forget that that the power is highly dependent on the true effect size, the sample size, and the significance level. As you choose a significance level closer to 1 the power goes to zero and vice versa.<br><br>But I don<sq>t think the authors are really suggesting what you say if you read it carefully. I note that they say <dq>as we explain below<dq>.</p>", 
                "question": "Does low power in a statistical analysis leads to false positive?"
            }, 
            "id": "cbhm570"
        }, 
        {
            "body": {
                "answer": "<p>Yes; power is the probability of rejecting the null, when the null is false.<br><br>It is the complement of a Type II error, so the power is equal to <br><br>*p = 1 - \u03b2*<br><br>But the overall point of the paragraph is valid, that is, having knowledge of negative results is very important, especially for reproducible research.</p>", 
                "question": "Does low power in a statistical analysis leads to false positive?"
            }, 
            "id": "cbhm8u1"
        }, 
        {
            "body": {
                "answer": "<p>> In what degree can statistical measures be applied when analysing and presenting the analysis of BIG DATA?<br><br>Statistical analysis is a basic tool to extract meaning from data. As soon as you calculate an <dq>average<dq>, you<sq>ve entered the realm of statistics. So answer<colon> always. <br><br>>Are there other methods/tools apart from statistical/econometric?<br><br>Yes, many. One really popular method<colon> visually present data (charts, graphs) and allow human brains to apply their amazing power to discern patterns and relationships.<br><br>Other methods/tools include machine learning techniques, compression, filtering, outlier alerts, sampling, etc.</p>", 
                "question": "How big data is being analysed?"
            }, 
            "id": "ca5jmmw"
        }, 
        {
            "body": {
                "answer": "<p>It is sometimes very hard to tell. Part of the reason for this is that many applied papers may have good statisticians working on them, but control over the manuscript resides with people with subject matter expertise, so the explanation of the method may be expressed very imprecisely making it hard to work out what was really done.</p>", 
                "question": "How can I learn to immediately filter out papers that have a poor statistical foundation regardless of the paper subject?"
            }, 
            "id": "c6rtbyd"
        }, 
        {
            "body": {
                "answer": "<p>One red flag to look for, read the abstract.  Read the conclusion/discussion section.  Then look at the results section.  Do the results actually support the conclusions?  Can you TELL if they do?  If not ....<br><br> I actually jumped from ecology to statistics for this very reason - the ecological literature was terrible with this...</p>", 
                "question": "How can I learn to immediately filter out papers that have a poor statistical foundation regardless of the paper subject?"
            }, 
            "id": "c6sabdh"
        }, 
        {
            "body": {
                "answer": "<p>The question is confusingly written. So there are three treatments and six dependent variables? </p>", 
                "question": "[Stats help for MSc student] My girlfriend laughed at me when I asked her that she should seek Reddit for help on her data analysis - let<sq>s prove her wrong"
            }, 
            "id": "c5vn4ql"
        }, 
        {
            "body": {
                "answer": "<p>What does the Box<sq>s test tell you about cell sizes? What are your cell sizes?<br><br>Homogeneity of variance is rarely relevant if you have very different cell sizes.<br><br>I<sq>m never a big fan of manova anyway - if your questions really are multivariate, then it<sq>s OK, but they usually aren<sq>t (and I don<sq>t think they are in a situation like this).  <br><br>It<sq>s rare to correct for multiple tests when you have multiple outcome variables - the tests are correlated, so you cannot do the usual Bonferroni correction.  </p>", 
                "question": "[Stats help for MSc student] My girlfriend laughed at me when I asked her that she should seek Reddit for help on her data analysis - let<sq>s prove her wrong"
            }, 
            "id": "c6tobnv"
        }, 
        {
            "body": {
                "answer": "<p>I too was quite bad at this when we had this course on college, I think what I was doing wrong was that every exercise we had on Bayesian I tried to map to the sentence <dq>What is the probability of A given B<dq> which then maps directly to the P(A|B) formula. This was easy because many of the exercises are told in this form, but unfortunately at the time, it meant for me that I have the result and I didn<sq>t need to think about it anymore.<br><br><br>What I would suggest (what I did and think that helped me) is to sit down, try to solve some of those exercises but try not to use the above mapping. Even try to forget the whole Bayes formula, try to think about what sets you have at hand and eventually you<sq>ll derive the formula by yourself (which I think is the most enlightening part of it). Venn diagrams should help you greatly.<br><br><br>Some exercises that seem to be good starters<colon><br><br>  http<colon>//www.math.hmc.edu/funfacts/ffiles/30002.6.shtml<br><br>  http<colon>//plato.stanford.edu/entries/bayes-theorem/supplement.html<br><br><br>When you have the formula, try to play with it. E.g. try to think what happens to it when A and B are independent variables. Or what can you do to the formula by observing that P(A intersection B) is the same as P(B intersection A)...<br><br><br>As is mentioned in the wikipedia article that cuginhamer mentioned in his comment, Bayesian probability is just an <dq>interpretation<dq> and IMHO, it is only after you know this formula through you can start thinking about what its interpretations are.<br><br></p>", 
                "question": "Bayesian theory"
            }, 
            "id": "c5t46vd"
        }, 
        {
            "body": {
                "answer": "<p>Congratulations to being in a programme that acknowledges Bayesian theory! It is the correct way to view things, has been my conclusion.</p>", 
                "question": "Bayesian theory"
            }, 
            "id": "c66vqrb"
        }, 
        {
            "body": {
                "answer": "<p>The first paragraph here is the most succinct way to put it I think<colon> http<colon>//en.wikipedia.org/wiki/Bayesian_probability  Follow up questions?</p>", 
                "question": "Bayesian theory"
            }, 
            "id": "c5qj46g"
        }, 
        {
            "body": {
                "answer": "<p>You can think of Baye\u2019s theorem as a given occurrence over all of its possibilities. For instance<colon><br><br>P(Heart disease| (given) They\u2019re a smoker}= P(Heart Disease and a smoker)/P(smoker)=P(Heart Disease and a smoker)/{P(Heart disease and a smoker)+P(No Heart Disease and a smoker)}<br><br>Notice how P(Smoker) turned into two complementary groups. Of course the probability of a smoker is made up of those with heart disease, and without. That would be all the smokers. This can go on for many different subgroups. So long as they add up to P(Smoker), then that\u2019s fine.<br></p>", 
                "question": "Bayesian theory"
            }, 
            "id": "c5tjf7z"
        }, 
        {
            "body": {
                "answer": "<p>Let<sq>s consider the problem in one dimension. You can do the experiment if you<sq>d like. Toss a coin and step forward for heads and backwards for tails. On average, after n tosses, you<sq>ll end up ~ sqrt(n) steps from where you started. This is the binomial distribution at work.<br><br>Here<sq>s the math. http<colon>//mathworld.wolfram.com/RandomWalk1-Dimensional.html<br><br>Roughly, after many random steps, there are many more ways to end up at a spot different from where you started than there are ways to get back to where you started. So, now the most likely outcome it<sq>s that you<sq>re in a different spot. From here, the initial origin isn<sq>t special, so if you keep random walking, you<sq>ll again end up somewhere else. Keep it up long enough, and that somewhere else will be quite far from where you started.<br></p>", 
                "question": "How do random processes like Brownian motion lead to mixing and diffusion in fluids? I<sq>m under the impression that colloidal molecules are likely to be shoved about in any direction with equal probability so why doesn<sq>t everything cancel out to produce particles that are stationary on average?"
            }, 
            "id": "c57moyv"
        }, 
        {
            "body": {
                "answer": "<p>Focus on one particle. It isn<sq>t locked in place. Water is more like a mosh pit than it is a jam packed elevator. </p>", 
                "question": "How do random processes like Brownian motion lead to mixing and diffusion in fluids? I<sq>m under the impression that colloidal molecules are likely to be shoved about in any direction with equal probability so why doesn<sq>t everything cancel out to produce particles that are stationary on average?"
            }, 
            "id": "c57kmgx"
        }, 
        {
            "body": {
                "answer": "<p>I don<sq>t do anything in fluid dynamics, but do work with stochastic processes a bit. It seems you may be conflating physical Brownian motion (<dq>jiggly pollen grains in a petri dish<dq>) with stochastic Brownian motion (the Wiener process; continuous, stable, normally-distributed paths). The expected value of a one-dimensional Brownian motion with zero drift is 0 (stationary on average). However, if you have replicate sample paths, their expected variance is non-zero, so you<sq>ll get mixing and diffusion in the case of fluid dynamics.</p>", 
                "question": "How do random processes like Brownian motion lead to mixing and diffusion in fluids? I<sq>m under the impression that colloidal molecules are likely to be shoved about in any direction with equal probability so why doesn<sq>t everything cancel out to produce particles that are stationary on average?"
            }, 
            "id": "c57rhsn"
        }, 
        {
            "body": {
                "answer": "<p>Quite frankly, to make the mathematics easier. The independence is assumed to express the simultaneous likelihood as a product of the marginals. That the samples is identically distributed makes the all the marginal likelihoods the same. Almost all <dq>standard<dq> tests rely on the i.i.d. assumption.<br><br>There are lots of examples where the assumptions is perfectly valid, there are lots of examples where it is not quite. Arguably, in such cases it is often the independence that is violated.<br><br>So, the reaseon why you really want your samples to be i.i.d. is to not violate the assumptions of the statistics you are computing.</p>", 
                "question": "Why is it so important that a sample is I.I.D.? (independently and identically distributed)"
            }, 
            "id": "c3w6yis"
        }, 
        {
            "body": {
                "answer": "<p>There are a lot of reasons. An overall gist is that statistics works because observing things provides you a certain quantity of information which you can use to improve your confidence about the answers to some questions.<br><br>IID data means that you<sq>ve assumed that the process generating your data is *very* simple. This means you can ask fewer questions, there are less knobs lying around to wonder about because the distribution is so simplified. It also means that all that information you get from observing the data compounds to make the answers to your questions more accurate.<br><br>Consider independent, not identically distributed. Now you have to guess about the mean and variance of each data point using just itself. Clearly that<sq>s a lot harder (or, just impossible, really) than iid.<br><br>Of course, iid is often violated. If it<sq>s not strongly violated, you assume the correlations and distribution changes are minor compared to the general message you<sq>re learning from your iid assumption. Maybe it is strongly violated, but you can uncover some kind of dependence structure which still keeps down the complexity of your model so that your data can compound to give you accurate answers. Examples of this are common, from Kalman Filters, to Hidden Markov Models, to Hierarchical Bayesian Models.<br><br>At the heart of all of this is the idea you have some amount of information currency (power), bought by observing things. You use it to invest in a certain number of parameters (degrees of freedom, complexity of hypotheses). The more parameters, the less you invest in each one, and the less certain you are of their values.</p>", 
                "question": "Why is it so important that a sample is I.I.D.? (independently and identically distributed)"
            }, 
            "id": "c3w5cp9"
        }, 
        {
            "body": {
                "answer": "<p>Many(most?) of your statistical tests depend upon the [Central Limit Theorem](http<colon>//en.wikipedia.org/wiki/Central_limit_theorem) and the [Law of Large Numbers](http<colon>//en.wikipedia.org/wiki/Law_of_large_numbers).  Both of these theorems require that the random variables be i.i.d., if not, they aren<sq>t valid.<br><br>Consider, for instance, a thousand people choosing their favorite shade of green (which we<sq>ll number 1 to 10).  Each person is a separate random variable.  If we don<sq>t assume that they are i.i.d. then even something as simple as taking the average (expected value) of the group doesn<sq>t make a lot of sense (it would be like summing apples and oranges).<br><br>I<sq>m a little tired right now, but I<sq>d love to go into more detail about this.  So your questions aren<sq>t answered by any of the people who are much brighter than me, please ask some more and I<sq>ll try to get back to you.</p>", 
                "question": "Why is it so important that a sample is I.I.D.? (independently and identically distributed)"
            }, 
            "id": "c3wgd1g"
        }, 
        {
            "body": {
                "answer": "<p>My understanding might not be the most precise and technical. But here<sq>s how I understand this.<br><br>It depends on what type of statistics you are doing. For classical statistical models (e.g., standard t-tests, chi-square, ANOVA, regressions), we basically rely on modeling everything against the normal distribution; or to be more precise, the sum of the variance of the distribution approaches the normal distribution. <br><br>This is partially theoretical; in that all in all, the world is pretty <dq>Normal.<dq> So much of our statistical models (in classical statistics theory anyway) rest upon the very fundamental Central Limit Theorem, which states that the *mean* of a *sufficiently large* number of *independent*, *random* variables will be normally distributed. The further we depart from this the less confident we are in our results.<br><br>Does this help?</p>", 
                "question": "Why is it so important that a sample is I.I.D.? (independently and identically distributed)"
            }, 
            "id": "c3w3jak"
        }, 
        {
            "body": {
                "answer": "<p>Here is a great article detailing those probabilities<colon> http<colon>//www4.stat.ncsu.edu/~jaosborn/research/RISK.pdf<br><br>Basically, if attacker and defender have an equal number of armies, the probability the attacker wins is over 50<percent> as long as both have at least 5 armies. As you approach 30 armies, the probability the attacker wins is almost 70<percent>. At fewer than 5 armies though the attacker is handicapped with probability of winning between .36 and .47. </p>", 
                "question": "How big of an advantage does the attacker have in the board game RISK?"
            }, 
            "id": "c2dr8se"
        }, 
        {
            "body": {
                "answer": "<p>In each roll the attacker seeks to win some number of <dq>points<dq> where a point is awarded for each die that beats the defenders die. Thus, each roll can end in +2, 0, or -2 points corresponding to winning on both dice, an equal trade, or losing on both dice. If the process is fair, then the expected number of points per roll is 0.<br><br>There are 6^5 possible rolls between the two players. If the dice are fair, then it<sq>s easy to compute the expectation<colon> just sum up the number of points earned in every possible roll then divide it by 6^5. It<sq>d be difficult to do this analytically, so I made a Python program.<br><br>    d = xrange(1,7)<br>    points = 0<br>    for arolls in [(a, b, c) for a in d for b in d for c in d]<colon><br>        for brolls in [(a, b) for a in d for b in d]<colon><br>            topa = sorted(arolls, reverse=True)[0<colon>2]<br>            topb = sorted(brolls, reverse=True)<br>            points += sum(map(lambda x, y<colon> x > y and 1 or -1, topa, topb))<br><br>The expected number of points is 1230/7776, approximately 0.16, so the attacker expects to make just under 1 point on every 6 attacks. I never played much Risk, but that seems very even.</p>", 
                "question": "How big of an advantage does the attacker have in the board game RISK?"
            }, 
            "id": "c2dn2mn"
        }, 
        {
            "body": {
                "answer": "<p>Yes but then, in a sense, all of statistics is data reduction. We can<sq>t really make sense of thousands of individual data points, so statistics is a way of compressing that information. The original data may not always be reconstructable, but it<sq>s a signal and noise issue, stats is about filtering the noise and reporting the signal.</p>", 
                "question": "This guy explains kinds of file compression. Can I call a PCA - Principal Components Analysis a lossy data compression?"
            }, 
            "id": "dg72zxc"
        }, 
        {
            "body": {
                "answer": "<p>Yes of course, just google for <dq>PCA image compression<dq> and you should find multiple hits.</p>", 
                "question": "This guy explains kinds of file compression. Can I call a PCA - Principal Components Analysis a lossy data compression?"
            }, 
            "id": "dg7f4fh"
        }, 
        {
            "body": {
                "answer": "<p>If I am technically flattening dimensions of a matrix within certain components, likely PC1 and PC2, with a certain <percent> of data variability explanation contained within, I guess it is just a compression where I lose small amounts of data. <br>Is that correct?</p>", 
                "question": "This guy explains kinds of file compression. Can I call a PCA - Principal Components Analysis a lossy data compression?"
            }, 
            "id": "dg6v82w"
        }, 
        {
            "body": {
                "answer": "<p>I think you<sq>re on to something here.</p>", 
                "question": "This guy explains kinds of file compression. Can I call a PCA - Principal Components Analysis a lossy data compression?"
            }, 
            "id": "dg729d2"
        }, 
        {
            "body": {
                "answer": "<p>Clustering. K-means is a fine place to start.</p>", 
                "question": "Analyzing all the house to bus stop distances in our city. What methods should we use to group districts together?"
            }, 
            "id": "df7fy1l"
        }, 
        {
            "body": {
                "answer": "<p>Let me start off by saying this<colon><br><br>I don<sq>t think there can be an exhaustive list, data science is still developing and evolving so your list will never include everything .<br><br>The list will include way more than frequentist, parametric, null hypothesis significance tests. When you work with weird or large amounts of data, Null hypothesis significance tests can be kinda meaningless.<br><br>That being said, I would try to learn overarching concepts more that specific topics<colon> least squares, central limit theorem, bayes theorem, parallel computing, error variance, basic learning algorithms, boot strapping/simulations...<br><br>The fields of stats, computer science, and physics are all probably good places to start! It<sq>s also going to depend on what you want to do within data science. *\u00af\\_(\u30c4)_/\u00af* the good news is that the world is your oyster, there<sq>s so much data to use and play with, you can do anything with it!<br><br></p>", 
                "question": "What topics in Statistics are important(or widely used) in Datascience?"
            }, 
            "id": "df668g8"
        }, 
        {
            "body": {
                "answer": "<p>As /u/GeTheeAShrubbery said, I don<sq>t think there is an exhaustive list. However I can give you a list of (what I believe to be) the important statistical methods to at least have heard of so you can evaluate whether they would be useful to use with your data.<br><br>Ordinary linear regression, generalized linear model, logistic regression, Lasso and ridge regression, principle components analysis, multi-level (mixed effects) models, generalized additive models, maximum likelihood, splines, machine learning methods (including random forest, neural networks, and support vector machines) kriging, inverse distance weighting, point pattern analysis, spatial/temporal lag regression model, spatial/temporal error regression model<br><br>As I said before, this is not an exhaustive list. There are plenty of things that I am missing. But these are the types of things that I use in my data analysis and that I recommend other people use. Obviously, there are more methods out there than any one person could ever understand. That is why learning the concepts is important. They help you suss out whether any given method is worth your time to learn. The one exception to that being ordinary linear regression. If you can get a rock solid understanding of ordinary linear regression that is going to help you a lot. <br><br><br></p>", 
                "question": "What topics in Statistics are important(or widely used) in Datascience?"
            }, 
            "id": "df76xgn"
        }, 
        {
            "body": {
                "answer": "<p>One thing I haven<sq>t seen mentioned is clustering techniques. Clustering is one of my favorite topics and is extremely relevant in very many data problems!</p>", 
                "question": "What topics in Statistics are important(or widely used) in Datascience?"
            }, 
            "id": "df7a269"
        }, 
        {
            "body": {
                "answer": "<p>It may also be a good idea to make some links in the sidebar with frequently asked questions.  Specifically, both this subreddit and /r/statistics get questions about degrees in stats/classes in stats/going back to school for stats a lot, and it may be worth it to have one enormous list of posts that people have made.  Something like that.</p>", 
                "question": "Adding another mod or two?"
            }, 
            "id": "ddg5ttx"
        }, 
        {
            "body": {
                "answer": "<p>What generally works is to take applications, especially from people who mod other places, and have a chat about style. You may also want to see if any of the /r/statistics mods are interested in double duty because I presume loads on both are light, you know they can be trusted, and this place is not going to be that different. </p>", 
                "question": "Adding another mod or two?"
            }, 
            "id": "ddg1mw2"
        }, 
        {
            "body": {
                "answer": "<p>Right, gonna lecture you first. You need to know how you will analyse data before you collect it. Statistics isn<sq>t just about crunching numbers - that<sq>s almost incidental - it<sq>s about the design and interpretation of experiments (and other studies, but this is an experiment). Sample size is the most important thing here, it drastically affects interpretation. You can do a retrospective power calculation and should be reporting it. You need to interpret p-values carefully, this paper explains why<colon> http<colon>//rsos.royalsocietypublishing.org/content/1/3/140216<br><br>The best way to analyse this is as a prospective meta-analysis. That will analye each group and also give you a pooled estimate which preserves the trial level data. The easiest way to do it is to download [RevMan](http<colon>//community.cochrane.org/tools/review-production-tools/revman-5). It<sq>s designed to manage systematic reviews as well as meta-analyses so you can ignore most of it. It<sq>ll crunch the numbers nicely and produce some pretty plots. The Cochrane Handbook is an excellent resource for understanding some of the (very simple) stats going on.</p>", 
                "question": "Unsure if I am allowed to perform statistical tests like I did"
            }, 
            "id": "dd1170m"
        }, 
        {
            "body": {
                "answer": "<p>I think it would be helpful if you provided links/examples of what you believe is his suspicious statistical reporting.</p>", 
                "question": "Is anyone here familiar with Stefan Molyneux? He throws around a lot of statistics. Is he accurately using or abusing statistics?"
            }, 
            "id": "dcir4uv"
        }, 
        {
            "body": {
                "answer": "<p>Seeing as he shows no commitment to non-statistical sciences ([like elementary biology](https<colon>//youtu.be/eiCCaiG8_4o)), I predict that he would lack rigor in statistics as well. </p>", 
                "question": "Is anyone here familiar with Stefan Molyneux? He throws around a lot of statistics. Is he accurately using or abusing statistics?"
            }, 
            "id": "dcjelgu"
        }, 
        {
            "body": {
                "answer": "<p>It<sq>s non-linear in the sense that the number of standard errors (or whatever is being used to calculate a p-value) does not map linearly onto the p-value. Halving the z-value does not halve the p-value.<br><br>The meaning of a p-value depends heavily on how likely the null hypothesis was to be false in the first place. It is a conditional probability, as is power, so it does not have any meaning in its own right.<br><br>Say we<sq>re testing a new drug. It looks promising in the lab and early non-randomised clinical trials so we design an RCT to have 90<percent> power to detect a 10<percent> difference with p<0.05. We know from similar drug trials that, say, about 10<percent> of new drugs discovered this way turn out to be at least 10<percent> better than control. So 90<percent> of the time we are trying to measure a difference which does not exist and the only positive results will be false positives. 10<percent> of the time there is a real difference and all our positives will be true positives. So in 1000 trials we expect (900\\*0.05=) 45 false positives and (100\\*0.9=) 90 true positives. 1 in 3 of our p<0.05 results are false positives.<br><br>Now suppose that we<sq>re doing the same trial again but this time we<sq>re trying to confirm the positive results of the original trial. Under the previous assumptions, we know that the positive result is a true positive around 2/3 of the time so we expect to get a second true positive for around 60<percent> of trials (90<percent> of 67<percent>) and a second false positive for around 1.5<percent> (5<percent> of 33<percent>). It is a lot more likely that the statistically significant results relate to true positives because a true difference was much more likely in the first place.<br><br>This is discussed in some depth here<colon> http<colon>//rsos.royalsocietypublishing.org/content/1/3/140216<br><br>Most scientists don<sq>t get this and not many statisticians do, yet. It<sq>s something that was recognised decades ago for analysing the results of diagnostic tests - their accuracy doesn<sq>t mean anything without accounting for prevalence of disease in the population - but it was only around 20 years ago that people started pointing out that we<sq>re in exactly the same situation with clinical trials. They need to be viewed as diagnostic tests for true differences. <br><br>Unfortunately, it<sq>s a lot harder to define the sucess rate of a given type of trial in the universe of all possible trials than it is to define prevalence of disease in a defined population. But this framework for thinking about results is important to prevent over-interpretation and also to discourage the kinds of tiny sample sizes and scattergun approaches to research which make false discoveries so much more likely.</p>", 
                "question": "p-value as a <dq>strong nonlinear transformation<dq>"
            }, 
            "id": "dcb2trd"
        }, 
        {
            "body": {
                "answer": "<p>> What exactly is the <dq>strong nonlinear transformation<dq> that Gelman is talking about?<br><br>To be sure you had the meaning Gelman intended you<sq>d *have to ask Gelman*. I would guess that he means that generally speaking the p-value is not only not linear but that the way the observations enter the transformation from data to p-value is not at all well-approximated by a linear transformation. For example, a first-order Taylor expansion wouldn<sq>t give you a good sense of what was going on (e.g. depending on the arrangement of the rest of the sample, sometimes small changes in some data values may have very large impact on p-value, while with a slight change in the circumstances large changes in data values may have little impact on the p-value)<br><br>>  If we do not believe the null hypothesis to be true, does that mean that statistical tests based on them are statistically invalid?<br><br>Don<sq>t put words in Gelman<sq>s mouth. He said something about *interpretation*. That<sq>s what he meant. <br><br></p>", 
                "question": "p-value as a <dq>strong nonlinear transformation<dq>"
            }, 
            "id": "dcbe34n"
        }, 
        {
            "body": {
                "answer": "<p>Not familiar with Gelman but this is a pretty standard explanation of frequentist statistics. Not sure if it<sq>s a <dq>burn<dq> because guys like Fischer would have just nodded and said, <dq>yes that<sq>s right.<dq><br><br>Second point first. When frequents design hypothesis tests, they set up a mathematical model of the <dq>null<dq> hypothesis. A really clear example is [Fischer<sq>s exact test](https<colon>//en.wikipedia.org/wiki/Fisher<sq>s_exact_test). Fischer analyzed the case of independent marginal probabilities and showed under this assumption the resulting distribution would be the hypergeometric distribution. At no point did he ever calculate what the distribution would be if the marginal probabilities were not independent (the alternative hypothesis); he didn<sq>t need to.<br><br>To apply the test, you look at a concrete contingency table, say [this one](https<colon>//en.wikipedia.org/wiki/Fisher<sq>s_exact_test#Example) from the Wikipedia article because I don<sq>t know how to format tables on reddit. You then ask, <dq>if the null hypothesis were true, what is the probability of getting this exact result, or a more extreme one?<dq> Which you can calculate by summing up over the hypergeometric distribution. This gives you a <dq>p-value,<dq> but this p-value came entirely from the analysis of the null hypothesis. At no point did we ever perform a calculation about any alternative hypothesis. This, I believe, is what Gelman means when he says <dq>the p-value [...] is interpretable *only* under the null hypothesis.<dq> (emphasis mine.)<br><br>As for the rest of his quote, <dq>the p-value is a strongly nonlinear transformation of data<dq> it<sq>s generally true that [distributions](https<colon>//en.wikipedia.org/wiki/Probability_distribution) are maps from some statistic space into the closed unit interval. And, except for a few edge cases ([Uniform](https<colon>//en.wikipedia.org/wiki/Uniform_distribution_\\(continuous\\)), [Linear](https<colon>//en.wikipedia.org/wiki/Linear_probability_model), [Beta\\(1, 2\\)](https<colon>//en.wikipedia.org/wiki/Beta_distribution), etc.) most of the *interesting* probabilities distributions we study (Normal, Poisson, Chi-squared, F, T) are all strongly non-linear. You can see this yourself by plotting the [CDF](https<colon>//en.wikipedia.org/wiki/Cumulative_distribution_function) of typical distributions with non-trivial parameters.<br><br>I guess what<sq>s Gelman<sq>s getting at is that if you know some test statistic, say a z-score, and want to report a p-value instead, then <dq>all<dq> you<sq>ve done is interpreted that statistic in the context of the null hypothesis, and that<sq>s somehow a bad thing. Like if I were to say <dq>women were 2.3 standard deviations shorter than men<dq> that would be clearer and make fewer assumptions than saying <dq>the probability that we would have observed these data under the null hypothesis that heights of men and women have equal mean and variance is p = .01.<dq> Yes, we did pass our z-score (2.3) through a [non-linear function](https<colon>//en.wikipedia.org/wiki/Normal_distribution#/media/File<colon>Normal_Distribution_CDF.svg) to get our p-value (.01) but usually this is considered to add enormous value.<br><br>I guess a Bayesian would have said something like, there are two hypotheses, 1) that men and women have equal height, and 2) men and women have a height difference of 10 cm to which I assign equal priors (0.5 and 0.5), after learned that the height of 10 males and 10 females differed by 8 cm on average I have adjusted the probabilities of these two hypotheses to 0.2 and 0.8 respectively. (Or some infinite family of hypotheses, in practice.) In particular, a Bayesian would be forced to A) actually calculate the probability of the observed outcome for every hypothesis with a non-zero measure in the hypothesis space, and B) actually compare the relative probabilities of the different hypothesis. So I guess the <dq>burn<dq> is that the Frequentist spends zero time calculating the implications of their *actual* hypothesis, and instead focuses on attacking a straw-man null hypothesis?</p>", 
                "question": "p-value as a <dq>strong nonlinear transformation<dq>"
            }, 
            "id": "dcb3h5n"
        }, 
        {
            "body": {
                "answer": "<p>You put them in the model as covariates. The model syntax doesn<sq>t care if covariates are measured at level 1 or level 2.<br><br>Here<sq>s an example (from http<colon>//www.ats.ucla.edu/stat/spss/examples/alda/chapter4/aldaspssch4.htm).<br><br>    mixed alcuse with coa peer age_14<br>       /print=solution<br>      /method=ml<br>      /fixed = coa peer age_14 peer*age_14 <br>      /random intercept age_14 | subject(id) covtype(un).<br><br>Peer is a level 2 covariate, age_14 is a level 1 covariate. </p>", 
                "question": "Mixed Linear Models question"
            }, 
            "id": "daf6luj"
        }, 
        {
            "body": {
                "answer": "<p>Generally, one should not remove lower-order terms while at the same time including higher-order terms using the same constituents. This is explained in detail in [Brambor, Clark and Golder (2006)](http<colon>//mattgolder.com/files/research/pa_final.pdf), for example.<br><br>So say  you want to keep the three-way interaction ABC in the model. This means you shouldn<sq>t remove the two-way interactions (AB, AC, BC) or the main effects (A, B, C) (see [Brambor et *al.* (2006)](http<colon>//mattgolder.com/files/research/pa_final.pdf) on page 66).<br><br>This applies to all regression models and not just glms, as far as I<sq>m aware.<br><br>Now I don<sq>t fully understand what your teacher meant by <dq>remove AB, BC and CA in order to make the ABC interaction singificant<dq>, but it sounds wrong, based on this limited information.<br></p>", 
                "question": "A question about p values in GLM"
            }, 
            "id": "dacazy9"
        }, 
        {
            "body": {
                "answer": "<p>So, a few things. <br><br>One<colon> Your professor is right in the sense that increasing factors and things to control and examine decrease our degrees of freedom, which increases our variance, which hurts our significance. Imagine you have a pie and you cut it into three slices - two main effects and an interaction. Large slice! Now imagine you cut it into six - two main effects and four interactions. Your pie slice just got halved.<br><br>Now, she may be right also that the three way interaction is largely explained by the two way interactions, and therefore if we remove all the two ways the three way comes out significance while when we had the two ways, it would be best controlled and it wouldn<sq>t be. <br><br>However, it is not responsible nor makes sense to ever examine an interaction without controlling for its main effects (or on higher orders, without controlling for its interactions it<sq>s built on). Because otherwise we can<sq>t explain the interaction. Imagine looking at the interaction between gender and some 2 level factor. You theoretically could run a GLM on strictly gender*factor, and not include gender and factor, but what would your result mean? We didn<sq>t control for the lower levels so it<sq>s impossible to answer. <br><br>So can you do<colon><br><br>Reg y a b c a*b a*c b*c a*b*c<br><br>And change that to<colon><br><br>Reg y a b a*b*c<br><br>? Sure, but it<sq>s illogical.</p>", 
                "question": "A question about p values in GLM"
            }, 
            "id": "dacb1bx"
        }, 
        {
            "body": {
                "answer": "<p>The whole business is wrong from the beginning, are you sure this is what she was telling you to do? If so you should develop some additional learning resources besides just this teacher so you don<sq>t end up with a completely screwy idea of how regression works.<br><br>It<sq>s a really bad idea to remove lower order parts of the interaction series based on something<sq>s <dq>significance<dq> (whatever that means - a rant for another time). Doing so forces the associated parameter to exactly zero when it is unlikely to be exactly zero in fact. This messes up the rest of the parameter estimates - for example, causing you to see an apparently significant p-value for a non-significant three way interaction <colon>)<br><br>Also, generally one should not remove predictors from the model at all, unless you are properly adjusting for the bias in parameter estimates and variances with cross validation or something afterwards.</p>", 
                "question": "A question about p values in GLM"
            }, 
            "id": "daclzps"
        }, 
        {
            "body": {
                "answer": "<p>Are you sure your teacher didn<sq>t remove the higher order? The circumstances where one would remove a lower order term and test a higher order term are practically nonexistent.</p>", 
                "question": "A question about p values in GLM"
            }, 
            "id": "dacplws"
        }, 
        {
            "body": {
                "answer": "<p>Depends on what you want to be able to do.<br><br>Will you need time series? Multivariate analysis? Survey methods? Epidemiology? Experimental design? ... etc etc </p>", 
                "question": "What are the essential books or papers for a statistician or data scientists personal library?"
            }, 
            "id": "da52gzx"
        }, 
        {
            "body": {
                "answer": "<p>The elements of statistical learning</p>", 
                "question": "What are the essential books or papers for a statistician or data scientists personal library?"
            }, 
            "id": "da5lbhb"
        }, 
        {
            "body": {
                "answer": "<p>Discovering Statistics Using R. Basically set up my career.</p>", 
                "question": "What is the best method book podcast youtube channel or whatever that helped you learn statistics?"
            }, 
            "id": "da1l15k"
        }, 
        {
            "body": {
                "answer": "<p>Having to actually do the work.  Contextualizing the informatoin into things relevant to me really helps.</p>", 
                "question": "What is the best method book podcast youtube channel or whatever that helped you learn statistics?"
            }, 
            "id": "da3fg4y"
        }, 
        {
            "body": {
                "answer": "<p>Thinking about it for yourself.<br><br>If you read/watch/listen to something without internalizing it will do you little good. Reading something sure helps and gives you ideas. But for me the biggest <dq>aha<dq> moments came when I started thinking about things myself.<br><br>As an example think about<colon> You are walking on a road and come upon 5 stones arranged in increasing order - what methods can you use to tell if this is just a random arrangement or did someone intervene? What are the difficulties in doing so? What are potential pitfalls? How something that you read about (maximum likelihood, null hypothesis) relates to that?<br><br>At least to me - that helped the most.</p>", 
                "question": "What is the best method book podcast youtube channel or whatever that helped you learn statistics?"
            }, 
            "id": "da1kqug"
        }, 
        {
            "body": {
                "answer": "<p>For me it was a combination of things. But one particular YouTube channel that I think is awesome is [Brandon Foltz](https<colon>//www.youtube.com/user/BCFoltz/playlists).</p>", 
                "question": "What is the best method book podcast youtube channel or whatever that helped you learn statistics?"
            }, 
            "id": "da20h4g"
        }, 
        {
            "body": {
                "answer": "<p>>  Would my time be better spent going back and getting a second bachelors in statistics then going and getting a masters? If not what options do I have (if any)?<br><br>You should ask the stats department at the schools you are interested in. They are going to know what your best bet is.<br><br>Personally, I think going back for another bachelor degree is going to be a lot of wasted time retaking general curriculum courses you<sq>ve already taken. I think a better idea is to take the math courses you need as a non matriculated student (ideally at a state school), and then applying to a grad program.</p>", 
                "question": "I want to get a masters in statistics but I don<sq>t have a very solid math background. What are my options?"
            }, 
            "id": "d9lp6dy"
        }, 
        {
            "body": {
                "answer": "<p>You will need Calculus and a basic Linear Algebra course.  Neither should be difficult to take care of, and some programs might actually allow you to take them while you are in the program.<br><br>However that is an <dq>at minimum<dq> requirement.  What are you trying to do long term with this degree?</p>", 
                "question": "I want to get a masters in statistics but I don<sq>t have a very solid math background. What are my options?"
            }, 
            "id": "d9lo1w1"
        }, 
        {
            "body": {
                "answer": "<p>You could try learning the basic background on your own; Calc 1&2 are pretty easy to learn, and introductory linear algebra as well. <br><br>With those you could probably take a semesters worth of higher level classes (math stats, Calc III, regression analysis, linear algebra) and have a decent enough background. Definitely talk to prospective schools; at least here in Canada, most only require an equivalent minor (10 math/stats courses), but even that could potentially be waived/lessened if you do well enough and show interest in the program</p>", 
                "question": "I want to get a masters in statistics but I don<sq>t have a very solid math background. What are my options?"
            }, 
            "id": "d9lyvt7"
        }, 
        {
            "body": {
                "answer": "<p>Just to echo what<sq>s going on here.  You<sq>re going to want a background in Calculus and Linear Algebra.  Take the time off to master those and you<sq>ll get much more out of whatever program you<sq>re seeking. <br><br>If that doesn<sq>t occupy you enough,  look into R and get comfortable with using statistical software. </p>", 
                "question": "I want to get a masters in statistics but I don<sq>t have a very solid math background. What are my options?"
            }, 
            "id": "d9m28io"
        }, 
        {
            "body": {
                "answer": "<p>I did a Bachelor<sq>s degree in Statistics (2014) and had a final GPA of 3.2. Finding a job was very difficult even though I interned at a big 4 bank in Australia during University. I worked in Secured Lending (Mortgages) during the internship and this really narrowed by job aspects to be Finance related only. I ended up unable to find a job in my home city of Melbourne despite how hard I looked.  <br><br>I went through a recruiter and he got me a role in Sydney in Finance as a Risk Analyst. My starting salary was 55k, for the first year I was just mainly doing Sales reporting in SAS. There is no stats in my role, only coding in SAS or SQL. <br><br>Now nearly finishing my second year I manage multiple data warehouses, I<sq>m also the main developer for developing Sales dashboards in SAS VA. I do lot<sq>s of other things too but these are not really noteworthy. My salary is now 65k.<br><br>If you have any questions about my role please let me know.</p>", 
                "question": "Statistics Majors... What kinds of jobs were you able to find with just a Bachelors Degree"
            }, 
            "id": "d96syvh"
        }, 
        {
            "body": {
                "answer": "<p>I am finishing my Bachelor<sq>s in statistics next semester, and have a job offer for after I graduate, from the company I interned at last summer. They do government contract work, R&D type stuff, and I<sq>ll be in their data department. Starting salary 80k. Probably not a typical experience, though, I got really reeeeally lucky. <br><br>I had a hell of a time getting that internship, they were one of 3 companies out of ~60 I applied to that gave me an interview. Without that internship I<sq>m not sure I could find a good position. I looked around a lot and there aren<sq>t very many companies that want Bachelor<sq>s degrees for math-heavy positions. There are some opportunities if you<sq>re good at coding and databases though. </p>", 
                "question": "Statistics Majors... What kinds of jobs were you able to find with just a Bachelors Degree"
            }, 
            "id": "d97i74e"
        }, 
        {
            "body": {
                "answer": "<p>Mandingo<br><br>But i also gave the odds of me winning.</p>", 
                "question": "Statistics Majors... What kinds of jobs were you able to find with just a Bachelors Degree"
            }, 
            "id": "d96l2l3"
        }, 
        {
            "body": {
                "answer": "<p>> What is the best way to calculate probability or confidence that a polymorphism present in all of these samples is associated with strain A?<br><br>If the polymorphism is present in the samples for strain A, then it<sq>s <dq>associated<dq> with strain A.  It<sq>s not clear what the actual question is that you<sq>re trying to answer.  Are you saying that you have 3 samples, each a mixture of different strains and that A is always one of those strains, and you want to show that A has a certain allele at a SNP based on this data?  </p>", 
                "question": "I<sq>m trying to match a nucleotide polymorphism to a bacterial strain given population information but I<sq>m not really sure where to start. Is this a conditional probability question? Should I treat the population info as a prior and do some bayesian calculation?"
            }, 
            "id": "d89v911"
        }, 
        {
            "body": {
                "answer": "<p><dq>The best thing about being a statistician is that you get to play in everyone<sq>s backyard<dq> - John Tukey.<br><br><br>I would say a MSc in Applied Statistics would get you what you want - or at least get you want you want and also keep your options open. R, WinBugs, Python would all be helpful. I would put SAS next on the list. I certainly think minoring in CS would be an asset, or at least taking as many classes as you can, it<sq>ll just make you that much more competitive.</p>", 
                "question": "What to expect from a career in Statistics?"
            }, 
            "id": "d83umjp"
        }, 
        {
            "body": {
                "answer": "<p>I don<sq>t know what you really want but i would recommend statistics. It<sq>s more exciting than math for me. Btw i am doing MSc in statistics. </p>", 
                "question": "Getting a MSc in Math or Statistics?"
            }, 
            "id": "d7althv"
        }, 
        {
            "body": {
                "answer": "<p>It depends on what you want. If you want to get a job as a business analyst later, I<sq>d recommend statistics. <br><br>Math is (especially in Germany) much more difficult and a much more broad based training. <br><br>Every mathematician can learn the statistical contents in a little while, but not vice versa. <br><br>I have a master degree in math. When I applied for a job I acquired  in the meantime all the statistics + R-Programming.<br>It tooks me less than 3 months and my skills in statistics and R programming are as the same level as the statisticians. <br><br>So, what I wanted to say is<colon> If you really like math, you can also continue studying math and catch up on stats and R after.</p>", 
                "question": "Getting a MSc in Math or Statistics?"
            }, 
            "id": "d7dw6gt"
        }, 
        {
            "body": {
                "answer": "<p>Edit<colon> I misunderstood the question, ignore me.<br><br>How do you know what the best score is?<br><br>There are 1024 possible combinations, and most of them are very unlikely, e.g. if you get 64^0.5 right, you<sq>r going to get 8*3 right.<br><br>You can do something like this with something like factor analysis, or item response theory (but you won<sq>t know if a high score is good or bad). </p>", 
                "question": "What is the proper name for this process?"
            }, 
            "id": "d76eq45"
        }, 
        {
            "body": {
                "answer": "<p>I think you **should** look at the individually.  Looking at the questions separately, you can run a regression<colon><br><br>Math Score = Bo + B1*Q1 +B2*Q2+ ...<br><br>/u/jeremymiles is right-- with so many different possible combinations of responses to the 10 questions (though you may only observe a small subset of these patterns in practice), it will be unlikely without a huge amount of data to learn much with confidence.  If you did have a huge dataset, then you could code each different response set with a different dummy variable (except leave out one of them), and still run a regression to see which combinations have the highest coefficient values.</p>", 
                "question": "What is the proper name for this process?"
            }, 
            "id": "d76n9h6"
        }, 
        {
            "body": {
                "answer": "<p>You should probably look into (repeated measures) MANCOVA. </p>", 
                "question": "Help needed with Multiple Multivariate Regression"
            }, 
            "id": "d6kd6z2"
        }, 
        {
            "body": {
                "answer": "<p>Minor quibble<colon> <dq>different recordings within context should be the same<dq> that<sq>s not a hypothesis you can test. You can test for differences, you can<sq>t test for sameness. If the results are not significant, you failed to find that they were different , you did not find that they were the same.<br><br>60 features is a heck of a lot. You could do something like a factor analysis to try to reduce them.<br><br>Collinearity is only an issue if your predictors correlate. <br><br>However, I think your best solution is a multilevel model. This lets you put everything into one model, do global hypothesis tests, and then do individual hypothesis tests.<br><br>Instead of<colon><br><br>    s1, c2, r1, 9.9, 32.65, 28, ...<br>    s1, c2, r2, 10.1, 28.3, 27.3, ...<br><br>Your data will look like<colon><br><br>    s   c  r  f  y<br>    1  2   1  1  9.9<br>    1  2   2  1 10.1<br>    1  2   1  2 32.65<br>    1  2   2  2 28.3<br>    1  2   1  3 28.0 <br>    1  2   2  3 27.3<br><br>And your model will by something like<colon><br><br>    lmer (y ~ s + c + f + (1|r))<br><br>(Plus some interactions, that I can<sq>t work out now.)<br><br></p>", 
                "question": "Help needed with Multiple Multivariate Regression"
            }, 
            "id": "d6kp4g0"
        }, 
        {
            "body": {
                "answer": "<p>If you run 20 statistical tests on a dataset, one of them will have p<0.05 by chance. If you are selecting a variable for inclusion into a multivariate model based on p<0.25 or something than for every 4 tests you have a chance of finding one so. These are limitations of the approach where you do multiple tests on a large dataset to <dq>find<dq> something of statistical significance.<br><br>The most important step is to have your hypothesis a priori, before any statistical testing, and not to do any extra testing outside that. The other thing you could do is to pre-select a group of variables based on real-life knowledge of what is important. This would narrow down the number of variables you are testing. Lastly, if you are doing multiple intergroup testings, you could apply bonferroni or other corrections.</p>", 
                "question": "General Question About P-Hacking"
            }, 
            "id": "d5uycy5"
        }, 
        {
            "body": {
                "answer": "<p>There are methods of detecting p-hacking. Check PLOS biology for a few interesting articles on the subject. It<sq>s unfortunate, but some evidence suggests up to 50<percent> of researchers engage in it. </p>", 
                "question": "General Question About P-Hacking"
            }, 
            "id": "d5uzx68"
        }, 
        {
            "body": {
                "answer": "<p>Are you doing hypothesis testing, calculating standard errors, estimating parameters, confidence intervals or prediction intervals using the same data as you used to select variables?</p>", 
                "question": "General Question About P-Hacking"
            }, 
            "id": "d5vm5o7"
        }, 
        {
            "body": {
                "answer": "<p>I<sq>d recommend [Borenstein et al. (2009)<colon> Introduction to Meta-Analysis](https<colon>//www.amazon.com/Introduction-Meta-Analysis-Michael-Borenstein/dp/0470057246/ref=sr_1_1?ie=UTF8&qid=1469437639&sr=8-1&keywords=introduction+meta+analysis).<br></p>", 
                "question": "recommendations for meta-analysis text?"
            }, 
            "id": "d5prrj8"
        }, 
        {
            "body": {
                "answer": "<p>I found Lipsey and Wilson<sq>s Practical Meta-analysis to be a very nice introduction.  That was a while ago, and I<sq>m not sure if there is an updated version of it.</p>", 
                "question": "recommendations for meta-analysis text?"
            }, 
            "id": "d5qu9sz"
        }, 
        {
            "body": {
                "answer": "<p>A lot of machine learning boils down to trying to minimize something (or maximize, which is equivalent to minimizing the negated function). If you are trying to fit a curve to a bunch of points, you might try to minimize the distance to each point given by a particular curve. Let<sq>s call that the <dq>error.<dq> <br><br>Let<sq>s assume for a given problem, we<sq>ve constrained our possible candidate solutions to some family of functions. For instance, maybe functions with the formula <dq>y=bx<dq>. Therefore, a possible solution is given by each unique value of b. And each solution has some associated error, such that we are trying to find the value of b with minimal error.<br><br>We can define a function that, for a given b parameter spits out the associated error. Let<sq>s call that <dq>error function<dq> E(b). We want to minimize this function. <br><br>Imagine we plot this function for some values of b and it turns out to be bowl shaped (i.e. it has a global minimum). If we draw a line tangent to the curve at its minimum, the tangent line will have zero slope. Tangents to the right will have positive slope, and to the left will have negative slope.<br><br>We can use calculus to construct a function that, for any value of b, tells us the slope of the associated tangent line. This is the gradient. You can use the gradient to walk <dq>downhill<dq> to find the lowest point on your error function, or at least something close to it.<br><br>In higher dimensions (i.e. more variables) the error function isn<sq>t a simple curve but a complex surface. But this general strategy still generalizes<colon> you can construct (or estimate) the gradient to figure out what direction you need to move along the error function to pick a new candidate solution that was better than your previous best guess, hopefully moving towards a global minimum. <br><br>Sometimes you get to the bottom of the valley and it turns out there<sq>s a lower valley somewhere else. Gradient descent won<sq>t be able to help you here<colon> the technique stops when you find the bottom of any valley. If you pick a new random starting position and run gradient descent again, you might end up at the bottom of a lower valley.</p>", 
                "question": "ELI5<colon> Gradient Descent Vectors"
            }, 
            "id": "d5aw8id"
        }, 
        {
            "body": {
                "answer": "<p>The easy way to describe Gradient Descent is to start with the simplest case<colon> A [parabola](https<colon>//en.wikipedia.org/wiki/Parabola) in two dimensions. I<sq>ll use this [example image](https<colon>//en.wikipedia.org/wiki/Parabola#/media/File<colon>Relationship_between_parabola_and_quadratic_Bezier.svg) to talk about this.<br><br>What you<sq>re looking for using gradient descent is the minimum - the lowest point on the graph.  In the parabola example, this is the rounded bottom - placed at (0,0) in the example graph.<br><br>When you do gradient descent, you pick example points on the graph and see what direction the tangent to the graph is at that point.<br><br>For our example, we<sq>ll look at the two points (-1, 1) and (2, 4) on the graph.  These are marked in blue (-1, 1) and red (2, 4) on the graph.<br>(-1, 1) means X=-1 and Y=1.  For the graph we<sq>re looking at, we also have the formula Y=X^2; I<sq>m going to just compute values for this in the examples, <br><br>For (-1, 1) (marked in blue), the tangent is the dashed blue line.  This line is sloping down to the right; that indicates that the minimum is to the right of the point (-1, 1).  Let<sq>s say we move right by 0.1.  This gets us to (-0.9, 0.81).  This is closer to the minimum, but still not quite there.  We repeat adding 0.1, and get to (-0.8, 0.64).  Still not there, keep going, (-0.7, 0.49) -> (-0.6, 0.36) -> (-0.5, 0.25) -> (-0.4, 0.16) -> (-0.3, 0.09) -> (-0.2, 0.04) -> (-0.1, 0.01) -> (0, 0) -> (0.1, 0.01) - oops - overshot, but now we know the minimum is somewhere between X=-0.1 and X=0.1<br><br>To find the actual minimum, we<sq>ll repeat moving but now with smaller steps - e.g, 0.01.  We<sq>ll get (-0.09, 0.0081) -> (-0.08, 0.0064) etc, very similar to the first range but with two more zeroes in front.  Eventually, we<sq>ll have the same step-over and we<sq>ll have limited to the minimum being in the range X=-0.01 to X=0.01<br><br>And we can repeat with smaller values of the step again, and keep doing that go smaller and smaller ranges.<br><br>This is the core idea in gradient descent.  Step in the direction where the tangent is pointing down, and sooner or later you<sq>ll get to find something that<sq>s around the bottom - or at least *a* bottom, because it is possible for the graph to have several <dq>bottoms<dq> (local minimums)<br><br>There<sq>s one problem that isn<sq>t solved in the example I gave above<colon> Where does the step value 0.1 come from in the first place?  And the answer to that for this example is that I just made it up.<br><br>However, there<sq>s a trick to finding a good step value.  When you<sq>re looking at the tangent, look at the steepness of slope it has. This is called the derivative of the graph.  If the slope is steep, you can normally do a fairly large step.  If the slope is flat, you need to do a smaller step.<br><br>Now, this was in 2D.  In more dimensions, the same apply.  You just try out in any random dimensions, and see if you can do a step there that takes you <dq>downwards<dq>.  If you can, do that step - and then try another dimension and look for the same.  You can also try changing several dimensions at the same time.<br><br>Please ask more questions if this didn<sq>t fully clarify!</p>", 
                "question": "ELI5<colon> Gradient Descent Vectors"
            }, 
            "id": "d5awm7o"
        }, 
        {
            "body": {
                "answer": "<p>The answers in this thread are awesome, but they aren<sq>t really ELI5. And I wanted to do that.<br><br><br>First, let<sq>s talk a bit about numbers and vectors.<br><br>We remember how to count from preschool, and we<sq>re just starting to tell time. We are also just starting to remember our way to Mrs. Robert<sq>s class from home, two blocks north and one block west.<br><br>Let<sq>s say we take this stick and draw out a map from home and school in the dirt. That<sq>s a little complicated so let<sq>s first let<sq>s just look at home to the Plaid Pantry down the road.<br><br><--- P --- | ---  --- | ---  --- | --- H --- | ---><br><br>Okay, if we start at P, how many blocks do we have to get to H? We can count them, 1 block, 2 blocks, 3 blocks.<br><br>If we started at H and wanted to get to P, same thing right? 1 block, 2 blocks, 3 blocks.<br><br>But if it<sq>s the same number of blocks no matter which way your going, how do you know which way to go?<br><br>Well, we normally say go three blocks east, or three blocks west. But we could also say go three blocks east and three opposite of west. So really, we only need one name for this whole line. It doesn<sq>t really matter what we call it, so let<sq>s call it e.<br><br>To say 3 blocks east we can write 3e. To say 3 blocks opposite of e we can use a short hand and say -3e.<br><br>So to get to P from H we go -3e, and to get to H from P we go 3e.<br><br>What if we were somewhere in between, say some place * like this?<br><br><--- P --- | --- * --- | --- --- | --- H --- | ---><br><br>To get to P from * we need to go one block opposite east, so -1e. To get to H from * we need to go two blocks east so 2e.<br><br>So now let<sq>s think about how we walk to school.<br><br>^<br>S<br>|<br>-<br>|<br>|<br>_<br>|<br>|<br>--- --- | --- H --- | ---><br><br>We need to go two blocks north and one block west (opposite of east). So that 2n and -1e.<br><br>We know how to add things like 2 + 4 to get 6. We<sq>ve effectively been adding things up along one of these lines the whole time, you can think of what you did adding those together like doing 2u + 4u = 6u. So when we have the same direction, they add together. When we have different directions, like n and e, they don<sq>t combine, but we still want to keep track of them. We could do this in a couple ways. The first one is just to use <sq>+<sq> to mean <sq>and<sq> so 2 blocks north and 1 block not east is just 2n + -1e. But this can get a little confusing when it goes <sq>+ -<sq><br><br>So maybe we can choose something a little different, why not just list them out? <br>[2n, -1e]. That is a little clearer, but if we have a bunch of these, we are going to be writing letters a lot, and Mrs. Roberts already had us practice handwriting for an hour today (good thing too, cause my ls and my 1s are hard to tell apart.)<br><br>So let<sq>s just order them some how and we<sq>ll always use the same thing. Just make sure to write down the order at the beginning for everyone else so they know too.<br><br>[n,e]<br><br>Now we can just write [2, -1]<br><br>That is as small as we can get it really. If we took out anything more, we wouldn<sq>t know enough to get to school!<br><br>But, Timmy, don<sq>t you also live up a hill?<br><br>Yea, Mom says it<sq>s good exercise though.<br><br>Haha. That it is Timmy! That it is. But if you just wen<sq>t 2 blocks north and -1 blocks east, then wouldn<sq>t you fall just like Will E. Coyote as soon as you looked down?<br><br>... (Timmy looks confused)<br><br>We should probably add another direction for up and down, shouldn<sq>t we, Timmy?<br><br>Yeah... Prolly.<br><br>Okay! So now we have [n, e, u] (u for up!)<br><br>So school is [2, -1, -1] from our home!<br><br>---------------<br><br>On Gradient Descent.<br><br>Okay Timmy! You<sq>ve graduated Kindergarten! Are you ready for your next big adventure?<br><br>Yeah!<br><br>First Grade is just at the bottom of the hill!<br><br>Yay! (Dashes off)<br><br>Woah, hold on there, Timmy. Aren<sq>t we forgetting something?<br><br>(Stops) Ummm, what?<br><br>How are you going to get there???<br><br>Oh.<br><br>Chin up, Timmy! We<sq>ll get there one step at a time!<br><br>Okay!<br><br>How big can you step Timmy? We<sq>ll need some big boy steps to make sure we don<sq>t fall into any potholes or trip on any mole hills. But not too big now, we wouldn<sq>t want to fly right past the school, would we Timmy?<br><br>No sir; I can do that, mister!<br><br>Alrighty then, let<sq>s take this rope and measure your step; confidently now!<br><br>... (Tongue peeks out from Timmy<sq>s lips as he focuses, plants a foot down)<br><br>Looks like about 1.5 feet! All right, let<sq>s try to keep these the same then.<br><br>So, we want to get down the hill as fast as we can; wouldn<sq>t want to be late for our first day of class would we, Timmy?<br><br>(Shakes head)<br><br>That<sq>s a good boy, now let<sq>s get started. Take this rope and stretch it out down the hill.<br><br>(Lays the rope down flat)<br><br>That looks pretty good, now where<sq>s that rope going? We want to make sure it<sq>s the steepest grade we can, even if it<sq>s not pointing right toward the school; wouldn<sq>t want to climb any small bumps if we can avoid them would we? That<sq>s better. Now we<sq>ll do that again from here until we get to the bottom of the hill, understand?<br><br>Yessir!<br><br>So even though we didn<sq>t have a vector like we did to get to kindergarten, we can still find our new school just by knowing it<sq>s at the bottom of the hill, can<sq>t we?<br><br>Yeah!<br><br><sq>Atta boy, Timmy, now hurry on down to class!<br><br>(Timmy performs a gradient descent to get down to school with a step size of 1.5 ft and makes it there on time)<br><br>---------<br><br>ELI25<colon><br><br>To find a local maximum or minimum, we can always take a small step size (so we don<sq>t overshoot), and follow the steepest path from our current point.  You calculate the direction by computing the partial derivatives for each of the orthonormal bases in your coordinate system at the current location, and then moving in that direction by a step size. Technically, this will put you above or below the surface of the hill, the rope in the example is essentially flexible in the height component and no others. We repeat this until the gradient is within some tolerable value. This is the reason that gradient ascent is commonly referred to as <sq>hill climbing<sq>.</p>", 
                "question": "ELI5<colon> Gradient Descent Vectors"
            }, 
            "id": "d5b0nv1"
        }, 
        {
            "body": {
                "answer": "<p>One tip at the outset<colon> You are much more likely to improve your understanding by formulating your questions carefully and with a particular goal in mind.  What is motivating you to look into gradient descent?  Do you have a function you want to optimize?  Or have you just heard the term and would like to know more about it?  I<sq>m not trying to criticize you or your question, but some things are often much easier to learn with an application to hang the concepts onto.<br><br>Nevertheless, here is a quick summary<colon> Gradients are multivariate extensions of the ordinary derivative.  When a function has a single output but multiple inputs (say *z* = f(*x*,*y*), for instance), we can compute the gradient to see how the function changes with respect to each of its input variables.  This is particularly easy, since for each variable with respect to which we consider the function<sq>s rate of change, we treat any other variables as though they<sq>re fixed constants.  A gradient will be a vector with as many components as there are input variables (so for my example mapping *x* and *y* to *z*, the gradient will be a vector of length two, with the first component the partial derivative of f with respect to *x* and the second the partial derivative of f with respect to *y*).  If our function has more than one output, there is a generalization of the gradient called the *Jacobian matrix*.<br><br>Gradients are interesting because their vector nature imposes a notion of <dq>direction<dq> on them.  (Modern treatments of vectors tend to deemphasize the direction notion, since general vectors can contain all kinds of things besides numbers, but gradient descent still depends on it.)  Specifically, the gradient of a function evaluated *at a particular point* gives the direction in which the function has its greatest rate of change *from that point*.  If you think of your function as mapping, say, latitude and longitude to altitude on a hill, the gradient would tell you which direction to face to find the steepest climb from wherever you are.  If you<sq>re at the top of the hill, the gradient will be zero, because the hill will be locally <dq>flat<dq> at the very top.  Your intuition might argue with that, since standing at the top of a hill, you may see *plenty* of steep options where you can fall to your death, but mathematically speaking, if the hill is <dq>well behaved,<dq> the curve at the top will be flat.  Think of it another way<colon> If you can get a marble to rest at the top of a hill, the local slope is zero.<br><br>The same intuition holds for ravines as well as hills, just with a flip of sign.  When we want to find the maximum or minimum of a function, we want to find a place where the gradient is zero.  (This is not the only condition that must hold, so it is not sufficient, as there can be other peaks or valleys higher or lower somewhere else.  It is, however, necessary.)  If we start somewhere, say from some random point, we<sq>d like to make quick progress, so we use the gradient as a guide.  If we<sq>re minimizing a function, we go in the direction *opposite* the gradient, since it will give us the steepest descent, or the fastest route to the bottom.  This isn<sq>t always the best thing to do, though, so we often take smaller steps and reevaluate the gradient along the way, lest we skip and hop over the low point we<sq>re looking for.</p>", 
                "question": "ELI5<colon> Gradient Descent Vectors"
            }, 
            "id": "d5ax0rz"
        }, 
        {
            "body": {
                "answer": "<p>looking at this actuarial life table<colon> https<colon>//www.ssa.gov/oact/STATS/table4c6.html#fn1 and this demographic study of reddit http<colon>//www.pewinternet.org/files/old-media/Files/Reports/2013/PIP_reddit_usage_2013.pdf, both from 2013, assuming the demographics of the thread answerers mirror those of reddit users and the probability of death of users mirrors that of usa citizens, I get that 6 or 7 of those answerers should be dead by know. Eight if you assume a significant amount of 75+ year old redditors, which I think is unlikely. Could do better with better demographic data though</p>", 
                "question": "If you go back to a reddit thread from a year ago with 3000 individual commenters how many of those people would be expected to be dead at this point in time?"
            }, 
            "id": "d54umd0"
        }, 
        {
            "body": {
                "answer": "<p>If the thread is primarily from young people, not many would be expected to be dead. If it discussed palliative care, perhaps more would be dead. </p>", 
                "question": "If you go back to a reddit thread from a year ago with 3000 individual commenters how many of those people would be expected to be dead at this point in time?"
            }, 
            "id": "d55q044"
        }, 
        {
            "body": {
                "answer": "<p>You<sq>d want life tables from each of the countries the commenters could be from and you<sq>d need to know the distribution of commenters across the countries, ages and genders (if possible, for the specific subreddit the comment is in). There<sq>s past survey data that might let you get a rough approximation for these reddit-wide, but it might be possible to get better information. You could then construct a mixture-distribution across life-tables for one-year survival probabilities.<br><br>Most of the necessary life tables could be obtained from the Human Mortality Database, or other sources.<br><br>To do this entire calculation to an accuracy that would give even a decently accurate answer would require quite a bit of work. <br><br>You could get a rough first approximation by using data just for the biggest half-dozen countries of redditors, but it would still take quite a while. Hours at the very least, I<sq>d think.<br></p>", 
                "question": "If you go back to a reddit thread from a year ago with 3000 individual commenters how many of those people would be expected to be dead at this point in time?"
            }, 
            "id": "d54z7vb"
        }, 
        {
            "body": {
                "answer": "<p>I would recommended <dq>A Lady Tasting Tea.<dq></p>", 
                "question": "What are some good books about statistics?"
            }, 
            "id": "d4jon06"
        }, 
        {
            "body": {
                "answer": "<p>The Signal and the Noise<colon> Why So Many Predictions Fail--but Some Don<sq>t<br><br>Nate Silver. <br><br>Really liked this one </p>", 
                "question": "What are some good books about statistics?"
            }, 
            "id": "d4k4sll"
        }, 
        {
            "body": {
                "answer": "<p>The theory that would not die--mcGrayne<br><br>an adventure in statistics--andy field<br><br></p>", 
                "question": "What are some good books about statistics?"
            }, 
            "id": "d4jsueu"
        }, 
        {
            "body": {
                "answer": "<p>I<sq>m a big fan of [The Drunkard<sq>s Walk](https<colon>//www.amazon.com/Drunkards-Walk-Randomness-Rules-Lives/dp/0307275175). Also, the author Leonard Mlodinow (PhD in physics from Berkeley) has a number of other really good books on different scientific fields.<br></p>", 
                "question": "What are some good books about statistics?"
            }, 
            "id": "d4k92ab"
        }, 
        {
            "body": {
                "answer": "<p>i hated number theory too, but i ended up loving statistics. i think that statistics is certainly worth a shot for you. check out some intermediate/advanced stat textbooks and see if you can get into it. since you<sq>re well versed in proofs, you won<sq>t have too much difficulty following the material. what kind of concepts did your stat class cover and at what depth? what textbook did you use? what areas of statistics interest you? </p>", 
                "question": "Should I go for my Master<sq>s in statistics?"
            }, 
            "id": "d4dnei3"
        }, 
        {
            "body": {
                "answer": "<p>Im majoring in statistics at the moment so have a pretty good grasp of what undergrad stats is. Pretty much the opposite as you, I want to start taking pure mathematics courses like Algebra, Topology, Analysis etc.<br><br>Personally, stats is pretty broad. I wasnt that big a fan of boring Z-tests and t-tests. But some of the measure theory behind the distributions sounds interesting. I<sq>m more interested in things like time series and stochastic processes.<br><br>If you do a masters in stats i suspect you<sq>ll be doing a fair amount of both. The testing aspect is similar to what you would have covered in an intro stats course but vastly extended to all types of data and different ways of approaching it as well as multivariable regression and much more.<br><br>time series is more about correlations between data and figuring out the best way to model it. like eliminating the seasonal bit from sales data which goes up during xmas and down during summer or whatever<br><br>if that sounds up your alley then go for it! have a read of some intermediate statistics books to check it<sq>s what youre after</p>", 
                "question": "Should I go for my Master<sq>s in statistics?"
            }, 
            "id": "d4dtvbb"
        }, 
        {
            "body": {
                "answer": "<p>See here<colon>  <br>http<colon>//stats.stackexchange.com/questions/16665/how-can-i-get-a-significant-overall-anova-but-no-significant-pairwise-difference</p>", 
                "question": "ANOVA results significant but post-hoc not?"
            }, 
            "id": "d3wf979"
        }, 
        {
            "body": {
                "answer": "<p>The post hoc test sets a familywise error rate.  When controlling for tyoe 1 error, the pairwise comparisons may not be statistically significant despite a significant ANOVA.  </p>", 
                "question": "ANOVA results significant but post-hoc not?"
            }, 
            "id": "d3wfckd"
        }, 
        {
            "body": {
                "answer": "<p>The tests are based on different distributions and will not always give you the same result. It is possible for a Tukey comparison to be significant and the ANOVA not to be. As you discovered, the ANOVA can be significant with none of the Tukey comparisons being significant. In your case, you can conclude that one or more population means are different from each other but you cannot draw a confident conclusion about which one(s). <br><br>It is just me, but I don<sq>t like calling something post-hoc when you planned a priori to do it. </p>", 
                "question": "ANOVA results significant but post-hoc not?"
            }, 
            "id": "d3xayhp"
        }, 
        {
            "body": {
                "answer": "<p>Simply put<colon><br><br>Standard deviation is the measure of spread *in your sample.*<br><br><br>Standard error is more of an estimate of the population\u2014it allows us to compute a confidence interval to estimate the location of the true population mean. So if you compute a 95<percent> confidence interval (SE * +/- 1.96), you can make the claim that if you computed the sample mean and infinite number of times, the true population mean will fall within the confidence interval 95<percent> of the time. <br><br>To sum it up<colon> standard deviation is a measure of spread in your sample. Standard error is used to estimate the population mean. </p>", 
                "question": "standard error vs standard deviation"
            }, 
            "id": "d3pf2tj"
        }, 
        {
            "body": {
                "answer": "<p>Actually, and this is completely pendantic, they are both basically similar. A standard deviation is a common measure of variability of individual cases around their mean, whereas the standard error is a measure of variabiliy--in other words, the SD-- of sample means around the population mean. The standard error formula and the standard deviation formula are basically the same, except when n = 1, which is the case for SD it is silly to include n in the formula.  In any case, i believe the question refers to the SD. </p>", 
                "question": "standard error vs standard deviation"
            }, 
            "id": "d3pkz6u"
        }, 
        {
            "body": {
                "answer": "<p>Can<sq>t tell for sure from the information you have provided. <br><br>that second number might represent sd, or se, or a margin of error (confidence interval half-width) or even typical measurement error on whatever device they were weighed on<br></p>", 
                "question": "standard error vs standard deviation"
            }, 
            "id": "d3q0cvq"
        }, 
        {
            "body": {
                "answer": "<p>In general it<sq>s not appropriate to give the number without specifying which it is - it<sq>s kind of a shitty problem if it doesn<sq>t tell you this. However there<sq>s probably a convention that<sq>s used in your book or class, and the convention is probably that this is the standard deviation. You<sq>ll have to double check with the book or the prof to be sure. Which is a pretty long-winded way of saying <dq>Who knows?<dq> <colon>)</p>", 
                "question": "standard error vs standard deviation"
            }, 
            "id": "d3pf6av"
        }, 
        {
            "body": {
                "answer": "<p>This looks to me like an interrupted time series problem. ITS methods including ARIMA models and segmented regression would allow you to model both the level and slope(change) in your data after you introduced your process. <br><br>Lagarde, M. (2011). How to do (or not to do)\u2026 Assessing the impact of a policy change with routine longitudinal data. Health policy and planning, czr004.</p>", 
                "question": "I give up - I need help with Hypothesis Testing and no FAQ I<sq>ve come across in 2 days has helped"
            }, 
            "id": "d1v7gf7"
        }, 
        {
            "body": {
                "answer": "<p>You have a practical problem, and you are working to meet a practical requirement. From your employer<sq>s point of view, the criterion is dollar value, and therefore quantities upstream from that, e.g. material consumed, is relevant. All the stuff about statistical significance is irrelevant.<br><br>I repeat<colon> statistical significance is an irrelevant waste of time. Significance is what you pursue when you don<sq>t know what the practical consequences might be. That is not the situation here.<br><br>Are you saving your employer money? If so, that is the best justification you could find. There are at least a couple of ways to approach that -- perhaps just the total material consumed or total product is relevant. Or perhaps it is relevant that you have reduced the variance because extreme values cost more than middling values, per unit. Is the new process a lot of extra work? Extra work costs money too, maybe it is, or is not more than balanced by the increased output. But those are questions you can answer from the business context.</p>", 
                "question": "I give up - I need help with Hypothesis Testing and no FAQ I<sq>ve come across in 2 days has helped"
            }, 
            "id": "d1v7zsh"
        }, 
        {
            "body": {
                "answer": "<p>I<sq>m totally wrong here r.e. permutation test.  Your samples are more than large enough to just use a two-sample t-test.  Forget about the permutation test... sorry about that.<br><br>Another option would be this<colon>  Is there a specific loss target?  (If not, you could even set that target to 0, or whatever makes sense.)  Then score each day with a 0 or 1, according to whether the target was met or not.  Calculate the proportion of 1<sq>s in each set.  If the intervention had no effect, those proportions should be about the same.  If the intervention had an effect, then the post-intervention set should have a higher proportion of days that meet the loss target.  So you want to do a test of equality of binomial proportions-- this is very straightforward, you can do it on a hand calculator.  </p>", 
                "question": "I give up - I need help with Hypothesis Testing and no FAQ I<sq>ve come across in 2 days has helped"
            }, 
            "id": "d1v56bz"
        }, 
        {
            "body": {
                "answer": "<p>I would tackle this with non-parametric statistics. That way you avoid needing to do transforms and cram your data into a normal or something like that.<br><br>The [Mann-Whitney U test](https<colon>//en.wikipedia.org/wiki/Mann<percent>E2<percent>80<percent>93Whitney_U_test) is a good option that tests whether <dq>two samples come from the same population against an alternative hypothesis, especially that a particular population tends to have larger values than the other<dq><br><br>You can also look at [randomization tests](http<colon>//davidmlane.com/hyperstat/B163479.html), which are [pretty useful](https<colon>//www.uvm.edu/~dhowell/StatPages/ResamplingWithR/Random2Sample/TwoIndependentSamplesR.html) for non-parametric statistics. I am trying to find a nice video that someone made on this topic that explains it really quickly and well, but I<sq>m having no luck. I<sq>ll let you know if I find it.<br><br>A third option would be to estimate some kind of Poisson or Exponential model that helps estimate the expected amount of yield loss each day. Check out the [Poisson Distribution wiki page](https<colon>//en.wikipedia.org/wiki/Poisson_distribution), especially the <dq>parameter estimation<dq> section. You can use the different estimates of lambda (and their confidence intervals or posterior distributions if you go Bayesian) to show how different your two sets of data might be.</p>", 
                "question": "I give up - I need help with Hypothesis Testing and no FAQ I<sq>ve come across in 2 days has helped"
            }, 
            "id": "d1v5gxu"
        }, 
        {
            "body": {
                "answer": "<p>http<colon>//onlinestatbook.com/2/index.html<br><br>For the basics</p>", 
                "question": "Online resources for self-learning"
            }, 
            "id": "d16tqrk"
        }, 
        {
            "body": {
                "answer": "<p>Google Introduction to Statistical Learning for a great textbook. The authors are also starting a free MOOC soon</p>", 
                "question": "Where can I learn (for free preferable) or read about (doesn<sq>t have to be for free) statistics passed the <dq>Stats 101<dq> material?"
            }, 
            "id": "cyvlyuo"
        }, 
        {
            "body": {
                "answer": "<p>[Free](https<colon>//openstaxcollege.org/books) Stats book.</p>", 
                "question": "Where can I learn (for free preferable) or read about (doesn<sq>t have to be for free) statistics passed the <dq>Stats 101<dq> material?"
            }, 
            "id": "cyvp9ef"
        }, 
        {
            "body": {
                "answer": "<p>Check out [Stat 414/415](https<colon>//onlinecourses.science.psu.edu/stat414/). I think this will be the most natural next step after Stats 101. Also, you can watch video lectures from [Stat 110 by Joseph Blitzstein in Harvard](https<colon>//www.youtube.com/playlist?list=PLLVplP8OIVc8EktkrD3Q8td0GmId7DjW0). Good luck!</p>", 
                "question": "Where can I learn (for free preferable) or read about (doesn<sq>t have to be for free) statistics passed the <dq>Stats 101<dq> material?"
            }, 
            "id": "cyvzktc"
        }, 
        {
            "body": {
                "answer": "<p>Go to Coursera or edX for good free online courses.</p>", 
                "question": "Where can I learn (for free preferable) or read about (doesn<sq>t have to be for free) statistics passed the <dq>Stats 101<dq> material?"
            }, 
            "id": "cyw632s"
        }, 
        {
            "body": {
                "answer": "<p>Lets<sq> take a big step back from going through the motions to calculate some numbers. Checking to see if this jump is <dq>statistically significant<dq> is actually kind of silly. Statistical significance is about **inference**<colon> taking data from a *random* sample to try to figure out if changes in a sample can be extrapolated to indicate that the population of differences are likely different. You are looking at sales of data at YOUR business.  Sales increased. In what way are you trying to generalize this one time increase at one business to a population?  In what way is your data a random sample? <br><br>This is sort of like seeing if the amount of pizza **I** am eating now is statistically significantly different from last year.  The question makes no sense. It would only make sense if we had a sample of data from many people at several points in time, and asked if there is an indication of increased pizza eating in the population as a whole.</p>", 
                "question": "How to tell if an increase in sales is significant."
            }, 
            "id": "cxtn9c9"
        }, 
        {
            "body": {
                "answer": "<p>There are a few things you could do. One would be to calculate the sales for every day in month X (5k on monthX_day1, 4.2k on monthX_day2, 6.6k on monthX_day3, etc) then do the same for month Y. Then use those two sets in a welch test. Lets say that the results come back with X > Y with p = .04 . With that you could say that <dq>Daily sales in month X were greater then in month Y and there<sq>s only 4<percent> chance that the difference is just due to noise.<dq> <br><br>Or you could calculate the sales for each month, before nov14, and the sales for each month since nov14. Then use the welch test to say that <dq>Monthly sales since Nov14 have been greater then before Nov14, and there<sq>s only a 4<percent> chance (or what ever the p-value is) that that difference is driven by chance.<dq><br><br>Does that make sense?</p>", 
                "question": "How to tell if an increase in sales is significant."
            }, 
            "id": "cxtfaxe"
        }, 
        {
            "body": {
                "answer": "<p>You should look at it in terms of units sold, not total sales in dollars.<br><br>Then you estimate the standard deviation as the square root of the number of units sold (poisson distribution).</p>", 
                "question": "How to tell if an increase in sales is significant."
            }, 
            "id": "cxtidor"
        }, 
        {
            "body": {
                "answer": "<p>Disclaimer<colon> I haven<sq>t read the article.<br><br>There is no <dq>general rule<dq> for statistically significance.<br><br>One should when creating a hypothesis pick an alpha and thereby also beta depending on whats being analysed (https<colon>//en.wikipedia.org/wiki/Type_I_and_type_II_errors). You will often see physic experiments go below the 0.05 and, likewise others will also go above.</p>", 
                "question": "P-value < 0.06 but results are statistically significant."
            }, 
            "id": "cxgoknc"
        }, 
        {
            "body": {
                "answer": "<p>The threshold for statistical significance is a matter of opinion. It<sq>s all about your risk tolerance really. If you think that that the upper limit for the probability of making a mistake should be 5<percent>, go ahead and use 0.05 and if you think 6<percent> is still tolerable, you should use 0.06 instead. Actually, there are situations where you should be very strict and using the value of 0.01 isn<sq>t unheard of.<br><br>This is a bit suspicious. If the limit is greater than 0.05 I would take a closer look at the following<colon><br>* Did they had a good reason to pick an exceptional limit like that?<br>* Is something else in the study just as exceptional?<br>* Did they publish their data and can I confirm the calculations myself?<br>* Are the scientists independent? Do they have some connections to some relevant companies?</p>", 
                "question": "P-value < 0.06 but results are statistically significant."
            }, 
            "id": "cxgoqnx"
        }, 
        {
            "body": {
                "answer": "<p>0.05 is just a rule of thumb/convention, aka; a line in the sand. 0.06 isn<sq>t bad.</p>", 
                "question": "P-value < 0.06 but results are statistically significant."
            }, 
            "id": "cxgpwo9"
        }, 
        {
            "body": {
                "answer": "<p>[Article](http<colon>//www.bmj.com/content/349/bmj.g4643) in question.</p>", 
                "question": "P-value < 0.06 but results are statistically significant."
            }, 
            "id": "cxgni8i"
        }, 
        {
            "body": {
                "answer": "<p>U.S. Census fact finder </p>", 
                "question": "Data sets for practicing data analysis?"
            }, 
            "id": "cx1t8bb"
        }, 
        {
            "body": {
                "answer": "<p>Kaggle for competing with others on some fun data analysis problems, Quandl for financial data</p>", 
                "question": "Data sets for practicing data analysis?"
            }, 
            "id": "cx23gre"
        }, 
        {
            "body": {
                "answer": "<p>Eurostat for European data, ONS for UK data, World Bank has a lot of good data. If you know what you<sq>re looking for and the right terms to search you can find most things through google, x-variable historical data, x-variable by year, x-variable by country/state etc. </p>", 
                "question": "Data sets for practicing data analysis?"
            }, 
            "id": "cx25g8d"
        }, 
        {
            "body": {
                "answer": "<p>Simplest answer<colon> you have more information about the estimated parameter. The more information you have, the more confident your estimation can be. </p>", 
                "question": "For a specific confidence interval (with fixed \u03b1) the larger the sample size the narrower the confidence interval. Why?"
            }, 
            "id": "cwk3cvy"
        }, 
        {
            "body": {
                "answer": "<p>It would be best for you to reason this out for yourself by looking at how confidence intervals are formed. Consider the confidence interval for a mean and the role the standard error plays. (Look also into the theory behind standard errors.) How is the standard error affected by sample size? <br><br>EDIT<colon> I suppose I was downvoted because it seemed I didn<sq>t answer the question, even though everything one needs to ultimately find the answer is in here.<br><br>I<sq>ll make it explicit, then, for the example above, namely a mean (probably the most common thing you<sq>ll have to find a confidence interval for)<colon> When you form the sum of independent random variables, the variance of the sum is the sum of the variances of the variables. And when you multiply a random variable by a constant, the resulting variable<sq>s variance is the square of that constant times the variable<sq>s variance.<br><br>When you form a mean, you are adding 1/N times each independent sample point you<sq>re taking. The variance of this sum will then be N/N^2 = 1/N times the variance of whatever distribution you<sq>re drawing from. In other words, your uncertainty in the mean, as measured by the variance, is inversely proportional to your sample size. The standard error is the square root of this variance, and it<sq>s this term that defines the limits of your confidence interval, which will then be inversely proportional to the square root of your sample size.</p>", 
                "question": "For a specific confidence interval (with fixed \u03b1) the larger the sample size the narrower the confidence interval. Why?"
            }, 
            "id": "cwk32i0"
        }, 
        {
            "body": {
                "answer": "<p>1) Correlation is not a <percent>.  You must have found +0.7, which is not a <percent> of anything.<br><br>2) What you can infer is that when one product goes up, the other one (more than likely) *usually* does.  However, this need not be the  case.  It would be very easy to create a data set where when one goes up, the other goes down every case but once, and you can get a +.7 correlation.  Please try to avoid over-interpreting what correlation tells you...<br><br>3) The only concrete interpretation that you can get out of a correlation is<colon><br><br>a) Square it, multiply by 100 to get a <percent><br><br>b) This tells you that <dq>49<percent> of the variation in one of the product<sq>s sales can be explained by the other product<sq>s sales<dq>.  By variation, we literally mean the variance  in the number, sum(xi-xbar)^2 /n-1.  If you use one variable to explain the other one, the remaining variation that is unexplained will be 51<percent> of the original variation.  This idea takes a while to digest.<br></p>", 
                "question": "Help me understand correlation."
            }, 
            "id": "cw03qyc"
        }, 
        {
            "body": {
                "answer": "<p>You can infer that the two products tend to move in a similar direction. So yes, when one product has a big increase/decrease in sales, so does the other. You can even use a regression to predict one based on the other, and it won<sq>t do too poorly.<br><br>However, you can<sq>t infer any reason *why* based solely on that correlation. Maybe people buy the two products together. Maybe the purchase of one product encourages buying the other. Maybe they<sq>re both luxury products and people tend to buy more of both when they have money to spare. Maybe, since I assume you<sq>re using data without any kind of seasonal adjustment, there<sq>s a regular and similar pattern to the sales of both (retail sales are known to peak heavily in December, for example, almost as if there<sq>s some kind of event where people are encouraged to engage in rampant consumerism). Even with seasonally adjusted data, if there<sq>s an underlying trend it will muck around with many correlation calculations.<br><br>Also, there<sq>s the fact that correlation looks only at linear relationships. I would highly recommend plotting the data (both as Product A versus Product B and as each one separately over time) to see if there are any obvious patterns.</p>", 
                "question": "Help me understand correlation."
            }, 
            "id": "cw04n70"
        }, 
        {
            "body": {
                "answer": "<p>It<sq>s hard to get to 100<percent> because that<sq>s certainty.<br><br>I<sq>d say the easiest counter argument is a concrete example. Imagine the first of those 46 people has a birthday on January 1st, and each progressive person<sq>s birthday is on the day after the previous. All 46 people have birthdays on different days. None of them share a birthday, therefore since this counter example exists we know for sure that the probability isn<sq>t 100<percent> with 46 people. We actually don<sq>t reach 100<percent> probability until we have 367 people, since only at that point can we be *absolutely certain* each person doesn<sq>t have their birthday on a different calendar day<colon> there *must* be at least one calendar day with at least two people. That said, the probability gets *very close* to 100<percent> well before 367 people. Closer to around 70 people (99.9<percent> probability).<br><br>You should show him the graph here<colon> https<colon>//en.wikipedia.org/wiki/Birthday_problem<br><br>Regarding the intuition behind why you can<sq>t add probabilities, present him with the following<colon><br><br>* First, assert that the probability of a coin getting heads in a single flip is 50<percent>.<br>* Explain that his reasoning would suggest that the probability of a heads in two coin flips is 100<percent>.<br>* Use a coin (he might demand two separate coins) to demonstrate experimentally that this is clearly not the case.<br>* Draw a contingency table to illustrate why the probability of at least one coin showing heads is 75<percent>. I.e. draw this<colon><br><br>                    coin2<br>             coin1  heads  tails<br>             heads     HH    HT<br>             tails     TH    TT<br><br>If the probability of one coin showing heads is 50<percent>, that doesn<sq>t mean the probability of two coins showing a heads is 100<percent>. You can<sq>t add probabilities that way. Probabilities are weird. <br><br>... The exception is mutually exclusive events (which, in the case of 100<percent> probability, must cover the event space). If the probability of showing a heads is 50<percent>, and the probability of showing a tails is 50<percent>, and the event of showing a heads is *mutually exclusive* of the event of showing a tails, then the probability of a coin showing either a heads or a tails is 100<percent>. This is Kolmogorov<sq>s third [axiom](https<colon>//en.wikipedia.org/wiki/Probability_axioms).<br><br>The birthday problem involves some combinatorics that are almost certainly over the head of your eight year old son. If you<sq>re feeling ambitious, read through the wikipedia article to understand how it works, and replace <dq>people sharing the same birthday<dq> with <dq>people sharing the same color shirt<dq> assuming that only some set number of colors exist, or more simply that the party theme is certain colors. Start with black and white. Then increase the number of colors and work out the combinatorics for each increasingly larger set until it gets tedious.</p>", 
                "question": "Birthday stat question for 8 year old son"
            }, 
            "id": "cvplisd"
        }, 
        {
            "body": {
                "answer": "<p>[deleted]</p>", 
                "question": "Birthday stat question for 8 year old son"
            }, 
            "id": "cvpl3cb"
        }, 
        {
            "body": {
                "answer": "<p>Take a simpler problem. In one coin toss I have a 50-50 chance of getting a head. Do I have 100<percent> chance of getting at least one head in two tosses? (No, since there<sq>s a 25<percent> chance of tail, tail, and if there<sq>s *any* chance of some other outcome than <dq>at least one head<dq>, I can<sq>t get its probability to 100<percent>)<br><br>Now this is a different problem, but the point is the same -- you still have some chance that none of those birthdays match. Consider you had 45 people who all have different birthdays and you add a 46th. What<sq>s the chance that person matches none of them? It<sq>s not 0. Similarly, if you had 44 people with different birthdays, and you add a 45th. If you add people one by one, there<sq>s always a nonzero chance of no matches (unless you fill every single date in the year, and then suddenly, you *must* get a match).<br><br>With 46 people (and completely random birthdays, which is only approximately true in practice), there<sq>s actually only about a 95<percent> chance of a match.<br><br>Working it out is easier to do if you say <dq>what<sq>s the chance that there are no matching birthdays?<dq>, and then add people to the room 1 at a time.<br><br>Person 1 has 100<percent> (365/365) chance of no-match.<br><br>Adding person 2 has 364/365 chance of no-match<br><br>Adding person 3 gives 364/365 x 363/365 chance of no-match<br><br>... 23 people has (365 x 364 x 363 x 362 .... x 343 )/(365^(23)) = 0.4927 probability of no-match <br><br>(so the chance of at least one match is 1-0.4927 = 0.5073)<br><br>... 46 people has (365 x 364 x 363 x 362 .... x 320 )/(365^(46)) = 0.0517  <br><br>(so the chance of at least 1 match is 1 - 0.0517 = 0.9483 = 94.83<percent><br><br>Checking my calculation in R<colon><br><br>    > pbirthday(46)<br>    [1] 0.9482528<br><br>Yes, odd as it may seem, the (free) language R comes by default with functions for playing about with the birthday problem. </p>", 
                "question": "Birthday stat question for 8 year old son"
            }, 
            "id": "cvpoz0i"
        }, 
        {
            "body": {
                "answer": "<p>First, if your son did this calculation alone at 8 years old, that<sq>s impressive. That shows a really good understanding of basic mathematical operations. Normally, this kind of deduction skill (23 people = 50<percent> so 46 =100<percent>) is obtained at around 13 years old. So make sure that while the reasonning is wrong and it<sq>s not a right answer, please don<sq>t discourage your son of trying that. As far as an 8 years old have probability knowledge, that<sq>s a really good guess.<br><br>Now for the real answer, here a good [ELI15 explanation](http<colon>//www.reddit.com/r/askscience/comments/3j81fq/came_across_this_fact_while_browsing_the_net_i/cun0yee) from /r/askscience<br><br>For a ELI5, just reduce the numbers. Does he have Lego<sq>s, marbles or any sort of thing that he have multiple items of, with different color ? Let<sq>s pretend he has few white, red & blue shirts. Draw a diagram with him. See what are the possibilities if he pick 2 random. BB, BR, BW, WB, WR, WW, RB, RR, RW. There are 9 possibilities. 3 of which are same color shirts. So 1/3 (33<percent>) possibilites of getting 2 shirts of the same color. With the reasonning of your son, that would mean that 3 times more shirts would make it 100<percent>.<br><br>However, if you do the diagram of picking 3 shirts, you<sq>ll have 27 possibilities where only 3 triplets doesn<sq>t share a color. So 24/27 (89<percent>) does share a color. We have 1 more shirt, but the probability more than doubled.<br><br>If he is still asking question, tell him to try the 4 shirts possibilities (81 outcomes) and he will see that it is 100<percent> sure you have 2 shirts of the same color.</p>", 
                "question": "Birthday stat question for 8 year old son"
            }, 
            "id": "cvpypao"
        }, 
        {
            "body": {
                "answer": "<p>It<sq>s not specifically biostats, but [OpenIntro](https<colon>//www.openintro.org/) is a free online-course in statistics. It comes with a textbook, online practice labs and videos. It uses R for calculations.<br><br>There are several free courses at Coursera that might be interesting<colon><br><br>* [Statistical Reasoning for Public Health 1](https<colon>//www.coursera.org/course/statreasoning)<br>* [Statistical Reasoning for Public Health 2](https<colon>//www.coursera.org/course/statreasoning2)<br><br>You might find even more.</p>", 
                "question": "Studying biostats on your own"
            }, 
            "id": "ctbu8hy"
        }, 
        {
            "body": {
                "answer": "<p>In short, you can<sq>t.  <br><br>There<sq>s rarely a clearly correct answer in statistics.  The whole point of statistical modeling is to simplify what happens in the real world in order to gain a better understanding of it.  That inherently results in assumptions and simplifications that are subjective in nature.<br><br>1. They say hundreds of thousands of users which is good enough for me.<br><br>2. Yeah<br><br>3. Yeah<br><br>4. Yeah, but over a large population this will even out (ignoring how this interacts with points 2 and 3).<br><br>5. They can make whatever conclusions they want, but yeah a 3 hour window isn<sq>t the best from an experimental design standpoint.  This is a tradeoff for obtaining more samples.  Everything is about tradeoffs.<br><br>6. Data is an asset.  Few companies are going to give away raw data for free.  These kinds of companies are built on the exclusivity of their data.<br><br>7. The whole meme of <dq>all graphs should start at zero to show true effect size!!!<dq> is way overblown.  Every choice is arbitrary, and effect sizes that appear small to us on a graph fro 0 to 10 may in fact be significant.<br><br>This isn<sq>t a rigorous scientific experiment, it<sq>s a blog post for the purpose of marketing.  Also, these are data scientists, not statisticians.  The line between the two is fuzzy at best, but in general data scientists play fast and loose with the rules more than traditional statisticians do.<br><br>When you start learning about statistics you look around and realize that every data analysis article you read on that Internet and treated as gospel is actually full of assumptions that may or may not be true and generally total BS.  This is universal across almost every discipline you can study.<br><br>Then when you start getting out into the working world you realize why it<sq>s like that.  These questions are complicated and answering them with 100<percent> rigorous statistical methodology is expensive.  Nobody cares if your marketing blog post is full of holes.  At the end of the day you put something out there while the other guy is sitting there trying to make everything perfect.  Perfection is a luxury.<br><br>This got a bit off topic, but hopefully it was helpful.<br><br>Edit<colon><br><br>>I am wondering what tools I can use, what questions I should ask, and what I should look for when looking at presentations of statistical data that make assertions about the data that would clue me in to those conclusions being wrong or faulty.<br><br>You<sq>re doing fine so far.  Just realize that conclusions are almost always wrong or faulty in some way.  It doesn<sq>t mean they<sq>re not useful.</p>", 
                "question": "How do I know if someone<sq>s statistical analysis can be trusted?"
            }, 
            "id": "csv9crc"
        }, 
        {
            "body": {
                "answer": "<p>One major issue I see with the survey using the app is that it sounds like it<sq>s a self-selected sample<br><br>https<colon>//en.wikipedia.org/wiki/Self-selection_bias<br><br></p>", 
                "question": "How do I know if someone<sq>s statistical analysis can be trusted?"
            }, 
            "id": "csvwsv8"
        }, 
        {
            "body": {
                "answer": "<p>KevJohnson made some very good points, I just wanted to link you to an article that discusses point 7<br><br>http<colon>//qz.com/418083/its-ok-not-to-start-your-y-axis-at-zero/<br><br>Generally yes, a healthy skepticism should go into receiving / reading any analysis, but then again nitpicking at every detail doesn<sq>t work, exactly because we are talking about statistics - since we can<sq>t measure everything we approximate and an approximation is never perfect.<br><br>An important thing to keep in mind is not to confuse healthy skepticism with confirmation bias as in <dq>This can<sq>t be true because it goes against what I have believed before<dq> which I often encounter for example when people are convinced they know what the cause is but the data tell a different story.</p>", 
                "question": "How do I know if someone<sq>s statistical analysis can be trusted?"
            }, 
            "id": "csvz33w"
        }, 
        {
            "body": {
                "answer": "<p>At least ten.<br><br>edit (how I came up with the answer)<colon> I overlaid a grid on the crowd, including 3D morphing at the edges to account for the subtle fish-eye effect. Then using MatLAB and Simulink I used an optical recognition and statistical counting script to estimate the density of each grid and extrapolate to six sigma the topology of the landscape. Finally after integrating over the results of yesterday<sq>s polls of Yes / No supporters, the answer quickly became obvious.</p>", 
                "question": "Roughly how many people are in this protest and you did you get to that answer?"
            }, 
            "id": "css32ct"
        }, 
        {
            "body": {
                "answer": "<p>Count the people along one of the sides then square it for a super rough estimate.</p>", 
                "question": "Roughly how many people are in this protest and you did you get to that answer?"
            }, 
            "id": "cssi38l"
        }, 
        {
            "body": {
                "answer": "<p>Try reading this<colon> http<colon>//polmeth.wustl.edu/media/Paper/gill99.pdf</p>", 
                "question": "Why is it necessary to support your results with null hypothesis significance testing procedures (NHSTP) especially if the sample size reaches the actual population number?"
            }, 
            "id": "cs7xluy"
        }, 
        {
            "body": {
                "answer": "<p>I<sq>m not sure I understand your question. You collect a measure from a population at some sample rate (98<percent>). At that point there is no hypothesis,  so you<sq>re not doing any testing right?<br><br>Certainly as your sampling rate goes up your sample mean and sd is increasingly likely to represent the true population mean and sd. This is represented in the variation of your sampling distribution. If you repeatedly sample at a rate of 98<percent>, you<sq>d expect the resulting distribution to have a much smaller standard deviation than if you<sq>re sampling at a rate of 5<percent>.<br><br>Now, if you mean that you collect the same measure at multiple points in time and want to compare the measure at different points in time, then your null is that they are not significantly different, and you do NHSTP to test whether they are or not.<br><br>EDIT<colon> I wrote a [document in Rmarkdown](https<colon>//github.com/potterzot/rstats/blob/master/samplingDisributionAndSamplingRates.Rmd) to demonstrate what I mean. If you use Rstudio, you can run it easily. Graphs don<sq>t show unfortunately, but they will in Rstudio.</p>", 
                "question": "Why is it necessary to support your results with null hypothesis significance testing procedures (NHSTP) especially if the sample size reaches the actual population number?"
            }, 
            "id": "cs7ctnk"
        }, 
        {
            "body": {
                "answer": "<p>Short answer <colon> Yes, it<sq>s still necessary.<br><br>Example <colon> I have a normal coin (50<percent> head, 50<percent> tail). I want to see <colon> *If I drink water 5 min before tossing the coin during summer 2015 will it affect the coin result ?* .<br><br>We can agree that these events are independant, so I should find no significant link. However, if I record every occurence of this phenomenon, it will have experimental probability. I might have 47<percent> tail and 53<percent> head over the 100 occurences during the summer.<br><br>Even if I have all recorded events, I still need to verify if this variation (47-53 instead of 50-50) is <dq>normal<dq> and expected. Otherwise I<sq>d say <colon> <dq>Oh! I can affect the result of the coin<dq>, when in fact, it<sq>s all good.</p>", 
                "question": "Why is it necessary to support your results with null hypothesis significance testing procedures (NHSTP) especially if the sample size reaches the actual population number?"
            }, 
            "id": "cs8901b"
        }, 
        {
            "body": {
                "answer": "<p>Doing Bayesian Data Analysis by John Kruschke is not just the best textbook on Bayesian methods that I have found, but it is the best textbook on any subject I have found.  The time it took me from being a complete noob to using Bayesian methods in real life was about a week.  You learn as you go, doing real programming in R for problems that you can easily adapt to real world situations.  I can<sq>t recommend it highly enough.<br><br>Most other books are going to be very academic, and while that may be appropriate in some circumstances, the result is that those books are short on practical advice.</p>", 
                "question": "Best textbook or book to teach the Bayesian approach?"
            }, 
            "id": "cs3y2d7"
        }, 
        {
            "body": {
                "answer": "<p>[deleted]</p>", 
                "question": "Best textbook or book to teach the Bayesian approach?"
            }, 
            "id": "cs3xlqp"
        }, 
        {
            "body": {
                "answer": "<p>Great question! Would love to see some suggestions from others - maybe posting this to a subreddit with more exposure. That actually reminds me of [an article I saw a while back](http<colon>//www.randalolson.com/2013/03/15/a-data-driven-guide-to-creating-successful-reddit-posts/), from a site that, based on this post, you might enjoy.</p>", 
                "question": "How can I use statistical methods to improve my everyday nonprofessional life?"
            }, 
            "id": "crxsn5o"
        }, 
        {
            "body": {
                "answer": "<p>You could look at [Quantified Self](http<colon>//quantifiedself.com/). People do a lot of collecting data about their daily activities and analyzing that to make behavior changes. Seems like a cool thing to be doing with others, especially if you live a city that has it.</p>", 
                "question": "How can I use statistical methods to improve my everyday nonprofessional life?"
            }, 
            "id": "cry6d92"
        }, 
        {
            "body": {
                "answer": "<p>If you have a proportion of procedures complied with you want something from the chi-squared family. You can do this across the whole data set and it will tell you if something has changed, and if it<sq>s significant you<sq>ll need to do some follow up to see where it is. </p>", 
                "question": "Physician awful at stats-help!"
            }, 
            "id": "cqi0aqc"
        }, 
        {
            "body": {
                "answer": "<p>It depends what are you looking for.<br><br>If you investigate through the 4 years, then yes, as /u/aelendel/ says, a chi-squared is what you look for. Something like <colon><br><br>COMPLIED\\YEAR|YEAR01|YEAR02|YEAR03|YEAR04<br><colon>--|<colon>-<colon>|<colon>-<colon>|<colon>-<colon>|<colon>-<colon><br>YES|34|50|57|50<br>NO|65|35|45|100<br><br>Where your chi-square will tell you if through the years, something has changed. It won<sq>t tell you what or when, but it<sq>ll tell you to look into.<br><br>However, if you are looking especially for the last year and compare if especially last year was different from other year, then a Z-test for proportion, 2 independent groups is what you are looking for. You<sq>d compare the proportion of group A (this year) against group B (previous years combined) and see if differs significantly. <br><br>Just a reminder that if something significant changed in your data (i.e. major changes in compliance policies) you shouldn<sq>t include data previous to that change except if you are trying to prove that the major change did affect the compliance rate.</p>", 
                "question": "Physician awful at stats-help!"
            }, 
            "id": "cqif575"
        }, 
        {
            "body": {
                "answer": "<p>I think you can think about it this way <colon> <br><br>First, from n genes you can obtain sequences ranging from 0 to n genes in length (as genes can be deleted). The number of possible sequences can be found [here](http<colon>//en.wikipedia.org/wiki/Partition_of_a_set#Counting_partitions)<br><br>Second, the genes that are present can be ordered in any way (this is a shuffling problem. We know that the number of outcomes for shuffling of n items is n!)<br><br>Then, the genes that are present can be either in the forward or reverse state. So 2 possibilities per genes, 2^n possibilities total (this is assuming that all genes reversed is different from all genes in the original direction, which is probably true if you have some intergene sequences ?)<br><br>If you combine all of this <colon><br>First you have to sum over the number of possible partitions k. For a given partition, let i be its cardinal (number of elements = number of genes present)<br><br>For each partition k of length i, there are i! possible orders<br><br>For each order of length i of a partition k, there are 2^i possible outcomes due to orientation<br><br>I<sq>m sorry I<sq>m at work so I<sq>ll stop here but hopefully this helped<br><br>Edit <colon> typos & grammar</p>", 
                "question": "Determining total number of outcomes in a genome engineering experiment..."
            }, 
            "id": "cqawx3b"
        }, 
        {
            "body": {
                "answer": "<p>Usually, the uppercase is for the random variable, and lowercase is for a particular value of the variable. A sample from X can take values x1, x2, etc. I don<sq>t know if everyone<sq>s always consistent on this, especially for a class lecture where you could easily make a mistake on a slide. <br><br>Capital X-bar is the population mean (the mean of the whole dang variable), while lowercase x-bar would be the mean of a sample, typically. So the slide seems right in this case!<br><br>There<sq>s a [cheat sheet here](http<colon>//en.wikipedia.org/wiki/Notation_in_probability_and_statistics) with some common notation you might run in to.</p>", 
                "question": "When to use lowercase or capital letters for variables"
            }, 
            "id": "cq676xu"
        }, 
        {
            "body": {
                "answer": "<p>See p. 12 for a start to understand the differences<colon> <br>http<colon>//www.stats.ox.ac.uk/pub/MASS3/Exegeses.pdf<br>Also, see Maxwell and Delaney (Designing Experiments and Analyzing Data) for a different take, but with great explanations of differences. There is a big discussion on the differences among the R community.  </p>", 
                "question": "What<sq>s the difference between Type I II and III Sums of Squares Analysis in ANOVA?"
            }, 
            "id": "cq4df4c"
        }, 
        {
            "body": {
                "answer": "<p>I<sq>ve always found [this](http<colon>//afni.nimh.nih.gov/sscc/gangc/SS.html) to be one of the briefest but most complete explanations of the types of SS.<br><br>While that<sq>s a site for neuroimaging tools -- that can be ignored. These pages address a broad topic within ANOVAs with respect to SS.</p>", 
                "question": "What<sq>s the difference between Type I II and III Sums of Squares Analysis in ANOVA?"
            }, 
            "id": "cq515am"
        }, 
        {
            "body": {
                "answer": "<p>Type I to IV is a SAS thing. You could read their manual too. I am not sure XLSTAT uses the same names&numbers, but SAS has a big name, so small programs tend to follow. <br><br>In SAS type III is the one most often used.</p>", 
                "question": "What<sq>s the difference between Type I II and III Sums of Squares Analysis in ANOVA?"
            }, 
            "id": "cq4fq36"
        }, 
        {
            "body": {
                "answer": "<p>Conceptually, the difference is how the other effects (including possible interactions) are treated when you<sq>re dividing up the sums of squares for your effect of interest and the error term. [This might help summarize](http<colon>//goanna.cs.rmit.edu.au/~fscholer/anova.php).</p>", 
                "question": "What<sq>s the difference between Type I II and III Sums of Squares Analysis in ANOVA?"
            }, 
            "id": "cq4rvhl"
        }, 
        {
            "body": {
                "answer": "<p>Correspondence analysis -- a SVD technique designed to analyze counts.</p>", 
                "question": "Is it okay to use Singular Value Decomposition in the following situation?"
            }, 
            "id": "cpyz1ot"
        }, 
        {
            "body": {
                "answer": "<p>Let<sq>s say you are interested in voting outcomes in the united states. More specifically, you want to understand the voting outcomes of the small town in which you live. For simplicity, let<sq>s assume people can vote in one of three ways<colon> Democrat, Republican, or they can decide not to vote. (Multinomal means you have more than two possible categorical outcomes, in this case three.)<br><br>There<sq>s a 1000 people in your little village, and you would like to understand what it is about people that makes them vote one way or another. At the day of the election you went by all the houses to collect information about the citizens. You now have data on the vote (or no vote) that each person cast, as well as their age, ethnicity, income, religion etc. etc.<br><br>Using logistic regression you now try to estimate how each of those variables (age, ethnicity) together influenced how a citizen voted.</p>", 
                "question": "ELI5 multinomial logistic regression. (if thats even possible)"
            }, 
            "id": "cp6olvf"
        }, 
        {
            "body": {
                "answer": "<p>Do you understand binary logistic regression? <br> <br>Binary classification using logistic regression uses a dot product between an input vector **x** and a parameter vector **w** to answer a yes/no question<colon> does the input belong to class 1 or doesn<sq>t it. Remember that the dot product between two vectors A and B is sum(A[1]\\*B[1], A[2]\\*B[2], ... , A[n]\\*B[n]) and results in a scalar value. The scalar value is meaningless on its own, however (what does it mean that input *x* yields 32.1?), so we use the sigmoid function to map the value to the interval [0;1]. We can then interpret the answer as a probability distribution over class membership, i.e. P(x belongs to class 1) = 1/(1+exp(-**x**^T **w**)), and conversely, P(x does not belong to class 1) = 1 - 1/(1+exp(-**x**^T **w**))<br><br>Multinomial logistic regression just extends this concept to more than two classes, and instead of a single parameter vector we now have a vector for each class. We then ask of our model if **x** belongs to each class by - you guessed it - computing the dot product between **x** and each of the class-specific parameter vectors. We do an additional step to compute the full probability distribution, but the main concept remains unchanged<colon> if we provide the model with an input **x** that truly belongs to class K, then the model is accurate when the dot product between **x** and class K<sq>s parameter vector is greater than the values of the other dot products. <br></p>", 
                "question": "ELI5 multinomial logistic regression. (if thats even possible)"
            }, 
            "id": "cp6peur"
        }, 
        {
            "body": {
                "answer": "<p>There<sq>s an expression for the variance of the sample variance in terms of kurtosis and squared variance here<colon><br><br>http<colon>//stats.stackexchange.com/questions/29905/reference-for-mathrmvars2-sigma4-left-frac2n-1-frac-kappan<br><br>... so now you just need to be able to account for the uncertainty in the sample kurtosis ...<br><br>Or you could use bootstrapping (or simulation under some reasonable assumption)</p>", 
                "question": "How do you estimate the error of a variance?"
            }, 
            "id": "co37ws7"
        }, 
        {
            "body": {
                "answer": "<p>If N is large enough then you can conduct a Monte Carlo, calculating variances on randomly selected subsets of the data.</p>", 
                "question": "How do you estimate the error of a variance?"
            }, 
            "id": "co2susj"
        }, 
        {
            "body": {
                "answer": "<p>It isn<sq>t clear what your goal is in your <dq>analysis<dq>. Are you trying to make some sort of inference?  Test some hypothesis?  Or just visualize and understand the responses?<br><br>Also unclear<colon> What does a <dq>trimmed mean<dq> have to do with making a confidence interval, and what do you mean when you talk about <dq>exclusionary<dq> and <dq>skews the data<dq>?<br><br>Since it is doubtful that your samples are a random subset of people from any population, it is unclear what a confidence interval would mean.  Perhaps just making a boxplot to visualize the data would be sufficient, but as I said, your goal is not clear.  Please elaborate! \u263a</p>", 
                "question": "Seeking the correct confidence interval to use for Poll results"
            }, 
            "id": "cnyiuc0"
        }, 
        {
            "body": {
                "answer": "<p>I think what /u/BurkeyAcademy is getting at is that the goal of a Confidence Interval (CI) is to estimate a parameter (true, underlying value) based on a statistic (from your sample). <br><br>In other words, say you want to estimate the average concentration of some chemical in a body of water. You take a sample, take the mean of that sample, and develop a CI using that sample statistic to determine the true mean concentration of that chemical. This is whats known as inference.<br><br>In your case, as I understand it, you aren<sq>t trying to make an inference. Rather you want to describe the results of the poll (applying these poll results to all players who play the same position in a given year would be an example of making an inference, and you would need to develop a CI). You say your goal is to <dq>find the average score of each player.<dq> Well by means of this poll you have achieved your goal! A confidence interval won<sq>t help you because you aren<sq>t trying to make some conclusion about a larger population.<br><br>I think what would help you is something like some line graphs or bar charts to visualize each players average scores over time.</p>", 
                "question": "Seeking the correct confidence interval to use for Poll results"
            }, 
            "id": "co3qhvc"
        }, 
        {
            "body": {
                "answer": "<p>There<sq>s the joke about the statistician^1 who is found taking a bomb onto a plane. When asked why he did so, he explained <dq>For safety - there<sq>s a 1 in 1,000 chance of there being a bomb on the plane, but the chance of there being a second bomb is 1 in a million!<dq><br><br>^1 Clearly not a good statistician if he fails to understand conditional probability.</p>", 
                "question": "Has anyone got some good examples of [fun] <dq>misuse<dq> of Statistics."
            }, 
            "id": "cmsbva6"
        }, 
        {
            "body": {
                "answer": "<p>The average person has 1.999 legs.<br>I<sq>ll leave it you to figure out why.<br><br><br>Source<colon> The Joy Of Stats, which you can watch for free online.<br><br><br>Three statisticians are hunting and they spot a duck. The first shoots too high, the second shoots too low, and the third jumps up and yells <dq>we got it!<dq><br><br><br>A statisitician is in a hotel room working when her wastepaper basket catches fire. Seeing this, she starts a number of other fires in the room, setting off an alarm and forcing her to open the door. The hotel manager yells into the room, <dq>WHAT ARE YOU DOING?<dq>. She replies stoically <dq>getting a bigger sample.<dq>.</p>", 
                "question": "Has anyone got some good examples of [fun] <dq>misuse<dq> of Statistics."
            }, 
            "id": "cmseot8"
        }, 
        {
            "body": {
                "answer": "<p>Have you heard about the correlation between ice cream and drowning deaths? Days with the most sales of ice cream also see the most people drown... But correlation does not imply causation! There must be many different cases of this kind of misuse.</p>", 
                "question": "Has anyone got some good examples of [fun] <dq>misuse<dq> of Statistics."
            }, 
            "id": "cmslulz"
        }, 
        {
            "body": {
                "answer": "<p>Even with all of modern medicine the average death rate among humans is still 100<percent>. </p>", 
                "question": "Has anyone got some good examples of [fun] <dq>misuse<dq> of Statistics."
            }, 
            "id": "cmsm0vh"
        }, 
        {
            "body": {
                "answer": "<p>I don<sq>t have an answer for you because I think you<sq>re me. I<sq>ve been thinking the same thing.  I want a degree in data analysis and stats is the closest I can think of.</p>", 
                "question": "Considering going back to school for statistics... only have a BA in Economics"
            }, 
            "id": "cmje3cw"
        }, 
        {
            "body": {
                "answer": "<p>I<sq>m not a statistician or anything but if you<sq>re going to make the decision, i<sq>ve seen people do a lot worse.  I<sq>m sure you<sq>ve at least had the intro stats stuff tossed out of the way.  Maybe an econometrics course too?  Econ and stats have a lot more overlap than you<sq>d think.  <br><br>What i<sq>d say is probably most important are the math classes.  Having close to/all of your calculus done is a step-up.  I don<sq>t know enough to say <dq>yes or no<dq> for you, but I don<sq>t think you<sq>re too out of line.  But going to graduate school - as you<sq>ve probably know - is a lot of time, and a lot of money.  If you consider going, make sure it<sq>s for a good reason, and not a <dq>spur of the moment<dq> type of decision.  </p>", 
                "question": "Considering going back to school for statistics... only have a BA in Economics"
            }, 
            "id": "cmje73y"
        }, 
        {
            "body": {
                "answer": "<p>My nephew had a BA in psychology and went to Carnigie Melon for a MA in statistics. It is a one year program. He spoke well of it and had several job opportunities right out the door.<br><br>http<colon>//www.stat.cmu.edu/academics/graduate/the-masters-in-statistical-practice-program </p>", 
                "question": "Considering going back to school for statistics... only have a BA in Economics"
            }, 
            "id": "cmjeb4f"
        }, 
        {
            "body": {
                "answer": "<p>This was me a year ago - In the northeast, 28, work as a research analyst w some excel<br>Skills, bachelors in Econ and interested in statistics. I researched some programs and went with the master of applied stats program at Villanova. It<sq>s my first semester so I can<sq>t really comment on the program as a whole, but I like it so far. 10 courses, or there<sq>s a 5 course certificate program. I know PSU has an online masters program, also west Chester university has a mater program. <br><br>This program doesn<sq>t require a stat background, but Calc I II and III and linear algebra and pre requisites. If you<sq>re like me you didn<sq>t take past calc I for Econ so you might need to get these done before you start, look into being a guest student at a community college to get them out of the way. <br><br>Other than that look at some programs and the courses they require and electives they offer, you can probably find course descriptions online. Might need some letters of recommendation and a BS statement of goals, also might need GREs, but I know the cert program at Villanova doesn<sq>t require GREs.</p>", 
                "question": "Considering going back to school for statistics... only have a BA in Economics"
            }, 
            "id": "cmjieou"
        }, 
        {
            "body": {
                "answer": "<p>Hey!<br><br>If you have access to EViews at all, that has a pretty good point and click automatic process for Granger Causality testing, otherwise you can use R which is free and probably would have something similar without worrying about Box-Cox etc. <br><br>Most of the time with these sorts of things you fit a whole tonne of models, and then choose between them using an information criteria, like the AIC or SIC. The general idea with these is that less is always better. <br><br>Also, with the p-value, if your p-value is less than your level of significance, then it is statistically significant.<br><br>Hope that helps a bit <colon>)</p>", 
                "question": "Help with Granger Causality test as a hobby?"
            }, 
            "id": "cmchslp"
        }, 
        {
            "body": {
                "answer": "<p>I think you calculated it more or less correctly. <br><br>Comparing the NYT numbers and yours.  The differences I see are that you didn<sq>t add in an estimate for miscarriages (pregnancy that doesn<sq>t result in live birth or abortion) and they used 2006 numbers, while you used 2010(?) numbers.<br><br>The document that NYT references has this as a source http<colon>//www.guttmacher.org/pubs/USTPtrends.pdf </p>", 
                "question": "Kristof in NYT writes 1/3rd of Teens Get Pregnant I came up with 18<percent>"
            }, 
            "id": "cm2snmi"
        }, 
        {
            "body": {
                "answer": "<p>There number is probably right but it is disingenuous in the sense that when people talk about teen pregnancy they dont mean someone old enough to vote.</p>", 
                "question": "Kristof in NYT writes 1/3rd of Teens Get Pregnant I came up with 18<percent>"
            }, 
            "id": "cm2s7og"
        }, 
        {
            "body": {
                "answer": "<p>So we<sq>ve got one that says I<sq>m close and another that says they<sq>re close.  Someone give me some more detail. <colon>)</p>", 
                "question": "Kristof in NYT writes 1/3rd of Teens Get Pregnant I came up with 18<percent>"
            }, 
            "id": "cm3jx1a"
        }, 
        {
            "body": {
                "answer": "<p>It has a collection of very nice properties relating to [efficiency](http<colon>//stats.stackexchange.com/questions/10578/intuitive-explanation-of-fisher-information-and-cramer-rao-bound), how it behaves under [transformations](http<colon>//en.wikipedia.org/wiki/Maximum_likelihood#Functional_invariance) (both of data and of parameters), nice results relating to [asymptotic distributions](http<colon>//en.wikipedia.org/wiki/Maximum_likelihood#Asymptotic_normality)  ... and several more things besides. </p>", 
                "question": "ELI5<colon> why is maximum likelihood so important?"
            }, 
            "id": "clu926c"
        }, 
        {
            "body": {
                "answer": "<p>Historically<colon> it led to the broader development of statistical science.<br><br>Parameter estimation<colon> if you are willing to assume the distribution family, it is the easiest method for finding the best parameter estimates.<br><br>Hypothesis testing<colon> there is no testing procedure more powerful than an ML-based approach.<br><br>EDIT<colon> And because maximizing a likelihood function enables you to be cool like me and /u/no_dammit</p>", 
                "question": "ELI5<colon> why is maximum likelihood so important?"
            }, 
            "id": "clu4iiz"
        }, 
        {
            "body": {
                "answer": "<p>[deleted]</p>", 
                "question": "I want to learn more about statistics and using computer tools for gathering and analysis. Where do I start?"
            }, 
            "id": "cjag6mo"
        }, 
        {
            "body": {
                "answer": "<p>To start, what experience (if any) do you have with applied statistical analysis?<br><br>If the answer is <sq>not much,<sq> then you might want to consider something like [Coursera<sq>s Data Science specialization](https<colon>//www.coursera.org/specialization/jhudatascience/1/courses)<colon> this is a series of courses offered by Johns Hopkins detailing the fundamentals of the data acquisition, analysis, and presentation process. Go through some good introductory statistics books like [Introduction to Statistical Learning](http<colon>//www-bcf.usc.edu/~gareth/ISL/index.html) and [The Elements of Statistical Learning](http<colon>//statweb.stanford.edu/~tibs/ElemStatLearn/). Find some [freely](https<colon>//github.com/rasbt/pattern_classification/blob/master/resources/dataset_collections.md) [available](http<colon>//www.data.gov/) [datasets](https<colon>//github.com/datasets) and play around with them<colon> figure out what statistical analyses are appropriate for a given dataset and how to apply and interpret them.<br><br>After you<sq>ve done all that, figure out a research question. Wanting to <sq>analyze the site elance.com<sq> is too broad<colon> what are your variables of interest? Are you seeking to predict something, to evaluate differences in populations, or something else entirely? Is this even a statistics problem? You want to find out about <sq>potential niche opportunities,<sq> but what does that mean? How do you operationalize that (i.e., how do you quantify a niche opportunity)?<br><br>Ask yourself if you even have access to the data that you would need. Does elance publicly publish the relevant data? Can you write a script to acquire it from their public-facing pages? Or do you just not have access to it at all?<br><br>All told, if you aren<sq>t going in with a set of fairly specific questions in mind, you<sq>ll more likely than not be lost. It<sq>s much easier to discover new avenues for analysis given an existing research question than it it to extrapolate relevant questions from a heap of data.</p>", 
                "question": "I want to learn more about statistics and using computer tools for gathering and analysis. Where do I start?"
            }, 
            "id": "cjab8qj"
        }, 
        {
            "body": {
                "answer": "<p>So if I<sq>m understanding correctly, you<sq>re attempting to reduce the 11 items you used in your research down into linear components.<br><br>First and foremost, factors and components are related yet different concepts (I saw you used them interchangeably). what is your overall mission in this reduction? To examine latent variables that may <dq>cause<dq> variance in these items, or just break them down into linear components (kinda like taking an average)?</p>", 
                "question": "Confusion about principal component analysis"
            }, 
            "id": "cibat3g"
        }, 
        {
            "body": {
                "answer": "<p>Not appropriate, due to the unbalanced design (eh) but more importantly because two of the effects are within groups (time and congruency). That 2x2x2 seems to be for all between groups. <br><br>Download R. It<sq>s free and not in a web page where I have no idea about the validity of the stats backing it up. <br><br>Then run something like <br><br>    Summary(aov(stroopscore~congruency*drug *time+ error(subject/time+congruent)), data=data))<br><br>If one was truly worried about the unbalanced, and wanted to be a good statistician try would move away from aov and use Rs lmer function from lme4</p>", 
                "question": "I<sq>m stumped."
            }, 
            "id": "cgbsu5o"
        }, 
        {
            "body": {
                "answer": "<p>I don<sq>t have an answer for your question at the moment, but I want to thank you for writing such a clear question. </p>", 
                "question": "I<sq>m stumped."
            }, 
            "id": "cgbwza4"
        }, 
        {
            "body": {
                "answer": "<p>Seriously, use R, not some webpage.</p>", 
                "question": "I<sq>m stumped."
            }, 
            "id": "cgbvfa9"
        }, 
        {
            "body": {
                "answer": "<p>I can see potential ways to approach this. <br><br>Are these transitions completely general, or is there (or even likely to be) some lower-dimensional subspace of transitions that both groups will fall into? (e.g. might the SVD of the transition matrix have fewer than 12 nonzero singular values, or at least be well approximated by it?)<br><br>In the most general case, power will be a problem. Are there any *particular* kinds of difference you want to have a good chance to pick up?<br><br></p>", 
                "question": "Statistics for Markov chains?"
            }, 
            "id": "cf7bqhc"
        }, 
        {
            "body": {
                "answer": "<p>I may be missing something, but I think that there is no concept of statistical difference between the two models because the parameters of the models are non-random.  They either are different or the same, no need for hypothesis testing.<br><br>On the other hand, if you estimated the components of your matrices from some data, then you have some basis for determining if the differences between the two models are purely due to chance or not.</p>", 
                "question": "Statistics for Markov chains?"
            }, 
            "id": "cf7frry"
        }, 
        {
            "body": {
                "answer": "<p>either your prior generally will penalize having too many clusters (overfitting) or you introduce a penalty or cost to adding another cluster. otherwise, you can end up with each datapoint being its own cluster. </p>", 
                "question": "How do modern nonparametric methods end up with good models?"
            }, 
            "id": "cdfowwv"
        }, 
        {
            "body": {
                "answer": "<p>Section 3.4 of Bishop<sq>s <dq>Pattern Recognition and Machine Learning<dq> (available for free online) has a discussion of how Bayesian models intrinsically penalize model complexity. I<sq>m going to try and give an intuitive explanation here - you should definitely check out the book if you want to try and nail down the math a bit more. It<sq>s fantastic.<br><br>The basic idea is that, for two models which fit the data equally well, the model evidence will be better for the model whose prior probability is more concentrated, and simpler models tend to have more concentrated priors. Recall Bayes<sq> theorem (sorry for the extremely tiny proportionality symbol, I can<sq>t find any better alternative)<colon><br><br>p(Model | Data) &#8733; p(Data | Model) p(Model)<br><br>For intuition<sq>s sake, think of the model space (i.e. the set of possible values for Model) as discrete and finite. The argument below will carry over to infinite and continuous model spaces under mild conditions.<br><br>Imagine you have a simple model and you<sq>re going to increase its complexity, say by adding more parameters. Then the number of possible parameter settings increases, being multiplied by the number of possible states for the new parameters (assuming they are independent of the old ones). Put another way, the set of possible values for Model expands. Since p(Model) has to sum to one over whatever model space you<sq>re in, the value of p(Model) has to get smaller as the set of possible values for Model gets bigger. For example, if the prior is uniform and the set of possible parameter values is of size N, we have p(Model) = 1/N.<br><br>Now suppose you have two models which explain the data equally well, so that the term p(Data | Model) in Bayes<sq> theorem is the same for both models. Which one will have a higher posterior probability? It will be the one with the higher prior probability. Assuming the prior is  uniform for both models, this will be the model with fewer possible configuration states, that is, the less complex model.<br><br>This explanation also shows the limitation of the Bayesian approach. As usual, everything depends on the specific form of the prior associated with each model, and usually the reason for the prior chosen in standard methods is nothing more than computational convenience. So the complexity penalty, while it falls out naturally given your prior assumptions, is still fundamentally subjective.<br><br>Hope this helps, and I am certainly very interested if anyone wants to refine or correct this.</p>", 
                "question": "How do modern nonparametric methods end up with good models?"
            }, 
            "id": "cdfq1er"
        }, 
        {
            "body": {
                "answer": "<p>Bayesian nonparametrics works by either truncating the limit (and proving this provides a good approximation) or by marginalizing over the thing that is going infinite. For DPMs, strategies based on the former give stick-breaking approximations, and strategies based on the latter give CRP based Gibbs samplers/variational schemes.<br><br>Computationally, people use MCMC or variational inference to get around the fact that they can<sq>t look at what is happening in every possible partition of the data. Like the EM algorithm for Gaussian mixtures, this is a difficult multi-modal problem, with the added difficulty of not knowing the number of clusters.<br><br>You should just read the NIPS Rasmussen paper from the early 2000s on this. He goes into plenty of detail for a CS person. [You can find it here](http<colon>//www.gatsby.ucl.ac.uk/~edward/pub/inf.mix.nips.99.pdf).</p>", 
                "question": "How do modern nonparametric methods end up with good models?"
            }, 
            "id": "cdfpi66"
        }, 
        {
            "body": {
                "answer": "<p>There<sq>s an identifiability problem with CRPs, but what you<sq>re interested in how often two of your data items are assigned to the same cluster anyway, and that<sq>s relatively easy to capture.</p>", 
                "question": "How do modern nonparametric methods end up with good models?"
            }, 
            "id": "cdfuozw"
        }, 
        {
            "body": {
                "answer": "<p>maybe I<sq>m overlooking/misunderstanding something, but is there any reason why you couldn<sq>t perform garden-variety OLS multiple regression? seems like the simplest way to get it done to me.</p>", 
                "question": "Multivariate Help - Tigers vocalizations and weather patterns"
            }, 
            "id": "cbcz9fu"
        }, 
        {
            "body": {
                "answer": "<p>I reckon that the tigers are clustered in a number of areas where the weather is the same and thus you need to account for that correlation somehow (robust standard errors, multilevel models, etc.). Furthermore, I am not sure whether you want to model all those vocalization variables simultaneously; in that case, you need a model that can accommodate multiple outcomes (-> multilevel models!).</p>", 
                "question": "Multivariate Help - Tigers vocalizations and weather patterns"
            }, 
            "id": "cbd9nx0"
        }, 
        {
            "body": {
                "answer": "<p>You can do PLS regression, that will take into account relations between vocal pattern characteristics and between weather data to relate these two together. msg me for details if you need them</p>", 
                "question": "Multivariate Help - Tigers vocalizations and weather patterns"
            }, 
            "id": "cc2zsgr"
        }, 
        {
            "body": {
                "answer": "<p>If you<sq>re looking for something numerically/computationally based, I worked in a group that used [Calvetti and Somersalo](http<colon>//www.dmmm.uniroma1.it/~luca.paulon/i2bsc_book.pdf) as our Bayesian reference. It<sq>s not necessarily what I<sq>d call a beginner<sq>s introduction but if you<sq>ve had a few applied frequentist classes you<sq>re probably the target audience anyway.</p>", 
                "question": "Beginners introduction to bayesian statistics"
            }, 
            "id": "cafdh3t"
        }, 
        {
            "body": {
                "answer": "<p>Maybe *Doing Bayesian Data Analysis<colon> A Tutorial with R and BUGS* ? Didn<sq>t read a lot of it, but it seems progressive and focused on applications <colon><br><br>http<colon>//www.indiana.edu/~kruschke/DoingBayesianDataAnalysis/</p>", 
                "question": "Beginners introduction to bayesian statistics"
            }, 
            "id": "cafgdac"
        }, 
        {
            "body": {
                "answer": "<p>I know almost nothing about statistics so this might be completely unhelpful but I found [some](http<colon>//yudkowsky.net/rational/bayes) seemingly [relevant](http<colon>//yudkowsky.net/rational/technical) information [online](http<colon>//lesswrong.com/lw/oj/probability_is_in_the_mind/).</p>", 
                "question": "Beginners introduction to bayesian statistics"
            }, 
            "id": "caftn49"
        }, 
        {
            "body": {
                "answer": "<p>Lessee now - what is the duty cycle of human poopage? <br><br>Let<sq>s say a person spends 15 min on the john twice a day (for #2)<br><br>Prob of a person pooping during a 24 period is 0.5 hours / 24 hours is .02<br><br>Prob of NOT pooping is 0.98<br><br>With 44 guys the prob of anyone pooping at any time is 1 - .98 ^ 44 or 0.588<br><br></p>", 
                "question": "What<sq>s the probability that there will be someone already pooping in my residential hall if I go inside the bathroom?"
            }, 
            "id": "c865xim"
        }, 
        {
            "body": {
                "answer": "<p>Is poop time deterministic?</p>", 
                "question": "What<sq>s the probability that there will be someone already pooping in my residential hall if I go inside the bathroom?"
            }, 
            "id": "c8634cl"
        }, 
        {
            "body": {
                "answer": "<p>I think this is more of a probability theoretical question. Specifically, this kind of problem can be analyzed via [queueing theory](http<colon>//en.wikipedia.org/wiki/Queueing_theory). You<sq>d have to specify what the probability distributions of the interarrival time (that is, the times between two visitors) and the service time (the time someone occupies a stall, which we can model as being random) are, and if there<sq>s a waiting room, etc. etc.</p>", 
                "question": "What<sq>s the probability that there will be someone already pooping in my residential hall if I go inside the bathroom?"
            }, 
            "id": "c86dkry"
        }, 
        {
            "body": {
                "answer": "<p>Here<sq>s a suggestion on how to structure your approach<colon><br><br>View the problem from a graph theoretic standpoint. Each person is a node, and a conversation between two people is an undirected edge. For a secret from person *u* to get to person *v*, there must be a series of conversations, in other words a route of edges, between them.<br><br>With this structure the problem becomes finding the minimum number of edges such that there is a path from any node to any other node. This in mind, I recommend reading up on the concept of a [spanning tree](http<colon>//en.wikipedia.org/wiki/Spanning_tree), the [shortest path problem](http<colon>//en.wikipedia.org/wiki/Shortest_path), and their intersection at the [minimum spanning tree](http<colon>//en.wikipedia.org/wiki/Minimum_Spanning_Tree). <br><br>You should be able to treat the numbers of people involved as fully connected graphs (with each edge as a <sq>potential conversation<sq> and having weight 1) and apply the appropriate algorithms from there.</p>", 
                "question": "A variation on the handshake problem"
            }, 
            "id": "c70a40g"
        }, 
        {
            "body": {
                "answer": "<p>Structure it like a balanced binary tree, such that person 1 and 2 are two leaves of a parent, 3 and 4 another, and so on<br><br>At each point, they share, and that bubbles up to the parent, who is then one of the two children (it doesn<sq>t matter which, for this explanation) - so you do N/2 of those evaluations to get to the next level of the tree, then you do N/4, etc. Eventually you get to two at the root, each with ~half the secrets, sharing to get 2 people with whole, then you can evaluate the whole thing backwards again - giving you 2(whatever the runtime of that is) as an upper bound.<br><br>You could probably do cleverer things involving the set of total secrets any given participant has at step N, but I don<sq>t think you<sq>ll get much better on the total number of conversations required, even if the people involved are more disparate.</p>", 
                "question": "A variation on the handshake problem"
            }, 
            "id": "c70facr"
        }, 
        {
            "body": {
                "answer": "<p>This problem is really interesting. I just tried experimentally to figure out the lowest amount of conversations and found the following (total people>conversations necessary). 2>1, 3>3, 4>4, 5>6, 6>8, 7>10. I just posted nonsense. Please let me know if you solve this though.</p>", 
                "question": "A variation on the handshake problem"
            }, 
            "id": "c70kulq"
        }, 
        {
            "body": {
                "answer": "<p>2n-4 works for 4 and up</p>", 
                "question": "A variation on the handshake problem"
            }, 
            "id": "c714ihm"
        }, 
        {
            "body": {
                "answer": "<p>1 - (61/64)^n<br><br>The probability of not having a stop codon the first trial is 61/64 (hope that<sq>s obvious). The probability of not having a stop codon in n trials is then (61/64)^n, since you can<sq>t have a stop codon in any of the n codons. Then 1 minus that is the probability of having at least one stop codon.</p>", 
                "question": "What<sq>s the probability of stop codons in random genes?"
            }, 
            "id": "c5tchzo"
        }, 
        {
            "body": {
                "answer": "<p>Famous statistician Tukey said \u201cThe best thing about being a statistician is that you get to play in everyone else\u2019s backyard.\u201d<br><br>When you learn stats you can work on projects in psychology, biology, seismology, economics.....obviously you could specialize in any of those fields later on in your academic career if you find something you really love, but having a strong math/stats background really opens up most of the sciences for you.<br><br>In the real job market for statisticians, you really have to have programming skills now a days, and certain fields like finance and computer science will probably continue to have more higher paying jobs. But if you focus on math and stats you will have the freedom to play around in different disciplines while building hard marketable skills<br><br>ps it<sq>s definitely a challenging field. You can choose to focus more on the theoretical mathematical side, or the more practical applied side. I love stats since it combines both</p>", 
                "question": "Career guidance"
            }, 
            "id": "dfy102o"
        }, 
        {
            "body": {
                "answer": "<p>A probability distribution is just a function that has certain properties (integral is 1, non-negative), and any function can be described by the set of points (x,y) that make up the line you see when you graph it out.<br><br>For a probability distribution you can obtain that set of (x,y) by knowing, instead, (x,p), where <dq>p<dq> here is the area under the curve to the left of the paired x value.  That is, p is the value of the corresponding *cumulative* distribution, and exactly one such distribution exists for any probability distribution.  p ranges from 0 to 1.<br><br>We define <dq>quantile<dq> as some value p, and we can say (for instance) <dq>x = 5 is the p = 0.3 = 30<percent> quantile<dq> if (5,0.3) = (5, 30<percent>) is a valid (x,p) pair in this distribution.<br><br>What a Q-Q plot does is it takes two distributions which are defined on the same x interval (say, from negative infinity to positive infinity), and it goes through each <dq>p<dq>-value and plots the corresponding pair of x-values.  So if our first distribution can be labeled with a k, and the second distribution a j, and we have these two points for the distributions (respectively)<colon><br><br>> (x*_k_*, p)<br><br>> (x*_j_*, p)<br><br>Then since the p<sq>s match, we plot the point (x*_k_*, x*_j_*).  The Q-Q plot is the total plot of all such points.<br><br>When you create a [normal] Q-Q plot, you are doing these steps<colon><br><br>1. For each point x*_k_* in your observed data, you calculate its quantile p*_k_* (the fraction of points that are less than or equal to x*_k_*).<br><br>2. You find the x value x*_n_* of the (standard) normal CDF that corresponds to p*_k_*.<br><br>3. You plot (x*_n_*, x*_k_*), or (x*_k_*, x*_n_*), depending on whether you want your data on the x-axis or on the y-axis.<br><br>The idea here is that if your distribution is precisely standard normal, then the x-values from your data should match precisely the x-values when you look up the standard normal CDF.  Then all of the points lie on the 45-degree x = y line.  If your data is not *standard* normal, but is still precisely normal (with some mean and standard deviation), then your Q-Q plot will still show a straight line, it will just have a different slope and different location.<br><br>We typically don<sq>t care about the particular slope or location of the plotted line, but we do care about departures from that line.  For a normal Q-Q plot, a non-linear pattern indicates that the data do not follow some normal distribution.  You can look up what sort of departures correspond to features of the data<sq>s distribution.</p>", 
                "question": "I don<sq>t understand Q-Q plots"
            }, 
            "id": "dfv0jhz"
        }, 
        {
            "body": {
                "answer": "<p>What about trying the basic sources (wikipedia, youtube...)?<br><br>Q\u2013Q plot<br>https<colon>//en.wikipedia.org/wiki/Q<percent>E2<percent>80<percent>93Q_plot<br><br>Interpreting the normal QQ-plot <br>https<colon>//www.youtube.com/watch?v=-KXy4i8awOg</p>", 
                "question": "I don<sq>t understand Q-Q plots"
            }, 
            "id": "dfuvwgo"
        }, 
        {
            "body": {
                "answer": "<p><br><br>Well, the theoretical quantiles in a normal Q-Q plot are the expected value\\* of the order statistics (the sorted values in a sample of size *n*) from a standard normal.  [With a standard normal, half its quantiles are negative]<br><br>If you plot the sorted observations against them (i.e. data on y-axis theoretical on x-axis) you should get points that are close to laying on a line with slope \u03c3 and intercept \u03bc<br><br>\\* or more precisely, a good approximation of this<br><br>Are [these](http<colon>//stats.stackexchange.com/q/101274/805) couple of [discussions](http<colon>//stats.stackexchange.com/q/111010/805) of any use to you? <br><br>Indeed there<sq>s dozens of posts on crossvalidated relating to [QQ-plots](http<colon>//stats.stackexchange.com/questions/tagged/qq-plot?sort=votes)</p>", 
                "question": "I don<sq>t understand Q-Q plots"
            }, 
            "id": "dfv1rir"
        }, 
        {
            "body": {
                "answer": "<p>For each observation (or group of observations) map the actual value vs the hypothetical expected value if the distribution were normal. If they line up perfectly (i.e. the actual observations are the same as the values from a normal distribution) this creates a diagonal line. If this doesn<sq>t happen your observations have some degree of non normality.<br><br>For more detail check out the sources linked by /alfredmichon</p>", 
                "question": "I don<sq>t understand Q-Q plots"
            }, 
            "id": "dfuw7te"
        }, 
        {
            "body": {
                "answer": "<p>>doing my MS degree has basically made me feel more stupid than I was when I started <br>  <br>This is the definition of being ready to practice statistics! It<sq>s only when you realise how little you know that you<sq>re really an expert. You<sq>ll be fine, I promise.  </p>", 
                "question": "Self doubt while job-searching"
            }, 
            "id": "dfj274u"
        }, 
        {
            "body": {
                "answer": "<p>Some things are under your control. Others are not under your control. And worrying about things you cannot change is not rational.<br><br>The interview already passed and you cannot influence the result. So don<sq>t worry about it.<br><br>If you won<sq>t get hired - that means people that were hiring found somebody who they thought was better fit for their job. This is also not under your control.<br><br>If you get hired - do the best job that you can. If your employers will be unsatisfied - they will let you know. But you cannot control their decision. The only thing you can do is to work the best way you know how. So I would concentrate on that.<br><br>If your skills are really not yet up to par with what is expected in your <dq>dream job<dq> then concentrate on improving. In general I think there is a shortage of <dq>data scientists<dq> so your chances of getting hired should be quite high.</p>", 
                "question": "Self doubt while job-searching"
            }, 
            "id": "dfipfoj"
        }, 
        {
            "body": {
                "answer": "<p>>made me feel more stupid than I was when I started.<br><br>This is quite common in some areas of study, including statistics. Impostor syndrome can be a rather serious thing and you should not let it get to you! I feel the same way, the more I study statistics the less I (know I) understand about it. </p>", 
                "question": "Self doubt while job-searching"
            }, 
            "id": "dfj3zo9"
        }, 
        {
            "body": {
                "answer": "<p>You<sq>ve got this. I<sq>ve been a statistician for 8 years now, and there<sq>s nothing like going to a stats lecture on a subject I know little about to make me feel dumb. Now don<sq>t get me wrong, I<sq>m totally comfortable with my area of expertise, but outside of my niche I still feel like I<sq>m constantly learning new things.</p>", 
                "question": "Self doubt while job-searching"
            }, 
            "id": "dfj445h"
        }, 
        {
            "body": {
                "answer": "<p>What is the scientific question?  Weight and length will be strongly correlated.  Adjusting for length in a model of weight will give you results that are not straightforward to interpret.  You<sq>ll end up testing hypotheses about how BMI or <dq>bird density<dq> differs across locations (or something like that).  If you just care about weight, don<sq>t adjust for length.<br><br>Regarding plots... what do you want to illustrate?  One way of visualizing the data is to show one scatterplot of length X weight for each location.  You can add the fitted line to each plot if you like.  Or you can do a single scatterplot, and tag observations from the three locations with different point shapes or colors.  But again, if you want to look at differences in weight across locations, I recommend just doing an ANOVA, which you can illustrate with a bar plot.</p>", 
                "question": "Predictive models with a continuous dependent variable and a categorical independent variable."
            }, 
            "id": "dcd103h"
        }, 
        {
            "body": {
                "answer": "<p>The most straight-forward way to plot this would be a set of three scatterplots, one for each location, with the ANCOVA prediction (i.e. straight) lines going through the data. The three lines will be parallel but with different y-intercepts. This should be fairly straightforward to do in R using ggplot.</p>", 
                "question": "Predictive models with a continuous dependent variable and a categorical independent variable."
            }, 
            "id": "dcd1k3q"
        }, 
        {
            "body": {
                "answer": "<p>Categoricals are typically graphed in terms of box or violin plots.</p>", 
                "question": "Predictive models with a continuous dependent variable and a categorical independent variable."
            }, 
            "id": "dcdepvg"
        }, 
        {
            "body": {
                "answer": "<p>Isn<sq>t is always this [formula](https<colon>//support.content.office.net/en-us/media/c96afb87-c05c-4d03-aca9-b24601d19661.gif) ?<br><br>What other ones are you seeing?</p>", 
                "question": "What exactly is Sample Variance and Sample Standard Deviation?"
            }, 
            "id": "dc1qw4o"
        }, 
        {
            "body": {
                "answer": "<p>You can compute the variance in a sample by using N in the denominator (which I believe is the MLE)  but if you want an unbiased estimate of the population variance you divide by N-1.</p>", 
                "question": "What exactly is Sample Variance and Sample Standard Deviation?"
            }, 
            "id": "dc1wdlj"
        }, 
        {
            "body": {
                "answer": "<p>Start by thinking about individual scores in a sample, and how they differ from the mean. The difference between any given score and the mean is a deviation. You can calculate the deviation for every score in your sample. This new set of deviation scores is interesting but not very useful, because if you try to summarize them, you will get zero. By definition, the sum of all the deviations = 0, and that is because of the balancing property of the mean - it minimizes the average deviation. So calculating the average deviation also yields zero and isn<sq>t useful.  <br>  <br>So if the deviation scores aren<sq>t useful, one way to make them more so is to square them. Now you have a set of squared deviations. If you add all of them up, you will get the <dq>sum of the squared deviations<dq> which is commonly called <dq>Sum of squares<dq> for short. If you divide this sum by N, that is basically taking the average of the squared deviations. This number is called Variance. Because it is calculated based on squared values, it isn<sq>t easy to compare to the original sample mean or sample data, so you can take the square root of the variance to undo the original squaring step. The result is the Standard Deviation, which can then be compared to the original mean, and represents something like the <dq>average distance any given score deviates from the mean<dq>.  <br>The only thing missing from this is that if you want to generalize from a sample to a population, this calculation tends to underestimate the true variance and standard deviation. To correct for this, it is necessary to divide by (N-1) instead of N. This correction is not necessary when you are studying the population directly. So divide by N when computing population variance.  <br>Another way to think of this is that the formula for the mean is not really Sum(x)/N but instead Sum(x)/df where df is the degrees of freedom. </p>", 
                "question": "What exactly is Sample Variance and Sample Standard Deviation?"
            }, 
            "id": "dc1ycnz"
        }, 
        {
            "body": {
                "answer": "<p>Sample variance can be calculated for instance by this equation<colon><br><br>V = (1/N)\u2211(Xj \u2013 X_bar)<br><br>This has certain properties depending on the distribution you<sq>re talking about.  For instance, it is virtually always biased; for normally distributed variables this can be corrected by dividing by N\u20131 instead.  Of course, the sample standard deviation you get from that (V = s^2 by definition) will still be somewhat biased, as the maintenance of a nice expected value doesn<sq>t translate well with non-linear transformations (i.e. the square root).  It helps some though.<br><br>Another property that that new <dq>variant<dq> of the sample variance equation has is that it makes the mean square error larger.  You can reduce the MSE for normally distributed variables by dividing by N+1 instead (this generates bias though).<br><br>What you<sq>ll find is that there are many ways to describe the idea that you are trying to get at.  <dq>Variance<dq> is strictly defined (for populations/distributions), but you can come up with a variety of ways to estimate that number that will all be good or bad in their own ways.  Conventionally the above formula is secondary to the N\u20131 correction, since people care about unbiasedness a lot and like to assume that variables are normally distributed.  However when this is not the case (for variables that are strictly positive, say), then there are methods like MLE that can get you closer to an estimator that has many nice properties.</p>", 
                "question": "What exactly is Sample Variance and Sample Standard Deviation?"
            }, 
            "id": "dc25j52"
        }, 
        {
            "body": {
                "answer": "<p>NCSU will probably have more cachet if you ever want a job at SAS</p>", 
                "question": "NCSU or Texas A&M for Online Masters Degree?"
            }, 
            "id": "dbvpk1i"
        }, 
        {
            "body": {
                "answer": "<p>I don<sq>t know what your career interests are, but NCSU is more respected for ecological statistics.</p>", 
                "question": "NCSU or Texas A&M for Online Masters Degree?"
            }, 
            "id": "dbvo6dm"
        }, 
        {
            "body": {
                "answer": "<p>I<sq>m currently attending the online Texas A&M program. I really enjoy it. Feel free to reach out if you have any questions about it. Can<sq>t speak to how good the NCSU program is, though I imagine you<sq>ll be fine regardless of whichever you choose.</p>", 
                "question": "NCSU or Texas A&M for Online Masters Degree?"
            }, 
            "id": "dc2wgzm"
        }, 
        {
            "body": {
                "answer": "<p>What do you mean equivalent? And do you have to pay, if so, how much?</p>", 
                "question": "NCSU or Texas A&M for Online Masters Degree?"
            }, 
            "id": "dbvfmk4"
        }, 
        {
            "body": {
                "answer": "<p>I loved my Master in stats I made in Sherbrooke, Qc.<br><br>They also have a good cluster in Machine learning too if you want to join that bandwagon.</p>", 
                "question": "What are good MSc in Applied Statistics programs at Canadian universities or free international universities? (criteria/interests inside post)"
            }, 
            "id": "db1kwef"
        }, 
        {
            "body": {
                "answer": "<p>RemindMe! Next Thursday at 4pm</p>", 
                "question": "What are good MSc in Applied Statistics programs at Canadian universities or free international universities? (criteria/interests inside post)"
            }, 
            "id": "db1y4kz"
        }, 
        {
            "body": {
                "answer": "<p>I would try to get involved with some projects while you are an undergrad. I have found that ugrad stats education is often not very practical, and it<sq>s good to be able to show some of your work. <br><br>Data cleaning and munging are tasks you might want to focus on given your limited experience too. Taking on that role might allow you to find a position somewhere. </p>", 
                "question": "Is Bachelor in Statistics good enough to get a job?"
            }, 
            "id": "dauowlb"
        }, 
        {
            "body": {
                "answer": "<p>Yes. There are tons of jobs for people with basic programming skills and an understanding of numbers. You<sq>d be surprised how few people actually have both of those. Of course, you<sq>re not going to be a Data Scientist at Facebook making 6 figures, but you can definitely find a good job with your degree. </p>", 
                "question": "Is Bachelor in Statistics good enough to get a job?"
            }, 
            "id": "dauqdyn"
        }, 
        {
            "body": {
                "answer": "<p>Thanks for the replies. I<sq>m hoping to go to graduate school right after my undergrad, but it<sq>s just the fear of having more debt. One more question, for those of you who went to stat grad, how did you pay for all the tuition and expenses? And how did the job hunting go after acquiring the degree?</p>", 
                "question": "Is Bachelor in Statistics good enough to get a job?"
            }, 
            "id": "daurd6g"
        }, 
        {
            "body": {
                "answer": "<p>Definitely. Look into finance/tech/energy companies and network well. A young professional with a background in programming and distributions/regressions/modelling is harder to find than many think... Managers are starting to realize how dynamic that skillset is, especially in the developing landscape where data gives firms a sharp competitive advantage. </p>", 
                "question": "Is Bachelor in Statistics good enough to get a job?"
            }, 
            "id": "davahwr"
        }, 
        {
            "body": {
                "answer": "<p>They are the deviations of the actual y from the predicted y. When forming a linear regression, the predicted Y follows a linear relationship. Deviations from this line, are called residuals; they measure the distance between the actual Y for each N and the line (predicted Y). <br><br>For a good lineal model, the residuals should be small and random. If they are really large it shows the model is not very accurate, and if there is a pattern then it would suggest a non-linear model would fit better for example. <br><br>In finance, residuals signify abnormal profit in asset pricing models, (capm, sim, fama french, etc)</p>", 
                "question": "Residuals?"
            }, 
            "id": "daidufb"
        }, 
        {
            "body": {
                "answer": "<p>Hmm, good question but tough one. My understanding comes out of knowing the math so I don<sq>t know if I can give a good answer. But, one way to say it for a regression model is that the residuals are the part of the outcome measurement that you aren<sq>t able to explain with whatever your inputs are.</p>", 
                "question": "Residuals?"
            }, 
            "id": "daiddr1"
        }, 
        {
            "body": {
                "answer": "<p>a residual is data  minus fit, so it<sq>s what<sq>s left in your data that<sq>s *not* in your model (or rather, the part of your model that yields the fitted value, most typically the part that is a model for the mean).<br><br>As a result, a residual shows what you missed about the data -- what you failed to describe. Ideally, that should be nothing but noise. You generally look at residuals to see if that<sq>s the case.<br><br>When it<sq>s not the case, the particular manner in which the residual isn<sq>t <dq>just noise<dq> tells you about the way in which the model is failing to describe the data. <br></p>", 
                "question": "Residuals?"
            }, 
            "id": "daikvzu"
        }, 
        {
            "body": {
                "answer": "<p>> What are some comprehensive introductions to the principles of statistical testing,<br><br>It really depends on what you mean by that, but I expect that you will at least want something that gets you to <br><br>- pivotal quantities (which is important for a lot of test statistics, confidence intervals, bootstrapping etc)<br><br>- the basics of likelihood ratio tests (which one way or another underlies a lot of the tests you know as well as a lot you don<sq>t -- and is one important tool toward understanding and developing hypothesis tests more generally), and probably also<br><br>- resampling tests (permutation / randomization-type tests and bootstrap tests)<br><br>The basic theory required to understand and apply these isn<sq>t hard, but you<sq>ll want some calculus and linear algebra.<br><br>As a first step, probably something at about the level of Mendenhall Wackerly and Scheaffer<sq>s *Mathematical Statistics with Applications* (though a number of similar alternatives are about as good). That won<sq>t quite get you all of the above but it will cover a lot of what you need to do them.<br><br>If you<sq>re going to be doing a lot of statistical analysis, I<sq>d highly recommend learning R, but choice of books depends partly on what things you<sq>ll need. Will you want regression? GLMs? nonlinear regression? GAMs? LMMs? GLMMs? etc etc <br><br><br></p>", 
                "question": "How can I start learning statistics seriously?"
            }, 
            "id": "dag4jjw"
        }, 
        {
            "body": {
                "answer": "<p>You probably need a self contained book that starts with the basics of probability and statistics, because I think that starting your studies with hypothesis testing and inference is like trying to run before you can walk.<br><br> I would therefore recommend an informal undergrad text such as Mathematical statistics with applications by Wackerly et al. This text falls in the category of <dq>statistics made easy for undergraduates who must know how to apply things rather than understand how and why things work<dq>, and it is fairly okay if your aim is that. But is ineffective if you expect to understand and master the concepts of Mathematical Statistics the proper way.<br><br>In order to fully and deeply understand statistics you must however possess a fairly high level of mathematical knowledge and maturity. </p>", 
                "question": "How can I start learning statistics seriously?"
            }, 
            "id": "daf0nps"
        }, 
        {
            "body": {
                "answer": "<p>Hello fellow Melbournite! Along with your reading which will improve your understanding of statistical theory, you should stop using SPSS as soon as you can. Progressing to script-driven analysis (instead of pointing and clicking) is really important if you want to make a go of this. Once you actually understand the basic methods at a deep level you<sq>ll probably arrive at this conclusion on your own, but do yourself a favour and start learning Stata or R this week - the employability gains alone are worth it. </p>", 
                "question": "How can I start learning statistics seriously?"
            }, 
            "id": "dafk0d4"
        }, 
        {
            "body": {
                "answer": "<p>I<sq>m not clear on your question.  The eigenvalues are the variances of the axes.  So the eigenvalues are the values that are maximized.  The eigenvalues sum to the total variance, which is the sum of the diagonals in the covariance matrix.  <br><br>For ELI5, the cov matrix represents the structure of the data, like a cloud of plotted coordinates (points).  To make the first axis, draw a line where the cloud is the widest, such that the line goes through the center of the cloud and stretches to the extremes of the widest points.  The variance of the data along that line is the first eigenvalue.  Then draw a line perpendicular to the first line in the direction of the next widest points.  That<sq>s the second axis, and the variance from these second widest points is the second eigenvalue.  Keep drawing perpendicular lines until you have the same number of lines as variables.  At this point, all of the variance will be explained by your new perpendicular (uncorrelated) axes.  Thus the sum of the eigenvalues will equal the sum of the variances (the diagonal of the cov matrix).  <br><br>Although PCA can be done iteratively, it can also be done pretty simply using linear algebra.  We maximize the eigenvalues according to the diagonals of the cov matrix.  The off-diagonals in the cov matrix help tell us which direction to draw our axes.</p>", 
                "question": "Relationship of Eigenvalues to Variance (PCA/SVD)?"
            }, 
            "id": "dadczlr"
        }, 
        {
            "body": {
                "answer": "<p>It can<sq>t be true in general for Gaussian r.v.s, but possible could be for zero mean Gaussian r.v.s. <br><br>A quick counter example<colon> If y1 has a large negative mean, y2,y3, and y4 all have small positive means, and the covariance matrix is diagonal with small variance, then<br><br>cov. (y1 y2, y3 y4) = 0 because of independence<br><br>but <br><br>E(y1 y3) E(y2 y4) + E(y1 y4) E(y2 y3) = something negative <br><br></p>", 
                "question": "Cov(AB CD) = E(AC) E(BD) + E(AD) E(BC) for normals? Why?"
            }, 
            "id": "d9wqe8f"
        }, 
        {
            "body": {
                "answer": "<p>Have a look at [this post](http<colon>//stats.stackexchange.com/questions/6350/anova-assumption-normality-normal-distribution-of-residuals) over at Cross Validated about that issue. Briefly, ANOVA doesn<sq>t assume that your raw variables are normally distributed. It assumes that the *residuals* are approximately normally distributed. My advice would be to continue with the parametric ANOVA and check the residuals.<br><br>As a side note, I<sq>m not a fan of normality hypothesis tests. Some of the reasons are discussed [here](http<colon>//stats.stackexchange.com/questions/2492/is-normality-testing-essentially-useless).</p>", 
                "question": "What do you do when part of your data fails a normality test?"
            }, 
            "id": "d9n0f1j"
        }, 
        {
            "body": {
                "answer": "<p>You might like to look into <sq>survival analysis<sq> - it is specifically designed to tackle problems where time-to-event is the outcome. </p>", 
                "question": "What do you do when part of your data fails a normality test?"
            }, 
            "id": "d9nbesb"
        }, 
        {
            "body": {
                "answer": "<p>What is your actual model; interpretation of parameters depends on how your model is actually setup and the type of variable you have.</p>", 
                "question": "How to properly interpret and use odds ratios from a logistic regression"
            }, 
            "id": "d9j9jxh"
        }, 
        {
            "body": {
                "answer": "<p>In the logit model, the linear combination of predictors+intercept can be used to predict the log of the odds (not odds ratio!) </p>", 
                "question": "How to properly interpret and use odds ratios from a logistic regression"
            }, 
            "id": "d9jnuhn"
        }, 
        {
            "body": {
                "answer": "<p>If I understand this correctly, the scenario you have in hand is 3 readers giving mark on 30 applications (ignoring the leftover you have in hand for simplicity), where each application has 10 essays. Each reader will mark 10 applications (i.e. 30 essays) and you want to normalize the score so that the reader<sq>s bias is minimized.<br><br>Unfortunately, if you have already done the marking, there simply isn<sq>t a good way to normalize the score I can think of. Suppose you have reader A giving a universally higher mark to the group of applications he marked, can you claim that that group of students are simply better? Or it is just the fact that reader A is more lenient in terms of giving the score? The answer is, sadly, inconclusive. The only way you can distinguish the cases is if you can **ensure that the split of all applications among readers is fair and thus, students quality in each reader<sq>s group is roughly equal in terms of distribution**, which is hard enough to prove.<br><br>Now, though, if you have not started the marking process, there are several ways to normalize the score. For example, instead of each reader marking 10 applications, have them mark 30 application but only look at 1 essay for each application. This way you can control somehow the bias by having more than one person marking one application. If budget permits, you can even have more than 1 person marking 1 essay, but I guess that will be too costly.<br><br>The bottomline is, you will need some form of **baseline comparison** to make influence on whether the score difference is student<sq>s ability or reader<sq>s bias. The setup you have right now has no overlapping meaning there is no baseline comparison to judge the score. You can of course remedy this, if possible, by having the readers mark some additional material (like past application, or simply have them mark another round of application) so you can infer their bias.</p>", 
                "question": "Normalizing reader scores for student essays"
            }, 
            "id": "d96mbm8"
        }, 
        {
            "body": {
                "answer": "<p>Not a statistician but I feel like the answer might be 4! x 6! / 10!.<br><br>If you drew numbered answers from a hat there would be 10! ways you could do this and only one would be perfect ascending order.<br><br>But you don<sq>t have to get them in the right order, all the <dq>true<dq> answers are interchangeable, there are 4! ways to interchange them. You can also swap any of the false answers, there are 6! ways to do that.<br><br>So that means your initial probability 1/10! is multiplied by 4! and by 6! giving 4! x 6! / 10! or one chance in 210.<br></p>", 
                "question": "You are given a series of 10 truth or false questions and were told 4 of the questions are true and 6 of them are false. Then you semi-randomly filled out the answers keeping the 4-6 rule in mind. What<sq>s the probability of getting every problem right?"
            }, 
            "id": "d8bwk4m"
        }, 
        {
            "body": {
                "answer": "<p>You<sq>ve got 50<percent> chance of getting any given one right. I think this question is way simpler than you intended out thought it was.       <br><br>1/(2^10 )</p>", 
                "question": "You are given a series of 10 truth or false questions and were told 4 of the questions are true and 6 of them are false. Then you semi-randomly filled out the answers keeping the 4-6 rule in mind. What<sq>s the probability of getting every problem right?"
            }, 
            "id": "d8bubmy"
        }, 
        {
            "body": {
                "answer": "<p>I am not sure how the winning percentages are calculated. p(Thunder) = 33/(33+13) = .717; p(Warriors) = 31/(31+14) = .689; p(Spurs) = 29/(29+15) = .659; p(Timeberwolves) = 28/(28+17) = .622. So I interpret your first question as<colon> p(winning x matches | losing y games). Since each game is independent, the conditioning should not matter. All the prediction is made based on the given winning percentage only. <br><br>For the spurs question, it seems to be a geometry distribution, which is to compute the probability if an event occur after x failures. Formula is p(x) = q^(x-1) p, where q = 1 - p. So we have<colon> p(spurs lose 2 games straight) = (1-.659)^2 (.659) = 0.0766. <br><br>However, is x > 2 and not equal 2, it becomes binomial distribution. Formula is p(x|k) = C_n_k * p^k * q^(n-k), where C_n_k is combination of n and k, n is the total number of events, k is number of successes, q = 1-p. So we have<colon> p(spurs lose 2 in x games) = C_x_2 * (.659)^(x-2) * (1-.659)^2. For example, if x = 3, C_3_2 = 3! / [(3-2)! * 2!], where x! is x factorial. <br><br>That is just my interpretation. It could be some other reasonable interpretations and leads to different answers. </p>", 
                "question": "A (probably) basic statistics question that I<sq>m too dumb to figure out"
            }, 
            "id": "d73mq74"
        }, 
        {
            "body": {
                "answer": "<p>I<sq>m wondering if you could just do this algebraically. Wins is the y intercept and Win Pct is slope for each line. Then find all the intersections and pick the one where Timberwolves tie for the highest score.<br><br>If that didn<sq>t work and you can<sq>t work it out from probability theory, I<sq>d try using Monte Carlo simulation to run a bunch of hypothetical <dq>seasons<dq> to see how things tend to go on average.  If you run it enough times that your average number of games doesn<sq>t change, there<sq>s a good chance that<sq>s close to the theoretical answer.</p>", 
                "question": "A (probably) basic statistics question that I<sq>m too dumb to figure out"
            }, 
            "id": "d73pf38"
        }, 
        {
            "body": {
                "answer": "<p>The clarifications you gave are pretty handy. Why do you want to reduce dimensionality?</p>", 
                "question": "How do I do dimensionality reduction in time-series data?"
            }, 
            "id": "d6teffq"
        }, 
        {
            "body": {
                "answer": "<p>You could try some kind of latent state modeling like a regime switching model or a kalman filter and then use the inferred latent state instead of your true observations. I<sq>ve never tried it, but googling aroung there seem to be [autoregressive variants on the LASSO](http<colon>//www.stat.cmu.edu/~arinaldo/papers/arLasso.pdf) you could try.</p>", 
                "question": "How do I do dimensionality reduction in time-series data?"
            }, 
            "id": "d6tzwov"
        }, 
        {
            "body": {
                "answer": "<p>I have no insights, but digging led me [here](http<colon>//stats.stackexchange.com/questions/23566/functional-principal-component-analysis-fpca-what-is-it-all-about) & [here](https<colon>//journal.r-project.org/archive/2013-1/shang.pdf).  Maybe someone else can say if it is in the right direction.</p>", 
                "question": "How do I do dimensionality reduction in time-series data?"
            }, 
            "id": "d6u2ayx"
        }, 
        {
            "body": {
                "answer": "<p>I haven<sq>t used it myself, but you could try the forecast package in R<colon> https<colon>//cran.r-project.org/web/packages/ForeCA/index.html</p>", 
                "question": "How do I do dimensionality reduction in time-series data?"
            }, 
            "id": "d6u85m3"
        }, 
        {
            "body": {
                "answer": "<p>I would examine literature in your field and see which technique is common. Many scientific fields stick to a certain method because it<sq>s the standard approach and for good reason.<br><br>From my understanding, with as many bonferroni corrections needed for calculating p-values correctly many of the borderline statistically significant factors could be found to fall outside the set alpha level. Through use of FDR you have more power to detect significance but at the cost of increased type 1 errors.<br><br> As an aside, are you using other methods of variable selection as well? From my experience, FDR is usually used along with other variable selection techniques to model. </p>", 
                "question": "Advantages of p-value over FDR?"
            }, 
            "id": "d6qe1s2"
        }, 
        {
            "body": {
                "answer": "<p>P-values are much older and more widely understood. I think there are few arguments in your application to use p-values/Bonferroni over FDR.<br><br>One disadvantage is that FDR does not offer any guarantee for the true discoveries that are missed; but p-values don<sq>t either ...<br><br>Also controlling FDR only controls the expected proportion of false discoveries; in practice false discovery proportion can be high variance and the proportion of false discoveries for a fixed dataset can be much higher or lower than the nominal FDR.</p>", 
                "question": "Advantages of p-value over FDR?"
            }, 
            "id": "d6qd1dk"
        }, 
        {
            "body": {
                "answer": "<p>The Journal of Quantitative Analysis in Sports (JQAS), an official journal of the American Statistical Association, publishes timely, high-quality peer-reviewed research on the quantitative aspects of professional and amateur sports, including collegiate and Olympic competition.</p>", 
                "question": "Looking for a stats journal to publish paper for our algorithm start up"
            }, 
            "id": "d6bbd5h"
        }, 
        {
            "body": {
                "answer": "<p>Part of the answer depends on what you want to do with stats. Like if you want to use Bayesian MCMC methods, learn Bayesian stats. But in general, the second step I would recommend after intro stats is a regression course. A course in linear regression will teach you about Sums of Squares, R2, residual plots, leverage, multiple regression, etc. Lots of basic skills that will help you with any subsequent course work. Also, learning to interpret a residual plot can be one of the most subjective and difficult concepts in stats for left-brain, technical-oriented people.<br><br>You didn<sq>t ask, but as a third step I would recommend a design of experiments / analysis of variance course. It will continue the modeling basics from your regression course, but start adding new connections between real world phenomena and the models used to represent them. It will also introduce concepts like random and fixed effects, nested vs crosses effects, etc. After intro stats, regression and design / ANOVA you have the foundation to learn any other stats topic<colon> generalized linear models, nonparametric methods, survival and reliability, multivariate methods, nonlinear regression, Bayesian stats, etc. </p>", 
                "question": "Where to go after intro statistics books?"
            }, 
            "id": "d5v5w5v"
        }, 
        {
            "body": {
                "answer": "<p><br>Applied Statistics Edition 2 by Rebecca Warner helped me fill in some gaps and tie concepts together in a way that I didn<sq>t get from my 3 previous grad level stats courses. </p>", 
                "question": "Where to go after intro statistics books?"
            }, 
            "id": "d5vafb7"
        }, 
        {
            "body": {
                "answer": "<p>Intro to statistical learning! And get on those regressions, poisson, geometric, stepwise, and lasso should all be good friends of yours. Further, don<sq>t turn your nose at econometrics. Difference in differences as well as regression discontinuity are extremely useful. </p>", 
                "question": "Where to go after intro statistics books?"
            }, 
            "id": "d5vltlr"
        }, 
        {
            "body": {
                "answer": "<p>ugh, why is it always like this with data science? it<sq>s like not having a stats background is a requirement for being a data scientist. Congrats, your self taught reading of a stats 101 book probably puts you at half the level of every other freshman business major taking an actual class at uni. Now you just have to read a few blog posts, download R and bam, data scientist! Looking forward to your post OP about gosh what do p-values really mean next month. </p>", 
                "question": "Where to go after intro statistics books?"
            }, 
            "id": "d5xahzi"
        }, 
        {
            "body": {
                "answer": "<p>Chernick<sq>s book is pretty clear and succinct, probably the first one I<sq>d reach for. I recall that Manly<sq>s <dq>Randomization, Bootstrap and Monte Carlo Methods in Biology<dq> gives some nice examples of how bias in sample collection reoccurs as bias in bootstrapping, thus necessitating bias correction.</p>", 
                "question": "Understanding bias correction in bootstrapping."
            }, 
            "id": "d5itiud"
        }, 
        {
            "body": {
                "answer": "<p>Ok - imagine you are in your kitchen and you need a hammer, which you keep in your basement. You have a small child in your house and you say <dq>can you go and get the hammer from the basement?<dq>  <br><br>The child comes back and says <dq>It<sq>s not there<dq> <br><br>As a grown up, you know that one of two things is possible. A) The hammer really is not there. B) The hammer is there and your child missed it. <br><br>Power analysis is what we have developed in order to design our experiments so that B) doesn<sq>t happen to you with your project. <dq>Your Child<dq> == <dq>Your experiment<dq> <br><br>1. Effect Size - You need to know how big the hammer is. If the hammer is 10ft long and your child missed it - it<sq>s probably not there! If it<sq>s a mini hammer that is 2cm long it is going to be harder to find so you need a longer search. This is analagous to effect size - you need to tell the analysis how big the thing is you<sq>re trying to find, because that impacts how many subjects you<sq>ll need. <br><br>(2. Sample size - Usually the <dq>unknown<dq> in the equation but not always, sometimes the sample size is fixed and we want to know how much power we have access to.)<br><br>3 - Alpha - this equates to<colon> If I did this exact experiment 100 times, how many times am I willing to accept saying <dq>I found the hammer!<dq> but in reality I found something else that is not a hammer. Less suitable for this analogy. 5<percent> is normal but not in every single field. <br><br>4 - Beta equates to<colon> How many times am I willing to say <dq>The hammer wasn<sq>t there<dq> but it really *was* there, you just missed it. Therefore 1-beta = how many times we expect to say <dq>I found the hammer<dq> and there really was no hammer. 1-beta=0.80 is considered pretty good in most fields. <br><br>So you need to decide on the effect size you<sq>re looking for, select alpha, select beta, and find something/someone to do the math to give you the sample size you need. </p>", 
                "question": "Baffled by Power Analysis"
            }, 
            "id": "d4ke63j"
        }, 
        {
            "body": {
                "answer": "<p>You can estimate N based on alpha, estimated population effect size, and desired power.  In novel research it is often difficult to estimate the population effect size.  If there is similar research, you can base it off of the effect sizes in that research. You can also decide what is the minimum effect size you might consider meaningful.  In some fields there are rules of thumb for small, medium, and large effect sizes that can be used as a last resort. </p>", 
                "question": "Baffled by Power Analysis"
            }, 
            "id": "d4ke68q"
        }, 
        {
            "body": {
                "answer": "<p>Here<sq>s a link to a somewhat old but good introduction to power calculations. http<colon>//www.unt.edu/rss/class/mike/5030/articles/Cohen1992.pdf</p>", 
                "question": "Baffled by Power Analysis"
            }, 
            "id": "d4ke8if"
        }, 
        {
            "body": {
                "answer": "<p>You give a good description of some of the problems with power analysis. The suggestion to do a pilot study is common but, as in your case, often not feasible. Even if you did a pilot study, your estimate of effect size would be very unreliable so any estimate of power would likely be very inaccurate. The problem with not knowing the power is you could invest a lot of time and effort with a low probability of a conclusive result even if your hypothesis is correct. <br><br>You might want to stop thinking in terms of the all-or-nothing approach to significance testing and do a moderately sized study. If the effect is not significant, you may have a strong hint of an effect even if you don<sq>t have conclusive evidence of it. A follow-up study either by you or another researcher in the future could clear things up. Not all studies have to be conclusive in and of themselves to make meaningful contributions. </p>", 
                "question": "Baffled by Power Analysis"
            }, 
            "id": "d4kqpd3"
        }, 
        {
            "body": {
                "answer": "<p>The probability will be between 60<percent> and 100<percent>, depending on the relationship of the predictors.<br><br>If the predictors are highly correlated then combining them doesn<sq>t offer any additional information beyond the 60<percent> that you get from the second predictor alone.  <br><br>If they are independent, then you can find the probability of both being wrong by multiplying<colon> (1-.55)*(1-.6) = .45 * .4 = 0.18. That gives the chance of being right when they agree as 82<percent>.<br><br>If they are anti-correlated, meaning that one predictor giving a false positive makes the other likely to be correctly negative, then the value of combining them is more than in the independent case.  The probability could be as high as 100<percent> when the two predictors agree.<br><br></p>", 
                "question": "Improving odds by combining them?"
            }, 
            "id": "d4h3mcr"
        }, 
        {
            "body": {
                "answer": "<p>Don<sq>t quote me on this, but if you are thinking strictly probability maths, I think you can assume that if both predict the same thing, you can just put 1 - (60<percent> * 55<percent>) to get the probability that they predict the correct, because probabilities tend to work multiplicatively. However, I am extemely uncertain about this so take it with a grain of salt.</p>", 
                "question": "Improving odds by combining them?"
            }, 
            "id": "d4gzbjg"
        }, 
        {
            "body": {
                "answer": "<p>I would probably use linear regression. Six mo score as the dependent variable, pre-test score and intervention indicator as the predictors.</p>", 
                "question": "Which Statistical Test would be better to use for my hypothesis?"
            }, 
            "id": "d3whyls"
        }, 
        {
            "body": {
                "answer": "<p>Just posted this on another thread<colon> here<sq>s my current thoughts<colon> <br>https<colon>//www.reddit.com/r/AskStatistics/comments/4mr4uf/comment/d3xoyrd<br><br>I also agree about the 80<percent>...that<sq>s too specific and not what stats are meant to do</p>", 
                "question": "Which Statistical Test would be better to use for my hypothesis?"
            }, 
            "id": "d3xp3zc"
        }, 
        {
            "body": {
                "answer": "<p>Your hypothesis is weird. No one is ever specific on <percent>. </p>", 
                "question": "Which Statistical Test would be better to use for my hypothesis?"
            }, 
            "id": "d3wrtn1"
        }, 
        {
            "body": {
                "answer": "<p>This is a general phenomenon of sums or means of random variables.<br><br>Suppose you have M sub-indices with mean 0 and SD 1, and suppose you obtain a global index by adding up the sub-indices. Then the global index has mean 0 and variance equal to the sum of the entries in the correlation matrix of the M sub-indices. Each off-diagonal entry in the correlation matrix is between -1 and +1, so M^2 is an upper bound on the variance of the global index, and M is an upper bound on the SD of the global index.<br><br>When the sub-indices are not perfectly positively correlated, the SD of your global index is strictly less than M. A subject who is +1 SD above the mean on each of the M sub-indices has a global index score of +M, which must be more than +1 SD above the global index mean.</p>", 
                "question": "Need help. I am searching for a statistic<sq>s technical term to label this phenomenon."
            }, 
            "id": "d3ncuhs"
        }, 
        {
            "body": {
                "answer": "<p>I<sq>m not aware of it having a name (however, I don<sq>t think it<sq>s in the least bit surprising).<br><br>But psych people love to give names to statistical things that I would think it odd to bother naming, so this effect could well have a name that<sq>s used among psychologists.</p>", 
                "question": "Need help. I am searching for a statistic<sq>s technical term to label this phenomenon."
            }, 
            "id": "d3mjqj2"
        }, 
        {
            "body": {
                "answer": "<p>It would be nice if it had a name but I am not aware of one. It is pretty much a statistical necessity so I wouldn<sq>t try to think up a psychological explanation. </p>", 
                "question": "Need help. I am searching for a statistic<sq>s technical term to label this phenomenon."
            }, 
            "id": "d3n3ktg"
        }, 
        {
            "body": {
                "answer": "<p>Doesn<sq>t ring a bell, but perhaps exploring other indices (beyond intelligence) might lead you toward an answer. <br><br>For example, the Gini Coefficient might be interesting to explore (https<colon>//en.wikipedia.org/wiki/Gini_coefficient), or check out the Statistical Annex of the Human Development Report (http<colon>//report.hdr.undp.org/). Just some thoughts. <br><br>But do please let us know if you find an answer - that<sq>d be great to know for future reference.</p>", 
                "question": "Need help. I am searching for a statistic<sq>s technical term to label this phenomenon."
            }, 
            "id": "d3n5z65"
        }, 
        {
            "body": {
                "answer": "<p>I<sq>m not sure how they come up with that <dq>94.1<percent> likely<dq>, but it sounds like he may have been trying to correct the prosecutor<sq>s fallacy. This might be handy<colon> http<colon>//www.dcscience.net/2016/03/22/statistics-and-the-law-the-prosecutors-fallacy/ The section headed <dq>The island problem<dq> deals with a precise analogy to Jobs<sq> claim.<br><br>He would be wrong, because the prior probability of him being the father is a lot greater than for a random, given that the mother is making a claim about him, not anyone who happens to have a high match probability.</p>", 
                "question": "Could somebody explain this line from <dq>Steve Jobs<dq> about paternity tests?"
            }, 
            "id": "d39ma3a"
        }, 
        {
            "body": {
                "answer": "<p>Incorporating expert opinion into a Bayesian model is usually done through *prior distributions* instead of an additional feature. (As an aside, doing so is considered *subjective* Bayesian inference versus *objective* Bayesian inference).<br><br>As a quick overview, Bayesians usually make inference on the *posterior distribution* - a combination of the *prior distribution* (in your case, expert opinion), and the *likelihood*. As a really basic example, consider a setting where you have data on MI outcomes (no covariates at this point) - a series of 1<sq>s and 0<sq>s. A frequentist would likely take the mean of the data. As a Bayesian, you would consider this binomial likelihood and likely combine it with a beta prior. The default (non informative) prior would be to use a beta(1, 1) distribution. However, if in a prior dataset, you had observed four patients, three with an MI and one without, you could use a prior of beta(1+3, 1+1). See [here](http<colon>//www.people.fas.harvard.edu/~plam/teaching/methods/conjugacy/conjugacy_print.pdf) for more details on beta-binomial.<br><br>In the above example, it<sq>s easy to incorporate prior information because we used a [*conjugate prior*](https<colon>//en.wikipedia.org/wiki/Conjugate_prior). While probably not exactly what you are doing for your dissertation, here<sq>s an overview of a conjugate prior with a linear regression from [wikipedia](https<colon>//en.wikipedia.org/wiki/Bayesian_linear_regression#Conjugate_prior_distribution). There are many more resources online for this that you can find by searching for something along the lines of <dq>bayesian linear regression subjective conjugate prior<dq>. For a more detailed (introductory) overview of bayesian statistics, check out this [book](http<colon>//www.amazon.com/Doing-Bayesian-Analysis-Second-Edition/dp/0124058884/ref=dp_ob_title_bk).<br><br>To be honest, as much as I<sq>m a Bayesian, I think that creating an automatic model that incorporates expert opinion will be really difficult. Usually, subjective priors are chosen carefully, and there not always as interpretable as the beta-binomial posterior presented above. I think this goal is possible, but it would require a lot of though about how the prior is automatically constructed from a data set of surgeon<sq>s predictions. If you have any followup questions/would like more resources, let me know!<br><br>Edit<colon> I guess I never really addressed the issue of predictive models. However, the difficult part will be constructing the prior automatically. If you can do this, predicting outcomes will be a simple change to make, especially in the case of linear model.</p>", 
                "question": "Incorporating expert predictions into a prediction model (x-post /r/statistics)"
            }, 
            "id": "d2rt3ym"
        }, 
        {
            "body": {
                "answer": "<p>What is Freedman<sq>s Paradox?  </p>", 
                "question": "What is the maximum number of variables I can have in a linear regression before I have to deal with Freedman<sq>s paradox"
            }, 
            "id": "d2pl1i6"
        }, 
        {
            "body": {
                "answer": "<p>After 1 variable you have to check for correlation. There is no hard and fast rule, instead you should check and try to make a principled judgement (based on the knowledge of what you<sq>re doing in scientific terms, hopefully.)<br><br>As a note, lasso, ridge and elastic nets and other tools like PCA are for dimension reduction. So, they are useful when you are attempting to understand processes. They are less useful for predictions.<br><br>Bayesian model averaging is for prediction. So, it is more useful for when you want to do predictions.</p>", 
                "question": "What is the maximum number of variables I can have in a linear regression before I have to deal with Freedman<sq>s paradox"
            }, 
            "id": "d2pkaao"
        }, 
        {
            "body": {
                "answer": "<p>The more data you have the more expressive your model can be with out over-fitting to the data and making incorrect predictions.<br><br>Models that are strictly more expressive than their simpler alternatives will always have better p-values, regardless of the number of variables or the amount of data, and you must take this into account.</p>", 
                "question": "What is the maximum number of variables I can have in a linear regression before I have to deal with Freedman<sq>s paradox"
            }, 
            "id": "d2pl25u"
        }, 
        {
            "body": {
                "answer": "<p>Hehe, Ljung-box test shows that there still a correlation between residuals, just run <br>><br>Box.test(model$residuals, lag = floor(sqrt(length(series))))<br><br>and see... So this model does not explain correctly the serie...</p>", 
                "question": "Time Series (ARMA & ARIMA)"
            }, 
            "id": "d2ki2ea"
        }, 
        {
            "body": {
                "answer": "<p>What research questions would you want to know the answer to? What outcomes are you looking for? Why do you have these data in the first place?</p>", 
                "question": "What statistical methods can i use with this data?"
            }, 
            "id": "d15bc3a"
        }, 
        {
            "body": {
                "answer": "<p>If I understand you correctly you have 2 variables. I don<sq>t understand what the purpose is for separating them in different pens, however, you could consider doing a regression analysis.<br><br>Where<colon><br><br>y=(weight in period t+1  -  weight in period t)<br><br>x= (bushels eaten)<br><br>y=b0+b1x+e<br><br>This will essentially tell you what effect the numbers of bushels eaten has on weight.<br><br>The other option is t-test or f-test. The t-test is used to compare means. So you could compare the  mean amount eaten in plot 1 vs plot 2 or plot 3. F-test is similar, it<sq>s used to compare variances. Similar to the t-test it you can test whether the variance in plot 1 is similar to the variance in plot 2 or 3. Hope this helps.</p>", 
                "question": "What statistical methods can i use with this data?"
            }, 
            "id": "d159c4c"
        }, 
        {
            "body": {
                "answer": "<p>Your question is<colon> why minimize negative log likelihood instead of the mean squared error when fitting a logistic regression model? This is a good question, and one that has received theoretical attention before ([Zhang, Annals of Stats](https<colon>//projecteuclid.org/euclid.aos/1079120130)).<br><br>First, these two criteria generally *do not give the same answer*, although they often are quite close.<br><br>[Here are some notes](http<colon>//www.ccs.neu.edu/home/vip/teach/MLcourse/2_GD_REG_pton_NN/lecture_notes/logistic_regression_loss_function/logistic_regression_loss.pdf) discussing this problem. Minimizing SSE is more robust to outliers (worst case loss is 1 for a given observation with squared error loss, but unbounded with likelihood). Unlike maximum likelihood, SSE for logistic regression is a non-convex objective, which makes it a harder optimization problem.<br><br>You can see this by evaluating your objective functions over a grid of possible values for your intercept and slope with your data and making a contour plot (I would not try this in Excel!). [I did a quick example with simulated data](http<colon>//i.imgur.com/urWYc6X.png), panel on the left shows relative values of the negative log likelihood objective, panel on the right shows the SSE objective, red dot is likelihood optimum, yellow dot is SSE optimum. You can see that the likelihood loss function is better behaved.</p>", 
                "question": "Logistic Regression<colon> Maximum Likelihood vs Minimizing SSE"
            }, 
            "id": "d12cv3y"
        }, 
        {
            "body": {
                "answer": "<p>I seem to recall that for logistic regression, these two are equivalent. Minimizing SSE yields a prediction which is just the expected value at the input point X. But that expected value is just P(Y = 1|X), which is also the output for logistic regression.<br><br>I think I saw that in the neural network literature in the 90<sq>s. (Remember that logistic regression = neural network with no hidden units.) No references come to mind, sorry.</p>", 
                "question": "Logistic Regression<colon> Maximum Likelihood vs Minimizing SSE"
            }, 
            "id": "d11qt1s"
        }, 
        {
            "body": {
                "answer": "<p>No.  They should not be the same, in general.<br><br>The distribution of results that *must* lie on [0,1] is most certainly not normal.  The logit transform takes that range and extends it to \u00b1inf.<br><br>What you are attempting is more in keeping with a probit analysis (but it is not that either).</p>", 
                "question": "Logistic Regression<colon> Maximum Likelihood vs Minimizing SSE"
            }, 
            "id": "d11kner"
        }, 
        {
            "body": {
                "answer": "<p>Logistic regression is not based on the *log* of you data-points, but the *[logistic function](https<colon>//en.wikipedia.org/wiki/Logistic_function)* of your data points which looks like _/\u203e and the parameters choose where the / part goes to best bisect your data.</p>", 
                "question": "Logistic Regression<colon> Maximum Likelihood vs Minimizing SSE"
            }, 
            "id": "d11hva5"
        }, 
        {
            "body": {
                "answer": "<p>Group A is more likely to have diabetes by 10 percentage points. It is 20<percent> more likely to have diabetes.</p>", 
                "question": "Group A has a 60<percent> chance of having diabetes. Group B has a 50<percent> chance of having diabetes. Is it more accurate to say that a person in Group A is 10<percent> more likely to have diabetes or 20<percent>?"
            }, 
            "id": "d0c4af0"
        }, 
        {
            "body": {
                "answer": "<p>Both are accurate. One is an absolute risk difference (10<percent>) and the other is a relative risk difference (20<percent>).<br><br>The important thing is to make it clear which you are using, it is easy to use relative risk to make the effect sound more important than it is.</p>", 
                "question": "Group A has a 60<percent> chance of having diabetes. Group B has a 50<percent> chance of having diabetes. Is it more accurate to say that a person in Group A is 10<percent> more likely to have diabetes or 20<percent>?"
            }, 
            "id": "d0cbddh"
        }, 
        {
            "body": {
                "answer": "<p>If these are probabilities then u/madrumos is correct.  If your data is a sample of relative frequencies from a population then you<sq>ll need to perform a hypothesis test that takes variance and sample size into account to see if the differences are significant.</p>", 
                "question": "Group A has a 60<percent> chance of having diabetes. Group B has a 50<percent> chance of having diabetes. Is it more accurate to say that a person in Group A is 10<percent> more likely to have diabetes or 20<percent>?"
            }, 
            "id": "d0c6cv1"
        }, 
        {
            "body": {
                "answer": "<p>In my experience trying to explain percentages to the typical adult reader, relative increases are always looked at as if you are trying to trick them. I only describe absolute changes now, and if necessary I might qualify it more vaguely like <dq>Group A were twice as likely to respond as Group B<dq> if 10<percent> -> 20<percent> for example</p>", 
                "question": "Group A has a 60<percent> chance of having diabetes. Group B has a 50<percent> chance of having diabetes. Is it more accurate to say that a person in Group A is 10<percent> more likely to have diabetes or 20<percent>?"
            }, 
            "id": "d0d3zbx"
        }, 
        {
            "body": {
                "answer": "<p>It seems okay to me, but I just started taking stochastics this semester, so I<sq>m far from proficient.<br><br>You might also want to try posting this in /r/learnmath<br><br>There<sq>s some fantastic help over there.</p>", 
                "question": "stochastic processes - Unsure whether my continuous time Markov Chain distribution is correct"
            }, 
            "id": "czvpxwg"
        }, 
        {
            "body": {
                "answer": "<p>OK never mind, I answered it myself and posted it.</p>", 
                "question": "stochastic processes - Unsure whether my continuous time Markov Chain distribution is correct"
            }, 
            "id": "czw7r9o"
        }, 
        {
            "body": {
                "answer": "<p>Depends on the competitiveness of the program.  I did an MS in Applied Stats with a history undergrad.  Most programs require linear algebra for admission but mine didn<sq>t and maybe you could waive it.  You can look for a program with a more applied than theoretical focus, but I<sq>d be cautious of <dq>data science<dq> programs unless the school seems legit.</p>", 
                "question": "Liberal Arts grad pursuing an MS in Stats...how can I prepare/"
            }, 
            "id": "cyw5qjf"
        }, 
        {
            "body": {
                "answer": "<p>1. He provokes you and wants to test your knowledge. He expects that if you have cheated, then you will admit in face of accusation (possibly because it<sq>s better to admit to cheating and do the work again, correctly?); if you have not cheated, he expects you to explain why this is perfectly normal for real-world data to have <dq>low<dq> score. Also - I guess you are talking about p-values, and p close to 0.01 is nowhere near <dq>too-good-to-be-true<dq>.<br><br>2. He has no idea what he is doing. What is the course name?</p>", 
                "question": "Pearson Chi-Square significance level at ~1<percent>... Professor says my data might be fake but it isn<sq>t"
            }, 
            "id": "cxyzb93"
        }, 
        {
            "body": {
                "answer": "<p>What data is collected from the questionnaires? What comparison was he making with a chi-square test?</p>", 
                "question": "Pearson Chi-Square significance level at ~1<percent>... Professor says my data might be fake but it isn<sq>t"
            }, 
            "id": "cxz2lpt"
        }, 
        {
            "body": {
                "answer": "<p>How many observations you have and how many degree of freedoms do you have ? Chi-Square are extremely sensitive to the number of observation. With a sample large enough, you can prove anything.</p>", 
                "question": "Pearson Chi-Square significance level at ~1<percent>... Professor says my data might be fake but it isn<sq>t"
            }, 
            "id": "cxz3pbn"
        }, 
        {
            "body": {
                "answer": "<p>When you say <dq>reliability<dq>, what exactly does he mean? Scale reliability?<br><br>And what does that .012 number mean? Is that your test statistic, or is that the p-value? If it<sq>s a p-value, then .012 is perfectly acceptable. In linear modeling applications, .001 is usually the reject criterion on a chi-square test because the test is so sensitive.</p>", 
                "question": "Pearson Chi-Square significance level at ~1<percent>... Professor says my data might be fake but it isn<sq>t"
            }, 
            "id": "cxz4wm9"
        }, 
        {
            "body": {
                "answer": "<p>Assuming you are talking about a single sample test where you are comparing your test set to some known distribution, the critical values on a one sided t-test are from the top (or bottom) alpha percentile out. On a two sided t-test, they are from the top and bottom 1/2 alpha out. The total critical area is the same in each, but it is distributed differently.<br><br>In a one sided t-test, if your p value is just past the alpha line away from the mean, then in a two sided test, it wouldn<sq>t have passed the 1/2 alpha line yet so you<sq>d reject it in case 1 but not in case 2. In general however, you should not substitute one test for another without good reason.</p>", 
                "question": "If you reject the null hypothesis for a one sided test with a specific significance level if you do a two sided test would you just always reject it?"
            }, 
            "id": "cwuyc0u"
        }, 
        {
            "body": {
                "answer": "<p>No. A two sided test is more conservative, you<sq>re more likely to accept the null. Test for power rather than playing with test types. Some luck you betad.</p>", 
                "question": "If you reject the null hypothesis for a one sided test with a specific significance level if you do a two sided test would you just always reject it?"
            }, 
            "id": "cwv4rjc"
        }, 
        {
            "body": {
                "answer": "<p>Chance of not dying on the first press<colon> 0.99<br>Chance of not dying through two presses<colon> (0.99)(0.99)<br>Chance of not dying through three presses<colon> (0.99)(0.99)(0.99)<br>...<br>Chance of not dying through 100 presses<colon> 0.99^100</p>", 
                "question": "Very very basic statistics question"
            }, 
            "id": "cvorrrt"
        }, 
        {
            "body": {
                "answer": "<p>The probability of dying at some point during *n* presses can be calculated with this formula<colon><br> <br>p = 1 - .99^*n*<br><br>Here<sq>s why, and it requires a little bit of backwards thinking<colon><br><br>1. You have a .99 chance of *surviving* each press<br>2. To find the probability of surviving *all* the presses, you multiply the probabilities together. Why? Because in general, whenever the word <dq>and<dq> is involved, you multiply the probabilities together*.<br>3. You now have .99^*n* as the probability of surviving *n* presses. Because you either survive them all or die at some point, you subtract .99^*n* from 1 to get the probability of dying. Why? Because whenever you only have a finite number of possibilities, their probabilities add up to 1.<br><br>\\*Multiplication, not addition! By the way, this only works if the events are *independent*, meaning neither can be used to predict the other. <br><br>EDIT<colon> fixed a mistake in my description</p>", 
                "question": "Very very basic statistics question"
            }, 
            "id": "cvovulu"
        }, 
        {
            "body": {
                "answer": "<p>> 96<percent> or something<br><br>Very roughly, it<sq>s about 2/3 (assuming the trials are independent of each other, like coin tosses or something).  The easy way is to work out the chance of surviving<colon><br><br>The chance of surviving one press is 99/100 -- i.e. 1- the chance of dying, (1/100). <br><br>The chance of surviving two presses is (99/100) x (99/100). The chance of surviving three presses is (99/100) x (99/100) x (99/100). ...<br><br>The chance of surviving 100 trials is (1-1/100)^100 = 0.99^100, so the chance of death is 1 minus that<colon> 1 - (1-1/100)^100<br><br>For large n, (1-1/n)^n is about 1/e (where e is 2.71828...), and so the survival probability is about 1-1/e = 63.2<percent><br><br>[For n=100, the exact value is nearer to 63.4<percent>, but the very-large-n answer is already very accurate. Most people are happy enough with <dq>a bit less than 2/3<dq>]<br></p>", 
                "question": "Very very basic statistics question"
            }, 
            "id": "cvpfzls"
        }, 
        {
            "body": {
                "answer": "<p>100 times = the average number of times before a person dies.<br><br>Same reason you can get cancer despite the risk of cancer being less than 100<percent>.</p>", 
                "question": "Very very basic statistics question"
            }, 
            "id": "cvouhdy"
        }, 
        {
            "body": {
                "answer": "<p>You could mess around with R, for example, the sample() function is very easy to use and can easily simulate dice rolls. For example,<br><br>    sample(1<colon>20, 100, replace=TRUE)<br><br><br>would give you a vector of 100d20 rolls.<br><br>There is also a package for R called [dice](https<colon>//cran.r-project.org/web/packages/dice/index.html) that will do a lot of what you ask.  There are some worked examples in the manual and in an answer to this [forum post](http<colon>//stats.stackexchange.com/questions/53154/using-r-for-dice-probabilities).<br><br>R would also be a great way to plot your results!</p>", 
                "question": "Applying Statistics in Dungeons & Dragons RPG"
            }, 
            "id": "cv49yz0"
        }, 
        {
            "body": {
                "answer": "<p>I second the recommendation to generate some random samples and take a look at the results.  In general, adding dice will have the following basic implications<colon> the minimum result will increase (if 0 is not represented on your dice), extreme values will become increasingly improbable, and your average result will become slightly higher (assuming you maintain the same maximum possible score).<br><br>For example, let<sq>s say you considered replacing your 20-sided die (d20) with two ten-sided dice (2d10).  You can still score a 20 using 2d10 (if you roll two 10s), but the lowest result you can achieve is a 2 (if you roll two 1s).  Furthermore, the chance of rolling a 20 has gone from 1/20 = .05 to 1/100 = .01.  Finally, the average result goes from 10.5 to 11.  <br><br>The gameplay implications here are that events that are contingent on very high or low rolls will happen much less frequently (if at all), and on average rolls will become increasingly likely to result in success (again, assuming that the maximum possible roll does not change, and also that success is not generally contingent on particularly high rolls).</p>", 
                "question": "Applying Statistics in Dungeons & Dragons RPG"
            }, 
            "id": "cv4cuns"
        }, 
        {
            "body": {
                "answer": "<p>Very interesting problem. Here<sq>s my R code to simulate the problem<colon><br><br>die_20<-floor(runif(10000,min=1,max=20))<br>barplot(table(die_20))<br><br>die_10<-floor(runif(10000,min=1,max=10))+floor(runif(10000,min=1,max=10))<br>barplot(table(die_10))<br><br>die_10plus5<-max(floor(runif(10000,min=1,max=10)),floor(runif(10000,min=1,max=10)))+<br>  floor(runif(10000,min=1,max=5))<br>barplot(table(die_10plus5))<br><br>You can use sample as /u/montgomerycarlos stated to get the same effect. Floor just rounds the value down to make it an integer. This validates what you mentioned about the triangle distribution vs. uniform. The third example is where I took the max of two rolls of 10 and added a roll of 5 to it. Interesting results!</p>", 
                "question": "Applying Statistics in Dungeons & Dragons RPG"
            }, 
            "id": "cv4dwqv"
        }, 
        {
            "body": {
                "answer": "<p>There is a pdf out there called [The Probability Distribution of<br>the Sum of Several Dice](http<colon>//digitalscholarship.unlv.edu/cgi/viewcontent.cgi?article=1025&context=grrj) <- PDF.<br><br>That shows how to get the probabilities analytically, using moment generating functions. A discrete moment generating function has the form<colon><br><br>mgf = sum of [p_k * e^(k * t)] for all k<br><br>Where k is the outcome (the value on the die), and p_k is the probability of seeing that value.<br><br>Let<sq>s work through an example using a d4. The moment generating function (mgf) is<colon><br><br>mgf = (1/4 e^(t) + 1/4 e^(2t) + 1/4 e^(3t) + 1/4 e^(4t))<br><br>For uniform probabilities, the coefficient can come out front.<br><br>You have to roll at least 5 times to get a 20, so let<sq>s get the mgf for 5 rolls<colon><br><br>mgf_5 = product i from 1 to 5 [ mgf ]<br><br>mgf_5 = 1/4^(5) (e^(t) + e^(2t) + e^(3t) + e^(4t))^(5)<br><br>Expanding the right hand side is a bit of work, so let<sq>s [cheat and use wolfram to do it](http<colon>//www.wolframalpha.com/input/?i=expand+<percent>281<percent>2F4*<percent>28exp<percent>28t<percent>29<percent>2Bexp<percent>282*t<percent>29<percent>2Bexp<percent>283*t<percent>29<percent>2Bexp<percent>284*t<percent>29<percent>29<percent>29<percent>5E5).<br><br>In other words, when you see<colon><br><br>e^(20t)/1024<br><br>That means that for k = 20 (k being the sum of the dice), p_k = 1/1024. For other die combinations, just change up the initial mgf and the power (so you can get a sum of 20).<br><br></p>", 
                "question": "Applying Statistics in Dungeons & Dragons RPG"
            }, 
            "id": "cv552no"
        }, 
        {
            "body": {
                "answer": "<p>Assuming timing within the hour for each event is random and independent of the other events...<br><br>P(only B happens)=(1-.03)\\*.04\\*(1-.05)<br><br>P(A+B happen, B first)=.5\\*.03\\*.04\\*(1-.05)<br><br>P(B+C happen, B first)=.5\\*(1-.03)\\*.04\\*.05<br><br>P(A+B+C happen, B first)=(1/3)\\*.03\\*.04\\*.05<br><br>Summing these, P(B happens and is first)=.03842.<br><br>Or conditional on B happening in the hour, .03842/.04=.9605. This may be counter intuitive, but is driven by the high probability that neither A or C happen, making B first by default.</p>", 
                "question": "Events A B and C have a 3<percent> 4<percent> and 5<percent> chance of occurring in the next hour respectively. What is the probability that the very next event in the hour should it occur will be B?"
            }, 
            "id": "cublk7z"
        }, 
        {
            "body": {
                "answer": "<p>The brute force approach would be to write a Monte-Carlo simulation.</p>", 
                "question": "Events A B and C have a 3<percent> 4<percent> and 5<percent> chance of occurring in the next hour respectively. What is the probability that the very next event in the hour should it occur will be B?"
            }, 
            "id": "cubl41y"
        }, 
        {
            "body": {
                "answer": "<p>Are the processes generating A<sq>s B<sq>s and C<sq>s independent? Do the events within each process occur independently of each other?</p>", 
                "question": "Events A B and C have a 3<percent> 4<percent> and 5<percent> chance of occurring in the next hour respectively. What is the probability that the very next event in the hour should it occur will be B?"
            }, 
            "id": "cubnbxh"
        }, 
        {
            "body": {
                "answer": "<p>I believe it<sq>s .04/(.03+.04+.05)= 1/3. The main point is that the question assumes that one of the events occurs, so you can ignore the possibility that no events occur, and it<sq>s basically the events racing against each other.<br><br>edit<colon> technically the given information is not enough to answer the question, since A B C could be more likely earlier or later in the hour. </p>", 
                "question": "Events A B and C have a 3<percent> 4<percent> and 5<percent> chance of occurring in the next hour respectively. What is the probability that the very next event in the hour should it occur will be B?"
            }, 
            "id": "cubl3yq"
        }, 
        {
            "body": {
                "answer": "<p>You<sq>d just set their glicko rating equal (1500) and have them play x amount of games together. I think USCF (elo) might be best because it handles large jumps rather well I believe. <br><br>Anyway USCF says 25 games before you<sq>re representative. But I<sq>m not sure. I saw someone do the math and beating Magnus Carlsen ten times in a row as a new player only brings you up to 1900 or something.</p>", 
                "question": "Can you help me to design an experiment to determine the best chess player out of a pool of 8."
            }, 
            "id": "cu6wnh7"
        }, 
        {
            "body": {
                "answer": "<p>What kinds of statistical software are you familiar with? That will drive the answer to this question, I think.<br><br>There are tons of ways to make a simple 1 input 1 output model. if the variation of the actual point looks normally distributed around a given projected value, then linear regression might be ok. If it<sq>s not normal, or if there is a complex shape, you might try some kind of kernel density method.<br><br>The first thing to do is to make a scatter plot of your data and look at the shape of it. Could you fit a straight line through the mean of the data? Maybe a quadratic or some other form? What do the errors look like? Etc. <br><br>If, for example, you only have Excel, you can fit a linear model (or use the solve function to fit a more complex model) first. Then you can look at the residuals and see if they have any kind of relationship, or if they look random. </p>", 
                "question": "Looking for help on how to generate a statistical model for fantasy football data"
            }, 
            "id": "cu70due"
        }, 
        {
            "body": {
                "answer": "<p>Look up the CAPM model, it is both easy and useful in finance. Also do a quick read about efficient market hypothesis. Otherwise just explain what you said above, it makes sense and they know the trade off they want to make when they hire. If you haven<sq>t lied on your resume so far then they already know.<br><br>Alternatively, take whatever your special interest area is in stats and just google those keywords with something like <dq> ___ and applications in finance<dq>, you<sq>re bound to find something. There are tons of applications for all kinds of common and esoteric stats stuff.</p>", 
                "question": "Financial data analyst interview"
            }, 
            "id": "cu5eks4"
        }, 
        {
            "body": {
                "answer": "<p>Generally, at least from what I<sq>ve heard, it<sq>s better to have a mathematical background before going into higher level statistics. I<sq>d suggest sticking with a math major. </p>", 
                "question": "Undergrad Considering Stat Ph.D program few questions"
            }, 
            "id": "ctxwfvs"
        }, 
        {
            "body": {
                "answer": "<p>Signed up to answer your question. <br><br>If your goal is getting into a good PhD program in statistics, you should work with a top statistician at your university and major in pure math. Take courses in analysis, first basic and then measure-theoretic probability, PDEs, statistics, and anything else you think you might enjoy. If possible, take a course on machine learning or probabilistic graphical models. Find out what the research strengths of your university are and learn from the best people you can find. You presumably have three summers of research, so do your best to get an REU every year and go somewhere you would consider doing a PhD (e.g., in California). <br><br>Also ask yourself why you want to do statistics. Job security? Being able to work in many different fields? <br><br>Source<colon> PhD in probability at tier 1 uni in northeast<br></p>", 
                "question": "Undergrad Considering Stat Ph.D program few questions"
            }, 
            "id": "cty1dhr"
        }, 
        {
            "body": {
                "answer": "<p>I, too, am an undergrad considering a stats PhD program (applying in the next couple of months, as a matter of fact). I<sq>m a mathematics-statistics major and everyone I<sq>ve spoken to in the department here (PhD students, my advisor, etc.) has basically said to take as many math classes as possible, as that<sq>s usually where PhD students hit their bottleneck. As such, I<sq>m basically fulfilling the <dq>bare minimum<dq> for the <dq>statistics<dq> part of my major and taking all the upper-level math classes I can (obviously, real analysis -- but also abstract algebra, complex analysis, PDEs, etc; none of which are, by any means, <dq>required<dq> of my major). FWIW, there is not a single PhD student in the stats department at my school (which is a major private research university in the northeast) who has come from a stats background (mostly math, some CS, some engineering). <br><br>Wow, that<sq>s a lot of parentheses.</p>", 
                "question": "Undergrad Considering Stat Ph.D program few questions"
            }, 
            "id": "ctxzz7w"
        }, 
        {
            "body": {
                "answer": "<p>I<sq>ll just echo what<sq>s already here and say that in my experience stats programs tend to favor math students even over their own undergrads.</p>", 
                "question": "Undergrad Considering Stat Ph.D program few questions"
            }, 
            "id": "cty3f5p"
        }, 
        {
            "body": {
                "answer": "<p>Just two thoughts<colon> <br>1. Did you have any a priori leaning - i.e. did you always intend to do bootstrapping in the primary analysis? <br>2. Is there an additional sensitivity analysis you could do? <br>3. Is this just a power problem?</p>", 
                "question": "Bootstrapping gives me significance but not bootstrapping it does not<colon> which one do i believe?"
            }, 
            "id": "ctkjjgr"
        }, 
        {
            "body": {
                "answer": "<p>Im not sure I exactly get what you mean, but couldn<sq>t you just run a series of univariate regression models with each categorical variable? I.e. Model 1<colon> avg life span = manufacturer; Model 2<colon> avg life span = year; etc. <br><br>You can then use a (somewhat arbitrary) threshold of, say, p=0.2, to include these variables in a mutlivariate regression model. I.e. the the p-value associated with model is 0.4 then do not include it in the multivariate regression model. <br></p>", 
                "question": "Significance of categorical variables in a regression analysis/ANOVA (x-post /r/math)"
            }, 
            "id": "ctcayhl"
        }, 
        {
            "body": {
                "answer": "<p>[deleted]</p>", 
                "question": "Significance of categorical variables in a regression analysis/ANOVA (x-post /r/math)"
            }, 
            "id": "ctd2312"
        }, 
        {
            "body": {
                "answer": "<p>My job title is currently lead data scientist, so I am not sure it counts as a professional statistician, but I think I can answer all of the questions<colon><br><br>1. PhD<br><br>2. Full time<br><br>3. I am a bit of a workoholic, but my fiancee is making sure I do not slip entirely<br><br>5. Cleaning / Preparing the data is most of the job and it can be both interesting and boring sometimes. Best approach is to automate as much as you can.<br><br>6. Deep learning is well defined, it<sq>s basically convolutional neural networks which have more layers than usually as well as use an algorithm that differs from the basic backpropagation  used in base neural nets. It<sq>s very prominent right now and it<sq>s used mostly prediction (and classification stemming from that, but not in the unsupervised learning kind of sense). It however requires a more data and computing power to be really efficient. I haven<sq>t tried it yet myself so someone correct me if I am wrong.</p>", 
                "question": "What<sq>s a day in your life like professional statisticians? (and other questions)"
            }, 
            "id": "ctbxwna"
        }, 
        {
            "body": {
                "answer": "<p>1. MSc<br>2. Fulltime<br>3. Play much more than I work. 8-430 everyday, extremely rarely stay late (only if i actually want to!)<br>4. Can be very mundane, or fun. Copying and pasting is boring, dealing with it all in R and programming it is fun. I work with small datasets so its usually mundane.<br>5. Que?</p>", 
                "question": "What<sq>s a day in your life like professional statisticians? (and other questions)"
            }, 
            "id": "ctc5yfm"
        }, 
        {
            "body": {
                "answer": "<p><br>>-What<sq>s your level of education? BS/MS/PHD?<br><br>MS<br><br>>-Do you work part time, or could you work part time?<br><br>I work full time but have unlimited vacation. I like my work so I don<sq>t really take advantage of it much. <br><br>>-What<sq>s your life/work balance like? <br><br>It<sq>s pretty good (for me). I work 6-10 hours a day depending on how I<sq>m feeling and how quickly something needs to get done.<br><br><br>>-What is cleaning/preparing data like?<br><br>Probably 80-90<percent> of what I actually do. It<sq>s not terrible but I spend more time doing that than I would like. It usually is pretty insightful though. <br><br>>-What would you consider <dq>deep learning<dq>? <br><br>Deep learning is latest flavor of neural nets. </p>", 
                "question": "What<sq>s a day in your life like professional statisticians? (and other questions)"
            }, 
            "id": "ctc773q"
        }, 
        {
            "body": {
                "answer": "<p>If you included all the observations in the model, then the analysis uses all the observations. It<sq>s hard to imagine how it could be otherwise. You are correct that the df can be used to document this. Your table should include significant and nonsignicant predictors with the p values (not asterisks). You can reply to the editor that more information is better than less information, that you don<sq>t want to accept the null hypothesis in cases where a predictor is not significant, and that non-signicant effects can provide interesting hints that can be examined in future research. As far as the criteria, just change a word or two and say you have no clarified how variable<sq>s were chosen (all were chosen). </p>", 
                "question": "Reviewer questioning my stats in my article need advice on how/what to respond"
            }, 
            "id": "ct5x21y"
        }, 
        {
            "body": {
                "answer": "<p>Yes, the regression uses all the data, and if the model was correctly specified (response is linear function of covariates...), distribution of explanatory variables shouldn<sq>t matter. But perhaps what the reviewer is getting at is whether the effect could be nonlinear - maybe even a small positive dose causes a <dq>jump<dq> in response compared to zero exposure. You could try to fit the model only on a subsample of people with positive exposure and check whether coefficients are similar. Another option could be to include dummy variable equaling 0 if there<sq>s no exposure, 1 if there<sq>s positive amount, as additional explanatory variable in the model.</p>", 
                "question": "Reviewer questioning my stats in my article need advice on how/what to respond"
            }, 
            "id": "ct5znei"
        }, 
        {
            "body": {
                "answer": "<p>You are globally right.<br><br>However, indirectly your reviewer bring up a valid point <colon> your model is a bit biaised. But not the way he thinks it is. In fact, your model is biaised toward the non-toxin group.<br><br>Let me explain with a poor flawed example <colon> if you were to mesure if the length of someone<sq>s hair would affect his score in a math test, how would you react if 80/150 participant were bald ? In a linear regression, each point worth as much as the other one. So more than 50<percent> of your data used for the regression has <dq>0<dq> in one variable.<br><br>Your results are <dq>true<dq>, but in layman<sq>s term, your model used 150 points to calculate your cross-variable coefficients, while it used only 70 points to calculate the coefficient for the toxin.</p>", 
                "question": "Reviewer questioning my stats in my article need advice on how/what to respond"
            }, 
            "id": "ct60odn"
        }, 
        {
            "body": {
                "answer": "<p>Does your university have a statistical consulting service? If so, that<sq>s your best bet.</p>", 
                "question": "Where can I get help from a professional statistician?"
            }, 
            "id": "csu1dco"
        }, 
        {
            "body": {
                "answer": "<p>You are probably going to want to find a statistical consulting service at your University. They probably (although not for sure) have one. <br><br>You don<sq>t want to go down the hiring a <dq>freelance<dq> statistician route. For a (masters) student, you are looking at north of 60 dollars an hour. For a PhD student/holder of MS, you are looking at 100 and for a PhD you are looking at 150 or more. <br><br>Your university will likely either offer these services for free or very low cost as it gets the stats students consulting experience. </p>", 
                "question": "Where can I get help from a professional statistician?"
            }, 
            "id": "csu7fzy"
        }, 
        {
            "body": {
                "answer": "<p>Try statistics.com<br><br>I believe they have consulting services.</p>", 
                "question": "Where can I get help from a professional statistician?"
            }, 
            "id": "csua6m9"
        }, 
        {
            "body": {
                "answer": "<p>Like others have said, you should ask your local university first. Start with your own adviser, then maybe the Psych, Education, Stats, Sociology, Econ departments. <br><br>If you run into a dead end with the resources mentioned, feel free to shoot me a private message. I was the consultant at my university for a couple of years. Most all dissertations I did for education research weren<sq>t too involved; most of the analyses could be done within a couple of hours. Pointing you in the general direction shouldn<sq>t take too long. </p>", 
                "question": "Where can I get help from a professional statistician?"
            }, 
            "id": "csumw1p"
        }, 
        {
            "body": {
                "answer": "<p>I<sq>d try to make use of the wasted space somehow and put something else there. Perhaps adjusted p-values for the hypothesis test of no correlation, another measure of correlation, or something else?</p>", 
                "question": "When you put in a correlation table where mirrored cells would equal the same number (e.g. Fire x Water = Water x Fire) which group of data would you leave blank? Above or below the diagonal?"
            }, 
            "id": "cstl65x"
        }, 
        {
            "body": {
                "answer": "<p>I am not aware of any rules. A quick [Google Image search](https<colon>//www.google.ch/search?q=correlation+matrix&biw=1080&bih=1835&source=lnms&tbm=isch&sa=X&ei=YR6aVfKVBobuUOu_vtAO&ved=0CAYQ_AUoAQ) of the terms <dq>correlation matrix<dq> reveals that both options are being used. Personally, I<sq>d leave the entries above the diagnonal blank.</p>", 
                "question": "When you put in a correlation table where mirrored cells would equal the same number (e.g. Fire x Water = Water x Fire) which group of data would you leave blank? Above or below the diagonal?"
            }, 
            "id": "cstkvx0"
        }, 
        {
            "body": {
                "answer": "<p>These results are the <sq>loadings<sq>, which are just the coefficients for your new variables . As such, you can have missing data.<br><br>Missing data simply means that no combination involving that part in your principal component can be found to explain the variance.<br><br>So, in your data - Component 3 is able to succesfully explain the variance in your data using all variables and teh coefficients are listed. Just as with any regression.<br><br>Component 1 was only able to use salinity, temp and depth. Grain and nitrogen are not part of the component.<br><br>And so on.<br><br>The SD and proportion of variance help show which components are most effective at describing the data. Component 1 will always explain the most variance and so on down. All components together as listed are able to explain the variance completely (or almost completely).<br><br>In this case, component 1 (salinity, temp, depth) explain 73.2<percent> of variance. Add in component 2 (temp, grain, depth and nitrogen) and you explain 87.7<percent> of the variance. etc etc<br><br>Thats about it. If you want to plot you just use what is there. so plotting component 1 only includes the variables in that component. You don<sq>t plot salinity, for example, as 1 vector.<br><br>Hope that helps. PCA can be confusing but it<sq>s just like any regression in principle when you break it down.<br></p>", 
                "question": "Need help interpreting Principal Components Analysis (PCA) results"
            }, 
            "id": "csfrddp"
        }, 
        {
            "body": {
                "answer": "<p>Actually, those look like suppressed values. Many statistical packages will suppress (i.e., not display) component loadings below a certain minimum value. I<sq>m guessing that whatever you<sq>re using to run the PCA is suppressing every loading under .100</p>", 
                "question": "Need help interpreting Principal Components Analysis (PCA) results"
            }, 
            "id": "csfvg0o"
        }, 
        {
            "body": {
                "answer": "<p>Excellent description of your problem. Do a regression of tail length ~ body length + body length * subpopulation + subpopulation. That is, just include a categorical term and an interaction term between the categorical population-defining variable and the quantitative predictor. That will give you a separate slope and intercept (with standard error) for each subpopulation that you can compare.<br><br>To do this *more* correctly, take a look at [generalized linear] mixed effects models <colon>-) </p>", 
                "question": "Comparing slopes/intercepts of regressions"
            }, 
            "id": "cseulvf"
        }, 
        {
            "body": {
                "answer": "<p>Agree with /u/fireflite - just adding that an ANCOVA is not appropriate here because it requires the use of a different covariate measured for the same subject. You can<sq>t use it to compare (potentially) different populations.</p>", 
                "question": "Comparing slopes/intercepts of regressions"
            }, 
            "id": "csexcya"
        }, 
        {
            "body": {
                "answer": "<p>Hey, I<sq>ve published many meta analyses over the past 15 years and can likely answer any question you have. Essentially, a meta analysis is a pooled analysis comprised of the outcome data from two or more pieces of earlier research. You need at least two groups per study and a numerator and denominator for each of those groups. There are several software packages that can perform meta analysis (I prefer stata), but review manager is free (you can dl it from the revman page on the cochrane collaboration website).<br><br>When I get to a computer (vs my phone) I can show you an online how to. M/A isn<sq>t all that daunting, you just need to know what you<sq>re pooling.  </p>", 
                "question": "Grad student looking for help with a meta-analysis"
            }, 
            "id": "csa5z6q"
        }, 
        {
            "body": {
                "answer": "<p>There will be no single answer to your question as it will depend largely on the data you collect from the literature. For example, [**this paper**](http<colon>//www.nature.com/nrg/journal/v14/n6/full/nrg3472.html) describes some meta analysis techniques for one specific type of study (GWAS)--while the discussion is highly tailored to GWAS data, you can also get some sense of the major ideas inolved (e.g. harmonizing data, collecting summary statistics and tracking down heterogeneity)<br><br>What does you committee have to say? They should be able and willing to offer cursory guidance.  <br><br></p>", 
                "question": "Grad student looking for help with a meta-analysis"
            }, 
            "id": "csa122r"
        }, 
        {
            "body": {
                "answer": "<p>Questions like <dq>What parameters are used?<dq> <dq>Are the models validated?<dq> <dq>How are the models validated?<dq> would not be answered by a meta-analysis I think. <br><br>A meta-analysis would take all the results from those models and attempt to extract an overall numerical results, i.e. an overall effect size (and associated p-value).</p>", 
                "question": "Grad student looking for help with a meta-analysis"
            }, 
            "id": "csaju76"
        }, 
        {
            "body": {
                "answer": "<p>You may be beyond this point, but my favorite introduction to the idea of narrative and quantitative meta-analyses and how they support your own study design is in<colon><br><br>Light, Richard J., Judith D. Singer, John B. Willett. 2009. [By Design<colon> Planning Research on Higher Education](http<colon>//acawiki.org/By_Design<colon>_Planning_Research_on_Higher_Education). Harvard University Press.<br><br>Good luck!</p>", 
                "question": "Grad student looking for help with a meta-analysis"
            }, 
            "id": "csan4l5"
        }, 
        {
            "body": {
                "answer": "<p>Reading quickly, the t-test sounds correct. Sounds like your anova isn<sq>t set up to account for the repeated measures of the same individual across conditions. Otherwise I believe the tests should be equivalent.</p>", 
                "question": "What is the difference between the interaction in a 2way ANOVA and using a t-test to compare the difference between differences?"
            }, 
            "id": "cs400w6"
        }, 
        {
            "body": {
                "answer": "<p>did you put a subject effect in your ANOVA? Your t-test might use within subjects variation as error, while your ANOVA uses between+within variation.</p>", 
                "question": "What is the difference between the interaction in a 2way ANOVA and using a t-test to compare the difference between differences?"
            }, 
            "id": "cs40xn6"
        }, 
        {
            "body": {
                "answer": "<p>[deleted]</p>", 
                "question": "What is the difference between the interaction in a 2way ANOVA and using a t-test to compare the difference between differences?"
            }, 
            "id": "cs43n5j"
        }, 
        {
            "body": {
                "answer": "<p>a stratified random sample is when you divide the population into groups, or strata, according to some criteria (location, age, income...) and select a simple random sample from each group. A cluster sample is when you already have sort of <dq>natural<dq> breaks between groups, like voting districts or blocks of a city. You then take a simple random sample of clusters and sample all elements within those clusters.<br><br>To sum it up<colon><br><br>* Stratified random sample<colon> take a simple random sample within each group<br><br>* Cluster sample<colon> take a simple random sample of groups and then sample all items within the selected groups (clusters).<br><br></p>", 
                "question": "Clustered vs Stratified difference?"
            }, 
            "id": "cr5luxg"
        }, 
        {
            "body": {
                "answer": "<p>/u/buttfoot has given a pretty good explanation of the difference between the two, but it might help to look at some of the properties of both, and why you would do either. Both sample designs are based on the same general idea, which is that you want your sample to, on average, contain a miniature version of your whole population - generally, you want it to capture the behaviour of your variable(s) of interest.<br><br>A stratified sample tries to divide your population into groups such that units in each stratum are *similar* with respect to your variable of interest. That way, you only need to sample a few units from each stratum to get a feel for how that variable behaves for the whole stratum, and hence by picking a few units from each stratum you get a sample that is distributed like the whole population.<br><br>For example, if you were trying to estimate something to do with wages, then you might want to stratify businesses by what industry they operate in (because you might suspect that people employed in law firms have similar wages, and so might people employed in fast food restaurants, but the two groups would be different from each other), and also by some measure of the size of the business (because the total amount of wages, and maybe even the spread of wages, is probably different for a business with thousands of employees compared to one that only employs ten people).<br><br>Stratified sampling allows for a few different things, too. (For example, if you also want to look at estimates divided in a particular way, you can stratify by that division so that you ensure a certain amount of sample or accuracy within each grouping.) And technically, stratification is a kind of meta-sample design, since after you<sq>ve stratified you can apply any kind of sample design you like *within* each stratum.<br><br>Cluster sampling wants you to create groups so that the units within each group have a big spread, and the groups themselves are similar to each other. For example, if you take a cluster sample of apartment buildings to get a sample of residents, then you<sq>re hoping that each building has a similar spread of, for example, families in bigger apartments, single people in studios, wealthier people in penthouses, etc. That way, by sampling just a couple of apartments you get the whole spread of the area<sq>s population.<br><br>Cluster sampling also typically assumes that there are a few levels of costs involved in sampling a unit - specifically, that there<sq>s a cost involved in sampling the cluster, and then a cost involved in sampling the units within that cluster - and that the per-cluster cost is much bigger than the per-unit cost. So maybe if you<sq>re taking clusters of towns, then it costs a lot of money to go to a town, but once you<sq>re there it<sq>s relatively cheap to interview the people in that town, so you may as well include all of them in your sample.<br><br>To sum that up<colon><br><br>* Stratified sample<colon> wants low variance *within* strata, high variance *between* strata.<br><br>* Cluster sample<colon> wants high variance *within* clusters, low variance *between* clusters. Also assumes it<sq>s cheap to sample *within* a cluster, expensive to sample many clusters.</p>", 
                "question": "Clustered vs Stratified difference?"
            }, 
            "id": "cr7c11l"
        }, 
        {
            "body": {
                "answer": "<p>1.5<percent> of the population being Gay, Lesbian or Bisexual is insanely low. I suspect that the majority of Gay, Lesbian or Bisexual people did not identify themselves on the first survey.   <br>  <br>Think about it logically. Just going by the presence of Gay, Lesbian or Bisexual people you see around you and in the media etc would it make sense that these people make up only 1.5<percent> of everyone? Are there 98 straight people for every two <dq>other<dq>? <br>   <br>The second figure is also suspect. These are people who were caught and charged so it does not include those who got away with the crime. We can safely assume hetrosexual persons in a position of trust are more likely to avoid detection/arrest than an <dq>out<dq> gay person who society in general can be harsher in their judgment. <br>  <br>Also, many pedophiles who identlfy as straight abuse boys and girls. Their sickness makes them target children not a particular gender. Once arrested for abusing little boys they would feel obliged to identify as gay or bi. If not I could easily see the arresting officer marking them down as such. These people would have answered the census identifying as hetrosexual.   <br> <br>Similar debates happen in America where the number of black men arrested and sent to jail would suggest black people are more likely to commit crimes. The picture is more complex with factors such as poverty , unfair targeting by police leading to greater arrest of blacks than whites , etc leading to a more nuanced and complex situation than the raw numbers might suggest. <br>  <br><dq>Never trust a black man<dq> is not the lesson to be learned here. <dq>Never allow non-hetrosexuals to care for children<dq> is not the lesson either for similar reasons. </p>", 
                "question": "Need help understanding a claim comparing two studies."
            }, 
            "id": "cqttcso"
        }, 
        {
            "body": {
                "answer": "<p>Two different studies in two different decades probably don<sq>t have a lot of methodological overlap. I don<sq>t know how it works in academia, but in market survey research it<sq>s generally not great practice to compare data from different surveys this way because there<sq>s no guarantee that both surveys are representative of the same population. That could be a good point for your argument.</p>", 
                "question": "Need help understanding a claim comparing two studies."
            }, 
            "id": "cqu7tg5"
        }, 
        {
            "body": {
                "answer": "<p>Simply add GDP as another independent variable.  You would use an interaction term if you believe that GDP and Labor interact in their relationship with sustainability (i.e. GDP has a different effect on environmental sustainability depending on what Labor is).  I would guess that it<sq>s not necessary in this case.<br><br>In multiple regression you interpret each coefficient as being independent from all other coefficients.  For example, you could say that a 1 unit increase in Labor results in an expected 3 unit increase in Environmental Sustainability *holding GDP constant*.  There are of course many assumptions that go along with this but that<sq>s the general idea.</p>", 
                "question": "How can I control for the effect of a variable in a regression?"
            }, 
            "id": "cqpndxg"
        }, 
        {
            "body": {
                "answer": "<p>Assuming GDP is not related to your labor variable, I would simply regress environmental sustainability as a function of labor and GDP. The coefficient of labor will be in the context of GDP. [I<sq>d probably also model environmental sustainability individually against labor and against GDP, and then in combination; I<sq>d use information theoretic methods (AICc) to rank the models.]</p>", 
                "question": "How can I control for the effect of a variable in a regression?"
            }, 
            "id": "cqpm9io"
        }, 
        {
            "body": {
                "answer": "<p>What you are describing is exactly what you get when you add GDP in as an interaction.  However, the order can be a problem<colon> GDP may not be simply related and it may not be a simple interaction.<br><br>So I think you might prefer to partition GDP into quartiles or quintiles and treat it like a classification variable rather than another continuous predictor.  Then you would run an analysis of covariance with GDP main effects and an interaction with Labor Index.  This will directly test whether GDP quartile affects the relationship between LI and ES.  It also simplifies the visual presentation.<br><br>This is the simplest analysis I can think of that will give you insight.<br><br>The best analysis might be a 2D nonparametric regression on the residuals after removing the planar effects of LI and GDP to see if there is much structure left to explore.</p>", 
                "question": "How can I control for the effect of a variable in a regression?"
            }, 
            "id": "cqps055"
        }, 
        {
            "body": {
                "answer": "<p>You describe several outcomes (count of dead, stage of cell cycle for the live ones) but not what hypothesis you want to test in relation to them.<br><br>The assumptions for an ordinary t-test don<sq>t hold, since counts will tend to have variance that<sq>s a function of the mean (and the other response variable is ordered categories, so again there<sq>s issues). The repeated measures in your design suggest that none of the options you offer would be sufficient ... but it depends on what you<sq>re testing. <br><br>It wouldn<sq>t be responsible to guess what that is.<br><br>Little help?</p>", 
                "question": "Can I carry out T-tests on percentages?"
            }, 
            "id": "cqj0fzd"
        }, 
        {
            "body": {
                "answer": "<p>The topic you are asking about involves de-identification of data in such a way that prevents the re-identification of that data. Here is an article in Science about this relating to credit card data, it might give you some leads.<br><br>http<colon>//www.sciencemag.org/content/347/6221/536.full<br></p>", 
                "question": "Ensuring Anonymity"
            }, 
            "id": "cq6u91z"
        }, 
        {
            "body": {
                "answer": "<p>One of the first formalizations of the concept that I<sq>m aware of is k-anonymity. It<sq>s since been superseded by different approaches, you can follow the links from this wiki article to the newer stuff<br><br>https<colon>//en.wikipedia.org/wiki/K-anonymity#See_also</p>", 
                "question": "Ensuring Anonymity"
            }, 
            "id": "cq79ykm"
        }, 
        {
            "body": {
                "answer": "<p>[This](https<colon>//www.aeaweb.org/articles.php?doi=10.1257/jep.28.2.75) paper describes challenges of anonymizing datasets, with some examples of badly-done anonymization in the past, could be relevant.</p>", 
                "question": "Ensuring Anonymity"
            }, 
            "id": "cq7m6bj"
        }, 
        {
            "body": {
                "answer": "<p>If we assume boys and girls are each 50<percent> of live births (I believe there<sq>s a little bit more girls in reality), then that exact outcome, like each other pattern of 10 siblings, has probability 1/(2^10)=1/1024. But that<sq>s almost certainly the wrong question to ask<colon> for example, the answer would be the same if you asked, what is the probability of 10 boys and 0 girls.<br><br>I think a good question would be, what is the probability of first having some children of one sex, and then only children of the other sex, for 10 children total? The answer to that is 20/1024, if I<sq>m not mistaken. (I count having, say, 10 boys as an example of this.) That is a little less than 2<percent>.</p>", 
                "question": "Odds of first 4 kids are girls and next 6 kids are boys?"
            }, 
            "id": "cq2blvb"
        }, 
        {
            "body": {
                "answer": "<p>Well strictly speaking it<sq>s 0.5^10 so 1/1024.<br>It<sq>s exactly the same probability as having 10 boys and the one of having 10 girls. This is because you specified that the 4 first were girls and 6 next were boys, so for each baby there is only one <dq>winning<dq> toss coin.<br><br>If you want to know what<sq>s the probability of having first 4 babies of the same sex and the next 6 of the same sex it<sq>s actually much higher (1/256 I<sq>d say)</p>", 
                "question": "Odds of first 4 kids are girls and next 6 kids are boys?"
            }, 
            "id": "cq2bl9x"
        }, 
        {
            "body": {
                "answer": "<p>Given that this actually happened, the odds are 100<percent> <colon>D</p>", 
                "question": "Odds of first 4 kids are girls and next 6 kids are boys?"
            }, 
            "id": "cq2o1u8"
        }, 
        {
            "body": {
                "answer": "<p>I<sq>d suggest using a linear regression model including an interaction term for the day with the control-status. These are both dummy variables. Then fit a linear regression model without the interaction. Use a likelihood-ratio test to see if there is evidence that the day affects the difference between controls and treatment subjects (i.e. if the interaction term is needed). If there is little evidence, you can drop the interaction term. Now, your model would include the control-status and the day-variable but not their interaction. Now use a likelihood-ratio test again to see if there is evidence for day-effect. If not, drop the day-variable which leaves you with an overall (i.e. pooled) t-test for the difference between controls and treatment animals.</p>", 
                "question": "Question<colon> Is it appropriate to pool/combine data sets to simplify analysis (use Student<sq>s t-test)? Please help!"
            }, 
            "id": "cpxmdgh"
        }, 
        {
            "body": {
                "answer": "<p>when you combined sets A and B and did t-test the control vs. treatment groups, did you make sure to use a weighted average of the sample variances (commonly noted Sp) multiplied by sqrt(1/n1+ 1/n2) in the denominator of the t-stat calculation?<br><br>see here for more<colon> https<colon>//onlinecourses.science.psu.edu/stat200/node/60<br><br></p>", 
                "question": "Question<colon> Is it appropriate to pool/combine data sets to simplify analysis (use Student<sq>s t-test)? Please help!"
            }, 
            "id": "cpxyptx"
        }, 
        {
            "body": {
                "answer": "<p>Could you elaborate on what your outcome measure entails? A single score from each condition? What type of variable is it? Categorical (e.g. nominal, dichotomous) or continuous (e.g. interval, ratio)?<br><br>If you are testing for independence of conditions, why are you testing a correlation? A lack of correlation (e.g. a non-significant r) does NOT imply independence. It just means that your data does not support correlation.<br><br>More info about the data would be useful. Thanks.</p>", 
                "question": "How to perform correlations when there are common factors involved?"
            }, 
            "id": "cpnvrty"
        }, 
        {
            "body": {
                "answer": "<p>Does Principal Component Analysis help with this?</p>", 
                "question": "How to perform correlations when there are common factors involved?"
            }, 
            "id": "cpnmy5c"
        }, 
        {
            "body": {
                "answer": "<p>Random forests is good when you have lots of possible predictor variables, don<sq>t need interpretable parameter estimates, and aren<sq>t worried about conventional hypothesis testing. Can be great for prediction or feature selection, though has some issues of the features are correlated.<br><br>Regression comes with all of the nice statistical features of the linear model (p-values, confidence intervals), including being the <dq>best linear unbiased estimator<dq> where the linear model holds. Not great for feature selection, and requires larger sample sizes relative to the number of possible predictors.<br><br>So definitely not the same thing, and the question of which will serve you better will depend on what you are trying to do. You<sq>re also likely to encounter resistance using one or the other depending on your field. Random forests falls into the field of machine learning, which is looked down on in some circles but is the new hotness in others.</p>", 
                "question": "When to use random forests vs multiple linear regression?"
            }, 
            "id": "cpnizfm"
        }, 
        {
            "body": {
                "answer": "<p>To add a bit to what /u/buttfoot said, and tweak a couple of things you said<colon><br><br>>This tells me the likely true range of values for the population I took the sample from.<br><br>You can<sq>t think about it like that.  There is **one** true value for the population.  You are collecting data to give you some *information* about what that true value is.  You can be 95<percent> sure that the interval contains the true value. <br><br>If you collect a different sample from the same population, you will get a different mean and a different interval, because the *information* you learned about the population was different.  Now, if this sample is truly from the same population, then you should probably combine the two samples, sine more information will give you a more accurate idea about where the true number is.<br><br>>This time I observe a mean of 75<percent>, which doesn<sq>t surprise me because 75<percent> was within my 95<percent> confidence interval.<br><br>The 75<percent> you observe the second time isn<sq>t the thing that the confidence interval is describing- that is another estimate and the confidence interval is trying to capture the truth.  What you should probably say instead is<colon><br><br>*<dq>If we later did a census of the entire population and learned that the true mean was 74, I wouldn<sq>t be surprised because I was 95<percent> sure that the mean was between 65 and 75.<dq>*<br><br><br></p>", 
                "question": "Why is a confidence interval always centered on the mean?"
            }, 
            "id": "co3pfgd"
        }, 
        {
            "body": {
                "answer": "<p>Remember the goal of a CI, to estimate a parameter (true, underlying mean value) based on a statistic (your sample mean). <br><br>Think of it this way, the way to interpret the confidence interval is something like <dq>we are 95<percent> confident true mean value is between .65 and .75. By <dq>95<percent> confident<dq> we mean that if we took repeated random samples of the population and created a confidence interval for each one, 95<percent> of the confidence intervals would contain the true mean value of the variable.<dq><br><br>The interpretation and idea behind CI<sq>s takes into account the variation in samples (that you get different sample means based on different samples).</p>", 
                "question": "Why is a confidence interval always centered on the mean?"
            }, 
            "id": "co3ll61"
        }, 
        {
            "body": {
                "answer": "<p>Some possible starting points are<colon><br><br>1. Basket Analysis (for the connection between Items A and B)<br>2. Penalized Regression (for predictive estimates)<br>3. Longitudinal Data Analysis (for the volatility in Consumer X)<br><br>Not that these are the only tools for their respective applications, but they are some things to consider looking into. As /u/Tartalacame said, though, you<sq>re trying to do quite a lot.</p>", 
                "question": "How to combine probability?"
            }, 
            "id": "cnv9bbq"
        }, 
        {
            "body": {
                "answer": "<p>What you ask for is quite a complex task for someone not very familiar with Stats.<br><br>Here is a 8min very entertaining [Probability 101 course](https<colon>//www.youtube.com/watch?v=YpvE0Co66nU).</p>", 
                "question": "How to combine probability?"
            }, 
            "id": "cnv58rt"
        }, 
        {
            "body": {
                "answer": "<p>Some thoughts<colon> <br><br>>The amount of time spent on each move. (Seconds, normally distributed)<br><br>This surprises me. Generally, response latency is log-normally distributed in such tasks. <br><br>>The similarity between their move and the top computer choice. (Arbitrary unit, normally distributed)<br><br>Help us understand this measurement scale. Sounds suspect.<br><br>> I<sq>m wondering what steps I need to take to build these profiles and then how I should apply a test such as MANOVA <br><br>It<sq>s not clear that MANOVA is appropriate for your goal. You have a binary (cheat, no cheat) outcome and multiple putative predictor variables. You<sq>d probably be better served using something like logistic regression (or even discriminant analysis). You might also consider thinking about this issue in signal detection terms. <br><br>What you need to come up with is a model that is both sensitive and specific to cheaters. You want a high hit rate and a low false positive rate. Take a random sample of your total games such that the proportion of cheat games is equal to the overal rate of cheat games. Build your regression model using the metrics for those games. Then test your model against the remaining games in the database where cheat/no cheat is known to see how it performs. Pay attention to the 2x2 classification matrix--again, you want a high rate of saying <dq>cheat<dq> when it<sq>s cheat but a relatively low rate of saying <dq>cheat<dq> when there is no cheat (otherwise you<sq>re back to wasting mods<sq> time)</p>", 
                "question": "Requesting assistance in applying MANOVA (or similar test) to discover cheaters on a chess website."
            }, 
            "id": "cnjbxff"
        }, 
        {
            "body": {
                "answer": "<p>Here<sq>s some good notes for your consideration<colon> <br><br>https<colon>//chess.stackexchange.com/questions/1521/how-would-you-determine-that-the-player-is-cheating-in-online-chess<br><br>I don<sq>t think MANOVA alone is going to help you out much here. You are making the assumption that you can statistically analyse sharp observables (time of move, similarity of move to a computer player, frequency of moves, web activity) to make a categorical decision on if someone is a cheater or not. This sort of thing is extremely hard in a game where it is much easier to find machine-assisted players (like counterstrike), and what you are going to find is you<sq>re going to get a lot of false-positives, degrade the experience of legitimate players that fall outside your model, and force the dedicated cheaters to use undetectable methods. Eventually, all a cheater would need to do is start up a local game with a decent chess engine, and mirror his human opponent<sq>s moves to a very high level machine player. This will never be detectable, and I assure you, people are already doing it. The best you can hope for is having a feedback system where games are recorded and someone who feels they are a victim of cheating can report a suspect. Then, it<sq>s a matter of getting good feedback from these reports and using them to punish the worst offenders. Players with high positive feedback can play against other people with high positive feedback, and insulate themselves from people who made an account for the sole purpose of cheating. <br><br>TL;DR Trolls suck, and this is going to be a losing battle. </p>", 
                "question": "Requesting assistance in applying MANOVA (or similar test) to discover cheaters on a chess website."
            }, 
            "id": "cndvibe"
        }, 
        {
            "body": {
                "answer": "<p>Since people are terrible random number generators, you can pretty much guarantee the answer is *not* what you<sq>d get by assuming they were good random number generators.<br><br>So anyone who says 1/10 is almost certainly wrong. The actual answer is higher in general.</p>", 
                "question": "What are the odds of two people saying the same number between 1-10?"
            }, 
            "id": "cn0vts4"
        }, 
        {
            "body": {
                "answer": "<p>1/10.  This is no different from the situation where one person guesses a number first (in their head) and then says <dq>Guess what number I am thinking of.<dq> Now, if a third person were to try to guess what **both** of the others guessed, there are 100 possibilities, so it would be 1/100.<br></p>", 
                "question": "What are the odds of two people saying the same number between 1-10?"
            }, 
            "id": "cn0tq4c"
        }, 
        {
            "body": {
                "answer": "<p>~~Assuming independence and each number is equally likely to be selected, each person chooses 1 out of 10 options so (1/10)*(1/10) = 1/100 is probability of them being the same~~<br><br>1/10</p>", 
                "question": "What are the odds of two people saying the same number between 1-10?"
            }, 
            "id": "cn0tqdj"
        }, 
        {
            "body": {
                "answer": "<p>The problem is that the WMW test is distribution free, so it<sq>s really hard to specify the size of the effect. <br><br>You say that the difference you<sq>re interested in is 2 points. 2 points of what? Means? You can<sq>t have means - you<sq>re doing a non-parametric test and means are parametric. Medians? You can have the same median and have a significant result from a MW test.<br><br>Why do you have to do an MW test? It<sq>s rare that you can<sq>t (a) transform, (b) use ordinal logistic regression, or (c) bootstrap.<br><br>If you really have to get a power estimate, use a simulation.<br><br></p>", 
                "question": "Question about sample size calculation for Wilcoxon-Mann-Whitney-U test?"
            }, 
            "id": "cmyaola"
        }, 
        {
            "body": {
                "answer": "<p>I don<sq>t entirely know what stats for engineers entails, but I would imagine that it involves two main topics, which if you can learn then learning the specifics in R and SAS aren<sq>t too bad. There<sq>s a lot of online example code for both topics.<br><br>The topics you might want to look into are regression and ANOVA, which is a special case of regression. As for resources, this [link](http<colon>//www.r-tutor.com/elementary-statistics/simple-linear-regression) should provide a decent background in the topics, while teaching you how to handle the problems in R.</p>", 
                "question": "What are the best free resources online where I can get a good foundation in Statistics."
            }, 
            "id": "cmva96l"
        }, 
        {
            "body": {
                "answer": "<p>Check out the [Data Science specialization](https<colon>//www.coursera.org/specialization/jhudatascience/1) on coursera. Specifically, the courses [R Programming](https<colon>//www.coursera.org/course/rprog) and [Statistical Inference](https<colon>//www.coursera.org/course/statinference) are relevant to what you<sq>ll be doing.<br><br>I<sq>ve completed the first two courses in the series, and now I<sq>m taking Statistical Inference.</p>", 
                "question": "What are the best free resources online where I can get a good foundation in Statistics."
            }, 
            "id": "cmwnveo"
        }, 
        {
            "body": {
                "answer": "<p>A good way to brush up on fundamental topics would be to watch this playlist of [~70 videos.](https<colon>//www.youtube.com/watch?v=uhxtUt_-GyM&list=PL1328115D3D8A2566) He also has a playlist on probability. There are a lot of great R tutorials out there. If you want another youtube video playlist and do not have a lot of programming experience I would [give this one a go.](https<colon>//www.youtube.com/playlist?list=PLqzoL9-eJTNBDdKgJgJzaQcY6OXmsXAHU)</p>", 
                "question": "What are the best free resources online where I can get a good foundation in Statistics."
            }, 
            "id": "cmv89wo"
        }, 
        {
            "body": {
                "answer": "<p>this is an online graphing calculator for ready-made formulas. check the statistics list<colon><br><br>http<colon>//www.fxsolver.com/browse/?oc=2&cat=7&formulas=on</p>", 
                "question": "What are the best free resources online where I can get a good foundation in Statistics."
            }, 
            "id": "cmvbvy2"
        }, 
        {
            "body": {
                "answer": "<p>Bayesian inference is made by adding a new data point to existing knowledge, hence the disagreement with the first.  E.g. every data point accumulated up to that time said the sun hadn<sq>t gone nova, adding one that says it did didn<sq>t change the outcome.</p>", 
                "question": "Could someone explain this comic to me?"
            }, 
            "id": "clrdk3z"
        }, 
        {
            "body": {
                "answer": "<p>Luckily, there<sq>s a site for that<colon><br>[http<colon>//www.explainxkcd.com/wiki/index.php/1132](http<colon>//www.explainxkcd.com/wiki/index.php/1132)</p>", 
                "question": "Could someone explain this comic to me?"
            }, 
            "id": "clrdxvb"
        }, 
        {
            "body": {
                "answer": "<p>http<colon>//stats.stackexchange.com/a/43408/24370</p>", 
                "question": "Could someone explain this comic to me?"
            }, 
            "id": "clrktj7"
        }, 
        {
            "body": {
                "answer": "<p>Because you<sq>re running this in a simulation, the best way to do this is to simulate it. That is, treat it as a hyperparameter and try to find the best value for your needs. What you<sq>ll want in the end is a few graphs showing precision vs. run time.<br><br>To do that, first make a simple scenario. Then, run the scenario once, sampling at every time step. Compute the total run time as well as the time taken to sample each step. Resample across the raw measurements taken across range of possible sampling rates the time steps repeatedly at random , recalculating the statistics of interest each time you do so. From there, you should be able to just use a scatter plot showing the estimated value of the statistic against the number of time steps that were used to compute it from.<br><br>As to the random versus uniform sampling question, uniform sampling is probably safer. It<sq>s less likely to miss changes over time, easier to explain and shouldn<sq>t have any issues unless there<sq>s really strong periodic patterns.<br><br>Good luck!</p>", 
                "question": "Choosing a sampling interval for real-time measurements"
            }, 
            "id": "cl66w24"
        }, 
        {
            "body": {
                "answer": "<p>My old work had a subscription to KeySurvey which I found to be very good and had excellent support.</p>", 
                "question": "[Question] what survey programme/site do you use for online surveys etc"
            }, 
            "id": "chn2z1q"
        }, 
        {
            "body": {
                "answer": "<p>Google drive Form</p>", 
                "question": "[Question] what survey programme/site do you use for online surveys etc"
            }, 
            "id": "chncp9p"
        }, 
        {
            "body": {
                "answer": "<p>Qualtrics</p>", 
                "question": "[Question] what survey programme/site do you use for online surveys etc"
            }, 
            "id": "ci09b66"
        }, 
        {
            "body": {
                "answer": "<p>Is this a research problem or a homework problem? - just curious.</p>", 
                "question": "Analytical formula for power to detect variance component with known correlation structure?"
            }, 
            "id": "cg4xiy6"
        }, 
        {
            "body": {
                "answer": "<p>1. Your first question is always a problem in multiple regression, interactions or no. You could always try to be less parametric, in which case you might include a whole host of interaction terms because that<sq>s what a second order Taylor series approximation tells you to do.<br><br>2. Theory tells you how to specify your regressions. Is there a model, formal or informal, that suggests that a particular cross partial derivative should have a particular sign? That an effect should be stronger for one subset of the data than another? Then include an interaction term. The adjusted R-squared will still be a useful indicator of whether your new variable adds much explanatory power, but you should always think about theory before jumping in and adding a ton of variables and interactions.<br><br>3. One cool use of interaction terms is the differences in differences. Basically you want to know the causal effect of x on y, and you have a treatment group and a control group based on values of z in a quasi-experimental setting. So you regress y on x, z, and the interaction of x and z. The causal effect will be estimated by the coefficient on the interaction term. This kind of research design is ubiquitous in applied fields, poke around on Google scholar and you<sq>ll see what I mean. http<colon>//en.wikipedia.org/wiki/Difference_in_differences</p>", 
                "question": "Question about interaction terms in multiple regression (x-post from /r/statistics)"
            }, 
            "id": "cda4287"
        }, 
        {
            "body": {
                "answer": "<p>For the lazy<colon> /r/statistics<br><br>---<br>I provide direct links to lesser known subs mentioned in the title if one isn<sq>t already provided.<br><br>Let me know if I need to try harder<colon> /r/LazyLinkerBot</p>", 
                "question": "Question about interaction terms in multiple regression (x-post from /r/statistics)"
            }, 
            "id": "cd9w2ai"
        }, 
        {
            "body": {
                "answer": "<p>One interesting use of interaction terms is if you have dummy variables for various things interacting with other terms. For example, say you are modeling lifetime income of americans with RHS education and dummy variables for things like born in US, born in UK etc. If you interact the dummy with the other variables you might get some interesting results by seeing if something like education level conditional on being born in a specific foreign state is significant and large. If you are interested in marketing these sorts of dummy interactions can be hugely useful. Think  likelyhood to buy = income + income * is 20 to 25 etc. </p>", 
                "question": "Question about interaction terms in multiple regression (x-post from /r/statistics)"
            }, 
            "id": "cd9zxb1"
        }, 
        {
            "body": {
                "answer": "<p>Think of interaction as being the case where the relationship between x and y differs by level of z.  A common variant is to see a strong association between x and y in one subgroup, but see no association in another subgroup.  Sex is a great example.  There are probably lots of advertising scenarios that will increase purchases in men with more exposure but have no effect on purchases in women.  When subgroups have big differences in the x/y relationship, you<sq>ve got interaction (aka effect modification).  </p>", 
                "question": "Question about interaction terms in multiple regression (x-post from /r/statistics)"
            }, 
            "id": "cdilryd"
        }, 
        {
            "body": {
                "answer": "<p>Yes, yes and sorta kinda but not quite.<br><br><br>If you understand the concept of r, that<sq>s great.  Let me just point out that there is a less well-known interpretation of r as the cosine of the angle between the variation in the response variable, Y, and the variation in the explanatory variable, X.  I have always found that definition very helpful.  And keep in mind, that r is always between 1 and -1.<br><br><br>If you know correlation, then R is - just as you say - the correlation between the Y<sq>s and the F(X)<sq>s.  I think this makes sense intuitively since a good linear regression should give you a good correlation between Y<sq>s and F(X)<sq>s.<br><br><br>Now for R^2 .  Numerically, to get R^2 , you just square the above R.  Since this R is between -1 and 1, R^2 must be between 0 and 1.  Keep this in mind, too.<br><br>Interpreting R^2 is a little different.  <br><br>Look at two numbers.  First, consider SUM(YI - YBar)^2 - the sum of the squares of the data values corrected for the mean of Y<sq>s.  This is a measure of the total variance of the Y<sq>s. Next, look at the SUM(F(XI) - YI)^2.  This is a measure of the variance of the YI<sq>s after correction for F(XI).  It is called the residual sum of squares.  Intuitively, if you have a good regression then the F(XI)<sq>s should be close to the YI<sq>s and the residual sum of squares should be small.   <br><br><br>Next look at (Residual Sum of Squares)/(Total Sum of Squares).<br><br><br>Again, for a good regression, this should be small but since there are a lot of squares there, it can never be negative.  On the other hand, you can also show it must always be less than or equal to 1.  So for a not-so-good regression, it will be close to 1.<br><br><br>R^2 turns this around.  R^2 is 1 - ( (Residual Sum of Squares)/(Total Sum of Squares) ).<br><br>Now you can interpret R^2 as a the square cosine of the angle between original Y<sq>s and the Y<sq>s predicted by the regression.  If the regression is good, R^2 should be close to 1.  If it isn<sq>t, it should be close to zero.<br><br>Since R^2 is between zero and 1, you can interpret it as <dq>The proportion of variance in the Y<sq>s that is explained by the regression on the X<sq>s.<br>  <br>Sorry for my ad-hoc notation.  I ain<sq>t no good at Latex.<br></p>", 
                "question": "r R and R-squared"
            }, 
            "id": "cd09w0n"
        }, 
        {
            "body": {
                "answer": "<p>It sounds like what you want to do is a power analysis.</p>", 
                "question": "Problem that has been stumping my company for ages"
            }, 
            "id": "ca6jfky"
        }, 
        {
            "body": {
                "answer": "<p>I am sorry to say, but the approach suggested by s460 is wrong. Conversion rate is a dichotomous variable (converts yes/no) and cannot be analyzed by a t-test. There seems to be a lot of confusion here. Let me try to explain. <br><br>You can always get some estimate, even with little data, like in your example you only have 50 clicks, you get an estimate of 10<percent> but you cannot trust it that much, right? So, power analysis as suggested can be done but with the goal of estimation and not hypothesis testing. The question you have to ask yourself, is how much precision you want in your estimate? In classical statistics this is called the width of the confidence interval. <br>However, in your case I would strongly suggest to have a look at Bayesian statistics. By doing Bayesian calculations you would end up with a posterior probability distribution for the conversion rate. For example, lets say you have the mentioned data; you will be able to see, visualize, analyze what is the probability distribution of the conversion rate, given your data so far. When new data are gathered you can update this distribution and stop when you have some required accuracy. Furthermore, you can use this distribution then to find the distribution of the break-even bid and so on and so forth. You can also incorporate relevant data (so, from similar keywords) with so-called prior distributions.<br>I would be glad to help further if you have difficulties with the above.</p>", 
                "question": "Problem that has been stumping my company for ages"
            }, 
            "id": "ca736ax"
        }, 
        {
            "body": {
                "answer": "<p>Aside occasionally using google/wikipedia, I don<sq>t generally use resources in my daily life for those questions; I know the transformations I<sq>d tend to use.<br><br>You might google these<colon><br><br>i) *Tukey ladder of powers* / *Box-Cox transformation* - there<sq>s only one thing to remember here.<br><br>ii) Variance-stabilizing transformation (these I can often work out from first principles, but I know several from memory)<br><br>Most transformations I use fall into one of these categories<br></p>", 
                "question": "What are some common data transformations and when are they used?"
            }, 
            "id": "ca0j1rq"
        }, 
        {
            "body": {
                "answer": "<p>log - because you want to do proportions in a linear model (ANOVA)</p>", 
                "question": "What are some common data transformations and when are they used?"
            }, 
            "id": "ca0ozwe"
        }, 
        {
            "body": {
                "answer": "<p>A log transform will convert a distribution with a long tail to the right into something more <dq>normal<dq> or Gaussian.  Power transforms can pull in a tail on the left.</p>", 
                "question": "What are some common data transformations and when are they used?"
            }, 
            "id": "cazibsx"
        }, 
        {
            "body": {
                "answer": "<p>Well I think the next experiment should satisfy this<br><br>* Take 100 coins<br><br>* Split them up in groups of 2 so 50 groups.<br><br>* Toss them.<br><br>* Take away the groups with only females. (Both head or tail depending on how you choose to do it.)<br><br>* Of the remaining groups take away one male.<br><br>* Look at the remainder of the groups you were left with. This should around 2/3 girls</p>", 
                "question": "Can someone please explain the Children<sq>s Puzzle to me?"
            }, 
            "id": "c9xwgsz"
        }, 
        {
            "body": {
                "answer": "<p>1. Substitute coins (pennies) for children, heads for boys and tails for girls.<br><br>2. Take 100 pennies and divide them into 25 groups of 4.<br><br>3. Randomize (toss/flip) all the pennies<br><br>4. Examine all 25 groups, discard all tails/tails<br><br>5. Do your stats on the remaining groups, only.<br><br>**Edit<colon>** Wrote that before my morning coffee(s).  /u/Lisandre got it right<colon> 50 groups of 2</p>", 
                "question": "Can someone please explain the Children<sq>s Puzzle to me?"
            }, 
            "id": "c9xuczi"
        }, 
        {
            "body": {
                "answer": "<p>Is this related to the Monty Hall problem?</p>", 
                "question": "Can someone please explain the Children<sq>s Puzzle to me?"
            }, 
            "id": "c9xsf3b"
        }, 
        {
            "body": {
                "answer": "<p>Semi-serious answer<colon> http<colon>//seanjtaylor.com/post/39573264781/the-statistics-software-signal <br><br>(In other words<colon> I think that anyone who knows about statistics software will assume that someone proficient in R will be able to adapt to Stata or SPSS if necessary.)</p>", 
                "question": "Statistical software comparisons?"
            }, 
            "id": "c91ix8j"
        }, 
        {
            "body": {
                "answer": "<p>Coming from R, SPSS and STATA will be stupid easy to learn.</p>", 
                "question": "Statistical software comparisons?"
            }, 
            "id": "c91ukt2"
        }, 
        {
            "body": {
                "answer": "<p>You can try, surely as just graduated they cannot expect you to know exactly what they want. Just make sure there is something to make you stand out from all other (non users).  <br><br>On SAS, using it is like SM, unfortunately the S is already reserved by SAS</p>", 
                "question": "Statistical software comparisons?"
            }, 
            "id": "c91kgo7"
        }, 
        {
            "body": {
                "answer": "<p>Distributions generally get introduced in a probability class and the relationships among distributions get elaborated on in a mathematical statistics course.  There are plenty if probability books out there.  Hogg and Craig is a decent mathematical statistics text.  Any introductory Bayesian statistics book will also cover distributions from another angle.</p>", 
                "question": "Could you suggest a resource to learn more about distributions?"
            }, 
            "id": "c8kdlwm"
        }, 
        {
            "body": {
                "answer": "<p>This is hardly a statistics question, but [the answer is plane](http<colon>//www.politifact.com/virginia/statements/2011/jun/11/peter-pantuso/bus-association-head-says-buses-safest-mode-commer/).<br><br>[This](http<colon>//shop.nsc.org/Reference-Injury-Facts-2012-Book-P124.aspx) is the book referenced. You can probably get the information straight from the horse\u2019s mouth on a couple of government agency websites, but I leave that as an exercise to the reader.</p>", 
                "question": "What<sq>s safer for a 400 mile intercity trip plane or bus?"
            }, 
            "id": "c755khd"
        }, 
        {
            "body": {
                "answer": "<p>This is what you want, I believe http<colon>//www.nature.com/nature/journal/v435/n7039/abs/nature03459.html <br>> Current models of human dynamics, used from risk assessment to communications, assume that human actions are randomly distributed in time and thus well approximated by Poisson processes1, 2, 3. In contrast, there is increasing evidence that the timing of many human activities, ranging from communication to entertainment and work patterns, follow non-Poisson statistics, characterized by bursts of rapidly occurring events separated by long periods of inactivity4, 5, 6, 7, 8. Here I show that the bursty nature of human behaviour is a consequence of a decision-based queuing process9, 10<colon> when individuals execute tasks based on some perceived priority, the timing of the tasks will be heavy tailed, with most tasks being rapidly executed, whereas a few experience very long waiting times. In contrast, random or priority blind execution is well approximated by uniform inter-event statistics.     <br>   <br>...    <br>> To provide direct evidence for non-Poisson activity patterns in individual human behaviour, I study the communication between several thousand e-mail users based on a data set capturing the sender, recipient, time and size of each e-mail11, 12. As Fig. 2a shows, the distribution of the time differences between consecutive e-mails sent by a selected user is best approximated with P(tau) approximately tau-alpha, where alphasime1, indicating that an individual<sq>s e-mail pattern has a bursty non-Poisson character<colon> during a single session a user sends several e-mails in quick succession, followed by long periods of no e-mail activity. This behaviour is not limited to e-mail communications. Measurements capturing the distribution of the time differences between consecutive instant messages sent by individuals during online discussions5 show a similar pattern. Professional tasks, such as the timing of job submissions on a supercomputer6, directory listing and file transfers (FTP request) initiated by individual users7, or the timing of printing jobs submitted by users13 were also reported to display non-Poisson features. Similar patterns emerge in economic transactions, describing the time interval distributions between individual trades in currency futures8. Finally, heavy-tailed distributions characterize entertainment-related events, such as the time intervals between consecutive online games played by the same user11.</p>", 
                "question": "Ask r/Stats<colon> Help finding <dq>human-scale<dq> examples of heavy tailed distibutions"
            }, 
            "id": "c6yghvo"
        }, 
        {
            "body": {
                "answer": "<p>It<sq>s usually because votes come in *by county*, and counties vary dramatically in their Republican/Democratic leanings, so you can often predict which way the last counties to report in will vote.  <br><br>I noticed on Tuesday that whenever the networks called a borderline state for Obama, almost always it was this situation<colon>  All of the heavily Republican (usually rural) counties had reported final counts, but the heavily Democratic counties (usually urban) had only reported partial counts. (It seemed like it was always in this direction - Democratic counties reporting in later - I suppose because the cities are slower at tallying up their votes). And early data from the Democratic counties often indicated massive advantages for Obama.<br><br>Ohio and Florida both panned out like this. When Fox News finally called Ohio, they had pretty much *all* the Republican counties<sq> final counts in, and were only still waiting for final counts from the urban/Democratic counties along Lake Erie. And early results from those Lake Erie counties were running a whopping 2<colon>1 in favor of Obama. Shortly after that they called Ohio for Obama even though at that point the statewide popular vote was nearly tied, on the theory that the remaining uncounted votes would also be 2<colon>1 in favor of Obama. (Then you got the spectacle of Karl Rove trying to argue with the analysts<sq> desk - he had gotten fixated on <dq>But the popular vote is nearly tied<dq> and seemed to complete miss the point that all the remaining votes were going to be coming from heavily Democratic counties.)<br><br>Similar situation in Florida - the million votes remaining to be counted were all from heavily Democratic counties.</p>", 
                "question": "During Elections night how are the results of a state projected even if only a small fraction of the votes have been counted?"
            }, 
            "id": "c6yaqmd"
        }, 
        {
            "body": {
                "answer": "<p>The chance one person doesn<sq>t have the flu is 90<percent>. The chance two people both don<sq>t have the flu is 90<percent> x 90<percent> = 81<percent>. The chance ten random people all don<sq>t have the flu is 90<percent> to the tenth power = roughly 35<percent>. The chance at least one person out of those ten *does* have the flu is therefore 100<percent> - 35<percent> = 65<percent> (roughly).</p>", 
                "question": "If 10<percent> of people have the flu and I meet a group of 10 random people what is the chance that I will meet someone with the flu in that group?"
            }, 
            "id": "c6w04d0"
        }, 
        {
            "body": {
                "answer": "<p>Since it<sq>s been tackled twice, I<sq>ll do it the rough way<colon><br><br>If *n* people each have 1/*n* chance of some attribute (and the occurence of the attribute is independent across people), then as *n* becomes large, the probability that none has the attribute is 1/*e* (about 37<percent>). <br><br>in your case, *n*=10, which isn<sq>t large, but it doesn<sq>t do so bad<colon><br><br>The approximate probability is 1 - 1/e = 63<percent><br><br>Pretty close to the true answer, but a lot less number crunching.<br><br>However, the chance that a group of people are truly independent with respect to <sq>flu is low, since it is readily communicated. The mere fact that they<sq>re together in a group may have resulted in cross-infection.<br></p>", 
                "question": "If 10<percent> of people have the flu and I meet a group of 10 random people what is the chance that I will meet someone with the flu in that group?"
            }, 
            "id": "c6w1y04"
        }, 
        {
            "body": {
                "answer": "<p>1-0,9^10</p>", 
                "question": "If 10<percent> of people have the flu and I meet a group of 10 random people what is the chance that I will meet someone with the flu in that group?"
            }, 
            "id": "c6wf1ym"
        }, 
        {
            "body": {
                "answer": "<p>I just wrote up an answer<br><br>Why don<sq>t you give me your answer first so I can see your reasoning and maybe correct it so in the future you have a better understanding of how to attack these problems.</p>", 
                "question": "If 10<percent> of people have the flu and I meet a group of 10 random people what is the chance that I will meet someone with the flu in that group?"
            }, 
            "id": "c6w07hx"
        }, 
        {
            "body": {
                "answer": "<p>(n-2)!/n!= 1/(n(n-1))<br><br>I think?<br><br>Edit<colon> changed my mind</p>", 
                "question": "For n individuals what<sq>s the probability that the last person to pick during a round of Secret Santa name picking will pick their own name."
            }, 
            "id": "c6v0cyj"
        }, 
        {
            "body": {
                "answer": "<p>Well, if you start with the senario where everyone just picks a name and you allow other people to pick their own name, then the odds that the last person would pick their own name would be 1/n<br><br>But if you make it more realistic where people prior to the last person pick again if they get their own name it becomes a little more complicated.<br><br>I don<sq>t know the formula so I<sq>ll try to work it out.  If you start with 3 <br>A,B and C<br>A starts and can pick B or C<br>B is next and can pick A or the remaining from above but there is a 1/2 chan<br>C is forced to pick the final which would be 1/4 chance their own.<br><br><br>So for each pick the person lets say the 5th, they have either 5 options to pick from or 4 if their name is still in the mix.<br><br>What is the chance they are still in the mix? it is the number of people before then and the odds that they were picked in each event.<br><br>So for the first person the odds of picking any number is 1/(N-1) <br>The next is<colon> 1/  ((N-1) x (1/(N-1)) + 1/((N) x (N-1)) ...<br><br>And After looking at my working I have made myself really confused.<br><br>Someone save me!<br></p>", 
                "question": "For n individuals what<sq>s the probability that the last person to pick during a round of Secret Santa name picking will pick their own name."
            }, 
            "id": "c6v8q6t"
        }, 
        {
            "body": {
                "answer": "<p>Wouldn<sq>t you know it, someone much smarter than me details a recursive solution<colon> <br><br>http<colon>//www.spontaneoussymmetry.com/blog/archives/232<br><br>I wonder if a closed formula solution exists?</p>", 
                "question": "For n individuals what<sq>s the probability that the last person to pick during a round of Secret Santa name picking will pick their own name."
            }, 
            "id": "c6w4hzs"
        }, 
        {
            "body": {
                "answer": "<p>Why not code it?<br><br>    selsan <- function(who,persons) {<br>      if (length(persons)==1) return(persons)<br>      sel <- sample(persons[persons!=who],1)<br>      return(c(sel,selsan(who+1,persons[persons!=sel])))<br>    }<br>    #selsan(1,1<colon>5)<br>    finselsan <- function(n){<br>      selsan(1,1<colon>n)[n]<br>    }<br>    nrep=1e4<br>    sa <- sapply(1<colon>nrep,function(x) finselsan(8))<br>    table(sa)/nrep<br><br>    sa<br>         1      2      3      4      5      6      7      8 <br>    0.1114 0.1138 0.1216 0.1333 0.1356 0.1594 0.1292 0.0957 <br><br><br>Either I made an error, or you have less than 1/n chance. At the same time, the last person has best chance to get the third to last person</p>", 
                "question": "For n individuals what<sq>s the probability that the last person to pick during a round of Secret Santa name picking will pick their own name."
            }, 
            "id": "c6vf9n3"
        }, 
        {
            "body": {
                "answer": "<p>Anybody?</p>", 
                "question": "Is this legitimate statistical analysis or hocus pocus? "
            }, 
            "id": "c6t595r"
        }, 
        {
            "body": {
                "answer": "<p>Your link seems to be down right now.<br><br>http<colon>//www.scribd.com/doc/109398239/2008-2012-Elections-Results-Anomalies-and-Analysis<br><br>This contains much of the information in the report, also with a discussion of 2008.</p>", 
                "question": "Is this legitimate statistical analysis or hocus pocus? "
            }, 
            "id": "c6x6su3"
        }, 
        {
            "body": {
                "answer": "<p>There was so much $$$ spent, it would skew the data.</p>", 
                "question": "Is this legitimate statistical analysis or hocus pocus? "
            }, 
            "id": "c7doewg"
        }, 
        {
            "body": {
                "answer": "<p>The Basic Practice of Statistics by David Moore is the accepted intro college text at the moment.</p>", 
                "question": "Any recommendations for a statistics book written for the layman?"
            }, 
            "id": "c6qw8d5"
        }, 
        {
            "body": {
                "answer": "<p>Self promotion alert<colon> Applying regression and correlation, by Jeremy Miles and Mark Shevlin, tries to take a pretty gentle and friendly approach.  It all depends on what you already know though, it might be too straightforward (and it<sq>s designed for social science students, and it really could do with an update).<br></p>", 
                "question": "Any recommendations for a statistics book written for the layman?"
            }, 
            "id": "c6qw76l"
        }, 
        {
            "body": {
                "answer": "<p><dq>Intuitive Biostatistics<dq> is a good one. Not sure how much it goes into the topics you need though. They also have a lot of good free info on their website (graphpad.com) along with some specialized software for nonlinear curve fitting (Prism - I use it a lot).</p>", 
                "question": "Any recommendations for a statistics book written for the layman?"
            }, 
            "id": "c6r4q9f"
        }, 
        {
            "body": {
                "answer": "<p>Is it a balanced scale; i.e. do you want an equal number of negative vs. positive possible responses?  Also consider if you need a 100<percent> neutral response in the middle of the scale.  Odds are you should be comparing 5pt to 11pt.<br><br>There is research on either side that will tell you 11pt scales have more statistical power and don<sq>t suppress an audience that avoids using endpoints, but that 5pt scales offer the most accuracy in test-retest situations where the test-taker is asked to repeat his/her responses.  So, you could go either way... or just use a 7pt scale because screw it.</p>", 
                "question": "Which is better for getting a more accurate performance review a 1-5 scale or a 1-10 scale?"
            }, 
            "id": "c4m4pr2"
        }, 
        {
            "body": {
                "answer": "<p>[This](https<colon>//lra.le.ac.uk/handle/2381/3937) is a pertinent research. Go for 7 to 9 items. </p>", 
                "question": "Which is better for getting a more accurate performance review a 1-5 scale or a 1-10 scale?"
            }, 
            "id": "c4mbtv4"
        }, 
        {
            "body": {
                "answer": "<p>It also depends on how the questions are worded and the intervals of the item stems. bayleo brought up some good points, but also keep in mind that this isn<sq>t exactly an exeriment, so there are different factors to consider.</p>", 
                "question": "Which is better for getting a more accurate performance review a 1-5 scale or a 1-10 scale?"
            }, 
            "id": "c4mfkua"
        }, 
        {
            "body": {
                "answer": "<p>The next outcome is independent of all past outcomes so a coin is no more likely to come up with a head on the 10th throw than it was on the 1st throw regardless of what happened on throws 1-9. However, you can use the past throws to make an estimate of how likely a head is to come up (in stats language, the coin<sq>s probability distribution). From the past throws we would estimate that the coin has an 80<percent> chance of heads and a 20<percent> chance of tails so you<sq>d do best to bet heads.<br><br><br>In effect, both the things you said are correct. Given you don<sq>t know for certain whether the coin is fair or not you should use past throws to estimate how likely a head is to come up; but also, on every throw the chance of the coin coming up heads is the same even if our estimate of that probability changes.</p>", 
                "question": "Question about coin tossing whether the next outcome is dependent on past outcomes."
            }, 
            "id": "c45717z"
        }, 
        {
            "body": {
                "answer": "<p>That<sq>s entirely dependent on if it is an unfair coin or not. If it<sq>s not an unfair coin it<sq>s entirely independent 50<colon>50 odds but if it is 80<percent> biased towards heads then you should use historical data to judge its bias, but it<sq>s still only entirely independent 80<colon>20 odds.<br><br>TL;DR<colon> don<sq>t bet your house on the 1000001th toss.</p>", 
                "question": "Question about coin tossing whether the next outcome is dependent on past outcomes."
            }, 
            "id": "c46fz00"
        }, 
        {
            "body": {
                "answer": "<p>This outcome (80<percent> to 20<percent>) would not happen if the coin was fair due to the Law of Large Numbers. As the number of fair coin flips increases, the number of heads and tails will become very close to equal. If we give heads a value of 0 and tails a value of 1, the average value of a coin flip would be very close to 0.5 as the number of coin flips increase. So if this did happen it is an unfair coin and you should bet on a head. http<colon>//en.wikipedia.org/wiki/Law_of_large_numbers</p>", 
                "question": "Question about coin tossing whether the next outcome is dependent on past outcomes."
            }, 
            "id": "c47hora"
        }, 
        {
            "body": {
                "answer": "<p>yes. coin flips are *independent* which means that the probability of the outcome of a flip at time `t` doesn<sq>t depend on the outcome of a flip at any other time. This is true for both fair and biased coins.</p>", 
                "question": "Question about coin tossing whether the next outcome is dependent on past outcomes."
            }, 
            "id": "c454ug8"
        }, 
        {
            "body": {
                "answer": "<p>This is actually a problem in game theory research and not solved yet.</p>", 
                "question": "Choosing the optimal number in a guessing game"
            }, 
            "id": "c43nxdn"
        }, 
        {
            "body": {
                "answer": "<p>Minor point, but for n = 2, 499 is an equally good choice.</p>", 
                "question": "Choosing the optimal number in a guessing game"
            }, 
            "id": "c42ghzs"
        }, 
        {
            "body": {
                "answer": "<p>Perhaps try simulating your opponents (e.g. using an overdispersed binomial model like [this one](http<colon>//en.m.wikipedia.org/wiki/Beta-binomial_model) )and see how probability of winning varies with your choice the number of opponents, and how closely they stick to the mean value. </p>", 
                "question": "Choosing the optimal number in a guessing game"
            }, 
            "id": "c42nlkt"
        }, 
        {
            "body": {
                "answer": "<p>[deleted]</p>", 
                "question": "Say you are the first person(of three) to spin the wheel on The Price Is Right. If you land on 70 cents should you actually spin the wheel again?"
            }, 
            "id": "c3lhaqs"
        }, 
        {
            "body": {
                "answer": "<p>I can<sq>t find a good explanation of the rules, anyone mind explaining them quickly?</p>", 
                "question": "Say you are the first person(of three) to spin the wheel on The Price Is Right. If you land on 70 cents should you actually spin the wheel again?"
            }, 
            "id": "c3ld2gf"
        }, 
        {
            "body": {
                "answer": "<p>Mojo17, would you happen to know the numbers on the wheel? Is it every 5 cents? And each number only appearing once?</p>", 
                "question": "Say you are the first person(of three) to spin the wheel on The Price Is Right. If you land on 70 cents should you actually spin the wheel again?"
            }, 
            "id": "c3lirh2"
        }, 
        {
            "body": {
                "answer": "<p>Do you know what strategy the second spinner will be using?</p>", 
                "question": "Say you are the first person(of three) to spin the wheel on The Price Is Right. If you land on 70 cents should you actually spin the wheel again?"
            }, 
            "id": "c3lcfsr"
        }, 
        {
            "body": {
                "answer": "<p>/u/psych_professor has answered your question so I<sq>m going to give you a lecture.<br><br>You need to consult a statistician *before* you collect the data. If this is the first time anyone has ever measured these sorts of muscle fibres in genetically comparable mice then fair enough, you could not have anticipated the problem and it would be an exploratory study. If the information was knowable beforehand a statistician would have made you look it up and helped you prespecify an appropriate analysis.<br><br>Without prespecification of the hypothesis *and how you will test it* you can<sq>t make valid claims about what you have found because you<sq>ve granted yourself licence to keep looking until you find something you like the look of.<br><br>This is at the heart of the so-called <dq>replication crisis<dq> [currently surfacing in multiple scientific fields](http<colon>//www.annualreviews.org/doi/full/10.1146/annurev-statistics-060116-054104). Your department should have access to statisticians specialising in the types of research your lab does to prevent these kinds of methodological errors occurring. If they don<sq>t then you should bring it up (power dynamics allowing, of course).<br><br>The specific problems vary by field. In animal studies AFAIK the primary (statistical) concern is too small samples, which is something forced on you by ethical restrictions. This is partially offset by using specific genetic strains to minimise variation between individuals but the downside of that is that results may not generalise.<br><br>Whatever the rationale, it<sq>s rarely going to be enough to justify sample sizes of 3 in each group, although I accept that this is pretty standard in your field (ie Not Your Fault). If there were more statisticians in animal research, it probably wouldn<sq>t be standard. It<sq>s a lot more ethical to do fewer studies with larger sample sizes. This gives a better chance of detecting realistic differences if they exist and much less chance of mistaking a wild over-estimate for truth when it more likely occurred by chance.<br><br>The link above has some good explanations and references to explore around these topics and the scientific blogosphere is producing some excellent critiques too. I<sq>ve only come across passing mentions of animal studies in this context but the principles generalise and there must be some specific literature out there somewhere.<br><br>I hope you take this in the spirit it is intended. We<sq>ve been banging on about these problems for 50 years but remarkably little has changed in most fields. If you<sq>re interested in experimental methods and in a position to push the point, do it. For science. <colon>)</p>", 
                "question": "Finding the P-value of the mean of a ratio of means (in R Prism Excel anything else free)"
            }, 
            "id": "dg0u2is"
        }, 
        {
            "body": {
                "answer": "<p>So when you say the values vary between mice regardless of the drug, are you saying the ANOVA was not significant because the within-groups variability was so high? Or are you saying the variances differed across the groups, violating the assumption of homogeneity of variance? If the former, it sounds like you are fishing for significance (p-hacking) which I would not advise. If the latter, you can try transforming the data. Taking the ratio and doing the t-test might work. You will just calculate the mean D/U ratio of control mice and drug mice and use this formula below. It is the mean difference divided by the pooled variance<colon>  <br>http<colon>//www.statsdirect.com/help/parametric_methods/unpaired_t.htm</p>", 
                "question": "Finding the P-value of the mean of a ratio of means (in R Prism Excel anything else free)"
            }, 
            "id": "dg0qd2x"
        }, 
        {
            "body": {
                "answer": "<p>https<colon>//en.wikipedia.org/wiki/Fieller<percent>27s_theorem#Approximate_confidence_interval</p>", 
                "question": "Finding the P-value of the mean of a ratio of means (in R Prism Excel anything else free)"
            }, 
            "id": "dg1049p"
        }, 
        {
            "body": {
                "answer": "<p>What are you trying to do? I<sq>m going to be upfront that I<sq>m not a neuronet guy, but if you are either trying to<colon><br>  <br>a) Scour a trained model to determine some kind of feature <dq>importance<dq> ranking or  <br>b) Accurately estimate a confidence interval for your model  <br>  <br>I know enough that it<sq>s not going to work. Both things are notoriously difficult to ascertain using neuro-nets.</p>", 
                "question": "How do neural networks work?"
            }, 
            "id": "dg0rixc"
        }, 
        {
            "body": {
                "answer": "<p>The BIC is similar to the AIC, where they both estimate how well a model fits with a penalty for additional terms.  They<sq>re trying to find the best model without overfitting.</p>", 
                "question": "Could someone tell me what the Bayesian Information Criterion tells me?"
            }, 
            "id": "dfnox2u"
        }, 
        {
            "body": {
                "answer": "<p>hey there--you can view your scenario as a bunch of binomial trials--you either get a 20 on your 20-sided die or you don<sq>t. As a result, your variance is given by p*(1-p), where p is the observed probability that you rolled a 20. So your variance should be equal to (.10)*(1-.10).<br><br>Check out some articles on binomial tests for more context!</p>", 
                "question": "p-value awesomeness! I<sq>m loving statistics but I can<sq>t seem to crack how to approach this simple problem. Significance of a sample result."
            }, 
            "id": "dfnbgw0"
        }, 
        {
            "body": {
                "answer": "<p>Contrary to the other comment, when doing this king of Z-test (i.e. comparison of an observed proportion to a theoretical proportion), you have to use the variance of the theoretical proportion. <br><br>So if I go back to your problem, you want to compare an observed proportion *p*=0.05 (obtained from a sample of *n*=500) to a theoretical proportion *p0*=0.1<colon><br><br>1/ you determine the null hpothesis, in this case<colon> *p*=*p0*<br><br>2/ you compute the statistic of the test, in this case<colon> <br><br>*Z= (p-p0)/sqrt(var(p0)/n)* with *var(p0)=p0(1-p0)*<br><br>3/ if you chose a type I error alpha of 5<percent>, you then exclude the null hypothesis if Z does not belong to the interval [-1.96; 1.96] (1.96 given by a table of standard normal distribution [as this one](https<colon>//statistics.laerd.com/statistical-guides/img/normal-table-large.png) using 1-alpha/2 = 0.97)<br><br>4/ profit, you just made your first test!<br><br>5/ if you want the corresponding p-value, you can use the same table with your value of Z in the reverse order (i.e. if Z=2.5, p=(1-0.9918)*2)<br><br></p>", 
                "question": "p-value awesomeness! I<sq>m loving statistics but I can<sq>t seem to crack how to approach this simple problem. Significance of a sample result."
            }, 
            "id": "dfnefqn"
        }, 
        {
            "body": {
                "answer": "<p>> \u03c3 is the square root of the variance, but the variance is something you can calculate from a list of results.<br><br>No, \u03c3^2 is the *population* variance (under the null hypothesis of a fair die), not the sample variance -- and we should be clear about what it<sq>s the variance of. If you mean the variance of an individual die roll where you count <dq>1<dq> if you get a 20 and a <dq>0<dq> if you don<sq>t (so that the average of outcomes would be a proportion of <dq>20<dq>s) then this is 1/20 x 19/20, and so \u03c3 is the square root of that. <br><br>The population variance of the sample proportion of 20s is 1/20 x 19/20 x 1/n where n is the number of rolls<br><br>[You *can* estimate the variance of the estimated proportion by s^2 (this would turn out to be  p(1-p)/(n-1) where p is the sample proportion), but that<sq>s not what<sq>s usually done.]<br><br>If it was me I<sq>d actually be counting how many 20<sq>s there were and then using the binomial distribution rather than the normal approximation. Note that you<sq>re so far into the tail with 10<percent> that the normal approximation isn<sq>t very accurate (but I<sq>ll talk about how to do it anyway).<br><br>So anyway,  the p-value depends on your alternative (and you have to pick this before you see any rolls, not after). Your only clue about an alternative is given in <dq>*I<sq>m not sure how fair my actual die is.*<dq>, which is not directional -- so that suggests a two-tailed test. <br><br>Now a p-value is the probability of a result <dq>at least as extreme as the one observed, given the null is true<dq>, so we<sq>d be interested in the results that were 10<percent> or more <dq>20<dq> results as well as 0<percent> (as far the either direction).<br><br>*Normal approximation*<colon> So anyway with 500 rolls the population standard deviation of the proportion of 20<sq>s for a fair die is \u221a[1/20 x 19/20 x 1/500] = 0.0097468<br><br>10<percent> heads is (0.1 - 0.05)/.0097468 = 5.13 standard deviations above the mean (5.03 if you use continuity correction). This yields an extraordinarily low p-value (the area above z=5.13 is 1.45 x 10^(-7), and you double that for the other end, so roughly 3 x 10^(-7), less than one in a million)<br><br>If you use the continuity correction you get about 5 x 10^(-7), still less than 1 in a million.<br><br>If you do the exact binomial calculation you get 3.6 x 10^(-6) for 50 or more <dq>20<dq>s and 7.3 x 10^(-12) for zero <dq>20<dq>s (which is so small we can ignore it). So the correct answer is about 3.6 in a million<br><br>To actually compute these p-values you either need a table or a computer program (Excel can do it if you have that, or R is free)<br><br>Here<sq>s a little table of p-values for the n=500 case, which I hope I did right<colon><br><br>       No.20s    exact p  norm   norm with<br>                 (binom) approx  cont.corr<br>      19 or 31   0.2581  0.21826 0.25908<br>      18    32   0.1809  0.15090 0.18228<br>      17    33   0.1223  0.10068 0.12381<br>      16    34   0.0797  0.06478 0.08113<br>      15    35   0.0501  0.04017 0.05125<br>      14    36   0.0305  0.02400 0.03120<br>      13    37   0.0179  0.01380 0.01829<br>      12    38   0.0103  0.00764 0.01032<br>      11    39   0.0057  0.00407 0.00560<br>      10    40   0.0031  0.00208 0.00293<br>       9    41   0.0017  0.00103 0.00147<br>       8    42   0.00092 0.00049 0.00071<br>       7    43   0.00049 0.00022 0.00033<br>       6    44   0.00026 0.00010 0.00015<br>       5    45   0.00013 0.00004 0.00006<br></p>", 
                "question": "p-value awesomeness! I<sq>m loving statistics but I can<sq>t seem to crack how to approach this simple problem. Significance of a sample result."
            }, 
            "id": "dfodn2y"
        }, 
        {
            "body": {
                "answer": "<p>There are a few reasons your covariance matrix is showing up as not positive definite. The most common is that your model is misspecified. It<sq>s possible that your model won<sq>t support second order factors, but not knowing more information about your sample size and observable correlations, I can<sq>t give better advice.l [Here<sq>s](http<colon>//www2.gsu.edu/~mkteer/npdmatri.html) a good read on the various reasons why this might happen. There are also lots of discussions on stack exchange about this issue (if you search, you don<sq>t need to specify that you<sq>re using Mplus as lavaan of AMOS issues would be resolved the same way). Good luck!</p>", 
                "question": "MPlus Factor Analysis - help with residual variance"
            }, 
            "id": "dfk10lq"
        }, 
        {
            "body": {
                "answer": "<p>Why not try to see what sort of accuracy you get with a more nonlinear technique? Random forests are a good place to start.</p>", 
                "question": "Hitting a dead-end with linear and logistic regression on messy data. Appreciate any advice"
            }, 
            "id": "dfk99yt"
        }, 
        {
            "body": {
                "answer": "<p>How low is your R squared? It might not be as bad as you think... the real world is not easy to model. </p>", 
                "question": "Hitting a dead-end with linear and logistic regression on messy data. Appreciate any advice"
            }, 
            "id": "dfknwgi"
        }, 
        {
            "body": {
                "answer": "<p>Is life perfectly controlled with only one thing on at a time, or do multiple effects occur at the same time and interact with each other on a daily basis?</p>", 
                "question": "Hitting a dead-end with linear and logistic regression on messy data. Appreciate any advice"
            }, 
            "id": "dfjxg2b"
        }, 
        {
            "body": {
                "answer": "<p>I think this is underspecified, in that you need to propose some kind of model of the switching process itself.  <br><br>In the simplest case, you might say that either or Alice and Bob is always flipping, and the estimation problem is to try to figure out which it is.  This isn<sq>t too hard, as long as Alice and Bob actually differ in their flipping ability.<br>  <br>In another simple case, if Alice and Bob switched off for every other flip, your problem would be reduced to trying to figure out whether odd flips are Alice<sq>s or odd flips are Bob<sq>s, which is still fairly straightforward.  <br><br>Both of the examples above are degenerate cases of a [hidden Markov model](https<colon>//en.wikipedia.org/wiki/Hidden_Markov_model), where the state space for the hidden Markov chain is {Alice, Bob}, and the transition matrices are respectively I (the 2x2 identity matrix) and J- I (J being the 2x2 matrix of 1<sq>s).  In both of these degenerate case, you can eventually figure out which state the chain<sq>s in after observing it long enough, because you can, respectively, just look at the raw autocorrelations, or compare even-odd and odd-even autocorrelations to infer which state it was initialized in with some certainty (again, as long as Alice and Bob actually have different abilities). <br><br>In the general hidden Markov model, you<sq>re going to have some other transition matrix.  Some of the easier cases would involve strongly asymmetric transition matrices -- for instance, if on each flip, Bob is unlikely to pass to Alice if he has it, and Alice is likely to pass to Bob if she has it, you might eventually be able to infer that Bob *usually* has it, which is much better than nothing even if you can<sq>t be sure whether Bob has it at any given time.  Matrices close to I should also be relatively easy, whether or not they<sq>re symmetric, because they will involve long runs of either Alice or Bob.<br><br>However, even with the simplifying Markov assumption, some transition matrices will make things very difficult.  I suspect the worst case is a symmetric matrix that<sq>s somewhat close to J-I -- the situation where Alice and Bob switch off very frequently, but not reliably, and each of them flips approximately half the time.  You might not be able to make much headway on that one at all.  <br><br>This is just conjecture, I actually have no experience using hidden Markov models (Bayesian or not), but that looks to me like a reasonable starting point on this problem.<br><br>EDIT<colon>  More concretely, I<sq>m saying that even if you start with fairly restrictive Markov/stationarity assumptions about the switching process, you<sq>ll need to put priors over the transition probabilities in the transition matrix, and priors over Alice and Bob<sq>s abilities, and maybe a prior on the initial state of the chain, and you<sq>ll probably have your hands full working with that alone.</p>", 
                "question": "Would Appreciate Help With Bayesian Approach"
            }, 
            "id": "dfek8hi"
        }, 
        {
            "body": {
                "answer": "<p>In terms of the music I would avoid using that as an ordinal variable, meaning that you assume that the avante garde jazz is less preferable than the control which is less preferable than classical.  Just treat it as a multilevel categorical.<br><br>You should also take into account your standard uncontrolled independent variables such as<colon><br><br>1) gender<br><br>2) age<br><br>3) education level<br><br>4) economic status<br><br>Not sure the best way to do that.  You could prescreen for these factors and select people to fit a structured factorial for this, or you could just take everyone and make sure you have enough replication in your full factorial that you can ferret out your uncontrolled variables afterwards.<br><br>I<sq>m guessing you<sq>re going to be doing this in SPSS since this sounds like social psych, and I don<sq>t know what tools exist in uncontrolled variables in a DOE for this kind of stuff.  JMP has some good tools for it though.<br><br>Anyways ignoring what I just said you have a fairly simple factorial model.  1 3 level categorical variable and 1 2 level, so in theory you only need 6 data points.  I would *highly* recommend a much more replicated design though.  Also fwiw there is no way to reduce the levels on a 2 factor design (no fractional factorials allowed).  If you prescreen and build a model using all of the above factors you may be able to do a fractional factorial, but you will have to keep in mind the colinearity of certain terms (such as age, education, and income)</p>", 
                "question": "Looking for help deciding what factorial study model to use for my research proposal"
            }, 
            "id": "deo7a9g"
        }, 
        {
            "body": {
                "answer": "<p>[deleted]</p>", 
                "question": "The bigger the sample size the more likely differences in the sample size will be seen as statistically significant?"
            }, 
            "id": "de1uhye"
        }, 
        {
            "body": {
                "answer": "<p>> I later got a reply that the sample size doesn<sq>t matter because the statistical significance depends on n/N (which would stand for sample size/population). This doesn<sq>t make much sense to me, it seemed like he was confusing statistical significance with statistical generalisation.<br><br>Statistical significance is a measure of the likelihood of observing some data due to chance. If you measure the average heights of people and measure a difference of 1 inch, you should ask <dq>given the two groups have the same average height, what is the probability of measuring >= 1 inch difference between a sample from each?<br><br>If your two groups are men and women and you assume they have the same average height and you only measure one person, you<sq>re fairly likely to measure a 1 inch difference given the variations in heights. If you measured 100 people, you<sq>d be very unlikely to get a difference of 1 inch (again assuming the groups have the same heights), so if you measured an inch of difference, you<sq>d conclude the difference is <dq>statistically significant.<dq><br><br>On the other hand, if your two groups are <dq>people you know named bob smith<dq> and <dq>people you know named alice jones.<dq> A single subject subgroup of each group measures the whole group, so unless they are the same height, they are statistically significantly different heights.</p>", 
                "question": "The bigger the sample size the more likely differences in the sample size will be seen as statistically significant?"
            }, 
            "id": "de1xq7v"
        }, 
        {
            "body": {
                "answer": "<p>In social science stats statistical significance is the likelihood of seeing your results/sample if there is in fact no relationship (i.e. your null hypothesis is true). </p>", 
                "question": "The bigger the sample size the more likely differences in the sample size will be seen as statistically significant?"
            }, 
            "id": "de20zdt"
        }, 
        {
            "body": {
                "answer": "<p>Tell him to look up the Dunning-Kruger effect. <colon>)<br><br>I see this confusion a lot. <dq>They only asked 1000 people, that doesn<sq>t say anything about what the whole country thinks.<dq> It seems to come from a confusion about the difference between a sample and an audit.<br><br>We use samples when an audit would be too expensive, or impossible. You<sq>re correct in your interpretation - the larger the sample size the greater the chance of detecting small differences. Sampling, power calculations and statistical vs practical significance are all topics which should help enlighten him.</p>", 
                "question": "The bigger the sample size the more likely differences in the sample size will be seen as statistically significant?"
            }, 
            "id": "de255jx"
        }, 
        {
            "body": {
                "answer": "<p>Might be that there<sq>s not enough difference. But I doubt it with that many subjects and repeated measures. And usually it should still converge, just increase your AICC. More likely, you have too many factors in your model. Or you haven<sq>t structured your random effect properly.<br><br>Can you post your syntax so I can see how you have the model configured? Or describe your fixed/random effects structure.</p>", 
                "question": "Chimp hormone thesis data<colon> GLMM vs. GEE and a Hessian Matrix problem"
            }, 
            "id": "ddu8a1z"
        }, 
        {
            "body": {
                "answer": "<p>The total variance can be partitioned into two parts<colon> that which is due to variation *within* groups and that which is due to variation *between* groups. ANOVA works by comparing the two<colon> if the variation between groups is large compared to variation within groups, the means must be different. Which is how come we can use variance to test for a difference between means.<br><br>You<sq>ve been given summary data to reverse engineer; everything you need is there.<br><br>You can get the total variation within groups (SSW) by reverse engineering the standard deviations.<br><br>You can work out the overall mean by reverse engineering the group means and sample sizes.<br><br>See if you can take it from there. It<sq>s a homework question so you<sq>ll be a lot better off if you can figure it out for yourself. It<sq>s an excellent question designed to make it <sq>stick<sq> so if you can solve it alone you<sq>ll benefit. [This is a nice straightforward page with all the standard ANOVA formulae you need](https<colon>//people.richland.edu/james/lecture/m170/ch13-1wy.html).<br><br>[Edited for some sloppy language]</p>", 
                "question": "How do I find Sum of Squared errors for ANOVA when I<sq>m only given the averages and no data points."
            }, 
            "id": "dds8dqr"
        }, 
        {
            "body": {
                "answer": "<p>Well, MS is variance, so if you are given the SD you can square it to get the variance.  <br>But...that will only get you the MS for each of the 2 groups. I<sq>m afraid I<sq>m stumped on how to get MS-error unless I<sq>m just being dense today.</p>", 
                "question": "How do I find Sum of Squared errors for ANOVA when I<sq>m only given the averages and no data points."
            }, 
            "id": "dds671d"
        }, 
        {
            "body": {
                "answer": "<p>Programming for sure. Even if you want to work in the health/bio or business world you will need to be able to program.</p>", 
                "question": "Best lower division duo for my undergraduate studies in Statistics?"
            }, 
            "id": "ddrdzqw"
        }, 
        {
            "body": {
                "answer": "<p>Programming will be useful to every field, but know that if you want to progress in those other sciences, for instance a masters, you will have to go back and take basic courses</p>", 
                "question": "Best lower division duo for my undergraduate studies in Statistics?"
            }, 
            "id": "ddrp3dg"
        }, 
        {
            "body": {
                "answer": "<p>I think you are missing the most important data element for this analysis<colon>  a customer id! (and transaction total but I<sq>m assuming you have that and just didn<sq>t show it)  <br><br>My thumb is only a small percentage of my body, but cutting it off will have a very big effect on my productivity.  Likewise, you need to know the value of the customers that would be most affected by a change in the service model.  With a customer ID, you could do the following<colon><br><br>  1.  Estimate a customer lifetime value for each customer using a Pareto/NBD model as outlined by Fader and Hardie.  Basically it will estimate the net present value of all of a customer<sq>s future transactions.  <br>  2.  Look at the distribution of pick-up times for those high-value customers.  You may find that while in terms of customers it is only 3.5<percent>, it is much more in terms of revenue.  <br>  3.  Develop strategies for pleasing high-value customers, but lower the service offerings for lower value customers.  This is what airlines do.  <br><br>Not that there<sq>s no insight that can be gained from your data.  I just don<sq>t really have the time to look at it now, but hopefully you will get some more actionable feedback.  </p>", 
                "question": "Drycleaner here could use some help. Am I understanding my data correctly?"
            }, 
            "id": "ddkowb9"
        }, 
        {
            "body": {
                "answer": "<p>Seems interesting might take a stab at it over the weekend. Question- do you have information for order size in terms of pieces? If not, any data in terms of classifying order sizes- like 1 to 5 in terms of quantifiable labour spent on the order? Asking because there May be a linear/ asymmetrical relationship between that and time customers have taken to pick up. Would be interesting to see if/how customers spread out large orders in terms of how quickly they need those. Would also help since you said you bust your ass off working to get these done in quickly- so possibly you could state larger orders may (if data supports) be given priority or if it doesn<sq>t, then a longer duration until pickup. Disclaimer- Not a statistician but i do use a lot at work. Let us know!</p>", 
                "question": "Drycleaner here could use some help. Am I understanding my data correctly?"
            }, 
            "id": "ddkgfcy"
        }, 
        {
            "body": {
                "answer": "<p>I took a look at it, and your numbers are correct.<br><br>    mean        8.970356<br>    std         7.474986<br>    min         0.071701<br>    25<percent>         4.956956<br>    50<percent>         6.334398<br>    75<percent>        10.197662<br>    max        81.081400<br><br>Does the 3 working days mean that if I show up 72 hours (excluding Sundays) after I dropped off a garment that I should expect it to be done then?<br><br>Here<sq>s a summary of the percentage of times people took some number of days or less to show up<colon><br><br>     1 days<colon> 0.12<percent><br>     2 days<colon> 1.89<percent><br>     3 days<colon> 3.49<percent><br>     4 days<colon> 11.52<percent><br>     5 days<colon> 27.36<percent><br>     6 days<colon> 42.98<percent><br>     7 days<colon> 54.49<percent><br>     8 days<colon> 62.14<percent><br>     9 days<colon> 68.29<percent><br>    10 days<colon> 73.32<percent><br><br>I think /u/heartastack<sq>s comment is spot on for things to consider. I<sq>m just here as numerical backup!<br><br><br><br></p>", 
                "question": "Drycleaner here could use some help. Am I understanding my data correctly?"
            }, 
            "id": "ddl4i0z"
        }, 
        {
            "body": {
                "answer": "<p>Both systems are explicitly designed to give you really high scores. <br><br>Because it<sq>s fun. The DM can balance this out by throwing more difficult monsters at the group than their level would suggest. <br><br>Or use normal difficulty and let them steam roll most encounters, because it<sq>s fun. <br><br>It<sq>s a completely subjective decision.</p>", 
                "question": "Dungeons and Dragons Statistics Help"
            }, 
            "id": "dd9jzln"
        }, 
        {
            "body": {
                "answer": "<p>I took a crack at modeling all of the possible outcomes, I already had a function in R that modeled damage using Great Weapon Master, but it used Kronecker to run the re-roll selection, so my laptop froze<br><br>I think the reason u/Tartalacame found that the second one is lower is going to stem from the fact that in method 2, after you re-roll, you are still stuck with the result even if you get another 1 or 2, and you get no choice if you roll a 3 or above (a la Great Weapon Master). On the other hand, method 1 lets you pick among the two <dq>columns<dq> even if you rolled mostly 3s (whereas method 2 is only <dq>powerful<dq> if you roll 1s and 2s)<br><br>I bet the distribution on method 2 is tighter though, so if you are risk averse, maybe go there...</p>", 
                "question": "Dungeons and Dragons Statistics Help"
            }, 
            "id": "dd9omu9"
        }, 
        {
            "body": {
                "answer": "<p>Who seriously did consider that ?!?<br><br>That<sq>s completely unbalanced for sure ! I mean just <dq>roll 4d6 and discard the lowest die<dq> gives you an average of 12.25 instead of 10.5 (traditionnal 3d6)<br><br>I didn<sq>t fully calculate for your <dq>Method 1<dq> but it<sq>s probably close to 14-14.5 on average.<br><br>I did calculate for <dq>Method 2<dq> and it<sq>s 13.92 on average.<br><br>I mean seriously, I play D&D and Pathfinder myself and I would never allow such ridiculously high starting values.<br>I personnally love <dq>Roll 8x 4d6, discard lowest die. Order your 8 results. Discard second best and second worst results. Arrange the other 6 results among your abilities<dq>. That gives you an average of 12.25 but you usually have 1 very good stat and 1 fair weakness.</p>", 
                "question": "Dungeons and Dragons Statistics Help"
            }, 
            "id": "dd9k83f"
        }, 
        {
            "body": {
                "answer": "<p>From the description I<sq>d guess method 2 is substantially higher on average. If I was offered the choice, I<sq>d have said I<sq>ll take method 2 without further calculation.<br><br>From simulation it looks like<colon><br><br>Method1 has a mean of about 13.86, and an 18 is about 7 times as likely as with 3d6<br><br>Method2 has a mean of about 14.91 and an 18 is about 16.5 times as likely as with 3d6<br><br>so my instinct that method 2 was considerably higher looks right<br><br></p>", 
                "question": "Dungeons and Dragons Statistics Help"
            }, 
            "id": "dd9pzu5"
        }, 
        {
            "body": {
                "answer": "<p>Create a compound measure of the three variables and use the distribution/standard deviations to form categories. </p>", 
                "question": "[x-post-r/statistics] Help creating a statistically significant VIP program for startup ?"
            }, 
            "id": "dd34zsd"
        }, 
        {
            "body": {
                "answer": "<p>1. The difference between (1-2) and (3-4) is driven by the reason that 4 is way lower than the other, therefore, the mean of 1 and 2 is different from the mean of 3 and 4. <br>2. With the evidence of the linear regression and posthoc both showing you that only one group is different from the others, so you should follow the main evidence. <br>3. Since you have 4 groups, HSD is more appropriate. </p>", 
                "question": "Anova tells me group 1-2 is different from group 3-4 but post hoc tells me that only group 4 is different from the others."
            }, 
            "id": "dc495ec"
        }, 
        {
            "body": {
                "answer": "<p>> For instance, if I had a skew of -1.87 this is demonstrative of not normal distribution is that correct?<br><br>(I assume you mean third-moment skewness rather than some other measure of skewness)<br><br>A population skewness of -1.87 would certainly indicate that the distribution would not be normal. <br><br>A sample skewness at least as far from zero as -1.87 can only occur very rarely in sampling from a normal distribution (even at small samples), so it would suggest that the sampling was from something other than a normal distribution.<br><br>> It basically has to be between -1.5 and 1.5 for it to be considered normal distribution?<br><br>Even a skewness of exactly 0 doesn<sq>t tell you that you are sampling from a normal distribution. A Laplace distribution has a skewness of 0 for example, but is distinctly non-normal. Indeed a population skewness of 0 (let alone a sample skewness of 0) doesn<sq>t even tell you the distribution you<sq>re sampling from is symmetric.<br><br>> If I had a Kurtosis score of 4.82, what does that mean.<br><br>Literally, that the average 4th power of a z-score\\* is 4.82. This tends to suggest relatively heavy tails (at a given standard deviation), but there<sq>s not a perfect relationship between tail-heaviness and kurtosis.<br><br>\\* <dq>internally<dq> standardized (i.e. by using the mean and standard deviation of the present sample)<br><br>> what would say a -.60 Kurtosis score mean?<br><br>Presumably you<sq>re asking about *excess* kurtosis rather than actual kurtosis (which is normally just the 4th standardized moment). It<sq>s important to be clear about that when you first mention a value for kurtosis or you may mislead part of your audience.<br><br>For an actual kurtosis, -0.6 would be impossible; for an excess kurtosis it may not suggest much (as a sample value) since sample kurtosis tends to be biased downward. For example, with sample sizes of 30, you<sq>re about twice as likely to see an excess kurtosis near -0.6 than you are near 0 when sampling from a normal population.<br><br>[For a population distribution, a kurtosis of -0.6 tends to indicate  lighter tails than for a normal distribution but (again) tail heaviness isn<sq>t perfectly related to kurtosis]<br></p>", 
                "question": "Please explain Skew and Kurtosis"
            }, 
            "id": "dafru98"
        }, 
        {
            "body": {
                "answer": "<p>Skew and kurtosis refer to the attributes of distribution (shape/symmetry) relative to a measure central tendency as a way of describing the data distribution pattern. Kurtosis is the distribution SHAPE (how flat or peaked the distribution is; flat=data is spread out farther from the middle measurement, peaked= data is closer to the middle measurement). Skew is the SYMMETRY (is the data distribution pattern relatively the same on both sides of the mean or does the data pattern look asymmetrical; symmetrical=think normal bell curve,asymmetrical=wonky bell curve). In the example above, the mean is the measure of central tendency and you start by looking at the measure of central tendency in the distribution and then visually inspect data patterns compared to the middle measurement.  Then use skew/kurtosis as the terminology to describe the characteristics of the shape and symmetry for the distribution.</p>", 
                "question": "Please explain Skew and Kurtosis"
            }, 
            "id": "dafykr8"
        }, 
        {
            "body": {
                "answer": "<p>It<sq>s not really clear to me what you<sq>re seeking here.<br><br>Can you explain in a bit more detail what you want?</p>", 
                "question": "What is the closest online equivalent/package to a Statistics 101 course?"
            }, 
            "id": "da6qzdk"
        }, 
        {
            "body": {
                "answer": "<p>If you mean an introductory class, I<sq>ve seen this taught 100 different ways. It depends on the field of study, and also usually whether it is more theoretical or applied. Can you be more specific?</p>", 
                "question": "What is the closest online equivalent/package to a Statistics 101 course?"
            }, 
            "id": "da73xjb"
        }, 
        {
            "body": {
                "answer": "<p>I think a mixed effects model would be appropriate here. That way you could calculate both the within and between individual correlation over time. You<sq>ll have to decide how you want to handle time though. Should it be a linear effect on the outcome? Categorical (like before or after an action)? Some exponential decay function? But my gut says you<sq>re right, there are ways to model this that are more powerful than eight independent correlations.</p>", 
                "question": "Salvaging an Unusual Case of Correlation Where Data Contains Repeated Measures"
            }, 
            "id": "d9y81jg"
        }, 
        {
            "body": {
                "answer": "<p>Any chance you can post the paper?</p>", 
                "question": "Salvaging an Unusual Case of Correlation Where Data Contains Repeated Measures"
            }, 
            "id": "d9ycp5b"
        }, 
        {
            "body": {
                "answer": "<p>I would need to see the paper. Maybe I think what they did makes sense <colon>) At any rate I need to understand the context better before I will have an opinion.</p>", 
                "question": "Salvaging an Unusual Case of Correlation Where Data Contains Repeated Measures"
            }, 
            "id": "d9yx58n"
        }, 
        {
            "body": {
                "answer": "<p>I do PCA in bioinformatics but I could imagine it applying to anything.  I use it for 2 purposes<colon><colon> (1) to visualize differences in the data; and (2) to reduce the dimensionality (essentially a generalization of (1)). I usually have multiple categories that I<sq>m looking and seeing how well they are linearly selectable in euclidean space.  It<sq>s a first pass to get a feel of the data before I proceed to downstream analysis. I plot the first 2 principal components on an xy plane (or 1 v 2 v 3 in 3D) then color them according to my categorical metadata.  You can also color the scatter plot based on continuous values to visualize the spread of certain values as metadata.  PCA for me is rarely to never a final analysis. You could color yours based on company, sex, age, region, or whatever data you have available.  <br><br>If you do this, important to see how much variance is accounted for in each axis to make meaningful conclusions.  Recall that each of the eigenvectors (the loadings I believe, I confuse components with loadings sometimes) are in the directions of most variance.  So if you have 1000 attributes and your first 2 principal components explain 99<percent> of your variance, then you can use those downstream instead of the 1000 attributes to essentially draw the same conclusions. <br><br>Hope this helps.</p>", 
                "question": "What is the business purpose of PCA?"
            }, 
            "id": "d9x9zfj"
        }, 
        {
            "body": {
                "answer": "<p>There<sq>s also PCoA which takes a distance matrix ( a form of mds) which I usually prefer bc of its flexibility in not confining the relationships to euclidean distance. tSNE is a newer more powerful ordination technique that<sq>s a little more difficult to tune and interpret. </p>", 
                "question": "What is the business purpose of PCA?"
            }, 
            "id": "d9xa19e"
        }, 
        {
            "body": {
                "answer": "<p>Well...  I<sq>m sure you<sq>ve been told this if you<sq>re taking a course but the reason to use these techniques is to uncover underlying structure in your data sets. <br><br>You mention using p value tests to confirm relationships right?  Well what if you have a really large number of relationships and it seems hard to target them individually (from a business perspective)?  Perhaps you suspect that a bunch of your variables probably tell you a simple story when looked at together.   Say your looking at financial data for a customer and asset size, liabilities and exposure and credit score all are correlated in some way or another.   After some factor analysis and some pca you can see more clearly that asset and liability resolve to one component and exposure and risk resolve to another. <br><br>As for what a business can do with that?  Well I guess it really depends on the area...  In marketing it<sq>s useful for distilling data into a more elegant story. You can<sq>t just send a long list of p values to a brand manager or a content editor.   You have to sit them down and tell them a story that makes sense to them. <br><br>Source<colon> am a senior marketing analytics specialist</p>", 
                "question": "What is the business purpose of PCA?"
            }, 
            "id": "d9xab1b"
        }, 
        {
            "body": {
                "answer": "<p>Anomaly detection<br><br>http<colon>//techblog.netflix.com/2015/02/rad-outlier-detection-on-big-data.html?m=1</p>", 
                "question": "What is the business purpose of PCA?"
            }, 
            "id": "d9xda3n"
        }, 
        {
            "body": {
                "answer": "<p>[One way to <dq>eye-ball<dq> it is to do a quantile-quantile plot.](http<colon>//www.itl.nist.gov/div898/handbook/eda/section3/qqplot.htm)<br><br>If it<sq>s the same distribution, the quantile plot should look like a straight line.<br><br>If you really want a p-value, you should probably use an [Anderson\u2013Darling test](https<colon>//en.wikipedia.org/wiki/Anderson<percent>E2<percent>80<percent>93Darling_test).</p>", 
                "question": "Goodness of fit test"
            }, 
            "id": "d9klfok"
        }, 
        {
            "body": {
                "answer": "<p>Im not 100<percent> clear on what you<sq>re asking, but in general you can fit a model (be it linear, polynomial, exponential...) then calculate an R^2 value. That will (essentially) tell you whether the points are generally close to or spread out from the line.<br><br>The hard part would probably be finding a good model.</p>", 
                "question": "Goodness of fit test"
            }, 
            "id": "d9ki9je"
        }, 
        {
            "body": {
                "answer": "<p>This steps into the realm of diagnostic check. You can check the model fit by checking against the assumption of the model. Since you are using a linear regression over transformed variable, you can conduct the following checks<colon><br><br>* Are the residuals showing any kind of pattern? <br><br>* Are the residuals normally distributed?<br><br>* Are the residuals independent?<br><br>For instance, you can do a standard white-noise test on the residual.  If your model has any additional assumption you can also check it in similar fashion.</p>", 
                "question": "Goodness of fit test"
            }, 
            "id": "d9l7v8l"
        }, 
        {
            "body": {
                "answer": "<p>There are lots of ways!<br><br>1) The simplest technique is to drop the participant entirely. However you have a very low *n* for the study. In these circumstances, the *df* added by a single participant can sometimes mean the difference between statistical significance and failing to reject the null. I always prefer to drop, personally. It<sq>s easier to explain how the variance I lost doesn<sq>t matter than to explain how the variance I made up (imputed) doesn<sq>t matter.<br><br>Here are some other ideas. <br><br>2) Substitute the mean as you said. The grand mean is considered conservative; the treatment/group mean is considered more liberal, but not as liberal as just giving your best guess based on experience (Tabachnik & Fiddell, 2013).<br><br>3) Estimate the value using regression, where the other means are used to develop a more adaptive estimation technique than the simple group mean.<br><br>4) Expectation Maximization, Multiple Imputation, Bootstrapping, etc. These techniques are much more sophisticated but not out of reach of the casual user. For example, SPSS has a handy interface that will perform expectation maximization (and regression) replacement. There are pros and cons to each technique that are beyond the scope of this comment; the statistical cons will be limited since you<sq>re estimating a single value, the bigger concern is the added utility vs time cost. Readily available techniques (such as those provided by your software) are probably better choices. In lieu of dropping a participant, these are my preferred techniques.<br><br>5) There are tons of other techniques, but you<sq>re estimating a single value, so this list will probably suffice.<br><br>Hope that gets you pointed in the right direction.</p>", 
                "question": "Need help with Repeated Measures ANOVA"
            }, 
            "id": "d8wn2fz"
        }, 
        {
            "body": {
                "answer": "<p>Best way is to use a mixed model instead with a random intercept for subjects to account for repeated measures. Mixed models can handle missing data without dropping cases like a conventional repeated measures ANOVA, and allow for far more flexible models. </p>", 
                "question": "Need help with Repeated Measures ANOVA"
            }, 
            "id": "d91blxc"
        }, 
        {
            "body": {
                "answer": "<p>I think you<sq>re mistaken and you do want to compare the means themselves. Your collaborator is correct, you want a t-test for this.<br><br>NHST<colon> You want to do post-hoc tests to examine all pairwise comparisons between the groups. It sounds like your planned comparisons showed that E1 and E2 are both significantly different from C, those are both t-tests, you<sq>ve already calculated the means. I<sq>m not sure which software you<sq>re using, but now you have to calculate the third t-test comparing E1 and E2. If you<sq>re worried about type 1 error and multiple comparisons, you should control with some form of correction (Bonferroni is the simplest.)<br><br>MCMC/Bayes<colon> Looks at the distribution of differences between each pair of group means. No multiple comparisons correction required, but you might want to include informed priors that shrink the group means slightly toward zero.</p>", 
                "question": "How do I test if two mean differences are different from each other?"
            }, 
            "id": "d8qbu1k"
        }, 
        {
            "body": {
                "answer": "<p>Skew is a well defined statistical measure (The third standardized moment). As per definition it takes ALL points into account. So you would be wrong to determine skew only by looking at the quartiles. You should take the entire box plot into account.<br><br>In some cases, a box plot doesn<sq>t even represent enough information to determine skew direction, (but there is clear cut cases). **edit<colon>** To underline this consider<colon> http<colon>//imgur.com/a/Zp1Mg The box plots are identical but one has left skew (negative skew) and the other has right skew (positive skew).</p>", 
                "question": "Do outliers matter when determining skew?"
            }, 
            "id": "d7vysr5"
        }, 
        {
            "body": {
                "answer": "<p>If you have a generative model which you can query, perhaps fetching more data points can clear that ambiguity.</p>", 
                "question": "Do outliers matter when determining skew?"
            }, 
            "id": "d7w1l76"
        }, 
        {
            "body": {
                "answer": "<p>I was going to type something up, but the [people here](http<colon>//stats.stackexchange.com/questions/2230/convergence-in-probability-vs-almost-sure-convergence) explain it better than I could.</p>", 
                "question": "Law of Large Numbers???"
            }, 
            "id": "d7quywt"
        }, 
        {
            "body": {
                "answer": "<p>There really isn<sq>t layman terms of explaining them. To understand the differences between converge in probability and almost surely convergence requires maths background, and that is why departments forces you to take those obnoxious real analysis course.<br><br>I think /u/DCI_John_Luther<sq>s link is explaining it as easy as possible.</p>", 
                "question": "Law of Large Numbers???"
            }, 
            "id": "d7tgc2z"
        }, 
        {
            "body": {
                "answer": "<p>[deleted]  <br> ^^^^^^^^^^^^^^^^0.3404 <br> > [What is this?](https<colon>//pastebin.com/64GuVi2F/22935)</p>", 
                "question": "Law of Large Numbers???"
            }, 
            "id": "d7r16if"
        }, 
        {
            "body": {
                "answer": "<p>Oh, answering my own question. Dell bought StatSoft (makers of Statistica) in 2014. <br>http<colon>//www.dell.com/learn/us/en/id/secure/2014-03-17-dell-acquires-statsoft-data-analytics-software</p>", 
                "question": "Why does Dell have a statistics ebook on their website?"
            }, 
            "id": "d7nysky"
        }, 
        {
            "body": {
                "answer": "<p>Wow, good find. </p>", 
                "question": "Why does Dell have a statistics ebook on their website?"
            }, 
            "id": "d7op66p"
        }, 
        {
            "body": {
                "answer": "<p>You have several questions embedded in here, but to begin me restate your scenario. You have *M* cities, and the i^th city has population N_i. Your observed data is number of cancers per city (x_i). You are interested in the cancer incidence rate (r_i). There are three ways you could summarize this scenario with a statistical model<colon> 1) completely pooling your data, 2) without pooling your data, 3) partial pooling (aka a multilevel model or hierarchical model). You could actually do any of these three from either a Bayesian or a Frequentist framework, but, especially for the partial pooling, Bayesian inference will tend to work better.<br><br>Complete pooling is equivalent to adding up all of your x_i<sq>s and your N_i<sq>s and coming up with one single estimate of cancer rate. This is probably a pretty bad model because obviously some cities are more smoggy, or happen to be downriver from where the government secretly built atomic weapons, or happen to have healing natural springs.<br><br>No pooling is a equivalent to using only the information observed in each city to estimate a local cancer rate. As you suggested you could use the MLE x_i/N_i. But an obvious problem here is that your cities have different population sizes, for example imagine Bumblefuck, Pennsyltucky with population 10 happens to get 4 cases of cancer. So you get an estimate of a whopping 40<percent> cancer rate with big standard errors. Clearly this probably isn<sq>t a reasonable estimate and it would be useful to include information on other cities (especially nearby cities or cities with similar environmental influences).<br><br>Partial pooling allows you to borrow information across cities. In this particular case a sensible parameterization would actually look fairly similar to binomial logistic regression. The only difference would be the addition of a random effect. In other words we would have logit(r_i) ~ a_0 + e_i, with e_i ~ N(0, sigma^2) with sigma to be estimated from the data (and in a Bayesian setting each parameter would be given a prior). So what we see here is that the cancer rate for the i^th city is a combination of two parameters<colon> a_0 the <dq>grand mean<dq> cancer rate and e_i the <dq>random effect<dq> for that particular city, and here sigma^2 is a hyperparameter. (I suppose a_0 is also a hyperparameter). <br><br>Side note<colon> this logit parameterization makes it a bit easier to see how the hierarchical(aka multilevel) model works to partially pool the data, it<sq>s also really convenient to work with and so in applied Bayesian data analysis tends to be more common that your Beta parameterization example. For the moment though, lets dispense with priors of Bayesian inference and think about just the likelihood model. <br><br>The model is going to try to simultaneous minimize the distance between two kinds of residuals. On the lowest level the model wants the inverse logit of (a_0 + e_i) to be as close as possible to x_i/N_i. At the same time though the model wants the smallest possible deviations of e_i from a_0. So this how you get each r_i related to each other in that they are treated as the inverse logit of draws from the hyperdistribution N(a_0, sigma^2). In this sense your e_i<sq>s are simply <dq>residuals<dq> from of your grand mean. And it should be obvious from this that they are conditionally independent draws from that hyperdistribution (unless you get hip to geography and make yourself a spatial hierarchical model). It should also be fairly obvious that this logistic regression parameterization makes it really easy to include city-level covariates (e.g. smog).<br><br>In hierarchical Bayes the next step is simply to put priors of the hyperparameters of a_0 and sigma^2. There are some good suggested defaults out there, or you might tune them to your problem.<br><br>For further reference here is a [short vignette that goes through the difference between complete, partial, and no pooling using the rstanarm R package](https<colon>//cran.r-project.org/web/packages/rstanarm/vignettes/pooling.html). I<sq>d also recommend perhaps picking up a book or two on Bayesian statistical inference ([BDA3](http<colon>//www.stat.columbia.edu/~gelman/book/) is awesome). Machine learning is a nice (if slightly creepy) buzzword, but it sounds like you might benefit from a pure Statistician<sq>s perspective.<br></p>", 
                "question": "Question about the role of hyper-parameters in hierarchical bayes."
            }, 
            "id": "d7j7v15"
        }, 
        {
            "body": {
                "answer": "<p>So let<sq>s first start by writing down the exact model you<sq>re considering here<colon><br><br>    for i in num_cities<colon><br>        r_i ~ Beta(a, b)<br>        x_i ~ Binomial(N_i, r_i)<br><br>which has a corresponding joint probability distribution<colon><br><br>`P(X, R, N, a, b) = \\prod_i Binomial(x_i | N_i, r_i) Beta(r_i | a, b)`<br><br>So given the above equation, it may be useful to think about the two scenarios you<sq>re considering.<br><br>**Scenario 1<colon> (a,b) fixed**<br><br>If both `a` and `b` are fixed, then for a given `x_i`, we clearly are independent of any other `x_j`. In this case, no matter what the other cities<sq> data turn out to be, you will in no way change your belief about `r_i` nor `x_i`. In fact, since we know `a` and `b`, we could just rewrite `P(X, R, N, a, b) = P(X, R, N | a, b)`.<br><br>**Scenario 2<colon> (a,b) unknown**<br><br>Now, if we treat `(a,b)` as unknown, we then have a different situation because now the joint distribution does not factorize. That is, the `a` and `b` <dq>tie together<dq> all the terms in that product. One way to look at it would be to think of the conditional distribution of `(a,b)`<colon><br><br>    P(a, b | X, R, N) = \\prod_i Binomial(x_i | N_i, r_i) Beta(r_i | a, b)<br>    = \\prod_i Beta(r_i | a, b)<br><br>Now, I<sq>m going to stop here. This example is a little unfortunately chosen, because given a bunch of independent, identically-distributed (IID) draws from a Beta distribution with unknown `(a,b)`, there is no closed-form solution to find the maximum likelihood solution. However, you can do a [convex optimization procedure](https<colon>//en.wikipedia.org/wiki/Beta_distribution#Two_unknown_parameters_2) to find the most likely values of `(a, b)`. And you could also try to put a prior on `a` and `b` and do Bayesian inference. All of that gets kind of messy in this case.<br><br>The point here though is that now to estimate `(a, b)`, you have to first look at *all* of your `R` values. So if you were to write a Gibbs sampler, for instance, to infer the values of `(a, b)`, your distribution would be affected by all of the `r_i` values. Then when you go to update `r_i` for a given city, you use your current `(a,b)` which now holds information from the other cities since it was drawn from a distribution affected by them. You can kind of think of information as <dq>flowing<dq> through the variables here.<br><br>If we had chosen a prettier problem, it may have been a bit easier to communicate. Like say a normal distribution with known variance and unknown mean which then generates a bunch of means which then generate some samples<colon><br><br>    for i in cities<colon><br>        r_i ~ N(a, sigma)<br>        x_i ~ N(r_i, tau)<br><br>where we assume `sigma` and `tau` are known. In this case, the update equation for the scenario with an unknown `a` is much cleaner<colon> it<sq>s the average of all the `r_i` values. So here the way you end up sharing information would be much clearer. First you would sample your global hyperparameter `a` from a normal distribution centered on the average of the local `r_i` values, then each `r_i` value would be sampled from a normal with a mean that includes the sampled `a` term in it. In effect, you<sq>re sort of <dq>shrinking<dq> each `r_i` towards the global mean by doing this. So the way the different `r_j` values are affecting `r_i` is indirect, in that they all contribute toward some global parameter which then changes the outcome of the local variables.<br><br>Hope that makes some sense. It<sq>s late here, so if it<sq>s a bit rambly, sorry!</p>", 
                "question": "Question about the role of hyper-parameters in hierarchical bayes."
            }, 
            "id": "d7j8dk9"
        }, 
        {
            "body": {
                "answer": "<p>Right now your fitting a model that predicts sales based on your factors with separate intercepts for ID (I assume that<sq>s salesperson). <br><br>If you<sq>re looking for factors that influence all sales people (like, month for instance, all sales people do well the month of June) then this is a good way to do it! <br><br>You can add the nesting if you think it would be helpful. Your syntax is correct!<br><br>Uneven sample sizes shouldn<sq>t be an issue!<br><br>If you<sq>re looking instead to see what things predict a good salesperson (years in school...etc) then you wouldn<sq>t necessarily want to do random intercepts for each person. </p>", 
                "question": "Mixed Effect Model Fit"
            }, 
            "id": "d7bvq0d"
        }, 
        {
            "body": {
                "answer": "<p>I<sq>m a bit unclear as to how a Master<sq>s would cost more than a PhD. Is this because you would get grants at the PhD level that aren<sq>t available to MSc students?<br><br>My advice, which is by no way authoritative, would be to take an MSc. You can work in clinical research with an MSc just fine. <br><br>You can also use the time needed to save up to make connections with potential supervisors, take any  additional undergrad courses they recommend (which you will have to do as an MSc or a PhD student otherwise), and improve your GPA if necessary.<br><br>This path also allows you to make progress towards a grad degree in biostats without making a committment right away. It<sq>s also a more stable timeline, as there is a LOT of variation in times to complete PhDs, and a lot of that variation has NOTHING to do with your own effort or success.</p>", 
                "question": "Can someone talk to me about a masters vs a PhD in biostatistics?"
            }, 
            "id": "d6xop16"
        }, 
        {
            "body": {
                "answer": "<p>If your overall GPA is around a 3.3, you have no research experience, and you<sq>ve only taken up through LinAl and Calc III (ie no probability, statistics, or real analysis), then your chances of getting into a funded PhD program in biostat are really small. Maybe try to get a job as a statistical programmer at a CRO, or find another way to get your feet wet in the field. Some programs offer funding to master<sq>s students, or at least tuition waivers, but given what you<sq>ve said it<sq>d probably be wise for you to make yourself a more attractive candidate first. Maybe take classes as a non degree seeking student or something.</p>", 
                "question": "Can someone talk to me about a masters vs a PhD in biostatistics?"
            }, 
            "id": "d6xqvhy"
        }, 
        {
            "body": {
                "answer": "<p>Hopefully my experience will be informative.<br><br>I was a biology major as an undergrad. I thought I wanted a PhD in genetics or ecology, but no one in my college really told me what to expect from the GRE or what kind of scores I would need. So, I didn<sq>t prep for the GRE (hey, I aced the ACT with no prep in HS, right?) and I didn<sq>t get the scores I<sq>d need for a PhD program. I ended up in a biology master<sq>s program and a very small school, which ended up being a great thing for me.<br><br>Like you with your chem degree, I found I didn<sq>t enjoy lab work or field work in my chosen field of biology. But I loved my required courses in biostats and philosophy of science. Philosophy doesn<sq>t pay the bills, so I started preparing for a PhD program in stats. I took 3 semesters of extra undergrad level math and stats courses to prepare myself for the transition. I applied to 5-10 schools and got assistantship / fellowship offers from two of them. I chose Penn State.<br><br>My first year was really rough. All the other PhD candidates had MUCH better math and stats skills than I did. My second year was much better, because I liked the material and teachers better. My third year was much tougher again, because I couldn<sq>t get the grade I needed in undergrad level Real Analysis or in my third year measure theory class. I failed my first attempt at the PhD candidacy exam that Spring and started applying for jobs. I completed my MS in stats and got an offer for a <dq>dream job<dq> during the summer of my third year. I accepted the dream job before my second attempt at the candidacy exam, which I also failed. But I had already more or less dropped out because I was GOING to take that job no matter what. I had been in school for 10 years straight and wanted to make some real money for a change.<br><br>I can only speak for my experiences at Penn State and applying to other schools. Penn State and the other schools I applied to in 2004 only had two real programs<colon> PhD with financial support (free tuition plus stipend) and Master of Applied Statistics (MAS) with no financial support (you pay tuition, no stipend or salary). No Masters of Public Health (MPH) programs offered any financial support either. The Master of Science (MS) degree at Penn State was an optional benefit for PhD students, but it mostly served as consolation prize for students who failed the program in their second year or later. They would not accept new students for a terminal MS degree at that time. You could only pick up the MS as a PhD student.<br><br>Truth be told, I was unprepared for my PhD program, even though I had a strong science background and almost as much math coursework as an undergrad math major. The people at Penn State wanted me to succeed. It seems like it<sq>s hard for them to recruit adequately prepared US students, so they take chances on motivated students from alternative backgrounds like bench science and engineering. A real science background is helpful in some areas of stats, specifically design of experiments. The things that came easiest to me were often struggles for my more mathematically inclined classmates. But ultimately a stats PhD candidate at a competitive school will be in classes competing with top level math students from around the world (China, Russia, India, ...). It<sq>s tough to thrive on a program when you<sq>re struggling with basics that your classmates aren<sq>t.<br><br>If you can afford to take out the student loans for MAS type masters programs, then it<sq>s probably a good deal for you. The classes aren<sq>t much harder than 400 level undergrad math courses and your employment prospects will be decent after graduation. You should be able to find entry level jobs in government, industry, etc.<br><br>If you really want the PhD, then do as much prep as you can. You<sq>ll need undergrad level math course work in <br><br>linear algebra / matrix algebra<br>calculus I, II, III and IV (if offered)<br>real analysis<br><br>Optional coursework in differential equations and any statistics courses will be helpful. You<sq>ll also need to learn some programming. Stats majors typically learn to program R or SAS, but learning any language like Perl, Python, Ruby, Java, shell scripting ... would be helpful. Learning databases would be helpful too.<br><br>Your first two years in a stats PhD program will throw a lot of different challenges at you. You<sq>ll need to write mathematical proofs (eg calculus and real analysis). You<sq>ll need to write basic computer programs in R, SAS or some other language to perform basic computational tasks like bootstrap and permutation tests. The more of these skills you can learn up front, the better.</p>", 
                "question": "Can someone talk to me about a masters vs a PhD in biostatistics?"
            }, 
            "id": "d6xyoom"
        }, 
        {
            "body": {
                "answer": "<p>There are very many types of estimation. I<sq>ll mention a couple of interesting examples. Statisticians might already be familiar with these but I don<sq>t expect many of these methods are well-known elsewhere (except for MaxEnt, which seems to have some popularity in ML/EE).<br><br>[Fiducial inference](https<colon>//en.wikipedia.org/wiki/Fiducial_inference) is an unpopular approach to inference, so it<sq>s more broad that estimation.<br><br>[Empirical Bayes estimators](https<colon>//en.wikipedia.org/wiki/Empirical_Bayes_method), where you plug in MLEs to determine your priors.<br><br>The [James-Stein estimator](https<colon>//en.wikipedia.org/wiki/James<percent>E2<percent>80<percent>93Stein_estimator) is an interesting case of super-efficiency although it is biased. It<sq>s related more broadly to [shrinkage estimators](https<colon>//en.wikipedia.org/wiki/Shrinkage_estimator).<br><br>[Maximum entropy estimates](https<colon>//en.wikipedia.org/wiki/Principle_of_maximum_entropy); again not popular in statistics.<br><br>[Iterative proportional fitting](https<colon>//en.wikipedia.org/wiki/Iterative_proportional_fitting) typically is equivalent to MLE, but has a different approach.<br><br>PS I think that you meant MAP estimation rather than MSP. I don<sq>t know of any <dq>MSP<dq> estimation.</p>", 
                "question": "Alternative estimation methods"
            }, 
            "id": "d6w8yij"
        }, 
        {
            "body": {
                "answer": "<p>> Ignoring the syntax, what is The Key Question trying to semantically ask? I am utterly confused by how there are two population means.<br><br>I don<sq>t see a question there - what is the Key Question?<br><br>You<sq>re conducting a t-test for the mean of some variable X.  X has some <dq>true<dq> population mean, \u00b5, which you cannot observe directly, since you only have a sample of X.  Instead, you *hypothesize* that the population mean of X is \u00b5_0.  So you are in fact dealing with two population means<colon> your <dq>hypothesized<dq> mean \u00b5_0, which is known; and \u00b5, the actual mean of X, which is unknown.  Your <dq>null hypothesis<dq> is that \u00b5 = \u00b5_0.<br><br>The basic question underlying the t-test is, <dq>Based on this sample of X, is there enough evidence to conclude that \u00b5 \u2260 \u00b5_0?<dq><br><br>If the answer is <dq>no,<dq> then we assume the null hypothesis is true (\u00b5 = \u00b5_0).  If the answer is <dq>yes,<dq> then we reject the null hypothesis, which then implies \u00b5 \u2260 \u00b5_0, and our best estimate of \u00b5 is then \u00b5 = X-bar.<br><br>> \u03b2_0 is a non-random, known constant which may or may not match the actual unknown parameter value \u03b2<br><br>Similar to above, \u03b2_0 is known because it is the <dq>hypothesized<dq> population mean, chosen by you.  \u03b2 is the true mean, which is unknown.  \u03b2-hat is the sample mean (which is known).<br><br>> which defeats the entire purpose of replacing the population standard deviation \u03c3 with the sample standard deviation s<br><br>The only population parameter needed for computing the t-statistic is the null-hypothesis mean, which you choose.  If you knew the population SD, you would use a Z-test.  We usually don<sq>t know the population SD, so we need to estimate it using the sample SD.  But the sample SD is subject to random noise (it has some non-zero variance).  The t-statistic takes this into account - in other words, it<sq>s similar to the normal distribution, but takes into account the extra variability involved in using a <dq>noisy<dq> sample SD instead of the (constant) population SD.<br><br>> When using the t-statistic on the population mean, do we use the central limit theorem to approximate the population mean, which we then insert into \u03b2_0 in the equation?<br><br>No; you insert your chosen value of \u03b2_0 into the formula.  The central limit theorem justifies the use of the t-statistic in the first place<colon> X-bar is (asymptotically) normally distributed, regardless of the distribution of X.  Without a central limit theorem, we wouldn<sq>t be able to use a t-test (or Z-test) on sample means, because we would have no idea how X-bar was distributed.<br><br>Hope that helps...<br><br>[Edit] One other clarification<colon><br><br>> At 0<colon>18 she says <dq>t is the sample mean<dq> while writing an \\bar{x}.<br><br>She<sq>s saying <dq>t is the sample mean, minus the population mean (\u00b5_0), divided by the standard error,<dq> which is correct.</p>", 
                "question": "Two questions about basic t-statistics. (Apologies for the length it really is basic.)"
            }, 
            "id": "d6uvbby"
        }, 
        {
            "body": {
                "answer": "<p>> At 0<colon>18 she says <dq>t is the sample mean<dq> while writing an \\bar{x}. I believe she meant to say <dq>x-bar is the sample mean<dq>.<br><br>No, she is correct but not completely formal and rigorous. A statistic is a function of the data and some known constants. She is saying that t is (a function of) the sample mean (amongst other things). <br><br>> At 0<colon>20 she states that \u00b5_0 is the population mean. At 0<colon>42 words appear saying \u00b5 is the population mean. I view this as inconsistent with 0<colon>20 and am confused why there are two population means.<br><br>It seems that the majority of your confusion stems from a lack of understanding of statistical hypotheses. There can be many population means of interest, in this case the mean under the null and under the alternative. <br><br>> At 0<colon>43 she asks <dq>The larger/smaller the value of x-bar, the stronger the evidence that the population mean that this sample comes from is greater than this population mean we<sq>re comparing it to<dq>. Symbolically, \u00b5 > \u00b5_0.<br><br>I agree that the use of larger/smaller is quite confusing, she should have only mentioned larger and smaller as separate cases. </p>", 
                "question": "Two questions about basic t-statistics. (Apologies for the length it really is basic.)"
            }, 
            "id": "d6tcnff"
        }, 
        {
            "body": {
                "answer": "<p>Originally it was portion of the perimeter under control, but it sometimes gets adjusted to be closer to how much of the work needed is done, much to the chagrin of [some purists](http<colon>//wildfiretoday.com/2009/04/29/contained-or-controlled/) <br><br>http<colon>//laist.com/2008/11/18/what_do_fire_officials_mean_when_th.php<br><br>http<colon>//www.nwcg.gov/glossary/a-z#Contained</p>", 
                "question": "Does anyone know what it means to say a fire is X<percent> contained?"
            }, 
            "id": "d6nbmc3"
        }, 
        {
            "body": {
                "answer": "<p>You<sq>d have to ask the people that make those statements to be sure.<br><br>I<sq>m pretty confident that <dq>70<percent> contained<dq> (or whatever) is not intended as some form of a probability statement.<br><br>My guess is it refers to the proportion of the fire-front that is contained (in whatever sense they intend <dq>contained<dq>).</p>", 
                "question": "Does anyone know what it means to say a fire is X<percent> contained?"
            }, 
            "id": "d6nbijq"
        }, 
        {
            "body": {
                "answer": "<p>Yes I would recommend looking for sociological journals, and if you want simplicity and possibly errors plus a lot of methodological explanation to help you follow what they did I would try searching for theses/dissertations. Most universities have digital repositories of all theses/dissertations you can search. <br><br>Source<colon> the stats in my masters thesis are effing horrendous despite being approved by committee. </p>", 
                "question": "Do you know of any interesting research papers that use introductory statistical methods?"
            }, 
            "id": "d6mkkpj"
        }, 
        {
            "body": {
                "answer": "<p>Most social science research uses simple statistical techniques. </p>", 
                "question": "Do you know of any interesting research papers that use introductory statistical methods?"
            }, 
            "id": "d6miu59"
        }, 
        {
            "body": {
                "answer": "<p>Databases. You should learn the theory behind different databases and how to write SQL queries. That will be more useful than learning a second or third language (eg Python, Perl, Java, Ruby, C variants, ...) IMO.</p>", 
                "question": "Which Computer Science Courses would supplement well with a Statistics undergrad degree?"
            }, 
            "id": "d6m6n4n"
        }, 
        {
            "body": {
                "answer": "<p>Hi, I would go on to your local Job search boards and checkout the type of job you want and look at the skill section.  If you want to get in to data analysis for example.  Jobs in my city commonly want a combination of Moderate to advanced Excel skills, SQL, SAS, and R.  I have on occasions seen Matlab too.<br><br>If you want to do data mining, data engineer etc, then there will be additional skills/software you should likely cover.<br><br>Here is an advert for a Data Warehouse Analyst Developer<colon><br><br>The ideal candidate will have<colon><br><br> * Can do attitude <br> * Excellent database query skills using SQL or MS Access<br> * Must be a team player and ability to work autonomously in a small team <br> * Excellent analytical and communication skills <br> * System analysis skills, such as writing requirements documents and functional specifications <br> * Understanding of Relational Databases, Entity Relationship Diagrams, types of SQL joins, Multi-Dimensional database concepts <br> * Data Warehouse development experience using Microsoft tools \u2013 SSIS, SSRS<br> * Solid MS Office skills \u2013 especially Excel <br> * Experience in any programming languages is bonus.<br> * Demonstrated initiative and ability to learn new systems quickly <br> * Tertiary qualification in Information Science, Computer Science, Information Management or other relevant discipline <br> * Outstanding time management and organizational skills, with the ability to prioritise.<br><br><br>This may not be what your interested in, but it might give you some ideas.  <br><br><br>In addition<colon>- Most if not all jobs will ask for high relationship skills of some sort, worth thinking about what you can do now to show this on your CV. </p>", 
                "question": "Which Computer Science Courses would supplement well with a Statistics undergrad degree?"
            }, 
            "id": "d6m3l8j"
        }, 
        {
            "body": {
                "answer": "<p>2 things <colon><br><br>1. Your assumption is right only if P2 & P1 are independent. If not, the probability of P1&P2 isn<sq>t P1xP2<br><br>2. Assuming independence, (1-(P1xP2)) = contrary of both happening = at most one event happens = ( P1 OR P2 OR  No event ) happens.<br><br>(1-P1)x(1-P2) = both P1 & P2 are not happening = only (No event) happens.<br><br>that<sq>s why the results are different.</p>", 
                "question": "Doubt is basic probability"
            }, 
            "id": "d6d2z8k"
        }, 
        {
            "body": {
                "answer": "<p>There are literally whole textbooks written on this subject. If you really want to dive into the rabbit hole, a good place to start is Friedman, Furberg, DeMets, Reboussin & Granger - Fundamentals of Clinical Trials (2015).<br><br>To expand my answer a bit<colon> clinical trials benefit from statistics at a high level by ensuring that the aims of the trial and analysis plan complement each other. One<sq>s analysis plan is useless if it cannot address the aims, and the aims are useless if a proper analysis cannot be done (or hasn<sq>t been planned). Beyond the obvious use of doing the analysis, another major aspect is how statistics can inform the experimental design, sample size selection and assurance of controlling type 1 and type 2 errors.</p>", 
                "question": "ELI5<colon> How is statistics used to design a clinical trial?"
            }, 
            "id": "d65dpyi"
        }, 
        {
            "body": {
                "answer": "<p>So your question is very extensive. I believe a lot of people are better fit to answer these questions than i am, but i can point you in the right direction for some of these. In general the used branch is duration analysis aka Survival analysis aka event history analysis, (i think it has a few other names)<br><br>>How does a single endpoint v. multiple endpoint change the power of the test and the way sample size calculation is calculated?<br><br>So, a single endpoint v. multiple endpoint requires different models (often the single endpoint is a special case of the multiple endpoint). Now your test, and sample size for a specific confidence interval depends on what you want to test - so its very hard to answer your question without getting something more specific.<br><br>>How would you account for mortality in calculations if the disease in question was particularly debilitating?<br><br>In essence one could have the endpoint be a scale from 1 - 10 on wellbeing where 1 was death etc. - Its very important that you are very clear on these definitions. If you are studying mortality and the disease in question was particularly debilitating it doesn<sq>t matter, death is death.<br><br>>How would a trial<sq>s results be affected if data were collected throughout the study v. at a fixed point of time?<br><br>In essence what you are asking about is in my field (econometrics) called stock sampling (fixed point) and flow sampling (throughout the study). I can<sq>t find an easy way to explain the effects, but flow sampling is preferable in this case! and im quite certain clinical trials almost allways use flow sampling (or atleast a pseudo kind, where the sampling might not be completely continous over time, but more on discrete steps). - I think the sampling methods has a different name in other fields.<br><br>As for the last question, i don<sq>t think i fully understand. If you are looking at the treatment effect this is the correct way to conduct the experiment. It<sq>s good that all families act alike. but im not certain i understand your question.</p>", 
                "question": "ELI5<colon> How is statistics used to design a clinical trial?"
            }, 
            "id": "d657zi8"
        }, 
        {
            "body": {
                "answer": "<p>Not sure about the SPSS implementation but statistical theory gives us a z-test for comparing two binomial means.<br><br> z stat= (p1 - p2) / sqrt( p (1 - p) (1/n1 + 1/n2))<br><br>where n1 and n2 are the number of respondents for each category, p1 is the proportion who said yes to hamburgers, p2 is the proportion who said yes to hotdogs and <br><br>p = (n1*p1 + n2*p2)/(n1 + n2)<br><br>In your example the z statistic is 6.78, which is going to reject the null hypothesis of equality at virtually every significance level. <br>The 0.05, 2 sided z critical value is 1.96 for reference.<br><br>See<colon> http<colon>//stats.stackexchange.com/questions/113602/test-if-two-binomial-distributions-are-statistically-different-from-each-other<br>Otherwise, if you wanted to work through the maths you could derive this from the likelihood ratio test, see the following for an example with gaussian means. <br>http<colon>//webpages.cs.luc.edu/~jdg/w3teaching/stat_305/sp11/twosamplelrtestsnormal.pdf</p>", 
                "question": "Comparing 2 dichotomous/binomial distributions"
            }, 
            "id": "d62eoip"
        }, 
        {
            "body": {
                "answer": "<p>For your edit<colon> your p-value has been truncated to an arbitrary 3 sigfigs. You should be able to edit it in the table and get the proper p-value. I<sq>m not sure what you mean by effect. It<sq>s not a t-test</p>", 
                "question": "Comparing 2 dichotomous/binomial distributions"
            }, 
            "id": "d632zkm"
        }, 
        {
            "body": {
                "answer": "<p>You could either do a z-test for equality of proportions, as /u/Fraxyz, or a <dq>contingency table<dq> chi-squared test for equality of distributions.<br><br>In this case, since you have two groups and two options per group (a 2x2 contingency table), they will produce the exact same results - the chi-squared statistic you get will be the square of the Z statistic from the proportions test.<br><br>> I also have a problem with interpretation. If I run a Chi-Square test, the significance returns .000. But does that mean there<sq>s a significant difference between the distributions, or that the one distribution has a significant <sq>effect<sq> on the other and thus they<sq>re technically the same?<br><br>It means we can reject the assumption that the distributions are the same - or more technically, that the <dq>hamburgers<dq> and <dq>hot dogs<dq> samples come from the same distribution.  Not sure what you mean by <dq>one distribution has a significant <sq>effect<sq> on the other.<dq></p>", 
                "question": "Comparing 2 dichotomous/binomial distributions"
            }, 
            "id": "d62jhbi"
        }, 
        {
            "body": {
                "answer": "<p>Your two variables are not independent so the z test and chi square are not valid. One possibility is to compute a new variable by subtracting the hotdog score from the hamburger score and use a t-test of whether the mean is significantly different from 0. Of course, your variable will not be normally distributed since it can take in only the values -1, 0, and 1 but the t test will still work well. It has been known since the mid 20th century that it works well even for dichotomous data. <br><br>On the other hand, the intra-ocular trauma test is valid for your example. </p>", 
                "question": "Comparing 2 dichotomous/binomial distributions"
            }, 
            "id": "d63ejn3"
        }, 
        {
            "body": {
                "answer": "<p>The probability of selecting a certain number out of n numbers is 1/n.<br><br>The probability of *not* selecting a certain number out of n numbers is 1-1/n.<br><br>The probability of *not* selecting a certain number if you select k out of n numbers is (1-1/n)^k.<br><br>The probability of selecting a certain number *at least once* if you select k out of n numbers is 1 - (1-1/n)^k.<br><br>Set n = k = 1.000.000 and you get the probability of selecting a certain number *at least once* if you select 1M out of 1M numbers<colon> 1 - (1-1/1e6)^1e6 = 0.632<br><br>So about 63.2<percent> of the possible numbers will appear at least once, or you<sq>ll get about 632.000 unique numbers.<br></p>", 
                "question": "Could statistics tell me what actual percentage of 6 digit numbers would be used if you asked a random generator to generate 10000000 random numbers?"
            }, 
            "id": "d612may"
        }, 
        {
            "body": {
                "answer": "<p>Yes. About 1 - 1/e = 0.6321206 = 63.21 <percent>. If I understand the question correctly.</p>", 
                "question": "Could statistics tell me what actual percentage of 6 digit numbers would be used if you asked a random generator to generate 10000000 random numbers?"
            }, 
            "id": "d611sec"
        }, 
        {
            "body": {
                "answer": "<p>what is a live number?</p>", 
                "question": "Could statistics tell me what actual percentage of 6 digit numbers would be used if you asked a random generator to generate 10000000 random numbers?"
            }, 
            "id": "d616lit"
        }, 
        {
            "body": {
                "answer": "<p>63<percent><br><br>1-(1-1/n)^n --> 1-1/e  for large n<br><br>This seems to come up about once a month (though usually it<sq>s for n=100 or something like that)<br></p>", 
                "question": "Could statistics tell me what actual percentage of 6 digit numbers would be used if you asked a random generator to generate 10000000 random numbers?"
            }, 
            "id": "d620fuo"
        }, 
        {
            "body": {
                "answer": "<p>GMM is a general method of estimation based on moment conditions of the form E[f(x,\u03b8)] = 0. The notation means that some function of both data x and parameters \u03b8 has expectation of zero (when the expectation is computed wrt. true data generating process, that is under true parameters). The idea is then for any candidate parameters to compute the sample mean of the above function based on actual observed data, and choose a candidate that drives this residual to zero. If there are more moment conditions than parameters, one would instead minimize a quadratic form to get <dq>close<dq> to zero, but the idea is similar.<br><br>OLS is a special case of GMM when the moment condition is E[(y-x\u03b2)x] = 0 (in other words, the error is orthogonal to explanatory variable), however GMM subsumes also many other estimators, such as instrumental variable regression (error is orthogonal to instruments). System GMM and Difference GMM are specific estimators used with dynamic panel models. In such settings, the typical random / fixed effect estimators will be inconsistent, so one must use further lags as instruments. I don<sq>t have much experience with such applications, but you may look at [Roodman (2009)](http<colon>//www.stata-journal.com/article.html?article=st0159), which is a nice overview (it<sq>s published in Stata Journal, but the discussion is more general). Another good reference is the text by [Cameron & Trivedi](http<colon>//cameron.econ.ucdavis.edu/mmabook/mma.html), which covers both GMM and its use with panel data.</p>", 
                "question": "Can someone ELI5 Generalized Method of Moments?"
            }, 
            "id": "d5t4hck"
        }, 
        {
            "body": {
                "answer": "<p>Funny... I was just looking into the difference between econometrics and <dq>pure statistics<dq> and ran across the GMM a bit.<br><br>Just googling, <dq>difference between econometrics and statistics<dq> gave some interesting results.  I particularly enjoyed this page<colon><br>http<colon>//robjhyndman.com/hyndsight/statistics-vs-econometrics/<br><br>Hyndman wrote a great intro to forecasting (https<colon>//www.otexts.org/fpp), so it was especially interesting to read his thoughts.<br><br>I can<sq>t help you specifically, but maybe seeing how the two approaches are different but have a lot in common might be helpful (sometimes it<sq>s just different terminology - other times, it<sq>s conflicting terminology).  It was interesting to read that while the GLM has <dq>taken over<dq> in statistics, it has not yet be fully adopted in econometrics.  It would be interesting to see how that would change the considerations of GMM vs. OLS.</p>", 
                "question": "Can someone ELI5 Generalized Method of Moments?"
            }, 
            "id": "d5sxazy"
        }, 
        {
            "body": {
                "answer": "<p>It<sq>s useful to think of statistical significance testing in terms of uncertainty. We are uncertain what the population is doing, so we sample from it. With the help of the central limit theorem we can make some assumptions about our sample, and using those assumptions we can do a test. The test is only necessary because of uncertainty in how well our sample describes the population.<br><br><br>That said, if you have complete information, you don<sq>t need significance testing. Statistical testing came about when we realized that we don<sq>t have the time/resources to collect complete information, but we still need to draw logical conclusions.</p>", 
                "question": "Philosophically what is a population? When do I use T statistic vs. Z statistic?"
            }, 
            "id": "d5mwwbg"
        }, 
        {
            "body": {
                "answer": "<p>On Z v. T statistics, if I am not mistaken you use the Z statistic only if you have an assumed population standard deviation you are using; if you are using the sample standard deviation, then you use the T statistic.</p>", 
                "question": "Philosophically what is a population? When do I use T statistic vs. Z statistic?"
            }, 
            "id": "d5mxjkt"
        }, 
        {
            "body": {
                "answer": "<p>On mobile now so I will expand later. A few observations. First, the probability distribution in question is the multinomial distribution (Wikipedia has a good article on it). Secondly, the probability changes based on his original guess; guessing that all 7 dice be ones is far less likely than, say, six separate numbers with one duplicate.</p>", 
                "question": "Odds of guessing 6/7 d6s correctly?"
            }, 
            "id": "d5az4rd"
        }, 
        {
            "body": {
                "answer": "<p>I<sq>m going to hedge by saying I might be wrong, since I<sq>ve felt confident with this kind of problem before only to overlook something important, but here goes...<br><br>First, note that if we assume each die is fair (i.e., that the probability of each side coming up is equal), then the probability of correctly guessing one die roll is 1/6, and the probability of incorrectly guessing one die roll is 5/6.<br><br>Next, because the order of the dice doesn<sq>t matter, we need to allow for the possibility that it could be any six of the seven dice. For this, we use the N-choose-k function, which is N!/(k!(N-k)!), where N is the total number of dice, k is the number of correctly chosen dice, and M! is the factorial function, which is M * (M-1) * (M-2) * ... * 2 * 1.<br><br>So, combining these two, the probability of guessing six of seven six-sided dice rolls is, according to Wolfram Alpha<colon><br><br>(7!/(6!1!)) * (1/6)^6 * 5/6 = 0.000125...<br><br>The Wolfram Alpha link has parentheses, so I couldn<sq>t get it to work with text<colon> https<colon>//www.wolframalpha.com/input/?i=(7!<percent>2F(6!1!))+*+(1<percent>2F6)<percent>5E6+*+5<percent>2F6<br><br>Edited to add that the formula above also assumes that the dice rolls are statistically independent. That<sq>s why you can raise 1/6 to the sixth power and multiply that by 5/6.</p>", 
                "question": "Odds of guessing 6/7 d6s correctly?"
            }, 
            "id": "d5baj9h"
        }, 
        {
            "body": {
                "answer": "<p>Finally time to expand. Sorry about the delay.<br><br>Exactly the point; /u/dinkum_thinkum had the perfect idea and best practical solution. The [Multinomial Distribution](https<colon>//en.wikipedia.org/wiki/Multinomial_distribution) counts the probability of receiving (<dq>A<dq> 1s, <dq>B<dq> 2s, <dq>C<dq> 3s, <dq>D<dq> 4s, <dq>E<dq> 5s, and <dq>F<dq> 6s) after rolling the dice n times. <br><br>The probability of receiving some combination (A,B,C,D,E,F) is as follows<colon> { [n! / (A! x B! x C! x D! x E! x F!)] x (1/6)^n }. In this case, n is 7, and we<sq>re interested in the probability of getting at least 6 of our 7 guessed numbers correct. That means getting either all 7 correct, or getting precisely 6 of our numbers right.<br>(If you ask about getting at least 5 correct, life gets substantially more challenging due to all the possibilities)<br><br>So in order to make this calculation, you must first look at their guesses. We<sq>ll illustrate the computation with an example<colon> one of the best guesses possible, (1,1,1,1,1,2), rolling one of each number except two sixes. <br><br>If that were your guess, any of the following combinations would also win<colon> (0,1,1,1,1,3), (1,0,1,1,1,3), (1,1,0,1,1,3), (1,1,1,0,1,3), (1,1,1,1,0,3), (0,1,1,1,2,2), (1,0,1,1,2,2)... etc, but we can boil them down to a few general cases<colon><br><br>+1 6, -1 anything else x5  <br>+1 non-6, -1 non-6 x20  <br>+1 non-6, -1 6 x5<br><br>Obviously here, we see quite a few difficulties in our mathematics. If your guess is something fancier, like (3,3,1,0,0,0), then the number of different types of possibilities gets a lot more challenging. The multinomial distribution is a bit limited here, but it still has its place at least in a preliminary way. There<sq>s almost surely a better way to approach this problem, but this solution (albeit slowly) works. <br><br>Let<sq>s finish up our example. You have (1/6)^7 x 7! x [(5 / (0! x 1! x 1! x 1! x 1! x 3!)) + (20 / (0! x 1! x 1! x 1! x 2! x 2!)) + (5 / (1! x 1! x 1! x 1! x 1! x 2!))] for your exactly six corrects, and (1/6)^6 x 7! / (1! x 1! x 1! x 1! x 1! x 2!) for your all seven corrects.<br><br>Since 0! = 1! = 1, we can simplify this to<colon><br>(1/6)^7 x 7! x [(5 / 3!) + (20 / (2! x 2!)) + (6 / 2!)] = 15.904<percent> chance of getting it right.<br><br>----<br><br>For the worst-case scenario, the guess of (0,0,0,0,0,7), only six winning possibilities exist [using /u/dinkum_thinkum<sq>s notation here]<colon> 666666, 566666, 466666, 366666, 266666, 166666. That gets your probability of winning at<colon><br>(1/6)^7 x 7! x ((1/7!) + (5/6!)) = 0.01286<percent><br><br>---<br><br>Otherwise, there<sq>s no real easy way to answer your problem. It depends on his guess, but once you know it, it is possible to run /u/dinkum_thinkum<sq>s algorithm to easily provide you with an answer. Hope this helped!</p>", 
                "question": "Odds of guessing 6/7 d6s correctly?"
            }, 
            "id": "d5o43bu"
        }, 
        {
            "body": {
                "answer": "<p>You can<sq>t really graph regressions, as every additional variable adds to the demensions of the plot. So given you have 5 variables, (where did fifteen come? Are you fully interacting? Do you really have good reason to investigate all of those interactions?) that<sq>s a fifth demensions like plot. In Stata, the best one can do is plot the predicted marginal means, holding all other variables besides your interest at  their means. I<sq>m sure there are similar ways to do this in R. It reduces you down to a single plot, but can still be very enlightening.</p>", 
                "question": "Can you graph a logistic regression with multiple categorical predictors"
            }, 
            "id": "d5aesmf"
        }, 
        {
            "body": {
                "answer": "<p>Your outcome variable is duration of time spent in the space and your explanatory variable is environmental condition.  Environmental condition has several levels that all participants will have spent a duration of time in (including zero).  The most straightforward way to run the test you want will be a repeated measures anova.  You can test for mean differences between the duration spent in each condition with that test.</p>", 
                "question": "Guidance with designing a statistical study and help choosing which test to use"
            }, 
            "id": "d50cf87"
        }, 
        {
            "body": {
                "answer": "<p>This is a really nice monograph where the authors used two large randomised, controlled trials to create historical and concurrent controls, in order to investigate bias and methods of adjustment. Adjusting generally made the bias worse<colon><br><br>http<colon>//www.journalslibrary.nihr.ac.uk/__data/assets/pdf_file/0007/64933/FullReport-hta7270.pdf<br><br>I don<sq>t think there is any magic method to prove causation where you can<sq>t do a true experiment. Establishing causation requires looking at the problem from a number of different angles and different study designs, combined with other evidence or widely accepted models which have weight in the argument.<br><br>This is one of the most important skills a researcher can have. It is rarely possible to prove anything with one study but a series of well thought out studies can combine to provide convincing evidence.</p>", 
                "question": "Examples where controlling for a covariate has been damaging/inappropriate"
            }, 
            "id": "d4zeh4e"
        }, 
        {
            "body": {
                "answer": "<p>Boston and Miami have the same temperature after you control for geographic location =P</p>", 
                "question": "Examples where controlling for a covariate has been damaging/inappropriate"
            }, 
            "id": "d4zkl31"
        }, 
        {
            "body": {
                "answer": "<p>I can<sq>t think of any on top of my head, but the way to tackle this is through simulations. </p>", 
                "question": "Examples where controlling for a covariate has been damaging/inappropriate"
            }, 
            "id": "d4zemiz"
        }, 
        {
            "body": {
                "answer": "<p>I saw a beautiful quote in a paper about all mountains having the same height when you control for air pressure</p>", 
                "question": "Examples where controlling for a covariate has been damaging/inappropriate"
            }, 
            "id": "d50996y"
        }, 
        {
            "body": {
                "answer": "<p>RemindMe! 1 day <dq>Good question.<dq></p>", 
                "question": "Lets say I have a 3D surface how can I express this surface as a set of additive Gaussian distributions? (even if someone could explain it in 2D that could be helpful too!)"
            }, 
            "id": "d48ahuh"
        }, 
        {
            "body": {
                "answer": "<p>I assume you<sq>re restricting the gaussian distributions to be normalized? And are you measuring the <dq>quality<dq> of the approximation as the L^2 -norm of the error?</p>", 
                "question": "Lets say I have a 3D surface how can I express this surface as a set of additive Gaussian distributions? (even if someone could explain it in 2D that could be helpful too!)"
            }, 
            "id": "d48c8cf"
        }, 
        {
            "body": {
                "answer": "<p>They do this in computational chemistry.  Represent molecular orbitals in terms of gaussians.<br><br>Start here https<colon>//en.m.wikipedia.org/wiki/Gaussian_orbital</p>", 
                "question": "Lets say I have a 3D surface how can I express this surface as a set of additive Gaussian distributions? (even if someone could explain it in 2D that could be helpful too!)"
            }, 
            "id": "d48u3d0"
        }, 
        {
            "body": {
                "answer": "<p>Do you just want to smooth a surface ([just fit splines](https<colon>//en.wikipedia.org/wiki/Spline_interpolation)), do you want to estimate local densities ([kernel density estimation](https<colon>//en.wikipedia.org/wiki/Multivariate_kernel_density_estimation)), or do you want to infer different subgroups in a population ([Gaussian mixture models](https<colon>//en.wikipedia.org/wiki/Mixture_model#Gaussian_mixture_model))? <br><br>At the moment is sounds like you want to re-invent something akin to kernel density estimation with Gaussian basis functions. The idea there however is not fit a minimum number of kernels individually to your local densities but to settle on one bandwidth and use sums of Gaussians to approximate larger peaks.<br><br></p>", 
                "question": "Lets say I have a 3D surface how can I express this surface as a set of additive Gaussian distributions? (even if someone could explain it in 2D that could be helpful too!)"
            }, 
            "id": "d4963pf"
        }, 
        {
            "body": {
                "answer": "<p>I haven<sq>t fully got my mind around your problem. But I can say - the sliding window approach does have the issue you describe. However, even your 4-time-points data has this issue<colon> round 2 is likely correlated with round 1, and so on. So you should probably account for it anyway.<br><br>The simplest nonlinear model for 4 time points is to treat time as an indicator, so you directly estimate the temporal response function. What is the model you<sq>re using and are you sure the linearity assumption is coming in where you think it is?</p>", 
                "question": "Sliding window analysis and violation of assumption of independence"
            }, 
            "id": "d44amw3"
        }, 
        {
            "body": {
                "answer": "<p>You do include yourself. An average is a summary statistic that summarizes the entire dataset, and it would be extremely inconvenient if it changes every time you looked at an individual data point. E.g. If you wanted to check how far away from the average the guy who scores 95 was than the guy who scored 75, you wouldn<sq>t want your baseline for the comparison to change. So yeah, if you are comparing your performance to the performance of the class as a whole, you do include yourself. If you<sq>re comparing your performance to the *rest of the class*, you do not. </p>", 
                "question": "Comparing yourself to a class you are a part of"
            }, 
            "id": "d40pknh"
        }, 
        {
            "body": {
                "answer": "<p>> the population size isn<sq>t in the confidence interval formula<br><br>Sure it is -- if you use the right formula for a finite population<br><br>https<colon>//en.wikipedia.org/wiki/Standard_error#Correction_for_finite_population<br><br>You can hardly blame the *formula* for you choosing to use the one for infinite populations when you don<sq>t have an infinite population.<br></p>", 
                "question": "Why isn<sq>t the confidence interval affected by the population size?"
            }, 
            "id": "d404pwh"
        }, 
        {
            "body": {
                "answer": "<p>BIC will normally favor a smaller model than AIC (look at the formula/calculation if you don<sq>t believe me). <br><br>Often this can help you if you want a smaller model or if you want a bigger model (often you want a smaller model). <br><br>Neither is wrong or right all the time but just consider how they do when you verify them on test data. Lasso regression is also another good method for model selection (again not always right but often does a good job)</p>", 
                "question": "Model selection? Chi-Sq? AIC? BIC?"
            }, 
            "id": "d4063d8"
        }, 
        {
            "body": {
                "answer": "<p>Are your models nested? LRT is fine to use if so and you are just adding covariates or terms to the model. </p>", 
                "question": "Model selection? Chi-Sq? AIC? BIC?"
            }, 
            "id": "d40fe58"
        }, 
        {
            "body": {
                "answer": "<p>They<sq>re functions which, when they exist in a neighborhood of zero, <sq>encode<sq> all the moments into a single function. One major benefit of mgfs is that convolution (which is how you find the distribution of sums of independent rvs) gets converted to multiplication. You can then <sq>invert<sq> the operation to get back to pdfs/pmfs<br></p>", 
                "question": "ELI5 - moment generating functions?"
            }, 
            "id": "d3vzk15"
        }, 
        {
            "body": {
                "answer": "<p>Think about statistics as a way of summarizing data and distributions. These distributions are often complex, so we wish to summarize their key features (for example, I can describe to you a person based on their age, race, height, appearance, etc.). These features of a distribution can be quantified by its moments (if they exist). The moment generating function is a nice way of computing those moments. Additionally as /u/efrique mentioned, they can be used to get back to the pdfs/pmfs that describe the distribution. <br><br></p>", 
                "question": "ELI5 - moment generating functions?"
            }, 
            "id": "d3w84df"
        }, 
        {
            "body": {
                "answer": "<p>http<colon>//online.stat.tamu.edu/programs.php - I hear about the Texas A&M online program all the time on Reddit. It<sq>s highly ranked overall and it<sq>s #7 among all public NCAA Division I FBS colleges.</p>", 
                "question": "Online Master<sq>s in Statistics Programs?"
            }, 
            "id": "d38du5c"
        }, 
        {
            "body": {
                "answer": "<p>Penn State has an online MS in Applied Statistics.<br><br>I<sq>m about halfway through, and I<sq>m not really very happy with it for a number of different reasons.<br><br>But...<br><br>* It is relatively inexpensive (about $2,500 per course)  <br>* Being online it<sq>s somewhat flexible  <br>* Statistics offers a pretty decent return on investment in today<sq>s world  <br><br>If you decide to go this particular route with PSU, be prepared to be hugely frustrated on a regular basis.<br><br>If all you really want to do is improve your knowledge of statistics, you might consider a graduate certificate program instead. They<sq>re normally just four courses.<br><br></p>", 
                "question": "Online Master<sq>s in Statistics Programs?"
            }, 
            "id": "d38kxz6"
        }, 
        {
            "body": {
                "answer": "<p>Both SMU and Berkeley have online data science programs. If you are looking toward the practical stats / ml side, this is not a bad option since they will teach the whole Python, NumpPy, SciPy, ScikitLearn bits, and probably also SASS, SPSS, and R.<br><br>If you are thinking more along the lines of quantitative finance, there are a few programs that specialize in that chiefly dealing with stochastic calculus and various computational methods.<br><br>If you are looking more toward the theoretical, such as the development of new algorithms etc, then an applied math degree is going to be your best bet.</p>", 
                "question": "Online Master<sq>s in Statistics Programs?"
            }, 
            "id": "d37u54z"
        }, 
        {
            "body": {
                "answer": "<p>Item response theory deals with designing questionnaires/tests... Hm the Rasch model?<br><br>Edit<colon> the Rasch model assumes binary responses (i.e. there is only a correct and incorrect answer). You could make up some binary questions, get some data on subjects, fit the Rasch model (proc irt in SAS) and exclude items with high threshold parameter estimates to obtain an <dq>easy<dq> test</p>", 
                "question": "What is the name of the type of statistics to get hard questions thrown out of tests?"
            }, 
            "id": "d2x0cgb"
        }, 
        {
            "body": {
                "answer": "<p>I expected a Joke <colon>(</p>", 
                "question": "What is the name of the type of statistics to get hard questions thrown out of tests?"
            }, 
            "id": "d2x0xjn"
        }, 
        {
            "body": {
                "answer": "<p>[deleted]</p>", 
                "question": "What is the name of the type of statistics to get hard questions thrown out of tests?"
            }, 
            "id": "d2wz0go"
        }, 
        {
            "body": {
                "answer": "<p>The  most common stats used to assess fairness are tests for differential item functioning.  There are no tests for identifying and removing difficult questions.  We like to have difficult questions on tests to help discriminate between test takers<sq> abilities.</p>", 
                "question": "What is the name of the type of statistics to get hard questions thrown out of tests?"
            }, 
            "id": "d2x1yya"
        }, 
        {
            "body": {
                "answer": "<p>Andrew Hayes has Krippendorf<sq>s alpha macros available for SPSS on his website<colon> http<colon>//afhayes.com/spss-sas-and-mplus-macros-and-code.html <br><br>If you don<sq>t know K<sq>s alpha, it<sq>s a design agnostic IRR statistic. Also, it doesn<sq>t really matter, because for the same design the alpha statistic won<sq>t be significantly different from Fleiss<sq> kappa. <br></p>", 
                "question": "Calculating Kappa for inter-rater reliability with multiple raters in SPSS"
            }, 
            "id": "d2mtpgv"
        }, 
        {
            "body": {
                "answer": "<p>I don<sq>t follow exactly what you<sq>re trying to accomplish, but Fleiss<sq> kappa is an extension of Cohen<sq>s kappa for more than two raters.</p>", 
                "question": "Calculating Kappa for inter-rater reliability with multiple raters in SPSS"
            }, 
            "id": "d2mpyu8"
        }, 
        {
            "body": {
                "answer": "<p>There is far weirder cross-pollination between disciplines all the time.  For example, my own area of systems biology often attracts electrical engineers.<br><br>Coursework doesn<sq>t unlock any secrets.  Unless there is something explicit stated in a department<sq>s guidelines and you have a good idea what you want to pursue as far as graduate-level work, can find a supervisor that has similar interests, and you can articulate your vision to him/her I would be surprised if the undergraduate background you listed would hold you back (quite the opposite).  This assumes a research-based, not coursework-based, graduate degree (I can<sq>t speak for the latter).  Talk to the professors directly - The Department^TM is just a bureaucratic structure.<br><br>Saying you want to study stats for a <dq>change of scenery<dq> sounds like a bigger problem.  If you go to graduate school, you want to be attracted to and passionate about a subject, not fleeing another <colon>P.</p>", 
                "question": "Getting into Statistics graduate studies from economics undergrad?"
            }, 
            "id": "d2izs1a"
        }, 
        {
            "body": {
                "answer": "<p>You have geometric Brownian motion. Consider the process ln(X(t)) and apply Ito<sq>s Lemma to it. You will then transform your process into arithmetic Brownian motion, which is just straight forward normal calculations.</p>", 
                "question": "Using Ito<sq>s Lemma in order to solve Brownian Motion Probabilities"
            }, 
            "id": "d2aqiaw"
        }, 
        {
            "body": {
                "answer": "<p>Your friend is confused.<br><br>It sounds like they are trying to do an equivalence trial. This is just a trial with a large enough sample size to exclude important differences. Essentially they need a very high power to detect a trivially small difference. There<sq>s a [CONSORT extension](http<colon>//www.consort-statement.org/extensions) discussing these designs (equivalence and non-inferiority trials).<br><br>The reason we set up a null hypothesis (of no difference) to test is because it is simple to define, whereas the myriad of possible alternative hypotheses are not. We know what <dq>no difference<dq> looks like so we can use it as a solid benchmark to compare with our observed results.<br></p>", 
                "question": "Can you have a difference in results as the null hypothesis?"
            }, 
            "id": "d29fxb4"
        }, 
        {
            "body": {
                "answer": "<p>This is another fine example of the muddled logic behind null hypothesis testing but I<sq>ve ranted about that enough on this subreddit so I<sq>ll save the soapbox for another time. <br><br>Here<sq>s what I would argue is the most reasonable way to approach this problem if you insist on null hypothesis testing. The null should still be a statement of no difference between groups. The alternative hypothesis in this case would be a simple statement of *some* difference without specifying direction. <br><br>This means that the t-test should be two-tailed rather than one-tailed in this case. While the language may feel a bit weird, a failure to reject the null hypothesis is a good thing in this case. Recall that all you can do with this method is reject or fail to reject the null. You are never accepting a research hypothesis anyway. <br><br></p>", 
                "question": "Can you have a difference in results as the null hypothesis?"
            }, 
            "id": "d29ek3n"
        }, 
        {
            "body": {
                "answer": "<p>If the test is one tailed, you can have a \u2264 or \u2265 null, but you can<sq>t have a two-tailed inequality null (instead you may want to look at equivalence tests and non-inferiority tests). <br><br>The reason you can<sq>t have a \u2260 null in an ordinary NHST is that you can<sq>t calculate the distribution of the test statistic under the null.<br></p>", 
                "question": "Can you have a difference in results as the null hypothesis?"
            }, 
            "id": "d2a55gn"
        }, 
        {
            "body": {
                "answer": "<p>Sounds like a two sample t-test. http<colon>//www.itl.nist.gov/div898/handbook/eda/section3/eda353.htm</p>", 
                "question": "Can you have a difference in results as the null hypothesis?"
            }, 
            "id": "d2ah1yz"
        }, 
        {
            "body": {
                "answer": "<p>The term <dq>unadjusted<dq> refers to the fact that these are univariate summaries (as opposed to estimates from covariate-adjusted models).  It<sq>s kind of standard to provide these summaries as part of a data description, and also to give context for interpreting more complex adjusted measures of association.  There<sq>s nothing hand-wavy about this-- readers will be legitimately interested in not only unadjusted measures of association, but also how those measures change in more sophisticated models.<br><br>The term <dq>unadjusted<dq> has nothing to do with correction for multiple testing.  It<sq>s not unusual to see a bunch of univariate tests presented with p-values.  If you<sq>re concerned about multiple testing issues, you can easily calculate corrections using Bonferroni or whatever.  But often, these p-values are not given in the decision-theoretic context of hypothesis testing.  Rather, they<sq>re thought of as measures of evidence in favor of association.  (i.e. a covariate with a small corresponding p-value may be worth further examination and inclusion in future models.)<br><br>I don<sq>t see anything wrong with the documentation on the modeling.  They discuss some details in the second paragraph of the analysis section.  And then they talk about some sensitivity analysis stuff in the paragraph before the discussion.  Yeah it<sq>s brief, but it would be unusual to expect more (it<sq>s not a methods paper, and they<sq>re using standard modeling strategies).<br><br>I<sq>m a little bit more optimistic than you guys regarding stats in medical publications.  First, methods are pretty thoroughly vetted in the review process, especially in the better journals.  And second, I don<sq>t know of any major medical research organization that doesn<sq>t maintain strong connections with biostat departments, many even have their own biostat working group.  Funding for statisticians is much more of a problem in psychology.</p>", 
                "question": "<dq>Unadjusted<dq> results in an academic paper - am I interpreting this correctly?"
            }, 
            "id": "d1zdkxf"
        }, 
        {
            "body": {
                "answer": "<p>>  Are they literally doing dozens of chi-square/t-tests between the two groups (depending on the underlying nature of the variable)?<br><br>Probably.  This is pretty common in medical research papers, where the researchers have a lot of different things that they<sq>re interested in and one major outcome or effect that they care about.  They<sq>re usually either trying to do dimension reduction (only selecting the variables with very low univariate p-values) or they care about a lot of related things (like in this paper).<br><br>> Is this appropriate (i.e. is it statistically sound to run dozens of separate tests between two groups)?<br><br>Generally you<sq>re taught that you don<sq>t want to do this because of the false negative and false positives rates.  Usually the researchers treat all of their tests as though they<sq>re independent as well even when they<sq>re really not (see the total mess that is GWAS studies for more examples of this).<br><br>> How is this <dq>unadjusted<dq>?<br><br>Most medical researcher have learned or heard of the Bonferroni correction, so this is to indicate that they haven<sq>t done any kind of correction for multiple testing.<br><br>This type of thing comes from the lack of statistics courses in most medical researcher or med school programs.  I know a lot of doctors doing research who either have taken 1 or 2 stats courses (and far too many of them think that they <dq>know statistics<dq>) and are doing serious, expensive research studies but they don<sq>t know how to design studies or how to analyze or interpret the results.  The best medical researchers I<sq>ve dealt with are the ones who haven<sq>t taken any stats courses because they feel comfortable asking questions and asking for help and generally want to learn.</p>", 
                "question": "<dq>Unadjusted<dq> results in an academic paper - am I interpreting this correctly?"
            }, 
            "id": "d1yyyc3"
        }, 
        {
            "body": {
                "answer": "<p>It sounds to me like you<sq>re trying to create a continuous variable from a categorical/ordinal one? This is possible, but may not be a good idea since the number of hours a person spends per week in religious education seems a bit like guesswork.<br><br>Have you considered treating this variable as an ordinal variable?</p>", 
                "question": "[Survey data analysis] How do I go about weighting and coding multiple responses?"
            }, 
            "id": "d1sucf8"
        }, 
        {
            "body": {
                "answer": "<p>If I understand you right, you probably want to define a null hypothesis that <sq>trees are removed independent of species<sq>.<br><br>Your *dependent (outcome) variable* is <sq>trees removed<sq>, and your *independent variable* is species, a categorical variable (buckets). Looking it up on the sidebar link, you get the [chi-squared test](https<colon>//en.wikipedia.org/wiki/Chi-squared_test) which is a good answer. You could also potentially use [Fisher<sq>s exact test](https<colon>//en.wikipedia.org/wiki/Fisher<percent>27s_exact_test).</p>", 
                "question": "Not sure what type of test to use..."
            }, 
            "id": "d0v4u4m"
        }, 
        {
            "body": {
                "answer": "<p>Loads of them. Methodological development is a very important part of what statisticians do, and a lot of that is done by <sq>applied<sq> statisticians working in a specific field of application. Research often includes elements of methodological research as well as the primary research question.<br><br>*Statistics in Medicine* is a good journal for methodological developments in health-related fields. <br><br>The extended <sq>explanation and elaboration<sq> papers for these three sets of guidelines should give you a good introduction into the kind of work that goes on<colon><br><br>http<colon>//www.consort-statement.org/<br><br>http<colon>//www.prisma-statement.org/<br><br>http<colon>//www.strobe-statement.org/<br><br></p>", 
                "question": "Are there statisticians who work on design of experiments/research methodology?"
            }, 
            "id": "d0sf9vk"
        }, 
        {
            "body": {
                "answer": "<p>Are you after something like Seltman<sq>s book Experimental Design & Analysis?<br><br>http<colon>//www.stat.cmu.edu/~hseltman/309/Book/Book.pdf</p>", 
                "question": "Are there statisticians who work on design of experiments/research methodology?"
            }, 
            "id": "d0sr07b"
        }, 
        {
            "body": {
                "answer": "<p>We can partition the total variance in the whole dataset as variance within and between groups<colon><br><br>Within groups = variance between individual observations and their group means<br><br>Between groups = variance between group means and the overall  mean<br><br>All the variance has to go somewhere and the greater the difference between group means, the greater the proportion of total variance falling between instead of within groups.<br><br>So the F test is just asking whether the variance between groups is large compared to the variance within those groups. If it is, it suggests a difference between the means of the groups. In terms of deciding what counts as <sq>large<sq> it is exactly the same as every other statistical test<colon> we define an (arbitrary) critical value based on how often we<sq>re happy to be wrong and compare our results to that.</p>", 
                "question": "Help understanding F-Tests.."
            }, 
            "id": "d0ge6vf"
        }, 
        {
            "body": {
                "answer": "<p>Give my video visualizing what an ANOVA does a try.  Skip forward to 19<colon>30 to get to the ANOVA discussion (I made the link start the video at 19<colon>30).  Make sure you are watching on a platform with annotations enabled, since my cursor didn<sq>t record, and I use annotations to draw attention to different parts of the graph. <br><br>http<colon>//www.youtube.com/watch?v=tq5JfEUcFBk&t=19m30s<br><br>Let me know if that helps- if not, I always appreciate suggestions/comments about how to improve.</p>", 
                "question": "Help understanding F-Tests.."
            }, 
            "id": "d0gd4u5"
        }, 
        {
            "body": {
                "answer": "<p>The MSB and MSE are two estimates of population variance and estimate the same quantity when the null hypothesis is true. When the null hypothesis is false, MSB estimates a larger quantity. If the ratio is sufficiently large, it is unlikely both mean squares are estimating the same quantity. Note that the null hypothesis refers to population and not sample means. See [this page](http<colon>//onlinestatbook.com/2/analysis_of_variance/one-way.html) for more details. </p>", 
                "question": "Help understanding F-Tests.."
            }, 
            "id": "d0hc03k"
        }, 
        {
            "body": {
                "answer": "<p>If the null hypothesis was true I understand this to meam that the means of each group would basically be equal to the population mean, and a s result the variance between groups would be small.<br><br>However , what about teh variation within groups ? Are we saying in theory if the null hypothesius is true then we<sq>d expect the variace within groups to give us a similar answer ?<br><br></p>", 
                "question": "Help understanding F-Tests.."
            }, 
            "id": "d0pxrc4"
        }, 
        {
            "body": {
                "answer": "<p>What exactly are you trying to do here? This isn<sq>t really a post-hoc analysis so much as a series of t-tests. And there are far too many comparisons for these to be meaningful. Run enough tests and you<sq>ll eventually find <dq>significant<dq> false positive results. <br><br>What is your experimental design? Be specific. Dependent variable, groups, n, hypotheses, etc.</p>", 
                "question": "Trouble understanding ANOVA post hoc data?"
            }, 
            "id": "d03fwax"
        }, 
        {
            "body": {
                "answer": "<p>Smaller takes its *usual* meaning -- i.e. it means <dq>further left on the number line<dq> not <dq>closer to 0<dq><br><br>If A<B then A is smaller than B</p>", 
                "question": "Negative values for -2LR AIC etc - how to interpret?"
            }, 
            "id": "czx3rwi"
        }, 
        {
            "body": {
                "answer": "<p>Absolutely lower is a better fit. But don<sq>t go fishing for better models just based on AIC/BIC, especially if you do not understand what the covariance structure actually is. This will lead you down a rabbit-hole of uncertainies.<br><br>Try running an unstructured covariance model - in essence, this is <dq>ideal<dq>, but potentially over-parameterised/complex. <br><br>Also, what is your random statement? Just intercepts or slopes as well? Do you need random slopes? What exactly are you trying to account for with your random statement? </p>", 
                "question": "Negative values for -2LR AIC etc - how to interpret?"
            }, 
            "id": "czwtb5k"
        }, 
        {
            "body": {
                "answer": "<p>I am actually in the same boat with a simulation for my thesis.  Negative BIC values pointing at one model, but I always worked with positive values.</p>", 
                "question": "Negative values for -2LR AIC etc - how to interpret?"
            }, 
            "id": "czwql26"
        }, 
        {
            "body": {
                "answer": "<p>These values are essentially determined up to an additive constant. It is whatever is lower and exact location on the number line does not matter. </p>", 
                "question": "Negative values for -2LR AIC etc - how to interpret?"
            }, 
            "id": "czws0nh"
        }, 
        {
            "body": {
                "answer": "<p>Before doing anything else, knowing the variance in the sample mean is just an important thing to keep in mind. It doesn<sq>t require fancy statistics, and xbar by itself is not an especially useful measure. <br><br>Thin about the difference between these two means<colon><br><br>1) sample mean = 10, standard error = 0.5<br><br>2) sample mean = 10, standard error = 50<br><br>Do you see how you think about those two very differently from simply knowing the variance attached to the number? When dealing with uncertainty, we don<sq>t care so much with the predicted estimate as much as the confidence interval</p>", 
                "question": "Introduction to Sampling"
            }, 
            "id": "czelrxe"
        }, 
        {
            "body": {
                "answer": "<p>I believe the point of the exercise was to demonstrate that you can construct a Population distribution consisting of all X-bars {X-bar_1, X-bar_2, ... , X-bar_n} based on the Population distribution of X. This Population distribution of X-bar has the same mean as the Population distribution of X, but its Variance is a fraction ( 1 / n ) of the Variance of the Population distribution of X.  <br>  <br>Perhaps your professor wished to demonstrate the importance of Variance in addition to the Mean of a distribution. This may aid your understanding when approaching further topics such as Mean-Squared Error and Consistency. </p>", 
                "question": "Introduction to Sampling"
            }, 
            "id": "czez5jb"
        }, 
        {
            "body": {
                "answer": "<p>Honestly you would probably get way better answers if you ask on www.crossvalidated.com</p>", 
                "question": "Introduction to Sampling"
            }, 
            "id": "czezvke"
        }, 
        {
            "body": {
                "answer": "<p>I know of [this](https<colon>//www.uncp.edu/sites/default/files/Images_Docs/Academics/Colleges_Schools_and_Departments/Departments/Sociology_and_Criminal_Justice/test510.pdf) little paper.</p>", 
                "question": "Teaching intro statistics to a blind student"
            }, 
            "id": "cz4edsc"
        }, 
        {
            "body": {
                "answer": "<p>I would think the first step is to talk to the student. They<sq>ve chosen to take the class and they know their coping mechanisms and learning style better than anyone else.<br><br>I teach statistics by using leading questions so that they work the theory out for themselves before I reinforce their confidence with slides confirming they were right. Harder to do with such a large group but it is very effective and it might help make the class more accessible for this student if it was feasible to do.</p>", 
                "question": "Teaching intro statistics to a blind student"
            }, 
            "id": "cz4f0pa"
        }, 
        {
            "body": {
                "answer": "<p>Haven<sq>t done this, but I teach stats and used to work with a blind guy, so here is just some ideas.<br><br>* Get legos to represent histograms.  <br>* Wire and a ruler to show continuous distributions.  <br>* Tape coins on a ruler and find the balance point to show mean, count the coins to show median.  <br>* Teach MAD and Range, explain standard deviation as a measure that accounts for both.  <br>* Teach MAD by turning each deviation into a length of paper.  Tape the papers together and then fold to get the MAD.<br>* Use the legos and different histograms to different standard deviations.  <br>* Use sheets cut into squares for standard deviation and a geoboard to get standard deviation.<br>* For regression, get a battleship game and use the pegs to represent data points.  Estimate x-bar, y-bar. Use a string to get general trend.<br></p>", 
                "question": "Teaching intro statistics to a blind student"
            }, 
            "id": "cz4gx6g"
        }, 
        {
            "body": {
                "answer": "<p>Hi OP- I know I am late to this thread, but I am a Teacher of the Blind and Visually Impaired. Two things- you may want to crosspost this over to r/blind to see if they have any suggestions for you. Also, is your student registered with disability services? There is a talking graphing calculator that will auditorily <dq>speak<dq> all of the graphs. It<sq>s a really strange thing to listen to, but it works well; I think it<sq>s actually just a plug-in that hooks on to the top of a TI-something or other. But it does exist and is actually a fairly new piece of technology.<br><br>If you need any other help, PM me - I can try to do my best. I know nothing about statistics, but I know a good bit about assistive technology, adapting curriculum, and I<sq>ve got a fairly large network that I can consult. <br><br>Edit<colon> Here is the [calculator](https<colon>//shop.aph.org/webapp/wcs/stores/servlet/Product_Orion<percent>20TI-84<percent>20Plus<percent>20Talking<percent>20Graphing<percent>20Calculator_1-07340-00P_10001_11051) - I would imagine that either the school or vocational rehab funds would be able to pay for this for the student.</p>", 
                "question": "Teaching intro statistics to a blind student"
            }, 
            "id": "czdog70"
        }, 
        {
            "body": {
                "answer": "<p>You<sq>re looking for power, right?</p>", 
                "question": "What is considered an accurate sample amount? (ie<colon> 25 50 100 etc) (Videogame win-rate related)"
            }, 
            "id": "cywfccs"
        }, 
        {
            "body": {
                "answer": "<p>define *accurate* in a sufficiently precise way</p>", 
                "question": "What is considered an accurate sample amount? (ie<colon> 25 50 100 etc) (Videogame win-rate related)"
            }, 
            "id": "cywi03q"
        }, 
        {
            "body": {
                "answer": "<p>[This calculator](http<colon>//www.sample-size.net/sample-size-conf-interval-proportion/) will estimate the required sample size (number of games) required to estimate your true win rate to within a given confidence interval width (i.e. margin or error) with a given level of confidence. It may help to [read more about confidence intervals](https<colon>//en.wikipedia.org/wiki/Confidence_interval).<br><br>For example, to get to 99.9<percent> confidence for that your true win rate is 55<percent> +/- 5<percent> would require ~4300 games. <br><br>Conversely, from [this calculator](http<colon>//epitools.ausvet.com.au/content.php?page=CIProportion&SampleSize=100&Positive=55&Conf=0.999&Digits=3), after winning 55/100 games the 99.9<percent> confidence interval puts your win rate between 38<percent> and 71<percent>, or 90<percent> confidence it<sq>s between 46<percent> and 63<percent>.<br></p>", 
                "question": "What is considered an accurate sample amount? (ie<colon> 25 50 100 etc) (Videogame win-rate related)"
            }, 
            "id": "cywmn5w"
        }, 
        {
            "body": {
                "answer": "<p>The odds of winning the powerball are 1<colon>292201338.<br><br>Let<sq>s put this in terms of time. 292,201,338 seconds is equivalent to<colon><br><br>4870022.3 minutes<br><br>or 81167.03833 hours<br><br>or 3381.959931 days<br><br>or 9.265643645 years.<br><br>This means that picking the winning numbers is akin to randomly picking a particular second in time over a 9 year 3 month period.<br></p>", 
                "question": "life situations with similar probability of winning the powerball"
            }, 
            "id": "cyttfwa"
        }, 
        {
            "body": {
                "answer": "<p>Disclaimer<colon> I am not a mathematician.<br><br>Shuffle a deck of cards well. If the top five cards are Ace, King, Queen, Jack, and Ten of Spades, *in that order*, you<sq>ve won the powerball.<br><br>Roll twelve normal (six-sided) dice. If they all come up the same, you<sq>ve won the powerball.<br><br>Play Russian roulette (six chambers, one bullet) seven times. If you live, play again *one hundred* more times. If you are still alive, you<sq>ve won the powerball. </p>", 
                "question": "life situations with similar probability of winning the powerball"
            }, 
            "id": "cytx7ow"
        }, 
        {
            "body": {
                "answer": "<p>OK first right down 5 numbers between 15-90 take a dice roll it 15 times add up the total. repeat this 4 more times. Then right down a number between 5-30 and roll the dice 5 more times. the likelihood of your roll totals matching to the numbers you guessed is about the same as you winning the lottery.<br><br>I have also heard it is roughly the same a flipping a quarter and calling it right 28 times in a row. </p>", 
                "question": "life situations with similar probability of winning the powerball"
            }, 
            "id": "cytwfv8"
        }, 
        {
            "body": {
                "answer": "<p>For a deeper understanding of Statistics you need to know Probability in depth as well. I don<sq>t have a book recommendation but I<sq>d definitely recommend Stat 414/415 by Penn State<colon> https<colon>//onlinecourses.science.psu.edu/stat414/ <br>It has definitions, motivating examples and in depth theory explained very well for your level. Concepts like probability density functions, distributions, expectation, likelihood are not usually discussed in a typical Statistical Inference course but in my opinion they are of fundamental importance to understand the stats behind <sq>significance<sq> and statistical testing. Good luck!</p>", 
                "question": "Book suggestions to get a deeper understanding of stastics?"
            }, 
            "id": "cy4ab30"
        }, 
        {
            "body": {
                "answer": "<p>Wow!  That<sq>s a pretty open-ended question!  It kinda depends on what you mean by <dq>deeper understanding<dq>.<br><br>If you mean, you want to be able to *do* statistics then I think you should focus on a statistics package and begin flinging data around.  Which book then depends upon which package.<br><br>If you mean, you want more advanced applications, I<sq>d have to ask whether you have a particular need in your area.<br><br>If you mean, you want more general theory, I<sq>d suggest you begin with inference and likelihood.  In that case, I<sq>d recommend _In All Likelihood_ by Yudi Pawatan.  It will presume calculus, though, as will any decent inference book.</p>", 
                "question": "Book suggestions to get a deeper understanding of stastics?"
            }, 
            "id": "cy3dwop"
        }, 
        {
            "body": {
                "answer": "<p>I had a great time reading [All of Statistics](http<colon>//www.stat.cmu.edu/~larry/all-of-statistics/)</p>", 
                "question": "Book suggestions to get a deeper understanding of stastics?"
            }, 
            "id": "cy3tit5"
        }, 
        {
            "body": {
                "answer": "<p>Gelman<sq>s Data Analysis Using Regression and Multilevel/ Hierarchical Models.  Very accessible regression book.</p>", 
                "question": "Book suggestions to get a deeper understanding of stastics?"
            }, 
            "id": "cy9s20l"
        }, 
        {
            "body": {
                "answer": "<p>http<colon>//ww1.cpa-apc.org/Publications/Archives/CJP/2003/december/streiner.pdf<br><br>Someone else may be able to give you a better ELI5 type answer.</p>", 
                "question": "p-value question"
            }, 
            "id": "cxgbqv7"
        }, 
        {
            "body": {
                "answer": "<p>You may be needing something more like an equivalence test or a noninferiority test<br><br>http<colon>//www.ncbi.nlm.nih.gov/pmc/articles/PMC3019319/<br><br>http<colon>//www.ncbi.nlm.nih.gov/pmc/articles/PMC2701110/</p>", 
                "question": "p-value question"
            }, 
            "id": "cxgj4wv"
        }, 
        {
            "body": {
                "answer": "<p><Deleted to avoid misinformation></p>", 
                "question": "p-value question"
            }, 
            "id": "cxgc4lr"
        }, 
        {
            "body": {
                "answer": "<p>1) PLEASE don<sq>t say that you are attempting to PROVE anything. If you care at all about science and the truth, then your goal is to learn information about that truth.  Your goal should never be to prove or disprove a thing. Also, except in extremely simplistic cases, statistics cannot PROVE things-  we can find evidence for, or evidence against. (Simplistic case<colon> Hypothesis<colon> There are brown sheep in the US.  We see two.  There you go!)<br><br>2)It may seem odd, but null hypothesis testing is only able to test whether information is consistent with, or inconsistent with a hypothesis.  Such a hypothesis must outline what we expect to observe in a certain state of the world-- therefore the equality part of the Null Hypothesis.  <br><br>3) It is impossible to prove a negative, and damn well hard to **PROVE** a positive as well in many cases. (Simplistic case<colon> There are no brown sheep in the US.  Even after taking hundreds of samples and finding no brown sheep; we have not proven that there are no brown sheep.)  E.g, it is not possible to prove that shampoo *does not* cause cancer-- however, after many repeated trials that *could* find a reasonably-sized increase in cancer risk; if we find no evidence of one, we can be reasonably sure that there isn<sq>t one. <br><br>4) The interpretation of p-values is extremely precise and very contentious since almost no one gets it right.  The simplistic, wrong, but <dq>sort of right<dq> way to interpret a 0.03 is that there is a 3<percent> chance that we made a type 1 error if we rejected.  The more correct way is to say that if we set an alpha of .05 before we started this process, then in repeated (properly done) tests we will have about a 5<percent> chance in the long run of committing type 1 errors.  The p=0.03 is just a decision rule consistent with rejecting in this long-run testing scenario.<br><br>5) And, every time someone interprets a p value for someone as I just did, the wolves descend and start attacking the describer (i.e., me).  Let<sq>s see what happens!<br><br>6) If your feeling is that there *should be* no difference, then collect as much data as you can, attempting to find a difference as honestly and earnestly as possible.  If you come up mostly empty, then you have probably achieved your goal of finding the truth, and that truth is consistent with your hypothesis. <br></p>", 
                "question": "p-value question"
            }, 
            "id": "cxgdkkh"
        }, 
        {
            "body": {
                "answer": "<p>Khan Academy has some really great videos on inferential statistics that really improved my conceptual understanding of statistics.</p>", 
                "question": "Does anyone have a source of help for someone struggling to wrap their head around statistics?"
            }, 
            "id": "cx1oc3c"
        }, 
        {
            "body": {
                "answer": "<p>It<sq>s not clear the kind of guidance you need. If you just need a reasonable introductory book, an older edition of Moore & McCabe<sq>s *Introduction to the Practice of Statistics* might help (the most recent editions don<sq>t seem to be as good; I don<sq>t think Moore is actually involved any more - 5th should be okay IIRC, but I wouldn<sq>t get 7th).<br><br>If you want something different from what that might get you, you may have to be more specific.</p>", 
                "question": "Does anyone have a source of help for someone struggling to wrap their head around statistics?"
            }, 
            "id": "cx1gvvf"
        }, 
        {
            "body": {
                "answer": "<p>[This source](http<colon>//onlinestatbook.com/) seems relevant. It has many examples from psychology. </p>", 
                "question": "Does anyone have a source of help for someone struggling to wrap their head around statistics?"
            }, 
            "id": "cx21kiw"
        }, 
        {
            "body": {
                "answer": "<p>I<sq>d recommend 40. With that level of missingness, the reduction in standard error from more imputations drops off sharply beyond 40.<br><br>The best reference to answer this question is Graham, Olchowski, & Gilreath (2007). DOI 10.1007/s11121-007-0070-9</p>", 
                "question": "31<percent> missing data but it<sq>s MCAR. How do I decide how many Bayesian imputated data sets to create?"
            }, 
            "id": "cx0eir4"
        }, 
        {
            "body": {
                "answer": "<p>As many as you have the patience and resources to run.</p>", 
                "question": "31<percent> missing data but it<sq>s MCAR. How do I decide how many Bayesian imputated data sets to create?"
            }, 
            "id": "cx0arz8"
        }, 
        {
            "body": {
                "answer": "<p>Some experts suggest up to100, but a lot of large government surveys in the US are moving to 20. In the end, it<sq>s a judgment call you have to make based on your data...my personal preference is between 20 and 40. (Sorry no links, but I just was part of a serious debate on imputation strategies for a multimillion dollar project at work, and our missing data experts went through all this and decided on 20.)</p>", 
                "question": "31<percent> missing data but it<sq>s MCAR. How do I decide how many Bayesian imputated data sets to create?"
            }, 
            "id": "cx0hxgp"
        }, 
        {
            "body": {
                "answer": "<p>How many records do you have?<br><br>You<sq>re probably fine with 10 datasets but you may want to run more iterations. </p>", 
                "question": "31<percent> missing data but it<sq>s MCAR. How do I decide how many Bayesian imputated data sets to create?"
            }, 
            "id": "cx0aq83"
        }, 
        {
            "body": {
                "answer": "<p>I can<sq>t imagine taking online stats course unless you have a really good math background.  I could not have completed my statistics courses online.  It was at the PhD level but still,  I found the courses extremely difficult and definitely needed face to face education </p>", 
                "question": "Online Masters programs"
            }, 
            "id": "cwseb13"
        }, 
        {
            "body": {
                "answer": "<p>The University of West Florida offers an online math/stat masters in which you actually attend lectures via computer. I can say that my education there was quite good, but the focus is on the application rather than heavy theory. </p>", 
                "question": "Online Masters programs"
            }, 
            "id": "cwsy7nd"
        }, 
        {
            "body": {
                "answer": "<p>Thanks, I appreciate your input. Sadly on campus studies aren<sq>t possible with my current job. But I do get to do all of the distance education I can handle for free. </p>", 
                "question": "Online Masters programs"
            }, 
            "id": "cwt5oih"
        }, 
        {
            "body": {
                "answer": "<p>There is nothing special about 380 and blanket statements that anything lower is not enough are completely wrong. It is in large part dependent on the dispersion in and distribution of the data considered. My guess is that within a certain field using a particular type of data to test certain types of questions 380 might often be necessary to achieve good statistical power, but these are very specific conditions.</p>", 
                "question": "Response distribution and why 380 for sample sizes?"
            }, 
            "id": "cwg2u5l"
        }, 
        {
            "body": {
                "answer": "<p>Bland-Altman plots are just a visualization technique. The paper you linked warns against naively treating repeated measurements on the same subject as independent, but isn<sq>t saying you shouldn<sq>t use Bland-Altman plots. Do you have a source for the claim that you can<sq>t use them for test-retest reliability? I think plotting the within-subject means of your measurements on the x-axis and the within-subject differences on the y-axis is a fair way to look at test-retest reliability.</p>", 
                "question": "Bland-Altman plots<colon> Double-check with you when to use."
            }, 
            "id": "cwcvipw"
        }, 
        {
            "body": {
                "answer": "<p>>how<sq>s that different from a GLM with the interaction between subjects and the predictor of interest?<br><br>The situation you describe would eat all all degrees of freedom by having subject as a predictor of interest. Additionally, it tests for the wrong question<colon> <dq>Do values vary between subjects?<dq>. This is almost certainly <dq>yes<dq>, and not particularly interesting in and of itself.<br><br>The difference is a random or repeated measure takes within-subject variation into account separately from between subject variation. Random effects do so via a different set of algorithms than the repeated measure. Generally speaking for simple models this ends up being the same thing (i.e. random intercepts but not slopes, assuming equal variances is the same thing as a traditional repeated measures). For more complex models, random effects in mixed models can account for heterogeneity of variance for intercepts and differing slopes for subjects, as well as other potential random effects in the model that are not primary effects of interest (e.g. nesting of subplots in plots). In other words, they not subject to the same limitations as repeated measures models (i.e. only modest departures from sphericity).</p>", 
                "question": "Mixed effects vs. repeated measures"
            }, 
            "id": "cwahw8w"
        }, 
        {
            "body": {
                "answer": "<p>The measurement error is already embedded within your variance of 0.05.  So I<sq>m not sure why you think you need to do this.<br><br>**Here is how you should have done it**<br><br>You can partition out the process variability from the measurement variability.  You would have done this by replicating the measurements on the 100 products and building this into the analysis.  But it sounds like you already missed that opportunity to take replicate measurements.  So your ship, the S.S. Accepted-Method, sailed without you aboard.<br><br>**Here is what you can do now (but, in my opinion, should not do -- see below)**<br><br>You could make certain assumptions about how accurate each measurement was and subtract this from the standard deviation of you results.  But you would need to know more than <dq>my measurement method varies between \u00b10.1.  If you think \u00b10.1 represents \u00b12 SDs in your measurement then your SD is estimated at 0.05 and your measurement variance is 0.0025.  You believe your measurement error is unrelated to your quantity so you get<colon><br><br>s_total^2 = s_process^2 + s_measurement^2<br><br>or <br><br>0.05 = s_process^2 + 0.0025<br><br>or<br><br>s_process = \u221a0.0475 = 0.218<br><br>compared to <br><br>s_total = \u221a0.05 = 0.224<br><br>Not much different.<br><br>**Why I think you should not do this**<br><br>1. The correct way to partition measurement error out was given first above.  The second way is ad-hoc.  It is fine if you want to know whether better measurement might improve your analysis.  Not so fine, otherwise.<br>1. There were a lot of potentially unjustified assumptions in that derivation I just did.  You may be able to justify some of them but all of them?  I assumed your measurement error is not connected to the quantity being measured.  This can be a bad assumption.  Often, bigger measurements have bigger errors.  I assumed that your <dq>varies between<dq> number represented 2 standard deviations.  I have no justification for this other than that those were the numbers you gave me.  Even if your \u00b10.1 value is correct in general, it may not be correct for this study. You get the picture.  <br>1. Measurement error is usually trivial compared to process error (as it is in your case).  It can pay to account for it but the correct way was given first above.<br></p>", 
                "question": "Errors in variance due to known design limitations"
            }, 
            "id": "cw0v1rg"
        }, 
        {
            "body": {
                "answer": "<p>Yes, you can represent the interaction using a [regression surface](http<colon>//www.math.yorku.ca/SCS/spida/lm/visreg.html). This will visualize variation in a dependent variable given simultaneous variation in the independent variables. Unfortunately, I am unfamiliar with whether SPSS is capable of producing such a visualization.<br><br>I have used the <sq>wireframe<sq> function in the R package lattice to do this.</p>", 
                "question": "Graphical presentation of a Multiple Regression Model"
            }, 
            "id": "cvxc9f6"
        }, 
        {
            "body": {
                "answer": "<p>You can use a 3D plot that would create a <dq>blanket<dq> or plane of vertices. But likely you<sq>ll just pick select values for all but one IV and represent the IV<sq>s relationship *[ceteris paribus](https<colon>//en.wikipedia.org/wiki/Ceteris_paribus)* <br><br>EDIT<colon> In that second suggestion you might overlay multiple lines that show different values for your second IV.</p>", 
                "question": "Graphical presentation of a Multiple Regression Model"
            }, 
            "id": "cvx87px"
        }, 
        {
            "body": {
                "answer": "<p>Each judge will have their own way of judging the players, but presumably any given judge<sq>s scale will be consistent across all the players that that particular judge rates.<br><br>Thus, for each judge, you can figure out across all players that the judge rated<colon> the judge<sq>s mean rating score  , the standard deviation for that judge<sq>s scores.  For that judge, you can then turn each player<sq>s score into <dq>z-scores<dq> - how many std deviations each player<sq>s score is above/below the mean.<br><br>Do that for all of the judges, and then you<sq>ll have each judge<sq>s z-score for each player.<br><br>You can average out all of the z-scores for every player across all judges, figure out who the best players are by figuring out which player has the highest average z-score across all judges.<br><br>To figure a comparison out on a state level,  find average the z-scores for the players in each state (e.g. find the average score of all the NJ players, and the average score of all the Texas players), and compare the average z-scores for all the NJ players vs the average z-score for all the Texas players, and see which is higher.</p>", 
                "question": "2 Sample TTest for the Means. Help Please!"
            }, 
            "id": "cvta7i2"
        }, 
        {
            "body": {
                "answer": "<p>1. What is the actual problem you<sq>re trying to solve? Why won<sq>t obvious solution A, B or C work?<br><br>2. Do you need to be able to reanalyze this or similar datasets in the future? Who will be responsible for this?<br><br>3. How was this data collected?<br><br>4. What do each of these columns mean?<br><br>5. Why is some data missing?<br><br>6. What conclusions do you want to be able to draw from this analysis?<br><br>7. What level of data security practices do you need? Anywhere from open source to only working on the company<sq>s secured computer is a possibility.<br><br>8. How much detail can I go into when describing this work to potential employers?<br><br>9. How do you think these variables relate? Why? Are there any pre-existing models in this field?<br><br>That should be a start <colon>)</p>", 
                "question": "Statistical Consulting Questions"
            }, 
            "id": "cvrppna"
        }, 
        {
            "body": {
                "answer": "<p>Just at a cursory glance, it sounds to me that you would want to calculate the kappa across the items BY each variable. Then if you needed to report a single metric, it probably wouldn<sq>t be unreasonable to average the kappa scores across the 14 variables.</p>", 
                "question": "Help with Inter-Rater Reliability (Cohen<sq>s Kappa)"
            }, 
            "id": "cvnyhoh"
        }, 
        {
            "body": {
                "answer": "<p>[Back transformation](http<colon>//www.biostathandbook.com/transformation.html)<br><br>Just be careful about back-transforming standard errors. It can<sq>t be done directly. You<sq>ll need to add the standard error to the mean prior to back-transformation to find the upper SE limit, and subtract it from the mean for the lower limit. Then backtransform these values and subtract the BT mean from the BT upper limit to find your upper SE and subtract the BT lower limit from the BT mean to find your lower SE. </p>", 
                "question": "Descriptives on log-transformed data?"
            }, 
            "id": "cvklhdr"
        }, 
        {
            "body": {
                "answer": "<p>If you were able to grade their performance on a scale from 0-100, ie as a percentage of <sq>perfect<sq> (= some tough but achievable goal), you could look at how much of the gap to perfection they<sq>d closed. So a student starting at 50<percent> and achieving 55<percent> has closed 10<percent> of the gap. A student starting at 90<percent> would only need to achieve 91<percent> to improve just as much.</p>", 
                "question": "Can a PE teacher get some help on developing a grading system?"
            }, 
            "id": "cvjv31m"
        }, 
        {
            "body": {
                "answer": "<p>Do you have any past data (from a gradebook or something) that can tell you what the improvements look like as a function of initial skill? Just going through that and looking at where (using a bench press as an example) the final weight is compared to the initial weight might give you some idea of a relationship.<br><br>That data might be able to lead to a regression that says <dq>this is the average performance for this starting skill<dq>. You could use that to assign grades based on above or below average. For the real stats-y folks, a quantile regression could be used to fit <dq>A<dq>, <dq>B<dq>, etc. grade levels. For simpler purposes, even knowing an average could tell you enough about whether or not someone is putting in enough effort. Measures of error in the regression would also tell you about your student<sq>s natural variability.<br><br><br><br><br></p>", 
                "question": "Can a PE teacher get some help on developing a grading system?"
            }, 
            "id": "cvk1sbr"
        }, 
        {
            "body": {
                "answer": "<p>Use a square root curve. Or really a 1 - Square root curve.  It will increase  lower values more than larger values, might give you what you are looking for.  You could also create a regression of expected improvement based off of initial scores and grade them relative to their expected performance.</p>", 
                "question": "Can a PE teacher get some help on developing a grading system?"
            }, 
            "id": "cvkgylu"
        }, 
        {
            "body": {
                "answer": "<p>I<sq>m not sure how Stata handles these things, but R may run into trouble with this part<colon><br>> I also have an interaction term that I believe has 20,000 potential values, the vast majority of which have no data. There are ~11,000 observations.<br><br>So if there really are 20,000 potential values, you<sq>re going to run into memory issues quickly. Since these are all factors, I suspect R is producing an entirely new column for every possible value of that interaction. Then estimating *beta* requires calculating *x^T X^-1* which is *k x k* where k=20,000. Also, you<sq>re going to run into non-identifiabile design matrix problems if many of your variables are 0 across the board. It makes more sense to construct your design matrix (*X*) carefully beforehand so that you don<sq>t try to calculate the full 20,000 x 20,000 matrix.<br><br>Also, this is a lot to ask of 11,000 data points. Why is a three-way interaction required? What<sq>s the objective? Will you be checking every single interaction term for hypothesis testing purposes? If this is a predictive model, you<sq>re better off simplifying things (or else you<sq>ll run into serious over-fitting problems).<br><br>As for why Stata can do this and not R - I don<sq>t know the answer to that. I suppose it<sq>s possible that Stata is hiding a variable selection step from you that drops the majority of the observations. It<sq>s also possible that Stata falls back on other methods to estimate the model (not solving the system of equations directly). It<sq>s also possible that Stata is simply coded more efficiently.</p>", 
                "question": "R performing 100x worse than stata?"
            }, 
            "id": "cvk1w1q"
        }, 
        {
            "body": {
                "answer": "<p>Are the variables facility_id, etc, factors before you run the model?  <br><br></p>", 
                "question": "R performing 100x worse than stata?"
            }, 
            "id": "cvju07j"
        }, 
        {
            "body": {
                "answer": "<p>You could try using [Revolution R Open](https<colon>//mran.revolutionanalytics.com/open/), also you say 3 categorical variables but there are 4 factors in your formula.</p>", 
                "question": "R performing 100x worse than stata?"
            }, 
            "id": "cvk8hrg"
        }, 
        {
            "body": {
                "answer": "<p>The short answer is that while logistic regression is linear in logit space, you can<sq>t interpret the parameters like that in non-logit space.  That particular trick which is so useful in linear regression doesn<sq>t really apply unless you<sq>re comfortable with odds ratios.<br><br>Imagine a logistic curve.  The slope at extremes will be near zero without much change, while the steepest slope will be in the middle with quite a bit of change when incrementing values.  To interpret the change in slope over your variable, you need to compare between specific values or estimate the derivative at a specific value.  If the main problem is communicating findings to others, I<sq>d recommend simply explaining that \\Beta > 0 means increasing chance (of <dq>success<dq>) with increase in age.  The prediction of the model for specific points is also usually going to be more important than the change in slope over the variable.</p>", 
                "question": "Simple question about logistic regression"
            }, 
            "id": "cvhag57"
        }, 
        {
            "body": {
                "answer": "<p>Did you include an intercept in the model as well? For the sake of the following explanation, let<sq>s assume that the coefficient of the intercept was -1. To calculate the probability of success P(Y = 1) for - say a 20 year old individual - we use the following expression<colon> exp(-1 + 0.03370 * 20) /(1 + exp(-1 + 0.03370 * 20)) = 0.419. Let<sq>s repeat the same process for a 30-year old<colon>  exp(-1 + 0.03370 * 30)/(1 + exp(-1 + 0.03370 * 30)) = 0.503. And so on. The increase in probability of success is not linear, so no general rule about the increase of probability per unit increase can be derived, afaik. You have to calculate the predicted probability of success over a range of ages which is easy with most statistical programs.</p>", 
                "question": "Simple question about logistic regression"
            }, 
            "id": "cvhag08"
        }, 
        {
            "body": {
                "answer": "<p>They *might* be talking about a scaled chi-square (which will be gamma, yes), or they *might* mean some form of noncentral chi-square. You<sq>re a bit light on details -- context would help</p>", 
                "question": "Parameters of common distributions in *Bayesian Data Analysis*"
            }, 
            "id": "cvgnl4q"
        }, 
        {
            "body": {
                "answer": "<p>You sure it<sq>s the chi-squared distribution and not the (EDIT<colon> *scaled*) *inverse* chi-squared distribution? Because the inverse distribution is parameterized by nu and s^2 in BDA3 (p.581).<br><br>If this doesn<sq>t sound right, could you maybe give a concrete example? I have the book so you can even just give a page number, but it would be better (i.e. for people who don<sq>t have the book) if you wrote it out or posted a picture of the relevant text.</p>", 
                "question": "Parameters of common distributions in *Bayesian Data Analysis*"
            }, 
            "id": "cvgkv1l"
        }, 
        {
            "body": {
                "answer": "<p>Why are these sorts of posts always downvoted? It<sq>s a totally valid question.<br><br>Bimodality is a really complicated thing to test for. Look up Hartigan<sq>s dip test for a somewhat rudimentary approach--at least it would be a good starting point. There used to be a bimodality test that uses Hartigan on R, but it has been removed from CRAN<sq>s list for a long time. You can still find the package of it online though, but you<sq>d have to manually install it. <br><br>When I tested for bimodality for part of my Master<sq>s degree, I went through all of this but I was unsatisfied with how much the tests were trying to fit a square peg in a round hole, so to speak. I ended up using visual inspections based on kernel density estimates along with using SD as a proxy measure, rather than a direct test of bimodality. This worked OK, but only worked because of the sort of data I was using. None of the direct tests for bimodality are really any good, IMO.</p>", 
                "question": "test for bimodal circular distribution? (info in comments)"
            }, 
            "id": "cvbsaq5"
        }, 
        {
            "body": {
                "answer": "<p>This is just a wild idea, but why not fit two finite mixtures models, one with one group and another with two, and decide based on fit index, like BIC?</p>", 
                "question": "test for bimodal circular distribution? (info in comments)"
            }, 
            "id": "cvc110n"
        }, 
        {
            "body": {
                "answer": "<p>I have a series of pairs of vectors, and the angle between them. Image shows histogram of the angles showing a bimodal distribution (around 100\u00b0 and - 120\u00b0). What statistical test could give me a p-value getting of my distribution assuming chance alone (which might look something like [this](http<colon>//imgur.com/6Zt1smW)).  <br></p>", 
                "question": "test for bimodal circular distribution? (info in comments)"
            }, 
            "id": "cvbhft0"
        }, 
        {
            "body": {
                "answer": "<p>I think you can run a repeated measures ANOVA, although I<sq>ve never tried it with a within-subjects factor with that many levels (50 temperature points). You would then have gender as the between-subjects factor, and if that main effect were significant, you could conclude that there is a difference in the happiness ratings for males and females across different temperatures. </p>", 
                "question": "Help with choosing appropriate test repeated measures ANOVA?"
            }, 
            "id": "cvar9mz"
        }, 
        {
            "body": {
                "answer": "<p>Absolutely no way would you should run all those independent t-tests alone. At the very least, it would require some very conservative family-wise correction.<br><br>This is a classic repeated measures test with one within subject factor (temperature) and one between-subject factor (sex). Whether or not you care about happiness and temperature you must account for it. It sounds like your boss knows what to do.</p>", 
                "question": "Help with choosing appropriate test repeated measures ANOVA?"
            }, 
            "id": "cvba3a8"
        }, 
        {
            "body": {
                "answer": "<p>I believe repeated measures is the right test for you. I<sq>m doing a similar analysis right now for ~6700 data points (16 subjects, 400 data point each) and this the analysis my professor chose for us.<br>One point worth mentioning is that I don<sq>t have any between subject variables (both my variables are within-subject) but I think repeated measures is still the way to go for you.<br></p>", 
                "question": "Help with choosing appropriate test repeated measures ANOVA?"
            }, 
            "id": "cvb644f"
        }, 
        {
            "body": {
                "answer": "<p>You want P(A and B and C). Unfortunately, you cannot calculate it unless you can assume that A, B and C are completely independent, which seems unlikely. </p>", 
                "question": "If our surveys say 68<percent> of customers will buy item A 54<percent> would buy item B and 44<percent> would buy item C is there a way to calculate what <percent> would be purchasing all three?"
            }, 
            "id": "curxpur"
        }, 
        {
            "body": {
                "answer": "<p>If you asked it on the same survey, can you revisit the original data, or can you link the data together? Like others said, it doesn<sq>t seem that it would be possible any other way.</p>", 
                "question": "If our surveys say 68<percent> of customers will buy item A 54<percent> would buy item B and 44<percent> would buy item C is there a way to calculate what <percent> would be purchasing all three?"
            }, 
            "id": "cus2yim"
        }, 
        {
            "body": {
                "answer": "<p>Not without additional information or assumptions.<br><br>e.g. you could calculate it if you assumed the three buying events are independent, but there<sq>s no good reason whatever to assume that (I expect it<sq>s not close to true).</p>", 
                "question": "If our surveys say 68<percent> of customers will buy item A 54<percent> would buy item B and 44<percent> would buy item C is there a way to calculate what <percent> would be purchasing all three?"
            }, 
            "id": "cusbqgq"
        }, 
        {
            "body": {
                "answer": "<p>At a minimum, 0<percent>.<br><br>Pretend you have 100 people named 1-100<br><br>1-68 will buy item A<br><br>1-54 will also buy item B<br><br>56-100 will buy item C<br><br>No one bought all 3.<br><br>At a maximum, 44<percent>.</p>", 
                "question": "If our surveys say 68<percent> of customers will buy item A 54<percent> would buy item B and 44<percent> would buy item C is there a way to calculate what <percent> would be purchasing all three?"
            }, 
            "id": "curwdkl"
        }, 
        {
            "body": {
                "answer": "<p>It depends on what your hypothesis is, what the research question is, what the study was about, the study design, what covariates were accounted for, etc...basically there<sq>s no way to answer that question without a lot more detail.</p>", 
                "question": "A study is performed in two countries; only when you add the data from both countries together are the findings statistically significant; is this ok?"
            }, 
            "id": "cuou719"
        }, 
        {
            "body": {
                "answer": "<p>You can reject the null hypothesis or fail to reject the null hypothesis; you cannot accept a hypothesis.  </p>", 
                "question": "A study is performed in two countries; only when you add the data from both countries together are the findings statistically significant; is this ok?"
            }, 
            "id": "cuou76q"
        }, 
        {
            "body": {
                "answer": "<p>Meta-analysis is a standard procedure for combining separate study results to obtain a larger sample size and therefore tighter confidence intervals. But it<sq>s ideally not done on an ad hoc basis, it<sq>s easy to cherry pick data which trend one way and ignore equally relevant data which trends the other.<br><br>There<sq>s nothing inherently wrong with it if the studies are reasonably homogeneous in terms of the question asked and populations studied but interpretation is affected by how those studies came to be combined. <br><br>Meta-analysis is often hugely important in picking up effects which would otherwise go unrecognised. [Example<colon> cot death](http<colon>//m.ije.oxfordjournals.org/content/34/4/874.full)<br><br>So, know what question you<sq>re asking and how rigorously you<sq>ve collected data to answer it. Interpretation depends heavily on how you came to combine the data.</p>", 
                "question": "A study is performed in two countries; only when you add the data from both countries together are the findings statistically significant; is this ok?"
            }, 
            "id": "cup26do"
        }, 
        {
            "body": {
                "answer": "<p>Does it have to do with having more power in the combined set because you have more replications?</p>", 
                "question": "A study is performed in two countries; only when you add the data from both countries together are the findings statistically significant; is this ok?"
            }, 
            "id": "cup1vq9"
        }, 
        {
            "body": {
                "answer": "<p>Logistic regression. You<sq>ll need more than Excel.<br><br>You can test the effect of one variable at a time using the Mantel-Haenszel approach which can be done in Excel. See<colon> http<colon>//sphweb.bumc.bu.edu/otlt/MPH-Modules/BS/BS704-EP713_Confounding-EM/BS704-EP713_Confounding-EM8.html</p>", 
                "question": "How do I test the effects of several variables on a 0 or 1 binary outcome?"
            }, 
            "id": "cuo2jt2"
        }, 
        {
            "body": {
                "answer": "<p>Multivariate Logistic Regression.<br><br>It<sq>s possible to [do it in Excel](https<colon>//www.google.com/search?q=logistic+regression+in+excel), but you pretty much have to implement all of the math yourself.  Doing it in R is very easy.<br><br>    fit <- glm(outcome ~ var1 + var2 + var3, data = mydata, family = <dq>binomial<dq>)<br>    summary(fit)</p>", 
                "question": "How do I test the effects of several variables on a 0 or 1 binary outcome?"
            }, 
            "id": "cuo82k8"
        }, 
        {
            "body": {
                "answer": "<p>A common solution would be logistic regression. Google it to learn more, but here<sq>s an applied walk through in R<colon> http<colon>//www.ats.ucla.edu/stat/r/dae/logit.htm</p>", 
                "question": "How do I test the effects of several variables on a 0 or 1 binary outcome?"
            }, 
            "id": "cuo943y"
        }, 
        {
            "body": {
                "answer": "<p>Why not use a Chi-Square test on the crosstab? Is the default option to analyze two dichotomous variable (cancel/no cancel vs invoice /no invoice).   </p>", 
                "question": "How to interpret A/B test results when populations may have underlying differences?"
            }, 
            "id": "ctyqw1o"
        }, 
        {
            "body": {
                "answer": "<p>Firstly, why are these groups split in the first place? Do you intend to do something different with one of them?<br><br>Secondly, I don<sq>t understand how you can have a <dq>rate<dq> for a person. As far as I can tell this is a binary condition - cancel or no cancel. Which means a z-test isn<sq>t appropriate. </p>", 
                "question": "How to interpret A/B test results when populations may have underlying differences?"
            }, 
            "id": "ctytjsu"
        }, 
        {
            "body": {
                "answer": "<p>I assume the design is the standard A/B marketing model where you start giving group A something before you give it to group B. I haven<sq>t ever done one myself but I don<sq>t think a between-groups comparison is what<sq>s usually expected (comparing A after the change to B before the change).  What you want is within-groups comparisons (A before the change to A after the change; B before the change to B after the change) because this will automatically control for differences in churn between the two groups.</p>", 
                "question": "How to interpret A/B test results when populations may have underlying differences?"
            }, 
            "id": "ctyvdb7"
        }, 
        {
            "body": {
                "answer": "<p>Ideally your control and test SHOULD have the same properties to run an AB test like this. You say there is no reason they should be different, but go on to list a reason they could be. <br><br>Seasonality should be no reason an AB test can<sq>t be run. You<sq>re comparing change in churn rate between the two, not a static number. <br><br>I see two options.  Both involve two steps<colon> running the test, find the one with the biggest change in churn (with confidence intervals); and<br> finding that difference in your sample. Are there more men in one group? If so, do the men see a higher effect of the test than women? If you don<sq>t care about observing these smaller segments, then at least use the segments you find to re-sample your population. If you do care, getting into even more precise segments of your customer base to determine effect of marketing efforts is a pretty common technique and may help take your test even further.<br><br>This may not answer your question since I don<sq>t prescribe an actual test to run here which seems to be what you<sq>re looking for. It is true that you may  not have the data underlying the actual reason for the different in churn rate you<sq>ve found, in which case I would just shoot for a higher confidence interval in the difference between the means of your AB test. If you have any specific questions about any part of this comment let me know. I<sq>m interested as well since I do this sort of thing pretty frequently.</p>", 
                "question": "How to interpret A/B test results when populations may have underlying differences?"
            }, 
            "id": "cu15hoz"
        }, 
        {
            "body": {
                "answer": "<p>[deleted]</p>", 
                "question": "Operator precedence in probabilities?"
            }, 
            "id": "ctled85"
        }, 
        {
            "body": {
                "answer": "<p>This sounds like a very complex question. What is this for? <br><br>Based in your description, you probably need a coauthor who does<br>statistics or a lot of training.</p>", 
                "question": "Beginning question regarding how to analyze ecological data sets!"
            }, 
            "id": "cth4qiu"
        }, 
        {
            "body": {
                "answer": "<p>Sounds like you are going to want to set up some indicator variables for the presence of animals at the site (1 for the presence of the animal, 0 otherwise), some indicator variables for the site and then performed a multiple regression on these variables and time. The problem you are going to encounter is the number of variables; judging from your comment you have a lot of variables to consider (the site, the time, the animals present) and if your sample size is not large enough, you might overspecify the model (keeping in mind you are probably going to include some cross-terms that elucidate the interaction between the site and the animals present). <br><br>Assuming this is a surmountable problem, you<sq>ll then want to look at the auto-correlation of the residuals for your model to check for any GARCH effects. If not, then you can just look at the t-stats for each coefficient of the model to determine its efficacy. Step-wise regression will help to eliminate any possible correlation between the variables and prevent your model from becoming inaccurate. </p>", 
                "question": "Beginning question regarding how to analyze ecological data sets!"
            }, 
            "id": "cth60ed"
        }, 
        {
            "body": {
                "answer": "<p>I think a little more detail as to what you<sq>re trying to understand might be helpful.</p>", 
                "question": "ELI5<colon> The Bland-Altman analysis (statistics)"
            }, 
            "id": "ctgai1r"
        }, 
        {
            "body": {
                "answer": "<p>I<sq>m not sure that there is even a question here, but Bland-Altman plots (also Tukey<sq>s mean difference) are pretty straight forward. I<sq>d be happy to comment if you have specific questions.<br><br>The Wikipedia article has all the info you need.</p>", 
                "question": "ELI5<colon> The Bland-Altman analysis (statistics)"
            }, 
            "id": "ctgdppl"
        }, 
        {
            "body": {
                "answer": "<p>Basically, a B-A plot is just a scattergram that has been reorganized to be easier to read.  If you have two machines that measure body weight (a digital scale and an old slide scale) and you measure the body weight of 100 people, they are both measuring the same thing so they should get the same weight.  But they won<sq>t quite.<br><br>One thing you could do is plot the digital measurement on the y-axis and the slide-scale measurement on the x-axis and you would expect to see the observations cluster around the 45 degree line.  To the extent that the observations don<sq>t cluster around that line, they differ.<br><br>But our eyes are better at seeing vertical differences or horizontal differences than 45-degree differences.  The B-A plot uses the fact that telling you the <dq>average<dq> and the <dq>half-difference<dq> between two points is the same as telling you the two points.  And when you plot these, they follow a <dq>flat<dq> line.  The deviations will be vertical and you can even put a <dq>band<dq> around the line indicating points that are unusually far away form chance explanation.  Points that fall outside the bands should be examined to see if there was something unusual that caused the disagreement -- to increase understanding of what is being measured, see?</p>", 
                "question": "ELI5<colon> The Bland-Altman analysis (statistics)"
            }, 
            "id": "cti4pi0"
        }, 
        {
            "body": {
                "answer": "<p>Are you talking about mixed effect models?</p>", 
                "question": "Post hoc ANOVAs for mixed model"
            }, 
            "id": "ct8vb1s"
        }, 
        {
            "body": {
                "answer": "<p>You should add, within the same analysis, the appropriate contrasts/estimate statements to test the hypotheses WITHIN the analysis that you ran.  Do not split the analysis apart and run marginal analyses.</p>", 
                "question": "Post hoc ANOVAs for mixed model"
            }, 
            "id": "ct998zz"
        }, 
        {
            "body": {
                "answer": "<p>It<sq>s easy but you need to use the emmeans command through syntax in SPSS. You are unable to select post-hoc effects for interactions through the GUI menus. DV = dependant variable, IV=independant variable<br><br><br>    MIXED DV BY IV1 IV2<br>      /CRITERIA=CIN(95) MXITER(100) MXSTEP(10) SCORING(1) SINGULAR(0.000000000001) HCONVERGE(0,<br>      ABSOLUTE) LCONVERGE(0, ABSOLUTE) PCONVERGE(0.000001, ABSOLUTE)<br>      /FIXED=IV1IV2 IV1*IV2 | SSTYPE(3)<br>      /METHOD=REML<br>      /PRINT=CPS CORB COVB G LMATRIX R SOLUTION TESTCOV<br>      /random= intercept  | subject(sp) COVTYPE(VC)<br>      /EMMEANS=TABLES(IV1) COMPARE ADJ(LSD)<br>      /EMMEANS=TABLES(IV2) COMPARE ADJ(LSD)<br>      /EMMEANS=TABLES(IV1*IV2)  compare (IV2) adj (lsd).<br><br>After adj in brackets it says LSD - this fishers least significant difference. It does not adjust for afamily-wise errors. If you wish to apply this correction, I<sq>d suggest using SIDAK instead of LSD.<br><br>The only issue you may find is that your SEs for these will be identical for between subject groups because it assumes homogeneity of variance. </p>", 
                "question": "Post hoc ANOVAs for mixed model"
            }, 
            "id": "ct9mz18"
        }, 
        {
            "body": {
                "answer": "<p>I have an update<colon><br><br>My former graduate school advisor found two articles that showed how a Bonferroni correct could be used in a multi-step regression approach. The two articles can be found [here](http<colon>//www.ncbi.nlm.nih.gov/pubmed/20238160) and [here](http<colon>//www.ncbi.nlm.nih.gov/pubmed/2071728).<br><br>In both cases they followed these general rules<colon><br><br>1. They perform the corrections only within each step, not across steps<br>2. They do not perform the corrections on their covariate step (usually step one)<br>3. They do not perform a correction on their last step **IF** said step only contains one variable. If the last step contains anything more than a single interaction term or a single main effect, the correction is performed.<br>4. This is more of a 3.5, but if they have a step four (usually reserved for a three-way interaction) they would perform the correction on step three of the regression which would contain additional main effects and interaction terms. However, rule #3 still applies if there is anything more than just a single term in the last step.<br><br>What does this mean for my results? It means I only run a Bonferroni correction on my step two variables using a new p value of p = .05/2, or .025.<br><br>I hope this helps anyone who stumbles upon this with a similar problem! <br></p>", 
                "question": "Reviewer Wants a Bonferroni Correction in a Multi-Step Regression? Unsure how to Apply..."
            }, 
            "id": "ct6p813"
        }, 
        {
            "body": {
                "answer": "<p>I come from an Undergrad Psychology background so forgive me if I<sq>m wrong. <br><br>Using multiple T-Tests accumulates family wise error. With each t-test you run you account for a certain <percent> of error correct? Well all that error from each test adds up. For example, you set you confidence level as 95<percent>? You have 5 t-tests to compare your groups? For the first comparison you<sq>re saying there<sq>s a 5<percent> chance you made an error. For the second comparison you<sq>re saying there<sq>s a 5<percent> chance you<sq>ve made an error, and so on so forth. So you<sq>ve done 5 t-tests, each at a 95<percent> confidence level. You<sq>ve got 5<percent> of a chance you made an error on each of those tests. Now you<sq>ve got 25<percent> chance of error for your entire case. <br><br>ANOVA allows you to compare 2+ groups/variables with one test. It will keep your confidence level at where you set it in the first place without accumulating more error due to more than one test being run. <br><br>Honestly whenever I had to compare 2+ variables, ANOVA was way less work than writing out every t-test for each comparison. </p>", 
                "question": "Help with one-way ANOVA relevance"
            }, 
            "id": "ct4ruco"
        }, 
        {
            "body": {
                "answer": "<p>u/v5ash5v nailed the idea. Here<sq>s a little more context for practice<colon><br><br>The experiment-wise error rate for testing multiple hypotheses is<colon><br><br>    1-(1- original \u03b1)^x = real \u03b1<br><br>\u03b1 is significance and x is the number of comparisons.<br><br>So if OP runs 10 t-tests at .05 significance, the chance of one of those tests producing a false positive is<colon><br><br>    1-(1-.05)^10 = .4012631<br><br>EDIT<colon> Instead of .05, \u03b1 is actually .4, meaning a 40<percent> chance of false positive.  To adjust \u03b1 for multiple hypothesis tests, there<sq>s the Bonferroni Correction, which looks like this on the experiment-wise equation<colon><br><br>    1-(1-\u03b1)^(1/x) = \u03b1<br><br>    1-(1-.05)^(1/10) = 0.005<br><br>We<sq>re really just dividing \u03b1 by the number of comparisons, so the equation for the adjusted \u03b1 is<colon><br><br>    \u03b1/x = adjusted \u03b1<br><br>    .05/10 = .005<br><br>If OP goes back to the original t-tests, significance should be gauged at the .005 level. But really an ANOVA with a Tukey adjustment saves a lot of work.  The problem with a Bonferroni Correction is that it limits Type I errors, false positives, but it<sq>s usually described as a pretty conservative method-- meaning it increases Type II errors, false negatives.  An ANOVA with post-hoc tests will adjust for Type I errors without increasing Type II errors drastically.<br></p>", 
                "question": "Help with one-way ANOVA relevance"
            }, 
            "id": "ct5dqh8"
        }, 
        {
            "body": {
                "answer": "<p>Great answers here already and I concur, you need to do an ANOVA and use family-wise error correction for post-hoc tests (e.g. sidak - Bonferroni is too conservative). But one thing that hasn<sq>t been addressed is the heteroschedasticity. My concern is that you are getting highly significant results because of a few very high or low values (hence the heteroschedasticity).  What type of data is it? Count or continuous? Count data often follows a poisson distribution, which means it either needs to be normalised through transformation (e.g. log transformation) or you need to use a non-parametric test equivalent (e.g. Kruskal\u2013Wallis). Continuous data can also be transformed if heteroschedastic to help shorten up those long right hand tails.<br><br>Normality isn<sq>t a big concern, but homogeneity of variance is. MAke sure your data meet assumptions for ANOVA because otherwise the results might be skewed.</p>", 
                "question": "Help with one-way ANOVA relevance"
            }, 
            "id": "ct5i93a"
        }, 
        {
            "body": {
                "answer": "<p>The chi square is designed to run frequency analyses on categorical data. Doing to the 1/0 dummy variable route produces a categorical (in this case, binary) value for each of those variables for each participant.<br><br>Of course, if the variable in question is worded such that coding it as 1-5 scale data would also make sense, why be pre-committed to the chi square? That would leave you with a continuous value for each participant, and put you in t-test/ANOVA/correlation land (depending on what other data you<sq>re working with)</p>", 
                "question": "Question about Chi square tests"
            }, 
            "id": "ct2vgi4"
        }, 
        {
            "body": {
                "answer": "<p>So, piggybacking on OP<sq>s post, to ask a question about the next step, presenting count data analyzed by chi square. To graphically show the number of responses for each category, I<sq>ve seen bar charts used most commonly with percentages indicating the frequency in the y axis.  <br><br>So my question<colon> say the response variable is actually continuous but has been coded to form <dq>categories<dq> (in this case, bins), which were used for an experiment.   Would it be wrong to include error bars in the y axis for each bar?<br><br>edit<colon> Nevermind, I found that yes, it is possible to do so, [here is an example](http<colon>//www.biostathandbook.com/chiind.html)</p>", 
                "question": "Question about Chi square tests"
            }, 
            "id": "ct4uf0t"
        }, 
        {
            "body": {
                "answer": "<p>When you code open-ended responses, that<sq>s pretty much a description of qualitative research.  I think you<sq>re trying to pound a square peg into a round hole because you<sq>re unfamiliar with qualitative research methodology, but you really should look into that more.</p>", 
                "question": "Question about Chi square tests"
            }, 
            "id": "ct2vgj9"
        }, 
        {
            "body": {
                "answer": "<p>Others have spoken about the validity of the t-test in this case with regards to sample size. I<sq>d like to say a few things about internal validity.<br><br>Internal validity is the extent to which a study measures the actual effect that you want to measure. For example<colon> If you want to measure whether extra tutoring causes better educational outcomes and just do a simple comparison between kids that receive extra tutoring and kids who do not, you<sq>d most likely find that more tutoring is associated with worst test scores - because tutoring is likely to be allocated to struggling kids who already have very low test scores.<br><br>In your case, let<sq>s say you<sq>ve got a really shitty ad campaign. So shitty, in fact, that it actually lowers sales. Let<sq>s also say that your company is selling Christmas decorations. You start the ad campaign in the beginning of November, and you have sales data from the end of each month. You find that the sales in the end of November is 200<percent> higher than the sales in the end of October. Since people buy more christmas ornaments in November anyway, you<sq>d expect a sales spike even without the ad campaign.<br><br>Ideally, you<sq>d want to look at a large sample of similar companies that instituted this ad campaign and compare them to a large sample of similar companies that did. In the absence of that, you could compare October and November in the year that you instituted the campaign versus October and November in the preceding year.</p>", 
                "question": "Question about using t-test to measure effectiveness of an advertising campaign."
            }, 
            "id": "ct40lxj"
        }, 
        {
            "body": {
                "answer": "<p>I don<sq>t think you should use a t-test here since you don<sq>t have enough data to make a valid appeal to CLT. As far as how to compare groups go<colon> you should compare the group means by testing the hypothesis that there is no significant difference in the mean sales for the before group compared to the after group. You could also do one sided tests under a hypothesis that one mean is greater than the other. If you did use a t-test (you shouldn<sq>t), Welch<sq>s t-test would be more appropriate as it allows there to be a significant difference in variance between the two groups. All in all, it might be better to use another test - possibly a permutation test - or measure the effect in a different way altogether.</p>", 
                "question": "Question about using t-test to measure effectiveness of an advertising campaign."
            }, 
            "id": "ct2nnt1"
        }, 
        {
            "body": {
                "answer": "<p>I just did a quick t-test and there is no significance in the difference between the means. However as SinnyCal mentioned, your sample size is just too small. The higher your N, the higher the chance of rejecting the H0 with a t-test. <br><br>Another problem are independent observations. Profits usually vary month to month, take december as an example. Maybe you could compare it to the profits made from april to june in the last year? Also take a look at internal validity and see if there might be other variables effecting your dependent variable.</p>", 
                "question": "Question about using t-test to measure effectiveness of an advertising campaign."
            }, 
            "id": "ct2pd0a"
        }, 
        {
            "body": {
                "answer": "<p>if it was normally distributed you could find the probability of the frequency of the port being accessed.</p>", 
                "question": "Stupid stats question"
            }, 
            "id": "csx45ah"
        }, 
        {
            "body": {
                "answer": "<p>Sort of, but not really.<br><br>In principle, if you have a large sample of observed ports you could take their mean and standard deviation and use those as the parameters of your assumed Normal distribution. This is all fine.<br><br>The problem is the Normal distribution is what<sq>s known as a continuous distribution, which means the probability of any given value is exactly zero. If you still wanted to assume normality (which doesn<sq>t really make sense because of this, but let<sq>s roll with it for a moment), then you can only calculate probabilities on *intervals* e.g. Pr(100 < port number < 110).</p>", 
                "question": "Stupid stats question"
            }, 
            "id": "csxiwwq"
        }, 
        {
            "body": {
                "answer": "<p>There is no rule.  You say that the question is not always applicable.  If it is inapplicable to 60<percent> of the respondents then 40<percent> answering it is 100<percent> response.  <br><br>But in a survey, it is highly doubtful that you can assume these observations are missing-completely-at-random (MCAR) or even missing-at-random (MAR) so I don<sq>t think imputation is applicable anyway (and I don<sq>t like it, generally, but that<sq>s my bias).<br><br>I think maybe your instrument needs work to avoid this dilemma.</p>", 
                "question": "Simple question on multiple regression"
            }, 
            "id": "cstey3m"
        }, 
        {
            "body": {
                "answer": "<p>If some questions aren<sq>t applicable to all respondents, then you want to step back and do separate analyses for subgroups.<br><br>For example, if one of your questions is # of years of marriage, but not all respondents are married, then you<sq>d do a separate analysis just for the subgroup of married respondents in which you assess the effect of that variable.</p>", 
                "question": "Simple question on multiple regression"
            }, 
            "id": "csu3e7i"
        }, 
        {
            "body": {
                "answer": "<p>A t-test is not appropriate as a test of two binomial proportions.<br><br>In large samples (which you should have no difficulty with in A/B testing), a chi-square test can be appropriate, though there are some other possibilities.<br><br>If your sample sizes are large enough and the proportions aren<sq>t too small, you can use a normal approximation (which will give a z-test).<br><br>There<sq>s no good justification for using a t-test over a z-test in this situation.<br><br>[Edited this to clarify what I was having a go at, and cut the ranting down to size]  <br>The fact that that Quora post got 12 upvotes and no downvotes suggests you should treat upvotes on that site as fairly meaningless. It<sq>s an object lesson in why you should not go to a marketing professor for stats advice -- any more than you should come to me for tax advice, or go to my tax accountant for legal advice or go to my lawyer for marketing advice.<br><br></p>", 
                "question": "T-test vs chi-square test for a/b testing"
            }, 
            "id": "csrrr8o"
        }, 
        {
            "body": {
                "answer": "<p>You get an interaction effect when the effect of one variable depends on the level of another other variable. In your case, it sounds like the effect of rain on the duration on a bicycle trip is dependent on the city. <br><br>e.g. suppose the average bike trip was 10 minutes for both cities. With no interaction (and assuming all other variables constant or accounted for), rain would increase the duration of the trip by say... +5 minutes, resulting in a 15 minute trip in both cities under rainy conditions. With interaction, the effect of rain could increase the duration of the trip in DC by 5 minutes but maybe because the roads in NYC have a higher concentration of oils and whatnot (resulting in slicker roads under wet conditions), the effect of rain on the duration is +10 minutes instead.</p>", 
                "question": "How to interpret interaction between 2 categorical variables in a regression model"
            }, 
            "id": "csrniau"
        }, 
        {
            "body": {
                "answer": "<p>As a side note, most statistical software allows you to specify that a variable is catgorical and makes up the dummy variables for you implicitly. This can be a <dq>significant<dq> time saver. </p>", 
                "question": "How to interpret interaction between 2 categorical variables in a regression model"
            }, 
            "id": "css1hv3"
        }, 
        {
            "body": {
                "answer": "<p>This may not be super helpful, but off the top of my head, measures of entropy and/or information gain come to mind.</p>", 
                "question": "Measuring concencentration"
            }, 
            "id": "cso40yq"
        }, 
        {
            "body": {
                "answer": "<p>Hmm this is going to be complicated by that covariance. Do you know that the grid is completely regular? I will type up some thoughts later.</p>", 
                "question": "What type of clustering algorithm should I use for clusters spaced on a grid?"
            }, 
            "id": "csnlqm0"
        }, 
        {
            "body": {
                "answer": "<p>To be frank, all of these analyses are somewhat on the more advanced side. I know you don<sq>t want to hear this, but I<sq>d strongly advice you take some time to familiarise yourself with the basics first, because it will be difficult for you to effectively use these methods without that understanding. It<sq>s not that you cannot understand the tests in principle, but you won<sq>t really understand whether the results are meaningful or not and why or do any troubleshooting (and there will almost certainly be a lot of that).<br><br>I took a spatial stats course during my PhD without much background. I could understand what we were looking at in broad strokes, but I was definitely hampered by my lack of fundamental understanding of descriptive and inferential statistics. Once I began to analyse my data in my PhD I spent months learning these stats. I<sq>ve come to be fairly knowledgeable for someone with little formal education on the topic but I<sq>m still a pleb here.<br><br>[K-means clustering](https<colon>//en.wikipedia.org/wiki/K-means_clustering) is a test to looks for spatially clustered data in euclidean space with similar means.<br><br>[Linear Discriminate Analysis (LDA)](https<colon>//en.wikipedia.org/wiki/Linear_discriminant_analysis) is kind of a reverse ANOVA, using a continuous independent variable to predict a categorial dependent variable.<br><br>[Multivariate analysis of variance (MANOVA)](https<colon>//en.wikipedia.org/wiki/Multivariate_analysis_of_variance) is a type of ANOVA that considers variance-covariance relationships between multiple dependent variables when determining if means are significantly different for an independent variable. <br></p>", 
                "question": "Need to understand classification statistics for dissertation- k-means cluster LDA MANOVA"
            }, 
            "id": "csia9zr"
        }, 
        {
            "body": {
                "answer": "<p>Until someone better gives an answer I<sq>ll have a go. This certainly isn<sq>t my area of statistics. I<sq>m not really sure what your covariates are, but given you<sq>ve done PCA, you must have a few and then you have a group, sex. <br><br>K-means<colon> You tell the computer how many groups you want and it will try to assign groups to each observation based on the covariates. It doesn<sq>t know anything about the data, just which points are closer to each other. You would then compare the group assigned by k-means to the known sex and see if it could separate them. (First thought is that there would be k=2 for male/female, but it could be that there are some distinct subgroups, so you might try with more.) This is good if you want to try to see if there are groups based on shape in the data and if they match to the sex.<br><br>LDA<colon> You tell the computer about the covariates and the sex of a training set of your observations and then it will tell you a straight line (or plane) based on your covariates which best separates them. You would then test with the rest of your observations which side of the line they are on and see if it gets it right. This is good if you know that they are dimorphic and you want to create a rule to determine the sex.<br><br>MANOVA<colon> Without knowing about your data, I<sq>m not sure what to say about MANOVA. You can test whether there is a relationship between the covariates and sex.<br><br>I would probably start with plotting your data based on the first two components of the PCA, with the sexes in different colours. If you can see that there is some kind of pattern or separation, then you<sq>re off to a good start!</p>", 
                "question": "Need to understand classification statistics for dissertation- k-means cluster LDA MANOVA"
            }, 
            "id": "csijbyy"
        }, 
        {
            "body": {
                "answer": "<p>You<sq>re right to be concerned - what you<sq>re looking to do is account for dependence based on repeated measurements of the same subject. If all you are looking for is whether there was a significant change in pre to post test values, then a paired t-test will suffice. This will generalise results across all factors. However, if you believe that different factors such as social workers or programs will affect the results, then these can be considered by including them as a either fixed or random factors in a general linear model or mixed model.<br><br>If you have a direct interest in evaluating differences between levels of these factors (i.e. program 1 vs program 2 vs program 3), then you would include program as a fixed factor in wither a GLM or a MM. If you do not have a direct interest in the differences but simply wish to account for the effect of program on the results, you would include it as a random factor in a MM.<br></p>", 
                "question": "Clustering standard errors for a t-test?"
            }, 
            "id": "cshe9tw"
        }, 
        {
            "body": {
                "answer": "<p>There may be a simpler way to go, but one option would be to use bootstrapping.</p>", 
                "question": "Error bars (SEM etc) on percentage data?"
            }, 
            "id": "csfvtjx"
        }, 
        {
            "body": {
                "answer": "<p>I hope I understood the question fully, but the standard error of a percentage is sqrt((p*(1-p))/n). n would be 20<sq>000 in this case. Further, SEM stands for <dq>standard error of the mean<dq> but you have no mean here.<br><br>You say <dq>I carried this out three times<dq>. What exactly did you carry out three times? You took 3 times 20<sq>000 cells on each day?</p>", 
                "question": "Error bars (SEM etc) on percentage data?"
            }, 
            "id": "csfwzu2"
        }, 
        {
            "body": {
                "answer": "<p>I would just do it the typical way, as you described<colon> take the mean of the three proportions (within a treatment, across the the replicates), and calculate sem from the sd divided by sqrt of 3-1.<br><br>It<sq>s probably normal enough for that as long as the percentage isn<sq>t near 0 or 100.<br><br>Alternatively, if you know for sure the number of cells in each treatment is the same each rep, you could just report the mean *number* of dead cells, so you don<sq>t worry about percentages.</p>", 
                "question": "Error bars (SEM etc) on percentage data?"
            }, 
            "id": "csgk8vr"
        }, 
        {
            "body": {
                "answer": "<p>https<colon>//twitter.com/researchmark/status/531937183842979840</p>", 
                "question": "Is this a violation of Chi Square assumptions?"
            }, 
            "id": "cs03jxm"
        }, 
        {
            "body": {
                "answer": "<p>depends on the binary question and the participants... within a given point in time the answers to the Q may be independent, so chi^2 could be fine.<br><br>because of correlation between time-steps though, repeating the test of significance for each time-step gives us less new information (the new information comes instead in the form of knowledge about how attitudes changed throughout a day) than it would if they were 3 independent groups.<br><br>at least that is the way I<sq>m thinking about it currently... I may be misunderstanding, though.</p>", 
                "question": "Is this a violation of Chi Square assumptions?"
            }, 
            "id": "cs0460x"
        }, 
        {
            "body": {
                "answer": "<p>While I don<sq>t know of anything exactly like what you<sq>re looking for, here are a couple of ideas you might try<colon><br><br>* clustering without the variable in question to see how it changes the results<br>* train a classifier to the clusters at whatever layer in the hierarchy is of interest, find the best fitting model, and use that model to give you some sort of idea of a variable<sq>s contribution -- like the variable importance metric in random forests</p>", 
                "question": "After hierarchical clustering is there a way to figure out how much each of the variables contributions to the clustering process?"
            }, 
            "id": "crusdnb"
        }, 
        {
            "body": {
                "answer": "<p>I like Luonnon<sq>s suggestions.  A more basic approach is to just look at variable averages by cluster, using the same z-score transformed data used in the input.  This will give you a summary like <dq>cluster 1 is high in variable A and low in variable B.<dq> This will give you an idea of what<sq>s driving each individual cluster, so you won<sq>t have to run multiple models.  </p>", 
                "question": "After hierarchical clustering is there a way to figure out how much each of the variables contributions to the clustering process?"
            }, 
            "id": "crvcvwa"
        }, 
        {
            "body": {
                "answer": "<p>So you<sq>ve already done a good job of identifying the problem (kappa is low because of of low prevalences) and finding the literature proposing a solution to this problem (I don<sq>t have full text access, but the abstract for your second article is specifically discussing alternative measures to work around this <dq>paradox<dq>). What else are you hoping we can answer here?</p>", 
                "question": "Low Kappa Statistic yet High Agreement in Data Set - what do I do?"
            }, 
            "id": "crm7rjw"
        }, 
        {
            "body": {
                "answer": "<p>If the golden tickets are truly distributed randomly, it wouldn<sq>t matter.</p>", 
                "question": "When Veruca Salts<sq> father is buying Wonka Bars in bulk to find a golden ticket is he any more likely to find one if he buys all the bars locally or world wide?"
            }, 
            "id": "crjcq5j"
        }, 
        {
            "body": {
                "answer": "<p>Instead of Survival Analysis, you should look at it<sq>s more flexible cousin, Time to Event Analysis. That<sq>ll allow you to handle recurring events for the same <dq>individual<dq>.</p>", 
                "question": "Neuroscientist here. I think I want to do a survival analysis but my dataset is a bit odd."
            }, 
            "id": "crbegm7"
        }, 
        {
            "body": {
                "answer": "<p>There are a number of ways this could be analyzed. However you need to first lay out the hypothesis. Are you more interested in<colon><br><br>The number of spots that change at least once?<br>The number of times that a spot changes?<br>The number of changes within a given neuron?<br>The number of changes within a treatment group?<br>The time to first change in a spot?<br><br>Survival analyses generally are appropriate for events with binary outcomes. There are also many other tests but it really boils down to the hypothesis. <br><br>Please more clearly state what you hypothesize. </p>", 
                "question": "Neuroscientist here. I think I want to do a survival analysis but my dataset is a bit odd."
            }, 
            "id": "crbj47v"
        }, 
        {
            "body": {
                "answer": "<p>Can you please elaborate about what you mean by point-set, individuals, and troop? I think you want a chi-square test, but I can<sq>t say for sure.</p>", 
                "question": "Multiple regression vs Anova"
            }, 
            "id": "craj9ox"
        }, 
        {
            "body": {
                "answer": "<p>That was a long time ago for me. I remember I once grasped it after reading about Vitali sets<colon><br><br>http<colon>//en.wikipedia.org/wiki/Vitali_set<br><br>I forgot the concept, but this thing did it for me back then. <br><br>If no one comes up with an intuitive explanation after me, maybe give this a try.</p>", 
                "question": "Does anyone have a good intuition for measurablility?"
            }, 
            "id": "cra5gnh"
        }, 
        {
            "body": {
                "answer": "<p>To be honest, you<sq>re not going to get an idea of what a measurable function is because it<sq>s such a broad class of functions that anything you come across in practice will be measurable. The only non-measurable functions that I can think of are ones explicitly constructed from the Axiom of Choice to be as terrible as possible, or functions on a space where the underlying sigma algebra has too few sets.<br><br>Here is probably the most useful example of a non-measurable function. It is taken from martingales in probability theory. Let<sq>s assume that you walk up to a casino game that is fair (bear with me). The outcome of each time you play the game is independent of your previous wins and losses. Let<sq>s say, X_n, is your result from playing the game the n-th time. It has an underlying sigma algebra A_n consisting of all possible events that could happen. For simplicity, X_n is either 1 or -1, you win a dollar or lose a dollar (let<sq>s say you put a dollar into a machine, and with probability 1/2 you get two dollars giving you a total of +1 dollars, or with probability 1/2 you lose that dollar you put in). <br><br>Your winnings at time n is the function F_n=X_1+...+X_n. Now, is F_n measurable? the question depends very much of which sigma algebra you are considering. It *is* measurable with respect to the sigma algebra generated by A_1, ..., A_n. It *is not* measurable with respect to A_1, ..., A_n-1. The reason is that we cannot know that the outcome of the n-th trial will be when we<sq>re playing the (n-1)-th time. <br><br>If it were measurable with respect to A_1, ...,A_n-1, we would know the outcome of the gamble before it ever happened. This is nonsense. The most useful examples of non-measurable functions are the ones generated by time-specific functions and their underlying sigma algebras. A non-measurable function is typically one that declares future values without the sufficient underlying information encoded in the sigma algebra (Note, this is different from using current information to make a prediction. My example is one where future events can be *known* only on present information, which is nonsense).<br><br>To answer your question, any function that makes an observation on currently observed data is going to be measurable. Thus, just about everything is measurable.  </p>", 
                "question": "Does anyone have a good intuition for measurablility?"
            }, 
            "id": "crat089"
        }, 
        {
            "body": {
                "answer": "<p>If you sample the whole population, then by definition the sampling error becomes zero because you know the true result for sampling the whole population. (Can still have other sources of error, e.g. measurement error, but the margin of error attached to sample size converges to zero).<br><br>What your boss may mean is <dq>what<sq>s the margin of error for a sample size of 6000 if you treat the population as infinite?<dq> (i.e. don<sq>t adjust for knowing there population is finite at 6000 people). In that case, assuming all your questions have just 2 response options and your boss really wants to ignore the existing data then you can take 50<percent>/50<percent> as the <dq>worst case<dq> response probabilities and compute from there (looks like SE=0.63<percent>, 95<percent> CI= +/- 1.2<percent>). *Really would not recommend that*, but that may be what your boss is thinking of.</p>", 
                "question": "Margin of Error for Sample Size"
            }, 
            "id": "cr8jbu7"
        }, 
        {
            "body": {
                "answer": "<p>That does sound a little unusual because the response rate is part of the calculation for margin of error - could you just assume a worst case scenario of a 50<percent> response rate? In which case I think the ME would be +/- 11.32<percent></p>", 
                "question": "Margin of Error for Sample Size"
            }, 
            "id": "cr8exhu"
        }, 
        {
            "body": {
                "answer": "<p>as long as n is a finite number then chi squared test will tell you if the results are consistent with what you expected.  What a Pvalue tells you is the likelihood of getting the result you obtained if you assumption was true.  Your alpha level is how far off you want a observed result to be for you to conclude that it is in fact different than what you would expect from your input probabilities.</p>", 
                "question": "How can I check my hypothesis?"
            }, 
            "id": "cr7nwe4"
        }, 
        {
            "body": {
                "answer": "<p>> What should I do to tell if there is some error in my program?<br><br>Evaluating probabilistic output is messy and never arrives at certainty.<br><br>A good practice is to separate the random parts of the program from the deterministic ones.  Use standard packages to generate random numbers and write your own code to make deterministic transformations of those numbers.  That way you can evaluate your code with test cases and be certain that it is correct.  <br></p>", 
                "question": "How can I check my hypothesis?"
            }, 
            "id": "cr7okrg"
        }, 
        {
            "body": {
                "answer": "<p>It<sq>s hard to tell from your description what your data are and what question you want to ask of them.<br><br>I gather that each observation (species) is associated with six values (behavioral scores).  The observations are in two groups, A and B.  You want to know whether there is a difference between the groups for each of the six values.<br><br>Without knowing anything about the behavioral scores, I<sq>d doubt they are normally distributed.  I nonparametric test would be a safe bet.  Something like a Wilcoxon Rank Sum Test (aka [Mann Whitney U test](http<colon>//en.wikipedia.org/wiki/Mann<percent>E2<percent>80<percent>93Whitney_U_test)).</p>", 
                "question": "Unpaired two-sample T-test or more?"
            }, 
            "id": "cr7723j"
        }, 
        {
            "body": {
                "answer": "<p>For someone on a low-income, I<sq>d suggest considering trying to come to grips with R (probably with RStudio) for your data analysis. Its free, and there<sq>s a host of online resources (including books, websites videos, documentation, mailing lists, and a very active stackoverflow community). There<sq>s also /r/rstats here on reddit.<br><br><br>By the way, don<sq>t *test* equality of variance and then choose the analysis based on that -- simply assume the variances differ. (And if you *must* test variances for some reason, don<sq>t use the F-test. Counts aren<sq>t actually normal.) <br><br>For a t-test the most commonly used unequal variance version is the Welch-test. (In R - and some other packages, like Minitab, for example - this is the default. You have to specify if you want an equal-variance test.)<br><br>[Wikipedia on Welch test](http<colon>//en.wikipedia.org/wiki/Student<percent>27s_t-test#Equal_or_unequal_sample_sizes.2C_unequal_variances)<br><br>However, your response seems to be *count data*; you wouldn<sq>t normally use t-tests for that (possibly Poisson regression). However I didn<sq>t follow enough of your design to be sure of what would be best.<br><br>For specific statistical questions (like <dq>I have situation X, data in form Y, and I want to get Z, how do I do that?<dq>), stats.stackexchange.com probably has an answer already (its search is pretty good but also try a site search using google because the internal search doesn<sq>t look at comments on posts where sometimes a gem of information or useful pointer resides). For general help (like <dq>what do I do now?<dq> style questions), somewhere like right here may be better.<br></p>", 
                "question": "Unpaired two-sample T-test or more?"
            }, 
            "id": "cr783qv"
        }, 
        {
            "body": {
                "answer": "<p>Not a hiring manager, but for question 1, lead with the MS in statistics and programming experience. People get hired from my program with just an MS Statistics quite a lot. Summer internship is great. People don<sq>t generally expect somebody with an MS to have job experience, though it<sq>s nice. They probably would very much like internship experience. But this all depends on the job.<br><br>For #2, top 20 helps. There<sq>s definitely a different between top 10 and 10-20 in terms of acceptance rates and you<sq>ll never get into a program you don<sq>t apply to. <br><br>For Q3, you definitely want to be looking for more applied programs, sure. I think after the top 10, the danger of being too theoretical has passed, but you should glance at degree requirements to get a feel for it.</p>", 
                "question": "Seeking advice on career in statistics."
            }, 
            "id": "cr6s1ww"
        }, 
        {
            "body": {
                "answer": "<p>I modeled this as an Absorbing State Markov Chain.  I calculated that it will take an average of about 640 steps until you get mythic sails (absorption). A step would be equivalent to one upgrade attempt from your current sail level (ex. a try from epic to divine or a try from grand to rare) .So, you will need about 9,600 gold and 640 regrade scrolls.  Also, I believe that you will fail and loose your sail about 203 times (if the sail is destroyed upon failure).<br><br>I am a statistics student, so I could have possibly made some mistakes.</p>", 
                "question": "how many sails would i need in archeage to get on to maximum quality"
            }, 
            "id": "cr65j48"
        }, 
        {
            "body": {
                "answer": "<p>> <dq>grand<dq> quality (green) and you can regrade these to <dq>rare<dq> (blue) this has a 90<percent> chance of success (if it fails sails go bye bye) this takes 15 gold and 1 regrade scroll per regrade attempt and then to <dq>arcane<dq> (purple) it is apparently 70<percent><br><br>Do you have any evidence that this percentage is correct? Most people have said it<sq>s 50<percent> all the way to Mythic starting at Grand. </p>", 
                "question": "how many sails would i need in archeage to get on to maximum quality"
            }, 
            "id": "cr6i7nq"
        }, 
        {
            "body": {
                "answer": "<p>I think that is just overcomplicating. It is all linear models. <br><br>By that nomenclature it seems<colon><br><br> * linear regression has one continuous independent variable.<br><br> * ANOVA has one or more categorical independent variables<br><br> * ANCOVA  has both categorical and continuous independent variables<br><br> * Multiple regression has several continuous independent variables</p>", 
                "question": "Could anybody clarify the difference between a linear model and a general linear model (Not a generalized linear model) please?"
            }, 
            "id": "cr5niqg"
        }, 
        {
            "body": {
                "answer": "<p>By my reckoning, all 4 of those are plain linear models.<br><br>What makes a general linear model more general than any of those is the variance-  <br>covariance matrix of the error term, which needn<sq>t be \u03c3^(2)*I*. That makes the  <br>problem effectively multivariate rather than univariate.<br></p>", 
                "question": "Could anybody clarify the difference between a linear model and a general linear model (Not a generalized linear model) please?"
            }, 
            "id": "cr68rk4"
        }, 
        {
            "body": {
                "answer": "<p>**Specific advice**<colon> Have you looked at the pool() function in the mice package? It should be able combine your GLM analyses. See the example that comes with ?pool and replace lm with glm, and change the variables and dataset names.<br><br>Similarly, you could use MIcombine in the mitools package to do the same thing, but it will output the collective parameter estimates, variances, and effective degrees of freedom as separate objects.<br><br>**General advice**<colon> You can also run separate GLMs on any software, even SPSS, on each of the imputed datasets, and combine the results using Rubin<sq>s Rules<colon><br><br>Let <sq>Param<sq> refer to the parameter value from all imputations collectively.<br>Let <sq>Param|Impute<sq> refer to value from one such Imputation.<br><br>* The estimate of any parameter is the mean of parameter estimates from your imputed data. <br> E(Param) = E[ E(Param|Impute)]<br><br>* The variance of any parameter is approximately<colon>  <br>Var(Param) = E[Var(Param | Impute)] + (1 + 1/m)*Var(E(Param | Impute)), for m > 1 imputations. <br><br>* In other terms, that<sq>s <br>Var(Param) = (Average variance estimate of each analysis) + (1 + 1/m)*(Variance BETWEEN the estimates in the analysis)<br><br>See this book from the author of the MICE package. Specifically, search for <dq>proper imputation<dq> to get to page 38. Everything you should need to combine imputed analyses is in pages 38-44 or so.<br><br>https<colon>//books.google.ca/books?id=M89TDSml-FoC<br><br>**Finally, one more thing**<colon> Please ask me about this again if you have issues, because I<sq>m developing a course in data management, which uses the mice package for a section, and I would love to hear about the problems that people run into.<br><br>Cheers!</p>", 
                "question": "Used MICE to impute data with R but now I want to run my GLM in SPSS. Do I run separate GLMs for each imputation and then combine them? Or do I combine the dataset and indicate that it<sq>s an MI dataset somehow?"
            }, 
            "id": "cr3028s"
        }, 
        {
            "body": {
                "answer": "<p>You<sq>ve told us what you<sq>re going to measure, but not what you want to know.  What hypothesis do you want to test?</p>", 
                "question": "Which statistical analysis should I use for this design?"
            }, 
            "id": "cr02rsm"
        }, 
        {
            "body": {
                "answer": "<p>Depends on what the cognitive tasks are. For example, if you<sq>re measuring accuracy in the memory task, that<sq>s a binary response and should not be modeled using an ANOVA (see [Jaeger 2008](https<colon>//wiki.hmdc.harvard.edu/LPLab/uploads/Main/Jaeger07stats1.pdf)). <br>If all your DVs are binary response, you<sq>ll need to do multiple logistic regression. If you have a mixture of binary and continuous DVs though, I<sq>m not exactly sure what you would need to do (perhaps someone else can weigh in on this). <br><br>Edit<colon> linked paper</p>", 
                "question": "Which statistical analysis should I use for this design?"
            }, 
            "id": "cr071lg"
        }, 
        {
            "body": {
                "answer": "<p>Sounds like a straight-forward MANOVA design.  Then you can break down the results using ANOVAs or linear discriminate analysis.</p>", 
                "question": "Which statistical analysis should I use for this design?"
            }, 
            "id": "cr03em5"
        }, 
        {
            "body": {
                "answer": "<p>For modeling three dependent variables you<sq>ll need three models, all with correction for multiple comparisons. That<sq>s three models, so your critical alpha (significance level) is going to be at least .05/3 =.016.</p>", 
                "question": "Which statistical analysis should I use for this design?"
            }, 
            "id": "cr2mvb5"
        }, 
        {
            "body": {
                "answer": "<p>Confidence Intervals are actually a pretty tricky concept that even  academics have trouble with, but BurkeyAcademy does a nice job below. <br><br>To respond to your post more<colon> the presence of the confidence interval says nothing about the accuracy or generalizability of the study. It just says, that from the data collected, we are fairly sure the mean risk of falls is between 1.02-1.11. They are not <dq>confident<dq> about the study, just where the average value probably lies. Any data can produce confidence intervals. </p>", 
                "question": "i<sq>m a idiot - can anyone please explain confidence intervals for me?"
            }, 
            "id": "cqzig25"
        }, 
        {
            "body": {
                "answer": "<p>Confidence intervals are just a way to make an inference from a sample to a population. Here the population is all the people that do take this medication, or who could ever take this medication in the future. Obviously it<sq>s not feasible to measure this population, so they took a sample of people that take this medication and, based on these people, developed an estimate for the population. So above when I said <dq>the true number...<dq> the word <dq>true<dq> refers to the population in question. So to answer your 2nd question, a confidence interval does generalize to the population in question. <br><br>The word <dq>confidence<dq> is tricky and means that if the authors were to take repeated random samples of the population in question and develop confidence intervals for each sample, 95<percent> of the confidence intervals would contain the true average number of falls. It does not mean that there is a 95<percent> chance the true average number of falls is between 1.02 and 1.11. It seems intuitive to conclude this, but don<sq>t do it.<br></p>", 
                "question": "i<sq>m a idiot - can anyone please explain confidence intervals for me?"
            }, 
            "id": "cr088tu"
        }, 
        {
            "body": {
                "answer": "<p>[deleted]</p>", 
                "question": "i<sq>m a idiot - can anyone please explain confidence intervals for me?"
            }, 
            "id": "cqzdbdc"
        }, 
        {
            "body": {
                "answer": "<p>1) Yes, it appears that they entered BB>50<percent> as a dummy variable =1 for the 14 communities they mention, =0 for the other 41.<br><br>2) They ran one regression with two observations for each MSA (for 55\u20222=110 observations)- this is what makes it a panel data set. [Here is my video on the basics.](http<colon>//www.youtube.com/watch?v=pQvhi60yN74)  They say that they ran a time and MSA fixed effects model (top of page 5), meaning that they entered in dummy variables for 54 of the MSAs, so this controls for things like the effect of education level and other characteristics of the MSAs.  They also include a time dummy, say, =0 for the 2011 observations and =1 for 2012.  They don<sq>t report the size/significance of this coefficient, and they should.<br><br>3) Here comes the garbage part- **they can<sq>t have actually done what they say they have done**. You can<sq>t have a dummy variable for the 14 BB>50<percent> cities **and** at the same time enter in MSA fixed effects dummies.  This creates variables that are perfectly collinear, and you cannot run a regression with perfectly collinear variables.  So, they are doing something that they are not telling us.  Admittedly, I didn<sq>t take an hour to read every word of this document, so, I may have missed something.  But, they report 52 d.f. in their F test, which means that they are pretending that they included all of these variables, which just isn<sq>t possible (110-54 msa dummies -1 time dummy - 2 independent vars. - 1 for the intercept=52).<br></p>", 
                "question": "Trying to understand a study."
            }, 
            "id": "cqx2pfc"
        }, 
        {
            "body": {
                "answer": "<p>If I were you I would just go to each chapter and try some of the questions again. But coursea might have what your looking for. </p>", 
                "question": "Online stats course where all the video lectures are available for download at once?"
            }, 
            "id": "cqupagq"
        }, 
        {
            "body": {
                "answer": "<p>Stats 101 at the University of Auckland is like that.  You get a DVD or CD containing recorded lectures.  Attendance at live lectures is optional but highly recommended, as you get extra learning from questions asked and answered.<br><br>I didn<sq>t do stats 201 so I cannot say if it is the same for that.</p>", 
                "question": "Online stats course where all the video lectures are available for download at once?"
            }, 
            "id": "cqupnv7"
        }, 
        {
            "body": {
                "answer": "<p>You have all the videos for Joe Blitzstein<sq>s Harvard Stat110 (<dq>Introduction to Probability) class up on YouTube.</p>", 
                "question": "Online stats course where all the video lectures are available for download at once?"
            }, 
            "id": "cqv3641"
        }, 
        {
            "body": {
                "answer": "<p>I suggest method 2.<br><br>I wrote a short R-function to to the job. It should be fairly quick<colon><br><br>    bayes.prop <- function(x, n, alpha1 = 1, beta1 = 1, alpha2 = 1, beta2 = 1, nsim = 1000, sig.level = 0.95) {<br>      <br>      p1 <- rbeta(nsim, x[1] + alpha1, n[1] - x[1] + beta1)<br>      p2 <- rbeta(nsim, x[2] + alpha2, n[2] - x[2] + beta2)<br>      rd <- p2 - p1<br>      <br>      quants <- quantile(rd, c((1-sig.level)/2, (1 + sig.level)/2))<br>      <br>      p.above <- sum(rd > 0)/length(rd)<br>      p.below <- sum(rd < 0)/length(rd)<br>      <br>      p.two.side <- 2*min(p.above, p.below)<br>      <br>      list(quants = quants, p.val = p.two.side)<br>      <br>    }<br><br>As another possibility<colon> Have a look at the [LearnBayes](http<colon>//cran.r-project.org/web/packages/LearnBayes/index.html) package for R. This is the companion package for Jim Albert<sq>s [Bayesian Computation with R](http<colon>//bayes.bgsu.edu/bcwr/). The function <dq>howardprior<dq> together with <dq>simcontour<dq> allows to flexibly model two proportions, and their dependecy.</p>", 
                "question": "Quickest Way To Compare Two Proportions With Bayesian Methods?"
            }, 
            "id": "cqqpkbv"
        }, 
        {
            "body": {
                "answer": "<p>The p-value is the probability of getting your (or <dq>stranger<dq>) results given that H_0 is true. To get the unconditional probability of getting results that deviate at least as far from H_0 by chance, you would need to know the prior probability distribution of the variable that H_0 is concerned with, and that is generally unknown.<br><br>I highly recommend reading <dq>The Earth is Round (p<.05)<dq> (Cohen 1994).</p>", 
                "question": "Basic question about definition of p value"
            }, 
            "id": "cqq8pwx"
        }, 
        {
            "body": {
                "answer": "<p>The confusion may stem from what is meant by <dq>results are due to chance alone.<dq> As tf113 said, what you<sq>re (correctly) talking about is probability of test statistic taking on extreme values (above observed value), *conditional* on null hypothesis being true. But if H0 is true, then all results are due to chance anyway, aren<sq>t they? The statement really describes probability that H0 is itself true, which is a different question, and cannot be answered without further assumptions. For example, one could start with some prior probability that H0 is true, then work out how the observed data changes that belief (which is what Bayesian statistics does).</p>", 
                "question": "Basic question about definition of p value"
            }, 
            "id": "cqqawut"
        }, 
        {
            "body": {
                "answer": "<p>The p-value is the likelihood that the null hypothesis is true, which is why below a certain threshold (commonly p<0.5) we reject H0 and accept H1.<br><br>It has to be interpreted this way, or the condition upon which we reject H0 becomes meaningless.</p>", 
                "question": "Basic question about definition of p value"
            }, 
            "id": "cqq9a69"
        }, 
        {
            "body": {
                "answer": "<p>The two errors you could make are<colon> 1) you include a variable that shouldn<sq>t be in there, and 2) you omit a variable that should be in there. As you include more variables, it becomes harder for any one variable to seem important, because uncertainty about the effect of each variable goes up with the number of variables.<br><br>Imagine that you have variables 1, 2, and 3. You run the full model and only 1 shows up as being important. What you<sq>re suggesting would be to rerun the model without controlling for 2 and 3 because they were not significant at some prespecified level. What if variable 2 would have met that criteria, had you not also been controlling for the effect of variable 3 at the same time?<br><br>That being said, many statisticians feel that both forward and backward selection models are an abuse of p-values, and that things like least-angle regression and information-theoretic approaches are more appropriate to variable selection in the first place.</p>", 
                "question": "Help with forward and backwards selection in regression"
            }, 
            "id": "cqq89ax"
        }, 
        {
            "body": {
                "answer": "<p>I think [these posts here](http<colon>//stats.stackexchange.com/questions/20836/algorithms-for-automatic-model-selection/20856#20856) provide answers for your question. Automated model selection procedures such as stepwise forward or backward model selection are heavily discouraged by many statisticians. They can easily lead to overfitting.</p>", 
                "question": "Help with forward and backwards selection in regression"
            }, 
            "id": "cqq1arz"
        }, 
        {
            "body": {
                "answer": "<p>Sorry one more question pertaining to forward selection<colon> Say the original model has 6 factors, and the final model given has 4 factors and all have acceptable p-values. When I run a model with those 4 factors, one becomes wildly insignificant. I know that p-values aren<sq>t always trust-worthy, but I<sq>m just not sure what is happening. </p>", 
                "question": "Help with forward and backwards selection in regression"
            }, 
            "id": "cqqcnsh"
        }, 
        {
            "body": {
                "answer": "<p>A few things to consider here. First, having 14 observations is really few. It<sq>s going to be difficult to believe any result unless it<sq>s true for almost every observation. Second, when you<sq>re running through that many bivariate analyses, you<sq>re bound to find some with a positive correlation. Even if the data was totally random, some would still be positively correlated. I think your best bet at this point is to break it down with averages. For example, you could say <dq>I found that the amount of foreign fighters leaving a country was greater in countries with a higher emphasis on learning the national language.<dq> You probably won<sq>t be able to say much about statistical significance (if you want to try, you can run a t-test) but you<sq>ll at least be able to generate some interesting possibilities. Feel free to message me if you have other questions!</p>", 
                "question": "Ran a correlation and found that almost nothing is significant. This is not what I expected. What to do from here?"
            }, 
            "id": "cqmame7"
        }, 
        {
            "body": {
                "answer": "<p>Are any of your analyses based on theory or you just datamining here?</p>", 
                "question": "Ran a correlation and found that almost nothing is significant. This is not what I expected. What to do from here?"
            }, 
            "id": "cqmidh7"
        }, 
        {
            "body": {
                "answer": "<p>It sounds like you<sq>re afraid of violating the assumption of no multicollinearity. If so, there are tests you can run, they<sq>re baked into SPSS (I<sq>m assuming you<sq>re using that software, because I also saw your post in /r/IOpsychology).<br><br>If your variables intercorrelate, that<sq>s not necessarily a cause for concern. Instead it<sq>s worth asking the following question<colon> may your control variable be part of a subset of one of your IV<sq>s, or the other way around. <br><br>For example<colon> I was once asked to advise on statistical analysis for a survey that used demographic location as control variables. They included both the street the participants lived on as well as the block where those participants lived. The streets were a *subset* of the blocks, and so, yeah, of course they correlated strongly. That, in short, is an example of multicollinearity. <br><br>Also, it<sq>s worth mentioning that if you violate certain assumptions associated with the linear regression model, the only thing that means is that the results of your analysis can only be used to make inferences about your sample, not the population from which it is derived. It really depends on your research question whether or not that<sq>s a huge deal or not.</p>", 
                "question": "I think I am violation the assumptions for controlling for a variable in MANOVA/MANCOVA."
            }, 
            "id": "cqm24g8"
        }, 
        {
            "body": {
                "answer": "<p>pretty sure for any dichotomous or categorical outcome, the more powerful approach would be a weighted least squares estimator. If using Mplus, just say estimator = WLS. I think they even have a WLSR (where R is robust).</p>", 
                "question": "Maximum likelihood for binary outcome model"
            }, 
            "id": "cql10sh"
        }, 
        {
            "body": {
                "answer": "<p>> Can I apply maximum likelihood estimation to a model with a binary outcome without specifying probit/logit?<br><br>It sounds like you<sq>re asking whether you can maximum likelihood estimation without specifying a likelihood model.  The answer to that question is no, you can<sq>t.</p>", 
                "question": "Maximum likelihood for binary outcome model"
            }, 
            "id": "cqlhgqf"
        }, 
        {
            "body": {
                "answer": "<p>Maximum likelihood estimation selects the parameters that are most likely to have generated your data, according to some probability distribution. So, by definition, this method of estimation requires you to assume some kind of parametric distribution. You can definitely estimate a linear model with a binary dependent variable via maximum likelihood, but do so you have to specify a distribution for the errors. If you want MLE-like methods without the assumption of a parametric distribution, you might want to take a look at empirical likelihood methods, which are the nonparametric analogue to MLE.</p>", 
                "question": "Maximum likelihood for binary outcome model"
            }, 
            "id": "cqliulm"
        }, 
        {
            "body": {
                "answer": "<p>Yes.  This is a generalized linear model with an identity link function.  I would recommend An Introduction to Generalized Linear Models by Dobson.<br><br>The difficulty you will have when applying such a model in a complex design is that the predictions will almost certainly wind up being invalid (they will fall outside [0,1]).  The logit or probit transformations prevent this.  You should keep this disadvantage heavily in mind.</p>", 
                "question": "Maximum likelihood for binary outcome model"
            }, 
            "id": "cqs38bj"
        }, 
        {
            "body": {
                "answer": "<p>From what you<sq>ve described, you have 1 ratio-scale dependent variable and 1 nominal scale independent variable (which treatment they received).<br><br>In your case, the approach you want to use is called survival analysis. It deals with time-to-event data like you have and handles censored data (you don<sq>t have complete information about whether or not the event *ever* reoccurs) very nicely.<br><br>Try to find someone local who knows statistics to help you run and interpret your analysis. You should be able to do it pretty easily in your statistical environment of choice (SPSS, SAS, R etc).</p>", 
                "question": "Am I using the correct statistical analysis?"
            }, 
            "id": "cqjov2n"
        }, 
        {
            "body": {
                "answer": "<p>What is your study question? That will definitely drive the stats approach you will need to use.</p>", 
                "question": "Am I using the correct statistical analysis?"
            }, 
            "id": "cqjyzbv"
        }, 
        {
            "body": {
                "answer": "<p>[deleted]</p>", 
                "question": "If a student gets 65<percent> correct answers on a 180-question test what is the probability getting only 40<percent> correct within a 5-question sequence/segment of that test. (2 right 3 wrong of those 5 questions)"
            }, 
            "id": "cqekol7"
        }, 
        {
            "body": {
                "answer": "<p>Frequentist confidence intervals are a very difficult concept to understand. Even some statisticians are sometimes confused by it. I<sq>ll try to clarify what they mean.<br><br>Let<sq>s assume that the true probability of getting the result Y is indeed 30<percent>. Further, let<sq>s assume that you perform your experiment 1000 times. Each time, you collect 100 measurements. For each of the 1000 experiments, the number of result Y varies, since it<sq>s a random quantity. For each of the 1000 experiments, you calculate a 95<percent>-confidence interval for the probability of getting result Y. Now comes the crucial part<colon> **On average, 950 of the 1000 95<percent>-confidence intervals will include the true value of 30<percent>.** The graphic on the [Wikipedia page](https<colon>//en.wikipedia.org/wiki/Confidence_interval#Interpretation) illustrates that further.<br><br>Normally, you don<sq>t perform an experiment/study multiple times, just once. So can we say that a **single** 95<percent>-confidence intervals contains the true value with a probability of 95<percent>? No, because the probability of 95<percent> refers to the performance of the confidence interval under repeated sampling. A single confidence interval either contains the true population value or not.<br><br>I don<sq>t know if that answers you question but I hope this clarifies some things.</p>", 
                "question": "How do we justify the use of confidence intervals?"
            }, 
            "id": "cqcx73q"
        }, 
        {
            "body": {
                "answer": "<p>I<sq>m a little confused with your variables. Your dependent is nest presence, is that numerical or possibly binary (0,1)? </p>", 
                "question": "I need help setting up my plan of analysis"
            }, 
            "id": "cqb4rem"
        }, 
        {
            "body": {
                "answer": "<p>Not trying to open up the can of worms about what p values mean here- forgive me if I give a pre-caffeine rough approximation to illustrate this case. \u263a But, in any hypothesis test, roughly speaking, the p value is related to the probability that you could see results that disagree with the null hypothesis by *this much or more* assuming that the underlying population comes from a world where the null hypothesis is true.  To take a slightly simpler case from yours<colon><br><br>Suppose we flip a fair coin 50 times and get 35 heads.  The null is that p=.5, but our p bar is .7.  The s.e. = sqrt(.5\u2022.5/50)=.071, so our zstat=2.83, 2 tailed p  using the normal approximation=.00468. Pretty darn unlikely.<br><br>If we happened to get the SAME pbar with a sample size of 100, the s.e. drops to .05, our zstat increases to 4.0, and the p value drops to .00006.  It is **extremely** unlikely this would happen with a fair coin.<br><br>Why must this happen?  Because even though the pbar is the same, the probability of getting this result **with a larger sample assuming that the null is true** is far lower.  It is quite easy to flip a coin 10 times and get 70<percent> heads, but quite rarer to do this flipping 50 or 100 times (with a fair coin).</p>", 
                "question": "Can a increase on sample size increase/decrease a p-value despite no change on an odds ratio?"
            }, 
            "id": "cq8tnb4"
        }, 
        {
            "body": {
                "answer": "<p>p-values are a function of the signal (OR) to noise (standard error of the OR, or more often the log(OR)).  Increasing the sample size, assuming everything else is equal, is more likely to decrease the SE, and make your p-value SMALLER.<br><br><br>The technical reason can also be seen by looking up the formula for the standard error for a log(OR)<colon><br><br>SE(log(OR)) = sqrt( 1/a + 1/b + 1/c + 1/d ) <br><br>where a, b, c, d are the cells in your 2x2 table.</p>", 
                "question": "Can a increase on sample size increase/decrease a p-value despite no change on an odds ratio?"
            }, 
            "id": "cq91ufd"
        }, 
        {
            "body": {
                "answer": "<p>So distribution per-die is<colon><br><br>    Number of <br>      successes        -1      0      1<br>    probability         0.1    0.4     0.5<br><br>and the total number of successes on *k* dice is simply a k-fold convolution of this.<br><br>I<sq>m pretty sure I could do 10 - 15 dice manually (though it<sq>s tedious enough that I<sq>d want to use Excel or R or something to actually do the multiplication and addition), by building up from smaller convolutions (or one could automate it via FFT and do any number at a time.)<br><br>by inspection, the convolution of two is <br><br>      -2   -1     0    1    2<br>    0.01  0.08   0.26  0.4  0.25<br><br>And of four is <br><br>       -4     -3     -2     -1      0      1     2     3    4<br>    0.0001 0.0016 0.0116 0.0496 0.1366 0.2480 0.2900 0.200 0.0625<br><br>Now, since I<sq>m feelng lazy, using `convolve` in R, we have for 5<colon><br><br>       ns      p5<br>       -5 0.00001<br>       -4 0.00020<br>       -3 0.00185<br>       -2 0.01040<br>       -1 0.03930<br>        0 0.10424<br>        1 0.19650<br>        2 0.26000<br>        3 0.23125<br>        4 0.12500<br>        5 0.03125<br><br>for 10<colon><br><br>         ns          p10<br>        -10 1.000000e-10<br>        -9 4.000000e-09<br>        -8 7.700000e-08<br>        -7 9.480000e-07<br>        -6 8.368500e-06<br>        -5 5.628480e-05<br>        -4 2.991960e-04<br>        -3 1.286928e-03<br>        -2 4.548357e-03<br>        -1 1.333746e-02<br>         0 3.262513e-02<br>         1 6.668732e-02<br>         2 1.137089e-01<br>         3 1.608660e-01<br>         4 1.869975e-01<br>         5 1.758900e-01<br>         6 1.307578e-01<br>         7 7.406250e-02<br>         8 3.007812e-02<br>         9 7.812500e-03<br>       10 9.765625e-04<br><br>for 15<colon><br><br>      ns          p15<br>     -15 1.056130e-15<br>     -14 6.004741e-14<br>     -13 1.755044e-12<br>     -12 3.332003e-11<br>     -11 4.612650e-10<br>     -10 4.958772e-09<br>      -9 4.305256e-08<br>      -8 3.099626e-07<br>      -7 1.885639e-06<br>      -6 9.827066e-06<br>      -5 4.432699e-05<br>      -4 1.743983e-04<br>      -3 6.019227e-04<br>      -2 1.830070e-03<br>      -1 4.915257e-03<br>       0 1.168113e-02<br>       1 2.457629e-02<br>       2 4.575176e-02<br>       3 7.524033e-02<br>       4 1.089990e-01<br>       5 1.385218e-01<br>       6 1.535479e-01<br>       7 1.473155e-01<br>       8 1.210792e-01<br>       9 8.408702e-02<br>      10 4.842551e-02<br>      11 2.252271e-02<br>      12 8.134766e-03<br>      13 2.142334e-03<br>      14 3.662109e-04<br>      15 3.051758e-05<br></p>", 
                "question": "Dice rolls in White Wolf<sq>s Storyteller system"
            }, 
            "id": "cq69d5f"
        }, 
        {
            "body": {
                "answer": "<p>The vocabulary I<sq>m familiar with is<colon><br><br>1. *Selected sample* - these are the people (or households, or businesses, or whatever) who you select to be in the survey. When designing the survey, also called *sample to approach*.<br><br>2. *Sample loss* - People who you select, but are out of scope of the survey (e.g. you<sq>re surveying males 20-25, you select a 40-year-old woman).<br><br>3. *Responding sample* - The people selected in the sample, who are in-scope and respond to the survey.<br><br>All the statistics are produced from the responding sample, and if you are designing based on accuracy constraints then they will usually be expressed in terms of the responding sample. However, once you have an idea of the responding sample you need, you then have to apply some assumptions about sample loss and response rates to reverse engineer an estimate of the sample to approach.<br><br>As for using Facebook to draw a sample, then it<sq>s never going to be a random sample so much of the theory about sampling doesn<sq>t apply. In any case, even if you did get a random sample you have two different populations - the facebook users who could have been selected in the sample, and the people who you<sq>re using your sample to draw inferences about - and the bigger the gap between those two populations, the less relevant any conclusions you try to draw will be.</p>", 
                "question": "When is sample a sample"
            }, 
            "id": "cq76k2v"
        }, 
        {
            "body": {
                "answer": "<p>You probably biased your sample by using the phone book but lets say that people-in-the-phone-book is the population you are interested in.  You randomly chose 1500 and 900 answered.  The 900 are your sample.   N=900, no matter what and your power will be low.  If you think non-response is an issue, you need to anticipate it and over-sample...or keep randomly selecting until you reach 1500.  Say it took 2500 selected names to get 1500 responses.<br><br>But you are not finished.  You need to keep track of the 1000 people who did not respond.  They are <dq>truncated<dq>.  This truncation causes your sample of 1500 to be potentially <dq>biased<dq>. <br><br>You then need to sample these 1000 non-respondents to find out why they failed to answer and whether they were like your respondents in predictors and responses.  You do not need to evaluate them all but 50 might be nice.  You are attempting to establish that your sample was not biased by the truncated data.<br><br>You might need to offer them money or inducement to do this.</p>", 
                "question": "When is sample a sample"
            }, 
            "id": "cqfna2i"
        }, 
        {
            "body": {
                "answer": "<p>It all depends on how accurate you want to be.  Yes, a simple method is just to take one page and multiply.  A little better, but more involved would be to count the firs pages of chapters and last pages separately, since they are different than interior pages of chapters.<br><br>It also depends on the kind of book-- novel, or textbook with tables and graphs and pictures-- pages in the latter will vary much more than the typical interior page of a novel.<br><br>What a statistician would probably do is randomly select a few pages (random sample) or pick several pages of the different types (first, interior, last) (stratified sample), to get an idea about how much variation there is in the word counts between pages.  If there isn<sq>t much variation, then counting just a few pages will give you a pretty good estimate of the true average per page.<br><br>I don<sq>t know how technical you ant to get, but here are a couple of my intro videos on confidence intervals.  \u263a<br><br>https<colon>//www.youtube.com/watch?v=7SRypF7upl8<br><br>https<colon>//www.youtube.com/watch?v=xggeInE2tMk<br><br><br></p>", 
                "question": "Would it be possible to use statistics to accurately estimate the number of words in a book?"
            }, 
            "id": "cq4bi3k"
        }, 
        {
            "body": {
                "answer": "<p>Each page has about the same number of lines.  Count the number of words per line on one page then get an average number of words per line.  From this average you can get a standard deviation and get some sense of how good your estimate is.</p>", 
                "question": "Would it be possible to use statistics to accurately estimate the number of words in a book?"
            }, 
            "id": "cq4ibrw"
        }, 
        {
            "body": {
                "answer": "<p>Your lizard brain is right <colon>)<br>Count as many pages as you can (the more the better), select them at random i.e page 7, 38, 92, 115 etc. (though it proberbly wouldn<sq>t matter if you do it random or not). Find the average words pr. page, multiply that with your number of pages!</p>", 
                "question": "Would it be possible to use statistics to accurately estimate the number of words in a book?"
            }, 
            "id": "cq4bicj"
        }, 
        {
            "body": {
                "answer": "<p>The best way (other than actually counting every word in the book), is to take a random sample of pages to comes up with an average number of words per page, then apply a simple proportion.  The larger the sample the better.<br><br>If you wanted to take it a step further, you could create a confidence interval based on your sample.</p>", 
                "question": "Would it be possible to use statistics to accurately estimate the number of words in a book?"
            }, 
            "id": "cq4t1jo"
        }, 
        {
            "body": {
                "answer": "<p>There is a package in R <dq>survsim<dq> that helps do this. <br><br>http<colon>//www.jstatsoft.org/v59/i02/paper<br><br>Here is a little tutorial that also looks helpful<colon><br><br>http<colon>//www.ms.uky.edu/~mai/Rsurv.pdf<br></p>", 
                "question": "I want to simulate right censored data in R and then compare survival and OLS methods. How do I do that?"
            }, 
            "id": "cq3idha"
        }, 
        {
            "body": {
                "answer": "<p>Hopefully [this](http<colon>//www.camdp.com/blogs/generating-exponential-survival-data) blog post by Cam dp helps!</p>", 
                "question": "I want to simulate right censored data in R and then compare survival and OLS methods. How do I do that?"
            }, 
            "id": "cq411fp"
        }, 
        {
            "body": {
                "answer": "<p>[deleted]</p>", 
                "question": "I want to simulate right censored data in R and then compare survival and OLS methods. How do I do that?"
            }, 
            "id": "cq3oegk"
        }, 
        {
            "body": {
                "answer": "<p>Why do want to do this? Survival methods are specifically tailored to deal with the partial information inherent to censored data. OLS cannot deal with this and its application to censored data will result in biased and unreliable estimates.</p>", 
                "question": "I want to simulate right censored data in R and then compare survival and OLS methods. How do I do that?"
            }, 
            "id": "cq3hh3o"
        }, 
        {
            "body": {
                "answer": "<p>I think I could help but I don<sq>t understand the question.<br><br>If I<sq>m interpreting your code correctly, you want to exclude observations (i.e. rows in the dataset) where <dq>areitiRT<dq> is larger than mean + 3SD?<br><br>Could you please clarify when exactly you want to exclude observations?<br><br>Optimally, you could upload a part of the dataset, so that I could check whether my code works.<br><br>Thank you.</p>", 
                "question": "Modifying a R for loop to work with three variables"
            }, 
            "id": "cpxslfl"
        }, 
        {
            "body": {
                "answer": "<p>Here is a quick and dirty implemenation that uses <dq>ddply<dq> from the <dq>plyr<dq> package. Please check carefully if this does what you want and that the right observations are excluded.<br><br>    # Load plyr for <dq>ddply<dq><br>    <br>    library(plyr)<br>    library(ggplot2)<br>    <br>    # Load dataset <br>    <br>    dat <- read.delim(<dq>Verkefni_1.txt<dq>, stringsAsFactors=FALSE)<br>    <br>    str(dat)<br>    head(dat)<br>    <br>    # Set up an indicator variable for each condition that is 1 if excluded and 0 when not excluded<br>    <br>    dat$exclude.markLiturEndurt <- NA<br>    dat$exclude.markStadsEndurt <- NA<br>    dat$exclude.litStaEndurt <- NA<br>    <br>    dat <- ddply(dat, .(subject, markLiturEndurt), function(df) {<br>      # df <- subset(dat, subject == 101 & markLiturEndurt == 0)<br>      <br>      lower.limit <- 100<br>      <br>      df$mean.markLiturEndurt <- mean(df$areitiRT, na.rm = TRUE)<br>      df$sd.markLiturEndurt <- sd(df$areitiRT, na.rm = TRUE)<br>      df$threshold.markLiturEndurt <- mean(df$areitiRT, na.rm = TRUE) + 3*sd(df$areitiRT, na.rm = TRUE)<br>      <br>      upper.limit <- mean(df$areitiRT, na.rm = TRUE) + 3*sd(df$areitiRT, na.rm = TRUE)<br>      <br>      df$exclude.markLiturEndurt[df$areitiRT > upper.limit | df$areitiRT < lower.limit] <- 1<br>      df$exclude.markLiturEndurt[df$areitiRT <= upper.limit & df$areitiRT >= lower.limit] <- 0<br>    <br>      # table(df$exclude.markLiturEndurt, df$areitiRT)<br>      <br>      return(df)<br>      <br>    })<br>    <br>    dat[which(dat$areitiRT > dat$threshold.markLiturEndurt)<br>        , c(<dq>subject<dq>, <dq>areitiRT<dq>, <dq>threshold.markLiturEndurt<dq>, <dq>markLiturEndurt<dq>, <dq>exclude.markLiturEndurt<dq>)]<br>    <br>    # dat$exclude.markLiturEndurt[is.na(dat$exclude.markLiturEndurt)] <- 0<br>    <br>    table(dat$exclude.markLiturEndurt, dat$markLiturEndurt, useNA = <dq>always<dq>)<br>    <br>    dat <- ddply(dat, .(subject, markStadsEndurt), function(df) {<br>      <br>      df$mean.markStadsEndurt <- mean(df$areitiRT, na.rm = TRUE)<br>      df$sd.markStadsEndurt <- sd(df$areitiRT, na.rm = TRUE)<br>      df$threshold.markStadsEndurt <- mean(df$areitiRT, na.rm = TRUE) + 3*sd(df$areitiRT, na.rm = TRUE)<br>      <br>      lower.limit <- 100<br>      upper.limit <- mean(df$areitiRT, na.rm = TRUE) + 3*sd(df$areitiRT, na.rm = TRUE)<br>      <br>      df$exclude.markStadsEndurt[df$areitiRT > upper.limit | df$areitiRT < lower.limit] <- 1<br>      df$exclude.markStadsEndurt[df$areitiRT <= upper.limit & df$areitiRT >= lower.limit] <- 0<br>      <br>      return(df)<br>      <br>    })<br>    <br>    # dat$exclude.markStadsEndurt[is.na(dat$exclude.markStadsEndurt)] <- 0<br>    <br>    table(dat$exclude.markStadsEndurt, dat$markStadsEndurt, useNA = <dq>always<dq>)<br>    <br>    dat <- ddply(dat, .(subject, litStaEndurt), function(df) {<br>    <br>      df$mean.litStaEndurt <- mean(df$areitiRT, na.rm = TRUE)<br>      df$sd.litStaEndurt <- sd(df$areitiRT, na.rm = TRUE)<br>      df$threshold.litStaEndurt<- mean(df$areitiRT, na.rm = TRUE) + 3*sd(df$areitiRT, na.rm = TRUE)<br>      <br>      lower.limit <- 100<br>      upper.limit <- mean(df$areitiRT, na.rm = TRUE) + 3*sd(df$areitiRT, na.rm = TRUE)<br>      <br>      df$exclude.litStaEndurt[df$areitiRT > upper.limit | df$areitiRT < lower.limit] <- 1<br>      df$exclude.litStaEndurt[df$areitiRT <= upper.limit & df$areitiRT >= lower.limit] <- 0<br>      <br>      return(df)<br>      <br>    })<br>    <br>    # dat$exclude.litStaEndurt[is.na(dat$exclude.litStaEndurt)] <- 0<br>    <br>    table(dat$exclude.litStaEndurt, dat$litStaEndurt, useNA = <dq>always<dq>)<br>    <br>    crosstab <- as.data.frame(table(dat$exclude.litStaEndurt, dat$exclude.markLiturEndurt, dat$exclude.markStadsEndurt, useNA = <dq>always<dq>)<br>                              , stringsAsFactors = FALSE)<br>    names(crosstab) <- c(<dq>litStaEndurt<dq>, <dq>markLiturEndurt<dq>, <dq>markStadsEndurt<dq>, <dq>Frequency<dq>)<br>    <br>    crosstab[-which(crosstab$Frequency == 0), ]<br>    sum(crosstab$Frequency)<br>    dim(dat)[1]<br>    <br>    # Create an overall exclusion indicator variable that is 1 if any of the three indicator<br>    # variables is 1<br>    <br>    dat$exclude.final <- 0<br>    dat$exclude.final[<br>      dat$exclude.litStaEndurt == 1 | <br>        dat$exclude.markLiturEndurt == 1 | <br>        dat$exclude.markStadsEndurt == 1<br>      ] <- 1<br>    <br>    # Now exclude the subjects<br>    <br>    table(dat$exclude.final)<br>    prop.table(table(dat$exclude.final))<br>    <br>    dat.final <- subset(dat, exclude.final == 0)<br>    <br>    table(dat.final$exclude.final)<br>    table(dat.final$markLiturEndurt)<br>    <br>    # Sanity checks<br>    <br>    dat[dat$subject == 117 & dat$markLiturEndurt == 1,]<br>    dat[dat$subject == 118 & dat$markLiturEndurt == 1, c(<dq>subject<dq>, <dq>areitiRT<dq>, <dq>markLiturEndurt<dq>, <dq>exclude.markLiturEndurt<dq>)]<br>    <br>    max(dat$areitiRT[dat$markLiturEndurt == 1])<br>    max(dat.final$areitiRT[dat.final$markLiturEndurt == 1])<br>    <br>    dat[which( (dat$areitiRT > dat$threshold.markLiturEndurt) & dat$exclude.markLiturEndurt == 0 & dat$markLiturEndurt == 0)<br>        , c(<dq>subject<dq>, <dq>areitiRT<dq>, <dq>threshold.markLiturEndurt<dq>, <dq>markLiturEndurt<dq>, <dq>exclude.markLiturEndurt<dq>)]<br>    <br>    <br>    # Plot the thresholds (i.e. mean + 3*sd) against exclusion indicators<br>    <br>    p <- ggplot(dat,  aes(y = areitiRT, x = threshold.markLiturEndurt)) + facet_wrap(~exclude.markLiturEndurt+markLiturEndurt) +<br>      geom_point(aes(colour = factor(exclude.markLiturEndurt)), alpha = 0.5) +<br>      geom_line(aes(y = threshold.markLiturEndurt, x = threshold.markLiturEndurt))+<br>      geom_hline(aes(yintercept = 100))+<br>      scale_y_log10()<br>    <br>    p<br>    <br>    p <- ggplot(dat,  aes(y = areitiRT, x = threshold.markStadsEndurt)) + facet_wrap(~exclude.markStadsEndurt+markStadsEndurt) +<br>      geom_point(aes(colour = factor(exclude.markStadsEndurt)), alpha = 0.5) +<br>      geom_line(aes(y = threshold.markStadsEndurt, x = threshold.markStadsEndurt))+<br>      geom_hline(aes(yintercept = 100))+<br>      scale_y_log10()<br>    <br>    p<br>    <br>    p <- ggplot(dat,  aes(y = areitiRT, x = threshold.litStaEndurt)) + facet_wrap(~exclude.litStaEndurt+litStaEndurt) +<br>      geom_point(aes(colour = factor(exclude.litStaEndurt)), alpha = 0.5) +<br>      geom_line(aes(y = threshold.litStaEndurt, x = threshold.litStaEndurt))+<br>      geom_hline(aes(yintercept = 100))+<br>      scale_y_log10()<br>    <br>    p</p>", 
                "question": "Modifying a R for loop to work with three variables"
            }, 
            "id": "cpxzwzt"
        }, 
        {
            "body": {
                "answer": "<p>Are you familiar with time series analysis?  This seems like the perfect usage of it, it is similar to regression but much better with this type of data.  I don<sq>t want to get too deep into it but here is a link to a book about it by Cryer and Chan, I would look at the random walk model and see what you can do from there.  R is an open source data analyzing software and if you install the package (TSA) you can try and fit a model to what you are doing, if you need any specific advice I can try to help but I am taking the class right now.<br><br>http<colon>//homepage.stat.uiowa.edu/~kchan/TSA.htm<br><br>Give it a look! Hope it helps (look at the random walk model and forecasting specifically.)  Even if you can<sq>t get the model fit right it is a helpful thing to know in general.</p>", 
                "question": "How to predict a random number every day."
            }, 
            "id": "cpu42ws"
        }, 
        {
            "body": {
                "answer": "<p>* I would *love* to see that data. Do you collect it in a file or something that you can share?<br>* Out of curiosity<colon> Is the pharmacy aware that it<sq>s just a game, or do they treat it as some sort of report?</p>", 
                "question": "How to predict a random number every day."
            }, 
            "id": "cpubxge"
        }, 
        {
            "body": {
                "answer": "<p>From the description it sounds like it<sq>s some kind of count or tally. Especially the fact that there are no numbers on the weekend, but a higher number on Mondays, suggests that it<sq>s the total of the weekend or something. So that means the number shouldn<sq>t be completely random and time series analysis would probably work well.<br><br>A simple approach would probably use something like a running average of the last few days, but you might want to correct for things like trends and perhaps seasonality. You<sq>ll want to decompose the data into its components, i.e. split out the trend, seasonality and random components to make your predictions.</p>", 
                "question": "How to predict a random number every day."
            }, 
            "id": "cpuqc44"
        }, 
        {
            "body": {
                "answer": "<p>Could it something like the number of prescriptions written by doctors from that hospital filled that day. With some days where they don<sq>t get round to totalling it so carry it over till the next day instead?</p>", 
                "question": "How to predict a random number every day."
            }, 
            "id": "cpuh0lh"
        }, 
        {
            "body": {
                "answer": "<p>The correct answer is that if you have all the data for the world, and all the data for your books, then NO HYPOTHESIS TESTING PROCEDURE IS NEEDED **and** DOING SUCH A TEST IS SILLY AND INAPPROPRIATE (sorry for yelling).  Statistical tests are used to see if one can generalize from a sample to a population.  But, if you are comparing two populations for which you have data, statistical tests do not apply.  For example, if we know the size of each state in the US and each province in China, and find that the average for the states is bigger than for the provinces, then we are done.  There is no <dq>random variable<dq> or sample to do a test about.</p>", 
                "question": "I<sq>m not a statistician but my boss wants me to be one -- help!"
            }, 
            "id": "cpmb80m"
        }, 
        {
            "body": {
                "answer": "<p>I<sq>d look into non-parametric tests. Specifically, I<sq>d use a Mann\u2013Whitney U test, comparing the citations of breakfast-authors with the citations of non-breakfast-authors. Don<sq>t use a t-test or other test assuming normality, since the distribution of citations almost surely is highly skewed. <br><br>Don<sq>t worry about the randomness of your sample - juat be careful about concluding anything about authors that are not in your database. </p>", 
                "question": "I<sq>m not a statistician but my boss wants me to be one -- help!"
            }, 
            "id": "cpmaddn"
        }, 
        {
            "body": {
                "answer": "<p>> This seems easy -- just compare the means.<br><br>To my mind, that<sq>s the right answer to that question.<br><br>> However I<sq>m also being asked to produce tests of significance. <br><br>I<sq>m not even sure it makes sense in this instance. it sounds like you have the entire population of interest. Since tests of significance are about inference from the sample you have to the population you want to talk about, if you already have the population, any difference whatever is <sq>significant<sq> in the intended sense.<br><br>(If you do have a non-random sample of the population of interest, unless its almost the entire population, your situation would be almost hopeless in any case.)<br><br><br><br></p>", 
                "question": "I<sq>m not a statistician but my boss wants me to be one -- help!"
            }, 
            "id": "cpmptuq"
        }, 
        {
            "body": {
                "answer": "<p>Crazy, I came to this forum to ask a very similar question just now. To your question, my first inclination would be to treat it like an A/B test (even though it<sq>s different time periods) and use a Chi-squared test to find out whether the proportions are statistically different. Have you tried [Evan<sq>s Awesome A/B Testing Tools](http<colon>//www.evanmiller.org/ab-testing/chi-squared.html)? FYI I recently tried to do this in Excel, but the CHISQ.TEST function requires an <dq>expected value<dq>, which stumped me a bit. I resorted to doing it in R.<br><br>I<sq>m curious to hear from others, because, as you said, you<sq>re drawing from two different populations. But the same could be said for a properly staged A/B test as well...</p>", 
                "question": "Significant statistical difference between two conversion rate proportions in e-commerce"
            }, 
            "id": "cpjhn1x"
        }, 
        {
            "body": {
                "answer": "<p>It really depends on what you want to do with your analysis.  Surely you don<sq>t merely want to predict information that you already have.  However, regression might make sense if you want to test a prediction model, or if you want to predict information that you don<sq>t have (e.g., the value of y in a year for which you have no value, though if this is a case of data missingness in your dataset, there may be other, better ways of handling the problem).</p>", 
                "question": "Regression vs Available data"
            }, 
            "id": "cpg20io"
        }, 
        {
            "body": {
                "answer": "<p>It can make sense for all manner of reasons --- what are you running the regression for?<br><br>On the other hand these are time series -- you have to watch out for spurious regression, for example.</p>", 
                "question": "Regression vs Available data"
            }, 
            "id": "cpg7euq"
        }, 
        {
            "body": {
                "answer": "<p>[These data](http<colon>//www.amstat.org/publications/jse/v3n1/datasets.dickey.html) collected by Galileo may be interesting for your students. They might even be able to collect some of their own. </p>", 
                "question": "High School Math Teacher looking for interesting Data to use in my class! (Those who wished your math class was more hands on be the change you wish to see in your math room!)"
            }, 
            "id": "cpd54yx"
        }, 
        {
            "body": {
                "answer": "<p>Well, this isn<sq>t high on the real-life practicality end, but theres a user on /r/Pokemon [who<sq>s been compiling counts of each flair sprite on the sub](http<colon>//www.reddit.com/r/pokemon/search?q=flair+statistics&restrict_sr=on&sort=relevance&t=all) and charting them. There are text versons in the comments of each one to make things easier. It might be an interesting project to give to the Pokemon fans in your class, you could let them come up with all sorts of projects, such as <dq>Pick your favorite 3 Pokemon and track the usage of that flair over time<dq>, you can even let them compare the three against each other.  <br><br>Statistics is also very important in engineering. I say this as an engineering student, and I<sq>m terrified of statistics XD But, for example, you could have students test the properties of two or three different products against each other, and you repeat the tests a number of times with each product. It doesn<sq>t have to be super-rigorous or anything. Eg<colon> testing the top speed and acceleration of different toy cars. If your school has a design club, perhaps you can run your experiments in conjunction with their activities.  <br><br>As another interesting activity you can try, you can try giving them [The Monty Hall Problem](http<colon>//en.wikipedia.org/wiki/Monty_Hall_problem). There are three closed doors; two of them contain a goat, one contains a car. The host knows which one is which. You pick a door (which stays closed for now). The host then opens one of the other doors, which must contain a goat. He then asks, <dq>Are you SURE you want to keep the door you<sq>ve picked?<dq> The best strategy is to switch, but your class may or may not figure that out right away; so ask your class if they think they should switch doors or stay, and why, and watch the debate begin ><colon>3 Then you can actually play several rounds of the game, while keeping track of the results. You can assign half to stay and half to switch, or everyone gets two turns to do each. Afterwards, you can discuss different methods of explaining why you got the results that you did. </p>", 
                "question": "High School Math Teacher looking for interesting Data to use in my class! (Those who wished your math class was more hands on be the change you wish to see in your math room!)"
            }, 
            "id": "cpd7z4p"
        }, 
        {
            "body": {
                "answer": "<p>Data on survival of passengers on the Titanic.<br>http<colon>//lib.stat.cmu.edu/S/Harrell/data/descriptions/titanic.html</p>", 
                "question": "High School Math Teacher looking for interesting Data to use in my class! (Those who wished your math class was more hands on be the change you wish to see in your math room!)"
            }, 
            "id": "cpehg17"
        }, 
        {
            "body": {
                "answer": "<p>You do need to recode the categorical variable, but subtracting the mean of a categorical variable from all of the category values doesn<sq>t cut it. The primary options are **effect coding** and **dummy coding**. It<sq>s a nontrivial choice because the different coding schemes can produce different stats for the same individual effects (even while producing the same omnibus F result) because the respective main & interaction terms are partitioned differently. That means the terms also cannot be interpreted exactly the same way between effect & dummy coding. </p>", 
                "question": "Do we need to center a categorical/dichotomous variable when doing a hierarchical regression for moderation?"
            }, 
            "id": "cp8ziov"
        }, 
        {
            "body": {
                "answer": "<p>http<colon>//fivethirtyeight.com/datalab/comparing-the-wwfs-death-rate-to-the-nfls-and-other-pro-leagues/<br><br>http<colon>//www.oliverkuss.de/science/publications/Kuss_Longevity_of_Soccer_Players_An_Investigation_of_all_German_Internationals_from_1908-2006.pdf<br><br>http<colon>//www.hindawi.com/journals/jar/2012/243958/<br></p>", 
                "question": "Seeking a life-expectancy table for various sportspeople."
            }, 
            "id": "cp8a9cm"
        }, 
        {
            "body": {
                "answer": "<p>This is very much [an open question in sports](http<colon>//en.wikipedia.org/wiki/Sports_rating_system#Rating_methods). The [Pythagorean Expectation](http<colon>//en.wikipedia.org/wiki/Pythagorean_expectation) could work here. Here<sq>s a nice [blog post](http<colon>//www.gautamnarula.com/rating/) on a multiplayer version of Elo rating that<sq>s available as an app. If you want to get really statistical, some brief googling turns up a bunch of papers on trying to solve this with posteriors from various bayesian models, but that<sq>s probably overkill and impractical. <br><br>In terms of simple to implement options, another idea that comes to mind is computing a t statistic for each individual<colon> mean score / (standard deviation of scores / sqrt(number of games)). Compared to the straight mean, this would give an advantage to players who score well more consistently (e.g. 11,12,13 over 8,12,16) and an advantage to high-volume players (eg. an average score of 10 over 64 games is treated as equivalent to an average score of 13.3 over 36 games, assuming equal standard deviations). Mean score could alternatively be replaced with mean margin of victory/defeat to give more weight to dominant victories if desired. Or treat games as poisson processes of scoring points, or ... or ... <colon>)</p>", 
                "question": "Looking for an equitable scoring model for our Croquet League 2.0"
            }, 
            "id": "cp5xp71"
        }, 
        {
            "body": {
                "answer": "<p>Maybe a variation on NCAA Basketball<sq>s RPI score? It would be something similar to<colon><br><br>((Your Points * .5) + (Opponent<sq>s Points * .25) + (Opponent<sq>s Opponent<sq>s Points * .25)) / Games Played<br><br>The scoring weights strength of schedule and rewards points won against strong players.<br><br><br></p>", 
                "question": "Looking for an equitable scoring model for our Croquet League 2.0"
            }, 
            "id": "cp5z0n2"
        }, 
        {
            "body": {
                "answer": "<p>How about something similar to the Elo system in chess?</p>", 
                "question": "Looking for an equitable scoring model for our Croquet League 2.0"
            }, 
            "id": "cp645dy"
        }, 
        {
            "body": {
                "answer": "<p>thanks for the input<br></p>", 
                "question": "Looking for an equitable scoring model for our Croquet League 2.0"
            }, 
            "id": "cpy72r9"
        }, 
        {
            "body": {
                "answer": "<p>I<sq>d say it depends on what you want to show in the graph. The standard error shows how precisely you determined the mean (another possibility to show that would be to plot confidence intervals). The standard deviation, on the other hand, shows the variation in the data. See [here](http<colon>//www.graphpad.com/support/faqid/201/).<br><br>Both approaches are certainly possible even with just 2 measurements.<br><br>With only two data points, plotting the raw data is also a possibility. This would be very <dq>transparent<dq>, in a way. Again, it depends on what you want to show in the graph.<br><br>Hope that helps.<br><br></p>", 
                "question": "Standard error bars on datasets with only 2 points?"
            }, 
            "id": "cp3wzxy"
        }, 
        {
            "body": {
                "answer": "<p>A good argument for plotting means and error bars is that plotting the raw data is cluttering. In your case that<sq>s unlikely, and plotting the actual data points should be fine. You can still choose to connect the means with a line if you<sq>d wish.</p>", 
                "question": "Standard error bars on datasets with only 2 points?"
            }, 
            "id": "cp3xlve"
        }, 
        {
            "body": {
                "answer": "<p>This will probably be of interest to you<colon> http<colon>//www.reddit.com/r/QuantifiedSelf/</p>", 
                "question": "What simple data can every individual collect to learn more about their life?"
            }, 
            "id": "cott9wx"
        }, 
        {
            "body": {
                "answer": "<p>* If you<sq>re someone who likes to meet fitness goals, try to bring your resting heart rate down as low as possible instead of tracking weight or BMI. Resting HR is a way better metric of the kind of <dq>fit<dq> that gives you extra years. <br>* If you want to be more productive, get an app on your phone that will randomly send you a notification that asks you to type in what you<sq>re doing. The very act of tracking your problem-behavior (e.g., procrastination) can really help to reduce/improve it. <br>* Along the same lines, got a pill you have a hard time taking regularly? Set your app to ask you if you<sq>ve taken your pill at the times you should<sq>ve have taken it. Not a reminder. A **question**. My app sends me to the medicine cabinet immediately so I can avoid saying <dq>no<dq>. <br><br><br>If you do decide to start collecting data, try to find something that will help you remember to record it.  Generally, the hardest thing for people to do is remember to track the data. I use an app called <dq>Reporter<dq> and it lets you make your own randomly timed and scheduled self-surveys. I use it to track all of the above in addition to questions about well-being. If you use something like a spreadsheet on your computer, its unlikely that you<sq>ll remember long stretches of time needed to collect valid data. </p>", 
                "question": "What simple data can every individual collect to learn more about their life?"
            }, 
            "id": "cotzo3k"
        }, 
        {
            "body": {
                "answer": "<p>For  anyone who is also interested in these questions<colon> <br><br>I found the solution to I)  [here](http<colon>//www.bigr.nl/OGMMReg/gmmpaper_website.pdf) and [here](http<colon>//luispedro.org/files/derivations/gaussian_integral.pdf) for the L2 distance.<br><br>The basic idea is to write down the integral as the squared difference between both GMMs. This can be decomposed to integrals over products of gaussians. (as GMMs are just weighted sums of gaussians).  The second link shows that the  integral of products of gaussians is just N(0,mu1-mu2,sigma1+sigma2) using the fact<br>that gaussian convolution has a closed form solution.  Thus the L2 distance for 2 GMMs can be written down as sums of this which is my Error function to minimize.<br><br></p>", 
                "question": "L2 Distance between Gaussian mixtures"
            }, 
            "id": "cotgqrj"
        }, 
        {
            "body": {
                "answer": "<p>The KL-divergence is not a metric. I think the most intuitive way to interpret KL(p, q) is in terms quantifying the expected increase in code length of a message x when using the <dq>wrong<dq> distribution q(x) instead of the <dq>true<dq> distribution p(x).</p>", 
                "question": "L2 Distance between Gaussian mixtures"
            }, 
            "id": "cosp7p2"
        }, 
        {
            "body": {
                "answer": "<p>Keep in mind that I<sq>m not a statistician, but a psychology student (we do have to know some statistics). <br><br>First of all, I would say they are both ordinal. But even if they weren<sq>t, I don<sq>t think you can simply add them up and report a mean. Even if they were both ordinal you shouldn<sq>t combine those answers to get a median (not mean) - because answers are different. I don<sq>t think you validated your test and came up with a way to combine those two different scales to get a one, meaningful number. I<sq>m siding with your collegue who says you can<sq>t combine those two. In practice however, if the answers to those questions are very similar - that is if they are highly correlated (their reliability is high), I would say you can report one number, but make it a median. It<sq>s an allowed central tendency measure for both ordinal and interval data and it is still meaningful. </p>", 
                "question": "How much trouble am I in by mixing same-scale ordinal and interval survey responses and taking the mean?"
            }, 
            "id": "coodx4k"
        }, 
        {
            "body": {
                "answer": "<p>More correct? They are all ordinal. Realistically? Taking sums/means of Likert items is a common and generally accepted practice for summarizing items on some scale. Can<sq>t say for sure what the impact will be without knowing what you intend to do with the items (other than means/descriptive stats), but in general I would expect the impact to be small.</p>", 
                "question": "How much trouble am I in by mixing same-scale ordinal and interval survey responses and taking the mean?"
            }, 
            "id": "coory9k"
        }, 
        {
            "body": {
                "answer": "<p>[deleted]</p>", 
                "question": "How much trouble am I in by mixing same-scale ordinal and interval survey responses and taking the mean?"
            }, 
            "id": "cooe3eo"
        }, 
        {
            "body": {
                "answer": "<p>I<sq>ll take a shot.  Here<sq>s a reference for choosing stats<colon> http<colon>//www.ats.ucla.edu/stat/mult_pkg/whatstat/<br><br>The Chi Square compares two categorical variables on the assumption that they aren<sq>t related to each other.  You should have a P-Value associated with this statistic.  If the P-Value is lower than your threshhold (P < .05, .01, .001.), then you should reject the null hypothesis that these items are unrelated.  This is evidence that they may be interconnected.  <br><br>If you don<sq>t have enough observations in your Chi Square contingency table (less than 5 in any cross category), the Fisher<sq>s Exact Test provides a similar P-Value that will suggest whether the null hypothesis can be rejected.<br><br>The Cramer<sq>s V statistic attempts to show the strength of any connection suggested by a Chi Square or Fisher<sq>s Exact. The values range from -1 to 1.  Values close to 0 indicate a weak association.  Values close to -1 and 1 indicate a strong association.  A -1 implies that when one category decreases the other tends to increase. A 1 implies that when one  category increases, the other also increases.<br><br><br></p>", 
                "question": "Differentiating Between Different Chi Square and Coefficient Tests"
            }, 
            "id": "coi5tzt"
        }, 
        {
            "body": {
                "answer": "<p>I believe the only reason this happens is because then a division by zero would occur.  The standard error is sqrt[p(1-p)/n].  I know this might not be the best solution, but perhaps you could just use (n-1)/n as the proportion instead of n/n, provided n was large enough, in order to avoid division by zero.</p>", 
                "question": "Stat test if column proportion is equal to zero or one"
            }, 
            "id": "coc0prk"
        }, 
        {
            "body": {
                "answer": "<p><dq>gerrymandering<dq></p>", 
                "question": "Does jerrymandering at the local level aggregate up to state level?"
            }, 
            "id": "coc7jv9"
        }, 
        {
            "body": {
                "answer": "<p>I tried simulating the game in Python. <br><br>Doing 1,000,000 trials, the player appears to go through the entire deck about 86<percent> of the time.<br><br>A probability that is straight forward to calculate for this problem is the probability that a player takes 0 drinks (only uses the first 10 cards) is nCr(36, 10) / nCr(52, 10) = 1.6<percent><br><br><br>    from random import shuffle<br>    <br>    def trial(deck)<colon><br>        <dq><dq><dq>Return True if the entire deck is used, False otherwise.<dq><dq><dq><br>        shuffle(deck)<br>    <br>        pile_size = 10<br>        deck_index = 0<br>    <br>        while pile_size > 0 and deck_index < 52<colon><br>            pile_size -= 1 #draw the next card from the pile<br>            <br>            next_card = deck[deck_index]<br>            deck_index += 1<br>    <br>            if next_card <= 10<colon><br>                continue<br>            else<colon><br>                pile_size += (next_card - 9)<br>    <br>    <br>        return deck_index >= 52<br>    <br>    def main(num_trials=10**6)<colon><br>        deck = range(2,15)*4 #J=11, Q=12, K=13, A=14<br>        full_deck_count = sum(trial(deck) for _ in xrange(num_trials))<br>    <br>        print <dq>{0} / {1} ({2<colon>.2f}<percent>) used the full deck<dq>.format(full_deck_count, num_trials, float(full_deck_count) / float(num_trials) * 100)<br>    <br>    if __name__ == <sq>__main__<sq><colon><br>            main()    <br><br></p>", 
                "question": "Question about a drinking game..."
            }, 
            "id": "co8wz4q"
        }, 
        {
            "body": {
                "answer": "<p>As far as I know, there is no way <dq>in simple terms<dq> to explain nor solve this (and I doubt there is a decent way, even in complicated terms).  The way I would explore this problem is to write some code to simulate 1,000,000 games, and then do some descriptive statistics on the outcomes (Histogram of number of drinks, total number of cards dealt, etc.)<br><br>I am swamped right now, but could write some code in a month or so if no one come up with an elegant solution.<br></p>", 
                "question": "Question about a drinking game..."
            }, 
            "id": "co8sqvn"
        }, 
        {
            "body": {
                "answer": "<p>This is the appropriate way to go about it, but you should realize the drawbacks. You estimated the error rate for the models *that were built using 4000 samples*, which means you have no guarantees of performance when they are trained on 5000 samples. <br><br>So for example, if you are choosing K for a KNN classifier, then chances are you might end up choosing K too small, since 4000 samples might mean that K=4 is best, but once you get to 5000 samples it is possible that K=5 will now be best.<br><br>In general model selection is a very difficult problem. A lot of people like leave one out (loo) cross-validation, since a model built with 4999 samples will behave very similarly to one built with 5000 samples. But obviously loo will be more computationally expensive than your proposed cross validation scheme. AIC and BIC are also tools that are commonly applied to model selection, and these are performed using ALL of your data at once. <br><br>If you want a book that goes into a lot of the details of error rate estimation and model selection, I suggest [this one.](http<colon>//www.amazon.com/Probabilistic-Recognition-Stochastic-Modelling-Probability/dp/0387946187)  **edit<colon>** This book also discusses some more advanced theoretical topics such as VC dimension, which leads to other tools like structural risk minimization that compete with cross-validation. Here is a [free online primer on SRM](http<colon>//www.autonlab.org/tutorials/vcdim.html).</p>", 
                "question": "Question about model selection with k-fold cross-validation and what to do afterwards"
            }, 
            "id": "co41w2b"
        }, 
        {
            "body": {
                "answer": "<p>Training + validation sets, or alternatively cross-validation, can be used to select tuning parameters. If your goal is to select a single model, the <dq>winning<dq> tuning parameter(s) should then be used in the model/algorithm, which is next fit to the entire training + validation set to yield parameter estimates. An independent test set can then be used to assess prediction error of the model with these particular estimates.<br><br>TL;DR<colon> yes.<br><br>source<colon> [Tibshirani the younger](http<colon>//www.stat.cmu.edu/~ryantibs/datamining/lectures/19-val2.pdf)</p>", 
                "question": "Question about model selection with k-fold cross-validation and what to do afterwards"
            }, 
            "id": "co49xcu"
        }, 
        {
            "body": {
                "answer": "<p>How large of a group do you sample the 200 from?<br><br>And is it the same questions every year?</p>", 
                "question": "Question on handling potential lack of independence for a test"
            }, 
            "id": "co19f38"
        }, 
        {
            "body": {
                "answer": "<p>assuming you don<sq>t want to get too fancy, first conduct anovas for each group to test null hypothesis that R and L side data do not differ (for each dv). if non-significant, combine R and L scores for each of six measurements. then run m/anovas to test differences between groups on those measurements.<br>if you want to control for other variables you can include covariates in ancova or switch to regression.</p>", 
                "question": "Question about which statistical tests to use."
            }, 
            "id": "cnw8xvy"
        }, 
        {
            "body": {
                "answer": "<p>To easily get out some basic, suggestive descriptive statistics one could do the following<colon><br><br>Create a variable that assigns a value of 1 if the patient was in group B and zero otherwise ( the group A guys ), call it e.g. dummy1.<br>Do the same for Left call it e.g. dummy 2, so that dummy2=1 if Left and 0 otherwise (right).<br><br>to compare the left section of group A and B you get something like this<colon><br><br>Tabulate all your 6 variables, given that dummy1=1 &dummy1=1<br><br>Tabulate all your 6 variables, given that dummy1=0 dummy1=1<br><br>You can use the (estimated) variances to see if there are significant differences in the means to conclude this for exampe group B had significantly more left roll than group A.<br><br>Descriptive statistics are usually easy to produce, yet can produce beautiful and powerful intuition with it.</p>", 
                "question": "Question about which statistical tests to use."
            }, 
            "id": "cnwq8cl"
        }, 
        {
            "body": {
                "answer": "<p>In my opinion, ranking means very little in the big picture. Most places that are looking to hire don<sq>t actually care where you got your degree, they just care that you have one. It<sq>s almost a checklist sort of thing<colon> <dq>Has a degree? Yes. Next question.<dq><br><br>Additionally, considerations are a little different pursing an MS than a BS. Firstly, you sound like you plan on pursuing the thesis route and actually have a specific field you want to research in. This should be a big part of what makes your decision. When searching for a school, try to find one that has at least one (preferably more) professors that are researching in the same field, or at least have knowledge and interest in it. As someone who is interested in voting (I assume that<sq>s what electoral forensics is), you wouldn<sq>t get near as much out of a program taught by biostatisticians as you would one that has statisticians focused on surveys, elections, and anything else relevant.<br><br>The second thing I would try to look for is a program that offers support for their graduate students. Although it requires more work, it is much easier on you since you are getting paid, may have the chance at simple benefits such as health insurance, and usually get tuition breaks. However, each program varies in how they handle this due to their funding. For example, when I inquired at Texas A&M, I found out they were short on funding and would only consider supporting candidates if they were on the PhD track. I had absolutely no intention or interest in pursuing a PhD, but they did not know this. My plan was to enter the program as a PhD student, and once I had the MS inform them that I would be leaving the program. This worked out great because I was accepted and offered a very nice assistantship, which I later turned down for different reasons.<br><br>Also in regard to support, here is a consideration for after you have begun attending your new school. Being supported by the department usually means teaching a class or being a teacher<sq>s assistant. This can get quite annoying because the department heads may be very strict, you may not like dealing with students, and you will likely find yourself having to try to divide your time between personal studying and class preparation. If you want to try to get out of this, start talking with the professors and getting to know them and their work. Sometimes researchers will have extra grant money with which they can higher a research assistant. This would get you away from teaching, still provide support for you, and give you more experience in research, which is more useful on a resume than being a regular department assistant.<br><br>I hope this is helpful! Good luck and feel free to ask any other questions you may have!</p>", 
                "question": "Seriously considering going for my masters...how important is a <dq>name brand<dq> uni?"
            }, 
            "id": "cnlkd4e"
        }, 
        {
            "body": {
                "answer": "<p>Ranking isn<sq>t important in the long run, but it can help open up opportunities right away. If you know someone you<sq>d like to work with at a non top 20 school then go for it. Otherwise use the ranking as a guide, often times better professors at better schools are worth it. Apply to a bunch of them then make a decision after you have options.</p>", 
                "question": "Seriously considering going for my masters...how important is a <dq>name brand<dq> uni?"
            }, 
            "id": "cnlqent"
        }, 
        {
            "body": {
                "answer": "<p>If you have a specific field you want to get into - eg sports statistics - it<sq>s a very good idea to go to a school with somebody in that field and which has a track record of placing people in those roles. </p>", 
                "question": "Seriously considering going for my masters...how important is a <dq>name brand<dq> uni?"
            }, 
            "id": "cnm1hwo"
        }, 
        {
            "body": {
                "answer": "<p>I spoke with a few people in the field and for the most part it does matter. However, it depends on where you want to end up. If you are looking at companies/unis that are more prestigious for work then the school matters. If you do not have a good school on your resume they will throw it out immediately. Does that suck? yes but with a more competitive market that is how it goes.</p>", 
                "question": "Seriously considering going for my masters...how important is a <dq>name brand<dq> uni?"
            }, 
            "id": "cnm8pig"
        }, 
        {
            "body": {
                "answer": "<p>Use theory (if you have any) to parse your items/indices and construct scales.  The degree to which your alphas are high and your scale inter correlations are low, and your theory strong, you can proceed.</p>", 
                "question": "Desperately searching for an alternative to Factor Analysis when sample size is extremely small!"
            }, 
            "id": "cnc4wc1"
        }, 
        {
            "body": {
                "answer": "<p>Do you need to use factor analysis? Why not try something like Multiple-criteria decision analysis which involves simple techniques like ranking and weighting the input variables to create a composite index?</p>", 
                "question": "Desperately searching for an alternative to Factor Analysis when sample size is extremely small!"
            }, 
            "id": "cnc55lz"
        }, 
        {
            "body": {
                "answer": "<p>I cannot claim that I entirely understood what you are trying to accomplish, but in any case there is no such thing as a free lunch. Either you get more data or strengthen your prior assumptions.</p>", 
                "question": "Desperately searching for an alternative to Factor Analysis when sample size is extremely small!"
            }, 
            "id": "cnby3q9"
        }, 
        {
            "body": {
                "answer": "<p>You are operating post event. So the probability that the two of you would meet is equal to the probability that the coin I just flipped would land on heads *after* I viewed it and determined that it did indeed land on heads. The probability is 1. 100<percent> <br><br>If we were to pretend that you didn<sq>t have these data, then the next question becomes, what is the scale at which you wish to estimate the probability? Are we going all the way back to the big bang? <br><br><br>The biggest problem here though is that trying to come up with probabilities like this, post-event is incredibly misleading. If you compare what happened to all of the possible things you can think of that *might* have happened it always looks like the probability is extremely low. For example, if I try to come up with a probability that I would have eaten the particular kind of cereal I just ate by comparing all of the brands of cereal that exist, assuming an equal probability that I could have eaten any of them, then it seems like the probability of eating froot loops is really low. But I did indeed eat froot loops. The probability is 1. It will always be 1. </p>", 
                "question": "How could I calculate the possibility of meeting someone?"
            }, 
            "id": "cn293jh"
        }, 
        {
            "body": {
                "answer": "<p>my two cents<colon> the way you phrase this question will influence whether or not you get the kind of <dq>cool<dq> answer you<sq>re looking for. the most global answer would be that the probability is zero, since the set of things that CAN happen is uncountably infinite (zero probability events happen all the time). but i guess it can be looked at a different way, and suppose you phrase it as <dq>what is the probability of two people meeting in [insert city] after living there for [insert number of years]?<dq>, then some answer might be possible. it could potentially be survival analysis, the same stuff used to figured out how long a sick patient has left to live<br>edit<colon> im just guessing on my lunch break here, but i don<sq>t think you can get anything realistic very easily. if you<sq>re just trying to impress your gf, then just make as many simplifying assumptions as possible and make sure that your answer is cute as hell, even if it isn<sq>t accurate.</p>", 
                "question": "How could I calculate the possibility of meeting someone?"
            }, 
            "id": "cn2gyxj"
        }, 
        {
            "body": {
                "answer": "<p>Ah, good ole, Doug Fir. I used to TA a graduate statistics class in a Forestry program, and this sounds like a familiar problem.<br><br>1. No, the typical assumption in a general linear model is that your fixed effects response variables are *fixed* and measured without error. In other words, they are not random, so it doesn<sq>t matter how they are distributed. But you might want to think carefully about how closely your data space matches your sampling frame. Are certain DBH classes or certain elevation underrepresented vis-a-vis your intended scope of inference?<br><br>2. My advice, which probably differs from the typical statistical snoot, is to not agonize over this relatively simple random effects structure being non-normal. The jury is certainly still out, but there a few papers out there (go on and get your google on) that imply that the bias under misspecification is typically small. So don<sq>t worry none.<br><br>Since I<sq>ve got a Forestry background, I wonder exactly what you<sq>re getting at with your hypothesis. Is there something else going here (fire?)? Absent more information about your sampling design, I fail to see how DBH should impact survival all by it<sq>s lonesome or in conjunction with elevation. In a young, dense, even-aged stand, during stem exclusion phase, you<sq>ll find lots of dead trees with a small DBH. In an old stand, especially after a disturbance (fire/wind), you<sq>ll find some trees with a large DBH dead.<br><br>My advice is to not worry so much about your model. All models are imperfect. They are merely tools for gaining understanding. What separate a good ecological statistician, or a statistically-minded ecologist from a bad one is not skill in fitting models and finding nifty transformations, but rather the ability to connect a research question to a sampling plan to a scope of inference. The rest is just mechanics.<br><br></p>", 
                "question": "Got a couple important but basic problems on some modern stats! Please help!!"
            }, 
            "id": "cmrz1lb"
        }, 
        {
            "body": {
                "answer": "<p>Considering that you can put binary variables like sex or cancer status in as independent variables without trouble, you should have no problem with something that is continuous but non normal.<br><br>As for your plot effect, you could try a more general family of transformations like the box-Cox transform, which includes sqrt and log among others.<br><br>How non normal are we talking here? Have you taken a histogram of the plot effects, or a qqnorm plot?<br></p>", 
                "question": "Got a couple important but basic problems on some modern stats! Please help!!"
            }, 
            "id": "cmroqha"
        }, 
        {
            "body": {
                "answer": "<p>The predictors (DBH and elevation) do not need to have a specific distribution but the presumption is that the random effects (residual logits) are normally distributed.  <br><br>However, if I may venture some speculation<colon><br>DBH and elevation are not significant and Plot shows non-normality because DBH and elevation do not really predict survival but something else within the plot probably does and you do not know what that is.  So the lack of normality in the random effects is probably telling you that something in plot is affecting survival but it is not in your model (perhaps because you do not know what it is or did not measure what it is).<br><br>The non-normality in plot is telling you that plot has other predictors that are affecting survival and you need to figure out what they are.  (If plot contained nothing, the random effects would look random, too...they don<sq>t look random...there is something going on besides DBH and Elevation that is not in your model).<br><br>One other item to wonder about, though...is Elevation not extremely correlated to Plot?</p>", 
                "question": "Got a couple important but basic problems on some modern stats! Please help!!"
            }, 
            "id": "cmru2v7"
        }, 
        {
            "body": {
                "answer": "<p>[deleted]</p>", 
                "question": "Using a t-test to predict next observation?"
            }, 
            "id": "cmn8hed"
        }, 
        {
            "body": {
                "answer": "<p>Your question beg for a bayesian approach. Do you think that your data are normally distributed? They looks like counts. Maybe a Poisson or a related distribution? <br>After that specification, you should set an a apriori distribution for your parameters. Later, we could calculate the a [posteri predictive distribution](http<colon>//en.wikipedia.org/wiki/Posterior_predictive_distribution).<br><br>You can do the same using a frequentist approach, but as you stated the question, I think you must use Bayes.</p>", 
                "question": "Using a t-test to predict next observation?"
            }, 
            "id": "cmn1err"
        }, 
        {
            "body": {
                "answer": "<p>Thanks for all your help everyone. I<sq>ll pursue the bayesian approach.</p>", 
                "question": "Using a t-test to predict next observation?"
            }, 
            "id": "cmnju1z"
        }, 
        {
            "body": {
                "answer": "<p>Here<sq>s my really simplistic approach<colon> <br><br>> library(sfsmisc)<br>> <br>> data<-c(3325, 475, 4485, -245, -2660, 3245, 6155)<br>> <br>> xy<-density(data)<br>> <br>> plot(xy)<br>> <br>> names(xy)<br>[1] <dq>x<dq>         <dq>y<dq>         <dq>bw<dq>        <dq>n<dq>        <br>[5] <dq>call<dq>      <dq>data.name<dq> <dq>has.na<dq>   <br>> <br>> indices<-which(xy$x<=0)<br>> <br>> y<-xy$y[indices]<br>> <br>> x<-xy$x[indices]<br>> <br>> integrate.xy(x,y)<br>[1] 0.2754843<br><br><br>So I used R to estimate the probability density function non-parametrically (I am not familiar with the algorithm but look up Kernel Density Estimation). <br><br>Now that I have the PDF, I integrate it from the lowest point to zero, giving me approximately the probability a point is lower than 0.<br><br>To all the real statisticians out there, sorry if I butchered it.<br><br>Edit<colon> Woops, couldn<sq>t read, you need to take 1 - 0.275 ~ 0.725 to be the probability something will be greater than 0, I calculated for something less than 0.</p>", 
                "question": "Using a t-test to predict next observation?"
            }, 
            "id": "cmnbn7u"
        }, 
        {
            "body": {
                "answer": "<p>I use [this](http<colon>//practicalstats.labanca.net/index.php?title=Selecting_a_Post_Hoc_test) as a rule of thumb.</p>", 
                "question": "When to use which post-hoc comparison test for ANOVA?"
            }, 
            "id": "cmg16dg"
        }, 
        {
            "body": {
                "answer": "<p>Disclaimer<colon> Undergrad who just added statistics as a major<br><br>From my understanding, for large datasets you would want to use Tukey. Fisher<sq>s is good for smaller datasets, but with large ones can generate <dq>false differences<dq> (type 1 error?)<br><br>Edit<colon> Please correct me if I<sq>m wrong</p>", 
                "question": "When to use which post-hoc comparison test for ANOVA?"
            }, 
            "id": "cmg3fjz"
        }, 
        {
            "body": {
                "answer": "<p>How many groups/conditions are we dealing with? You have 2 time points, but is there only 1 group? 2 or more? If just one group, a t-test is fine. If more than one, I would suggest an ANOVA.<br><br>To your question about assumptions, an ANOVA and a t-test share the vast majority of assumptions about normal distributions, independence of observations, homogeneity of variance, etc...so that isn<sq>t really a big deal here. The bigger deal is the number of comparisons required to draw your conclusions. That is, if it<sq>s just A vs. B, that<sq>s exactly what a t-test is good for. But if it<sq>s A vs. A<sq> vs. B vs. B<sq>, go with the ANOVA.<br><br>To your question about same results...for the sake of simplicity, let<sq>s say you have a 2x2 (AxB) design. Generally speaking, if you find a main effect of A, the pairwise t-test would also be significant under most circumstances (and so on for an effect of B). Things get a little more complicated for interactions, and frankly, it<sq>s mathematically/conceptually inappropriate to use t-tests to parse an interaction following an ANOVA anyhow (this is a separate and honestly somewhat pedantic story, which I<sq>m happy to go into if you<sq>d like). So in a nutshell, t-tests and ANOVAs will converge on the same result 9 times out of 10, except interactions (which t-tests aren<sq>t suited to describe anyhow).<br><br>Hopefully this helps! Sorry if I over-explained or took things too basic.</p>", 
                "question": "is it correct to use ANOVA repeated measures rather than paired t-test having only 2 time points?"
            }, 
            "id": "cmequtm"
        }, 
        {
            "body": {
                "answer": "<p>Case A<colon> You are saying that the robot picks randomly.<br><br>Case Bi and Bii<colon> It is unclear what you are saying.  It is unimportant what the program <dq>knows<dq>, but is important what it <dq>does<dq>. What is the process the program uses to select the color?  Infinite possibilities exist. Several interesting cases<colon><br><br>a) select deterministically yellow-blue-yellow-blue<br><br>b) select probabilistically with probability =R (e.g. R=2/3<colon> roll die, if 1-4, b else y) <br><br>c) select deterministically with probability =R (e.g., if R=2/3 pick b,b,y,b,b,y,b,b,y...)<br><br>d) select either probabilistically or deterministically with probability not = R.<br><br>In cases b) and c) you will approach ratio R. In case a) you won<sq>t unless R=.5.  In case d) , you won<sq>t by design.</p>", 
                "question": "Proof construction help"
            }, 
            "id": "cm2bprq"
        }, 
        {
            "body": {
                "answer": "<p>In Case A, it is clear that the ratio of balls in the sample will go to the ratio of the balls in the pit as the balls are selected at random (e.g., Pr(x = 1) = R and Pr(x = 0) = (1- R)) and the draws are iid. <br><br>This is pretty straightforward. If blue = 1 and yellow = 0 and R = fraction of pit that is blue, R = Pr(ball = blue) = Pr(x = 1) and the MLE of that is simply the fraction of the sample that is blue divided by the sample size. MLE are [asymptotically consistent](http<colon>//en.wikipedia.org/wiki/Maximum_likelihood#Properties). <br><br>In Case B, it doesn<sq>t matter that the arm knows or doesn<sq>t know the value of R. The program does nothing with that information. So B_i and B_ii are the same. <br><br>The robot selects x = 1, say, with Pr(x = 1) = A. The sample drawn by that program will have a ratio of sum(x = 1) / total equal to A as the number of draws increases. This will only be the same as Case A when R == A by chance.</p>", 
                "question": "Proof construction help"
            }, 
            "id": "cm2hw83"
        }, 
        {
            "body": {
                "answer": "<p>You<sq>ve got it backwards<colon> the things you do (add constructs, add metals to substrate) are the independent variables, the things you measure (metal uptake(?), root length) are the dependent variables.<br><br>At the simplest, you<sq>ve got a 3 factor Anova, if I understand correctly.<br><br>Pretend you are only measuring cadmium uptake in the plants.  The three treatments are genotype, cadmium added, and fe added.  <br><br>It is unclear to me if every possible combination is present.<br><br>If you wanted to analyze this in R, you<sq>d do<br><br>M1 =aov(uptake ~ genotype\\*cadmiumadded\\*feadded)<br><br>    summary(M1)<br><br>Interpreting the model is another story.<br><br>Furthermore, you actually have two dependent variables, which you could analyze separately or together in a manova.  I<sq>d avoid the manova if you don<sq>t understand anova. Good luck!</p>", 
                "question": "How does ANOVA apply to my data? Bonus<colon> How do I do it in R?"
            }, 
            "id": "cm0lakc"
        }, 
        {
            "body": {
                "answer": "<p>The shapes around the origin are a set of coefficients with the same penalty.  Since ridge penalizes according to the squared 2 norm of the coefficients, this is a circle.  Since lasso uses the 1 norm, it is a square.<br><br>The ellipsoid contours represent level sets of square error sums for values of coefficients.  So each set of betas in one ellipsoid has the same SSE. </p>", 
                "question": "Contour plots of penalized regression"
            }, 
            "id": "clz6wox"
        }, 
        {
            "body": {
                "answer": "<p>I would say that the data are not normally distributed.</p>", 
                "question": "<dq>Abnormal<dq> data distribution or <dq>Not Normal<dq> data distribution?"
            }, 
            "id": "clyzr0o"
        }, 
        {
            "body": {
                "answer": "<p>They mean two very different things<colon> not normal is specifically in reference to the normal distribution. Abnormal would be in reference to some other defined or implied context i.e. more like everyday use of the term, and definitely not in relation to the normal distribution. Of course, an abnormal distribution is likely not normal.</p>", 
                "question": "<dq>Abnormal<dq> data distribution or <dq>Not Normal<dq> data distribution?"
            }, 
            "id": "clz0fnt"
        }, 
        {
            "body": {
                "answer": "<p>Between<br><br>* <dq>Abnormal distribution<dq><br>* <dq>Not Normal<dq> distribution<br><br>and<br><br>* NOT <dq>normal distribution<dq><br><br>I would go with the third.</p>", 
                "question": "<dq>Abnormal<dq> data distribution or <dq>Not Normal<dq> data distribution?"
            }, 
            "id": "clz3t3q"
        }, 
        {
            "body": {
                "answer": "<p><dq>The data aren<sq>t normally distributed<dq>.<br><br>Of course, without some further qualification, it<sq>s an uninformative statement, since no non-artificial data are going to really be normally distributed. Some data may be from distributions that are close enough that it<sq>s not unreasonable to assume it for some purpose, but that<sq>s not that same thing.<br><br>George Box put it this way<colon> <dq>*All models are wrong, but some are useful.*<dq><br></p>", 
                "question": "<dq>Abnormal<dq> data distribution or <dq>Not Normal<dq> data distribution?"
            }, 
            "id": "clz7e8b"
        }, 
        {
            "body": {
                "answer": "<p>I<sq>ve seen that done where the scale is intentionally kept ambiguous. For example, if all that is marked is 0 = Not at all and 5 = Quite a lot, and numbers 1,2,3 & 4 don<sq>t have labels. That means the scale is treated as a continuous variable from 0-5, and therefore amenable to parametric testing. It<sq>s not perfect, but that<sq>s the rationale. </p>", 
                "question": "ANOVA and the Likert scale"
            }, 
            "id": "clvlnz8"
        }, 
        {
            "body": {
                "answer": "<p>Likert (and likert type) scales are intervals, not ordinal\u2014we can have an arbitrary zero (eg, 0-6 scale) and we assume that the difference between each point is equal ie, the diff between 3 & 4 = the diff between 5 & 6). The use of parametric tests like ANOVAs is appropriate, as interval scales can be normally distributed. There is also variance in people<sq>s scores. In other words,  let<sq>s say that on a 1-7 scale, the random sample mean is 5.3. In collecting these data, we hope to lean about a particular population (perhaps how Americans feel about legalizing weed). This, we infer that 5.3 is the parametric, or population mean. <br><br>When we don<sq>t necessarily know what a population<sq>s distribution would be, like with an ordinal checklist, then we turn to non-parametric tests.</p>", 
                "question": "ANOVA and the Likert scale"
            }, 
            "id": "clvp2qy"
        }, 
        {
            "body": {
                "answer": "<p>Simulations have demonstrated likert type scales of 7 intervals or more approximate an interval scale and parametric testing is appropriate. When dealing with less than 7 intervals, traditional analyses can provide biased results. I<sq>m in my phone so I can<sq>t link to the article (s) </p>", 
                "question": "ANOVA and the Likert scale"
            }, 
            "id": "clwftqo"
        }, 
        {
            "body": {
                "answer": "<p>I think it depends on what the independent and dependent variables are like. I have read about parametric tests like binary logistic regressions being used on Likert scale data. Not sure about ANOVAs though.</p>", 
                "question": "ANOVA and the Likert scale"
            }, 
            "id": "clvkt83"
        }, 
        {
            "body": {
                "answer": "<p>I should also say that if I use a permutation test (bootstrapping for a difference in medians), it works a treat - BUT, it doesn<sq>t hold up against an  N of 8 for multiple comparison testing.</p>", 
                "question": "Not sure what statistic to use!"
            }, 
            "id": "cls85p7"
        }, 
        {
            "body": {
                "answer": "<p>These are very tricky terms whose meaning varies by discipline. Can you clarify [which version of <dq>fixed effects<dq> and <dq>random effects<dq>](http<colon>//andrewgelman.com/2005/01/25/why_i_dont_use/) you intended, please?</p>", 
                "question": "Random vs. Fixed effects?"
            }, 
            "id": "cllcrbb"
        }, 
        {
            "body": {
                "answer": "<p>I<sq>m not sure whether or not the way you<sq>re using the terms is correct, but you would use a fixed effect analysis (e.g Peto) when you are certain that you have all the available data on all the variables being studied and you would use the random effects model (e.g. Dersimmonian and Laird) when you are pretty sure you are only working on a sample of all the available data.<br><br>The difference is that the fixed effects model only considers between-subject variance (assumes the between study variance is zero) while the random effects model considers both as a source of variance.<br><br>If you consider the normal curve produced by each of these, the fixed effects is narrower, making it easier to detect a stat sig difference.<br><br>As the random effects model considers a second source of variance, the normal curve is wider and flater, making it more difficult to detect a stat sig diff.<br><br>Very simplistic explanation, does this make sense to you?</p>", 
                "question": "Random vs. Fixed effects?"
            }, 
            "id": "cllfolu"
        }, 
        {
            "body": {
                "answer": "<p>make sure you learn the tools of the trade...excel, R, SAS, etc .. ... </p>", 
                "question": "a career in statistics?"
            }, 
            "id": "clj3g7b"
        }, 
        {
            "body": {
                "answer": "<p>There are actually a lot of jobs in statistical field. I<sq>m a M.Sc. Statistics and I work in a big company. I do performance analysis. <br><br>I have some friends working as <dq>contractual statistician<dq> with several scientists. They do the <dq>stats<dq> part of the research of biologist/physician/psychologist/etc.<br><br>I also have friends working as actuaries for assurances companies.<br><br>Government also hire statisticians in their research center and for census.<br><br>A lot of company hires Statistician to perform analysis on Big Data. Credit Card company, Loyalty programs, big data warehouse (Google, Facebook, etc) all have tremendous quantity of information and want to exploit it.</p>", 
                "question": "a career in statistics?"
            }, 
            "id": "cliqxbi"
        }, 
        {
            "body": {
                "answer": "<p>In essence, two things are of interest to you in the problem you posed - either the outcome occurs at least once over 18 trials or it doesn<sq>t. There are ways to directly calculate the probability of the outcome occurring at least once in 18 trials but that<sq>s lengthy. <br><br>Instead, consider the probability of the outcome not occurring at all over 18 trials. Intuitively you should see the probability of that happening is (7/9)^18 = 0.01085 (1<percent> chance). It is then easily seen that if there<sq>s roughly a 1<percent> chance of the outcome not occurring in 18 tests, then there<sq>s a 99<percent> chance of it occurring at least once.<br></p>", 
                "question": "If I know the chance of an event occurring in a given test how would I find out the probability of said event occurring given a specific number of tests?"
            }, 
            "id": "cl9v7yf"
        }, 
        {
            "body": {
                "answer": "<p>The way of determining odds for multiple occurrences is to multiply them together.  However, you want to determine odds for one occurrence in a certain number of times which is really just the reverse of determining the odds of no occurrences in that number of times.  A 2/9 chance means a 7/9 chance that it won<sq>t happen.  (7/9)^18 gives the odds that it won<sq>t occur all 18 times.  1-(7/9)^18 gives the odds that it will occur at least once within those 18 times.  That is approximately 99<percent>.  </p>", 
                "question": "If I know the chance of an event occurring in a given test how would I find out the probability of said event occurring given a specific number of tests?"
            }, 
            "id": "cl9we5t"
        }, 
        {
            "body": {
                "answer": "<p>The chance that it *doesn<sq>t* happen at all is 7/9 x 7/9 x ... x 7/9 = (7/9)^18 so the chance it happens at least once is 1-(7/9)^18 </p>", 
                "question": "If I know the chance of an event occurring in a given test how would I find out the probability of said event occurring given a specific number of tests?"
            }, 
            "id": "clanips"
        }, 
        {
            "body": {
                "answer": "<p>If you have basalt samples from known volcanoes, and if individual volcanoes produce basalt with distinct pXRF spectra, then you would expect this to show up in Principal Component Analysis as distinguishable clusters of points, and by projecting your altar spectra onto the first two principal components you can see which cluster they are closest to. I<sq>d be happy to write you an R script if you sent over the data, PM me if you<sq>re interested. <br> <br></p>", 
                "question": "Need help with multivariate analysis and PCA for archaeological analysis - ELI5"
            }, 
            "id": "cl7rr62"
        }, 
        {
            "body": {
                "answer": "<p>Attempt at ELI<sq>ve-got-little-stats-background of PCA (which you seem to struggling with)<colon> The idea here is that you don<sq>t compare Pb with St or Br directly at all. That would be difficult, since you end up doing many comparisons, which might even be conflicting. What you are attempting is to describe the general profile of your samples as simply as possible. That is, you don<sq>t want to describe the profile with many variables (as in 1 per element) but with few.<br><br>How this works is about as follows. Let<sq>s say you have two variables, a and b. Both these variables contain *information* about your basalt. You could choose to look at both. However, it turns out that a and b are *correlated*, like super-strongly. This means they *share* a lot of the information. If you make a scatterplot there<sq>s almost a straight line! You might now just as well say that you will only use one variable. That is, you make a new variable that contains as much information as possible from both a and b (there<sq>s math to do that). Since they shared so much information, there is little loss in combining them.<br><br>You can do this also with many variables at once, trying to come up with only a few new variables that contain as much information from your original ones as possible. Now these new variables are a good measure of the *overall profile* of your sample.<br><br>If you reduce your dataset to two variables, usually called principal component 1 and principal component 2, you can use those two variables in a scatter plot. You can then place each sample in this plot as a point. You might then look where your stone altar is in that plot, and see whether it is particularly close to some of your other data points.<br><br>Note that PCA is usually not very easy to interpret and it<sq>s good to have someone look at it that has a bit more experience.<br><br>Hope that helps, otherwise just reply here.</p>", 
                "question": "Need help with multivariate analysis and PCA for archaeological analysis - ELI5"
            }, 
            "id": "clazg8j"
        }, 
        {
            "body": {
                "answer": "<p>First, this is not a statistics question, it<sq>s a game theory question. Second, there<sq>s a huge literature on signaling games, cheep talk, bluffing, babble, etc. starting with Cho and Kreps in the 80s. Start there. </p>", 
                "question": "Suggestions on how to model bluffing behavior at the final decision node of round one of a twice-repeated signaling game."
            }, 
            "id": "ck7fg4u"
        }, 
        {
            "body": {
                "answer": "<p>Easy way<br><br>    colMeans(sample,na.rm=TRUE) # For means, deleting NAs<br>    apply(sample,2,sd) # For sd<br>    apply(sample,2,function(x) {sd(x,na.rm=TRUE)}) # For sd, deleting NAs<br><br>Package <dq>psych<dq> have *describe*, which do the trick, too<br><br>    library(psych)<br>    describe(sample)<br><br>UPDATE<colon>  /u/iacobus42 noted that we don<sq>t need an anonymous function on apply<br><br><br>    apply(sample,2, sd, na.rm=TRUE) # For sd, deleting NAs<br></p>", 
                "question": "Getting a mean and SD for multiple variables in R"
            }, 
            "id": "cjhrl2g"
        }, 
        {
            "body": {
                "answer": "<p>     cbind(mean=apply(sample,1,mean,na.rm=TRUE),sd=apply(sample,1,sd,na.rm=TRUE))<br><br>or if you prefer it the other way around<colon><br><br>     rbind(mean=apply(sample,1,mean,na.rm=TRUE),sd=apply(sample,1,sd,na.rm=TRUE))<br><br>If your columns are labelled, you can apply those as row or column labels (respectively) to the output.<br><br>If you prefer your results in a data frame<colon><br><br>     data.frame(mean=apply(sample,1,mean,na.rm=TRUE),sd=apply(sample,1,sd,na.rm=TRUE))<br></p>", 
                "question": "Getting a mean and SD for multiple variables in R"
            }, 
            "id": "cji6hwa"
        }, 
        {
            "body": {
                "answer": "<p>Since this is something I do a lot, I define a function to make a decent-looking table of summary stats.  It is easily customizable to add/remove specific summary statistics.  Copy/paste this into R, then run it with sumstats(data)<br> <br>sumstats= function(y){<br>sumst=sapply(y, function(x){ sumstat=c(mean(x,na.rm=TRUE),median(x,na.rm=TRUE), sd(x,na.rm=TRUE),min(x,na.rm=TRUE),max(x,na.rm=TRUE))<br>names(sumstat)=c(<dq>Mean<dq>,<dq>Median<dq>,<dq>SD<dq>,<dq>Min<dq>,<dq>Max<dq>)<br>sumstat})<br>aperm(sumst)<br>}   <br></p>", 
                "question": "Getting a mean and SD for multiple variables in R"
            }, 
            "id": "cjloxi1"
        }, 
        {
            "body": {
                "answer": "<p>Are you looking for a masters or a bachelors in statistics? <br><br>My undergraduate had nothing to do with math/biology either, but I ended up getting accepted for a masters biostatistics program. What the university did was accept me on a provisional basis until I had completed the pre-reqs for the program. I<sq>m getting a decent grasp on statistical theory, but I<sq>ve also loaded up on the more applied side of statistics. <br><br>I would say personal experience could benefit more than just the degree, but you need to know why things work before you can say you<sq>ve got decent experience in using statistics. A combination of classes and side projects can help you in the long run. </p>", 
                "question": "Late to the game- how to pursue career in statistics as a humanities college graduate?"
            }, 
            "id": "cjdnswi"
        }, 
        {
            "body": {
                "answer": "<p>Most biostatistics programs will require 3 semesters of calculus and a one semester course in linear algebra.  If your undergrad degree is from a solid school, you<sq>ll look OK in general.  Consider either extension courses or community college courses at a decent school to save some money.  General stats programs might require more math.</p>", 
                "question": "Late to the game- how to pursue career in statistics as a humanities college graduate?"
            }, 
            "id": "cjmhuue"
        }, 
        {
            "body": {
                "answer": "<p>Answer unanswered questions on stats.stackexchange.com - there are questions at every level on almost any topic ... and if you make the odd mistake people tend to be quite nice when they tell you you<sq>re mistaken (on average noticeably more so than on stackoverflow or math.stackexchange). If you don<sq>t see any suitable questions there<sq>ll be another 50-60 the next day.<br><br>I<sq>ve had numerous answers there that were dramatically improved by kind but insightful comments by other users.<br><br>Reading other answers is also useful.<br><br>Answering questions on the edge of your skill set gets you to do some research, learn new things, and helps with communication skills.<br></p>", 
                "question": "Like Project Euler but for probability and statistics?"
            }, 
            "id": "cj8y3ru"
        }, 
        {
            "body": {
                "answer": "<p>I don<sq>t know! Kaggle is something. Or. Treat stack overflow as that, and every question is a project Euler problem. Otherwise, open a book on the subject and solve problems... </p>", 
                "question": "Like Project Euler but for probability and statistics?"
            }, 
            "id": "cj8hrp3"
        }, 
        {
            "body": {
                "answer": "<p>At a bare minimum, it<sq>s a good idea to have your algebra and arithmetic skills at the front of your brain, so a calculus course can help with that.  Additionally, it<sq>s simply not possible to fully understand basic regression methods without at least some calculus.<br><br>You don<sq>t necessarily need that level of understanding if you<sq>re only planning to do quantitative analysis as a <dq>secondary<dq> part of your job (I got away with it for 7 years).  If you want to be more of an <dq>expert<dq> in quant methods, though, you need probability and inference, and those require calculus and a little linear algebra.<br><br>I<sq>m now entering my 4th year in a quant-heavy social science program, and everyone in my program is required to take multivariable calc/linear algebra.  The probability and inference is either baked-in to higher-level stats courses or can be taken as <dq>pure<dq> probability/inference in my university<sq>s stats department.  However, my school has an applied stats progression that doesn<sq>t require any calculus at all, which seems to work fine for most master<sq>s students and doctoral students outside my program.<br><br>In terms of a general path, I think most people in my program take the intro & intermediate applied stats courses while simultaneously taking calc/linear algebra in their first year, and then take advanced stats courses (which often include some probability and inference) in second year and beyond.  I did things a little differently because I took calc/linear algebra night classes while I was still working, before entering the program.<br><br>I will say that having calc/linear algebra and probability/inference under my belt made the advanced applied stats courses MUCH easier for me, so even if they<sq>re not strictly required, they can be very helpful.</p>", 
                "question": "Self study<colon> calculus before statistics?"
            }, 
            "id": "cj1uszt"
        }, 
        {
            "body": {
                "answer": "<p>Like the other two I actually got my Masters in Stats, with a minor/emphasis in statistics. I never took any calc, and now hold a position as a biostatistician at a hospital. <br><br>Calc is required if you really want to understand all of the math happening in some of the complex models/methods. If you simply want an applied understanding of many many stats methods, it<sq>s not so important. In all of my classes I just glazed over when they talked about derivatives and such. I just wanted to know how to use it/when to use it/what to check to make sure I used it appropriately.<br><br>And in real life those are the things that matter anyway, you<sq>ll never need to do these things by hand.</p>", 
                "question": "Self study<colon> calculus before statistics?"
            }, 
            "id": "cj1uu5a"
        }, 
        {
            "body": {
                "answer": "<p>You will understand statistics much better after taking calculus. Statistics before calculus consists of memorizing a bunch of formulas and test procedures. Statistics after calculus is when you really learn why the formulas exist in the first place, which allows for a much deeper understanding.</p>", 
                "question": "Self study<colon> calculus before statistics?"
            }, 
            "id": "cj1v3jw"
        }, 
        {
            "body": {
                "answer": "<p>I just finished a stats heavy degree in psych, and haven<sq>t taken a single college calc class. <br><br>Unsure about anyone else<sq>s experiences, but I<sq>ve not found the lack of calc to be a hinderence, so far. <br><br>Unless there<sq>s something beyond ANOVAs, Willcoxian, and Mann-Whitney that requires a calc background to understand, I don<sq>t think it<sq>s really essential. <br><br>At least, for us since I<sq>m assuming your public health program is probably similar to psych in its relationship to statistics -- practical application for conducting/understanding research -- where as a stats degree/minor would include a lot more theory/background where it<sq>s probably needed.  <br><br>For building a solid foundation, check out the [Khan Academy section on probability and stats](https<colon>//www.khanacademy.org/math/probability).<br><br>I did my first two research methods (stats) classes online, and basically self-taught the material using that site and the textbook. </p>", 
                "question": "Self study<colon> calculus before statistics?"
            }, 
            "id": "cj1oy2r"
        }, 
        {
            "body": {
                "answer": "<p>Probably decreased. They would be unchanged if the events were independent, but that<sq>s not the case. The system responds to events. The entire airline industry, including carriers and manufacturers, is engaged in reducing any preventable risks.</p>", 
                "question": "Are the odds of a 3rd Malaysian Airline crash increased or decreased after 2 crashes?"
            }, 
            "id": "cj095rw"
        }, 
        {
            "body": {
                "answer": "<p>There<sq>s no good answer that can be gleaned purely from statistics.  Statistics at its core deals with conditional probabilities (and distributions and means).  When you apply statistics to the real world, you need to throw in a lot of assumptions; the stats will be correct under your assumptions (as long as you don<sq>t screw them up), but if your assumptions are wrong, then they<sq>re not very useful.<br><br>If we assume everything is static (i.e., all humans involved will continue to act just as they have prior to, during, and after the crashes), then nothing has changed, because crashes are independent of each other.  This would be similar to flipping a coin<colon> the probability of getting tails on the next flip doesn<sq>t increase or decrease even after we<sq>ve flipped heads 10 times in a row.<br><br>[Our *estimate* of the probability that the next flight will crash is definitely going to increase, compared to what our estimate would have been if there had been no crashes (this should be true from both a frequentist and Bayesian perspective).]<br><br>On the other hand, if you assume the crashes were caused by some recent negligence or incompetence on the part of Malaysian Airlines, then it<sq>s quite possible that the underlying probability of a Malaysian Airlines crash has recently increased, and the two recent crashes are an indicator of that.<br><br>On the third hand, if you assume that Malaysian Airlines has learned its lesson and is going to start taking safety precautions that are much more stringent than ever before, it<sq>s possible that the underlying probability of a crash has decreased.<br><br>So...assumptions make all the difference. <colon>)</p>", 
                "question": "Are the odds of a 3rd Malaysian Airline crash increased or decreased after 2 crashes?"
            }, 
            "id": "cj14n5b"
        }, 
        {
            "body": {
                "answer": "<p>The simplest and maybe best option is just to make a histogram showing the frequency of responses for each question. Overlay before and after using translucent colours. There are two good reasons to do this. First, you<sq>re using all the data available and should be able to catch any more complicated patterns (mean stays the same, variance increases for example). Second, it<sq>ll be much easier for your class to see and follow along.<br><br>Along with this, you should report the median and / or mode before and after the semester in a table so they can figure out what<sq>s changed in a glance.<br><br>If you<sq>re looking for a significance test to verify that these differences are <dq>real<dq>, you<sq>ll need to use a nonparametric statistical test.<br><br>Basically, the traditional t-tests, ANOVAs etc. make a number of assumptions that don<sq>t really hold up here. First, they assume that the distance between the values you recorded is consistent. That is, going from 4 on your ranking to 6 after is exactly as big of a change as going from 7 to 9. Because your scale is fairly subjective, all you can say with confidence is that 4<6<7<9. That is, your data is ordinal, rather than interval or ratio. Second, as you probably remember, t-tests assume that your data follows a normal distribution. Unfortunately, this doesn<sq>t make sense here because the distances between your values don<sq>t behave.<br><br>Nonparametric tests assume that only order (are data points relatively higher or lower?) matters. To compare two distributions (like before and after for one of your questions) and ask if one is <dq>typically greater<dq> than the other, you should use a [sign test](http<colon>//en.wikipedia.org/wiki/Sign_test). Basically, it tallies, for each of the possible comparisons between distribution A and B, if data point A or data point B is better.  Your output (test statistic) will tell you what fraction of these comparisons favored distribution A / B. You<sq>ll also be able to produce a p-value from this.<br><br>If you know which student is responsible for each data point, you should used a paired test. A paired sign test in this case. Basically speaking, because the data points from before and after match, you want to look at how much they<sq>ve changed , not just whether the distributions by themselves are different.<br><br>Hope this helps! Let me know if you have any other questions <colon>)</p>", 
                "question": "Help Needed Determining Best Method To Analyze Data"
            }, 
            "id": "cit2dx5"
        }, 
        {
            "body": {
                "answer": "<p>If you<sq>re just trying to show the class how much they<sq>ve learned, just use simple histograms comparing average responses at the beginning and end of class visually. I would expect your students could care less about statistical significance if you<sq>re just trying to demonstrate to them how much they<sq>ve learned/changed, especially if you<sq>re teaching a class other than statistics. <br><br>If you are interested in statistical inference, I<sq>d recommend a Wilcoxon signed rank test as a nonparametric alternative to the paired t-test. If your class is relatively small, the nonparametric approach is the way to go. <br></p>", 
                "question": "Help Needed Determining Best Method To Analyze Data"
            }, 
            "id": "cit1s2y"
        }, 
        {
            "body": {
                "answer": "<p>I was thinking a T Test?  But it has been years since I have taken statistics.  I can<sq>t remember the difference between 1 or 2 tails and Paired, Two-sample equal variance, and two-sample unequal variance.  Thanks.</p>", 
                "question": "Help Needed Determining Best Method To Analyze Data"
            }, 
            "id": "ciszo51"
        }, 
        {
            "body": {
                "answer": "<p>Consider using Bayesian estimates, which can be designed to be sensitive to factors like number of votes or passage of time (or many other factors - it<sq>s quite flexible). A number of sites use Bayesian estimates to good effect, including [IMDb](http<colon>//masanjin.net/blog/bayesian-average) and [BeerAdvocate](http<colon>//www.beeradvocate.com/lists/top). The general idea is that items with few votes are heavily weighted towards the average of all items in the list (given that we have little information about the particular item, the safest assumption is that it is <dq>average<dq>), whereas items are weighted more towards the average rating for that item as more votes accrue.<br><br>I am not an expert on this approach, so I will not attempt to mangle the math, but you can read more about it [here](http<colon>//andrewgelman.com/2007/03/21/bayesian_sortin). An alternative useful approach is using the lower bound of the Wilson score confidence interval, as described [here](http<colon>//www.evanmiller.org/how-not-to-sort-by-average-rating.html).</p>", 
                "question": "What<sq>s the best way to weight reviews?"
            }, 
            "id": "cip5nnk"
        }, 
        {
            "body": {
                "answer": "<p>You<sq>ve got a population.<br><br>You take a sample from that population. You calculate a mean. That<sq>s a sample mean. <br><br>Do it again.<br><br>Do it again.<br><br>Do it again.<br><br>Keep going.<br><br>You<sq>ll have lots of sample means. Plot a histogram of them, and they<sq>ll have a normal distribution. That<sq>s the normal distribution that we care about.<br><br>Take one of your samples, calculate the standard error. That<sq>s the estimated standard deviation of all of the sample means that you have.<br><br>Take one of your samples. Calculate the 95<percent> confidence intervals of the mean. In 95<percent> of samples, the population mean will lie within the 95<percent> confidence intervals.<br><br>Whenever we talk about things like p-values, we<sq>re talking about the probability of an event, over lots of samples. Samples that you didn<sq>t take, but you have to pretend that you took them.<br><br></p>", 
                "question": "I<sq>m not understanding this sentence- could you please translate?"
            }, 
            "id": "cib13jw"
        }, 
        {
            "body": {
                "answer": "<p>This, by the way, is the beloved, powerful, and often misunderstood Central Limit Theorem. </p>", 
                "question": "I<sq>m not understanding this sentence- could you please translate?"
            }, 
            "id": "cib1q2i"
        }, 
        {
            "body": {
                "answer": "<p>[deleted]</p>", 
                "question": "Highest possible IQ?"
            }, 
            "id": "ci8np28"
        }, 
        {
            "body": {
                "answer": "<p>IQ scores, as I understand them, are **normally** distributed with (typically) mean 100 and SD 15.  (The binomial distribution, with large enough n, can be approximated by the normal distribution -- that may be part of the confusion.)<br><br>In theory, a normally distributed random variable has no upper bound. In practice, though, IQ tests of course have upper (and lower) bounds.<br><br>That said, what I think you<sq>re asking for is the (7.2*10^9 - 1)/(7.2*10^9) quantile of a N(100, 15^2) distribution, which isn<sq>t really the highest IQ score possible but would be, probabilistically speaking, the upper limit of the scores of everyone except the smartest person in the world.  This comes to approximately 194.6605 (using [WolframAlpha](http<colon>//www.wolframalpha.com/)).</p>", 
                "question": "Highest possible IQ?"
            }, 
            "id": "ci8npv0"
        }, 
        {
            "body": {
                "answer": "<p>IQ is *normally* distributed, not binomial. The upper bound of a normal RV is infinity (and so is the lower bound, so technically IQ is a truncated normal random variable). For every finite value x, there is some non-zero probability that a random draw from the normal distribution will be greater then x. It is not possible to calculate an upper bound. <br><br>You can say, however, that there is a given probability that in a draw of size 7.2 billion from a N(100, 15) that at least one person will have an IQ that large or larger. You can also calculate the expected number of people with an IQ that large or larger. The expected number one is easier to calculate. <br><br>First, you would standard IQ scores to Z scores using a mean of 100 and a sd of 15. You would look up the density of the standard normal that falls below this value using a table like this (http<colon>//image.mathcaptain.com/cms/images/95/normal-table-large.png). If an IQ of 145, that would be (130-100)/15 = 2 standard deviations. I would find the 2.00 point on the table and see that 97.72<percent> of the population falls under that value. So ~2.25<percent> have IQs higher than that value. I would take the product of that number and 7.2 billion to find the expected number of people with IQs of 130 or more. <br><br>Using R to do the math, you get the following<colon><br><br>     IQ    Expected number of people currently alive with higher IQ<br>    100    3,900,000,000<br>    110    1,817,946,270<br>    120      656,720,782   <br>    130      163,800,950<br>    140       27,578,740<br>    150        3,089,234<br>    160          228,033<br>    170           11,020<br>    180              347<br>    190                7<br>    200                0.09<br><br>IQs don<sq>t really follow a normal distribution once you start getting to the extremes. For IQs, say, 60-130 or even 45-145, IQs are reasonably normally. However, when you start getting into the limits, especially the upper limits (>= 150), the approximation isn<sq>t great and this probably overestimates the number of people with IQs that large or larger. </p>", 
                "question": "Highest possible IQ?"
            }, 
            "id": "ci8nzty"
        }, 
        {
            "body": {
                "answer": "<p>It would be something like 6.3 standard deviations above the mean, so 194.  You can use wolfram alpha to calculate this http<colon>//www.wolframalpha.com/input/?i=6.3+standard+deviations</p>", 
                "question": "Highest possible IQ?"
            }, 
            "id": "ci8nyen"
        }, 
        {
            "body": {
                "answer": "<p>conditional on your asking this question; 1</p>", 
                "question": "My wife and I were born on the same day in the same hospital. Our daughter was born on our birthday. What are the odds?"
            }, 
            "id": "ci2y9j1"
        }, 
        {
            "body": {
                "answer": "<p>Chance of three people sharing a birthday = 1 / 365^2 = 1 / 133,225<br><br>Gives new meaning of term <dq>birthday<dq> for your wife.<br><br>Edit<colon> I ignored leap years...maybe should use 365.25?</p>", 
                "question": "My wife and I were born on the same day in the same hospital. Our daughter was born on our birthday. What are the odds?"
            }, 
            "id": "ci2y2k5"
        }, 
        {
            "body": {
                "answer": "<p>The odds this would happen to YOU are small. But unless this had happened to you, you<sq>d never even think about this particular thing. <br><br>The odds that this would happen to someone are pretty good. And whoever that happens to be will think<colon> wow, what are the odds?</p>", 
                "question": "My wife and I were born on the same day in the same hospital. Our daughter was born on our birthday. What are the odds?"
            }, 
            "id": "ci34qv6"
        }, 
        {
            "body": {
                "answer": "<p>Infinitesimally small depending on your ethics, fertility rates and life travel arrangements...</p>", 
                "question": "My wife and I were born on the same day in the same hospital. Our daughter was born on our birthday. What are the odds?"
            }, 
            "id": "ci2zxu1"
        }, 
        {
            "body": {
                "answer": "<p>Let R equal return (which we want to equal 0 to make it break even) and I equal income (winnings before tax). Then E[R] = 3Y - E[I|I>0] * T * P(I>0).  So T should equal 3Y / (E[I|I>0] * P(I>0)).  To solve, you need to know E[I|I>0] and P(I>0), but to know these you need to know the distribution of winnings, not just its mean and standard deviation.  If we know that Y is very large, the central limit theorem will come into play and I<sq>s distribution will be approximately normal with mean 3Y and variance Y * X^2.  In this case, E[I|I>0] * P(I>0) = YX^2 / sqrt(2piYX^2 ) * exp(-9Y/(2X^2 )) + 3Y/sqrt(2piYX^2 ) * F(3Y/sqrt(2YX^2 )) where F is the normal CDF.  So the tax would be 3Y divided by that number.    </p>", 
                "question": "Slightly complicated stat question"
            }, 
            "id": "ci0eguq"
        }, 
        {
            "body": {
                "answer": "<p>Sounds like a coding error.</p>", 
                "question": "How to include binary data in my longitudinal regression?"
            }, 
            "id": "chzk4wz"
        }, 
        {
            "body": {
                "answer": "<p>It<sq>s not commonly used in the medical literature so I haven<sq>t seen a <dq>standard<dq> wording, but the correct interpretation is that if the test is not significant the <dq>distributions are not significantly different<dq>.<br><br>In my experience, the K-S test is often too sensitive, finding statistical significance in the shapes of distributions that are clearly not clinically important ... that may be why it isn<sq>t used a lot.</p>", 
                "question": "How to report Kolmogorov-Smirnov results?"
            }, 
            "id": "ch9q2q7"
        }, 
        {
            "body": {
                "answer": "<p>On APA style, you should end the sentence with the p-value obtained</p>", 
                "question": "How to report Kolmogorov-Smirnov results?"
            }, 
            "id": "ch9szu0"
        }, 
        {
            "body": {
                "answer": "<p>This is the APA format I<sq>ve used before<colon><br><br>A Kolmogorov-Smirnov test was used to test for normality on the main dependent variable x. The percentage of x for the A group, D(12) = 0.131, p > .05, and the percentage of X for the B group, D(10) = 0.201, p > 0.5, were both normal, indicating that the data was normally distributed in both groups.</p>", 
                "question": "How to report Kolmogorov-Smirnov results?"
            }, 
            "id": "ch9yyf7"
        }, 
        {
            "body": {
                "answer": "<p>Hi,<br><br>It won<sq>t answer completely your question but<colon><br>You want a power far larger than .173. The power is the probability to detect a significance effect when there is actually one (with an effect size of .03). A more usual power is around 80<percent>.</p>", 
                "question": "Trying to find optimal sample size for future studies to get significant results."
            }, 
            "id": "ch2h7hw"
        }, 
        {
            "body": {
                "answer": "<p>[deleted]</p>", 
                "question": "Trying to find optimal sample size for future studies to get significant results."
            }, 
            "id": "ch2i51h"
        }, 
        {
            "body": {
                "answer": "<p>Of the ~6 billion passengers that flew commercial flights in the world over 2013, 176 died in crashes. That<sq>s how many people die in traffic accidents in the US every ~36 hours.<br><br>You are more likely to die on a plane while in flight of a medical condition, than you are to die due to a crash. You are more likely to die in a car accident on the way to or from the airport. If you grew up in the USA, you are more likely to play in the NBA All Star game, than you are to die in a commercial plane crash. If you<sq>re a male and grew up outside of it, you<sq>re more likely to play in the World Cup (which is held every 4 years) than to die in a plane crash.</p>", 
                "question": "Brother in law won<sq>t ride an airplane because it might crash or ride a rollercoaster for the same reason... Skydiving etc... But he probably does other unnecessary things that are much higher risks. How can I convey to him the risk isn<sq>t worth considering?"
            }, 
            "id": "cgotw5m"
        }, 
        {
            "body": {
                "answer": "<p>I don<sq>t know your brother-in-law, but I<sq>d guess that his is not a terribly rational fear, and a sound statistical argument likely won<sq>t have much effect.  Maybe, though, I<sq>m just exposing my bias, as someone with a stats degree who still requires a small pile of psychoactive medication to board a plane. </p>", 
                "question": "Brother in law won<sq>t ride an airplane because it might crash or ride a rollercoaster for the same reason... Skydiving etc... But he probably does other unnecessary things that are much higher risks. How can I convey to him the risk isn<sq>t worth considering?"
            }, 
            "id": "cgocspk"
        }, 
        {
            "body": {
                "answer": "<p>Show him this clip<colon> https<colon>//www.youtube.com/watch?v=JwUhTlnOKh8</p>", 
                "question": "Brother in law won<sq>t ride an airplane because it might crash or ride a rollercoaster for the same reason... Skydiving etc... But he probably does other unnecessary things that are much higher risks. How can I convey to him the risk isn<sq>t worth considering?"
            }, 
            "id": "cgobksu"
        }, 
        {
            "body": {
                "answer": "<p>The p value gives you an idea of P(D | N). You want to know the probability of the null being true P(N) or 1 - P(H) = 1 - P(N^(c)) where N^c means the [complement of N](http<colon>//en.wikipedia.org/wiki/Complement_(set_theory\\)). From Bayes Rule, <br><br>P(N | D) \\propto (P(D | N) * P(N)) <br><br>Since P(N | D) + P(N^c | D) = 1, <br><br>P(N | D) = (P(D | N) * P(N)) / ((P(D | N) * P(N)) + (P(D |N^(c)) * P(N^(c))))<br><br>However, you don<sq>t know P(N), so you can<sq>t readily say what P(N | D) is. [Bayesian Statistics](http<colon>//en.wikipedia.org/wiki/Bayesian_statistics) will directly approach this problem but [Frequentists](http<colon>//en.wikipedia.org/wiki/Frequentist_inference) typically stay away from it.  <br></p>", 
                "question": "What does rejecting the null tell me about the probability of the null if anything?"
            }, 
            "id": "cgiuenx"
        }, 
        {
            "body": {
                "answer": "<p>Least squares is the maximum-likelihood estimator under the assumption that the noise (residual) is sampled independently from an identical Gaussian random variable.  This can be straightforwardly derived by maximizing the Gaussian log-likelihood function. </p>", 
                "question": "Why is least *squares* the maximum likelihood estimator? Or more generally why not some other least ____ estimate?"
            }, 
            "id": "cg9yjzn"
        }, 
        {
            "body": {
                "answer": "<p>Least squares is ML for the normal because the exponent of a normal has \u2212(*x*\u2212\u00b5)^2 in it. That makes minimizing the squares the same as maximizing the likelihood.<br><br>If you have a different distribution, maximizing the likelihood is minimizing something else.<br><br>So for example, minimizing the sum of absolute deviations is ML for the Laplace distribution.<br><br>More specifically, if the density is of the form *k.e*^(\u2212*g*\ufd3e*x*\u2212\u00b5\ufd3f) for some suitable function *g*, then maximizing the likelihood is the same as minimizing a sum of *g* terms.<br><br>In robust estimation, these g-functions are often called <dq>rho-functions<dq>, though the corresponding density for which they<sq>d be ML doesn<sq>t necessarily exist.<br><br>http<colon>//en.wikipedia.org/wiki/Robust_statistics#M-estimators<br><br>http<colon>//en.wikipedia.org/wiki/M-estimator#.CF.81-type<br></p>", 
                "question": "Why is least *squares* the maximum likelihood estimator? Or more generally why not some other least ____ estimate?"
            }, 
            "id": "cga8656"
        }, 
        {
            "body": {
                "answer": "<p>Maximum likelihood estimation and OLS both minimize the sum of squared residuals so the estimators are equivalent in this case (linear model with independent normal errors). You can see this if you write out the log likelihood function.<br><br>EDIT<colon> Just to be clear this happens to be true for this particular case but not in general.</p>", 
                "question": "Why is least *squares* the maximum likelihood estimator? Or more generally why not some other least ____ estimate?"
            }, 
            "id": "cg9q050"
        }, 
        {
            "body": {
                "answer": "<p>There are other methods. Least squares is used because to makes the estimation more sensitive outliers. I think your qubic method might be more so.  I know that sometime the sum of absolute errors is minimized instead. There is also the very frequent case of maximum Ilikleyhood estimation which is used with logistic, exponential, and normal distributions.   In many of these cases some violation of the assumptions needed for OLS is present.  If you have reason to believe that those assumptions are valid then it can be shown that OLS will produce the best performing results.  In the case where the distribution is normal but the absolute deviation was minimized the estimation would make the confidence intervals to tight. Such an estimation would not predict extreme values as often as they really would happen. </p>", 
                "question": "Why is least *squares* the maximum likelihood estimator? Or more generally why not some other least ____ estimate?"
            }, 
            "id": "cg9pggn"
        }, 
        {
            "body": {
                "answer": "<p>Given the information in your example, I<sq>m having trouble seeing the shortcoming of a Venn diagram with the number of books in each distinct subdomain written in the appropriate region of the chart, maybe with the totals of the three types listed below.  Is there additional information you<sq>re trying to convey?</p>", 
                "question": "How to visualize non-exclusive categories?"
            }, 
            "id": "cg9cky8"
        }, 
        {
            "body": {
                "answer": "<p>You can think of a marginal pdf as giving approximate probabilities. That is, if X has a pdf f(x), then P(x<X<x+dx) is approximately f(x)\\*dx for small values of dx. <br><br>You can then think of a joint density for two random variables X1 and X2 as the approximating the probability that they lie in a very small rectangle. That is, P(x1<X1<x1+dx1,x2<X2<x2+dx2) is approximated by f(x1,x2)\\*dx1\\*dx2. <br><br>The joint pdf for three random variables can be thought of as approximating the probability these three random variables lie in a rectangular prism of very small dimensions; that is, P(x1<X1<x1+dx1,x2<X2<x2+dx2,x3<X3<x3+dx3) is approximately f(x1,x2,x3)\\*dx1\\*dx2\\*dx3.<br><br>For an arbitrary choice of n random variables, you need to stretch your imagination enough to envision an n-dimensional rectangular prism. </p>", 
                "question": "Calculating the likelihood of multiple data points for a given probability function."
            }, 
            "id": "cg825b6"
        }, 
        {
            "body": {
                "answer": "<p>Use dummy variable for make and a dummy variable for model. Make sure you do the left-out dummy variable correctly. Interact these with continuous variables as desired.</p>", 
                "question": "Grouping continuous covariates?"
            }, 
            "id": "cg6oevx"
        }, 
        {
            "body": {
                "answer": "<p>>We<sq>re trying to determine if acceleration times can accurately predict age.<br><br>Not an expert, but I<sq>m curious about the answer as well. Contemplate this OLS spec<colon><br><br>Age = A + B * Time + C * ToyotaCamry + D * ToyotaCelica + (seven more dummies for car type)<br><br>A moment<sq>s thought reveals this dummy variable approach won<sq>t work<colon> Time is dependent on the dummies for car type. <br><br>Instead, maybe recast each trial as difference from the mean for each applicable car? We only have 50 data points for each mean<colon> this may be the weak link in my idea and could maybe cause us to make statements with too much confidence. Anyone have thoughts on that issue?<br><br>So here<sq>s the revised OLS model<colon><br><br>Age = A + B * DiffFromMeanForCarType<br><br>Other problems to consider<colon> Is this a linear relationship? Are we missing key variables (e.g., weather, gender, driving experience, familiarity with vehicle)?<br><br>Edit<colon><br><br>>I have data for 10 different cars, but there are 4 groups here - Ford, Honda, Hyundai, and Toyota. <br><br>No, you have 10 car types. Period. Make and model should be considered one thing. Make by itself is meaningless and adds nothing above the model of the car in terms of predictive power. </p>", 
                "question": "Grouping continuous covariates?"
            }, 
            "id": "cg6y76e"
        }, 
        {
            "body": {
                "answer": "<p>If your satisfied with the number of iterations in your MCMC routine, you most likely have something that is generally unimodal and not that irregular in shape.  I would guess that as sample size in your posterior rises, there will be less smoothing needed.  Unless you<sq>ve got severe autocorrelation, it shouldn<sq>t have that much of an effect.  That being said, the bandwidth is correct when you think it<sq>s smooth enough.  Unless you<sq>re going to be automating this for tons of different distributions, I wouldn<sq>t lose sleep over this.</p>", 
                "question": "Kernel Smoothing and Sample Size"
            }, 
            "id": "ceyqfdf"
        }, 
        {
            "body": {
                "answer": "<p>http<colon>//www.health.state.mn.us/divs/idepc/dtopics/stds/stats/stdsurvrpts.html</p>", 
                "question": "Can someone help me find a chart/map of STD statistics in the US?"
            }, 
            "id": "ce00hgy"
        }, 
        {
            "body": {
                "answer": "<p>We got the answer from reading the replies to [this thread](http<colon>//www.reddit.com/r/math/comments/1ror3d/given_an_alphabet_of_n_symbols_how_many_passwords/). I<sq>ve coded it up in python for convenience!<br><br>\t#!/usr/bin/python<br>\t# this library provides an optimised nCr<br>\timport gmpy<br><br>\tdef probopt(n, m)<colon><br>\t\tresult=0<br>\t\tfor k in range(1, m+1)<colon><br>\t\t\tresult = result + ((-1)**(m-k) * gmpy.comb(m, k) * k**n)<br>\t\treturn result<br><br>\tdef prob_of_q(n, k, q)<colon><br>\t\tx = probopt(n, q)<br>\t\ty = gmpy.comb(k, q)<br>\t\treturn x*y<br><br>\t# if we have<colon><br>\t#  a sequence of length n<br>\t#  a set of values to choose from of size k<br>\t# then<colon><br>\t#  prob_of_q(n, k, q) is the probability that my sequence will contain exactly <br>\t#  q unique values from the set q whether or not they are repeated</p>", 
                "question": "Calculating probability that a sequence won<sq>t contain a value"
            }, 
            "id": "cdpokpy"
        }, 
        {
            "body": {
                "answer": "<p>This is similar to the [Coupon Collector<sq>s Problem](http<colon>//en.wikipedia.org/wiki/Coupon_collector<sq>s_problem).</p>", 
                "question": "Calculating probability that a sequence won<sq>t contain a value"
            }, 
            "id": "cdp2l2b"
        }, 
        {
            "body": {
                "answer": "<p>That<sq>s a kind of interesting problem.  I took a quick run at it.  Not 100<percent> sure I<sq>m right, but I<sq>d be interested if you checked my solution against your simulation results.<br><br>Taking a and b as the cardinality of the corresponding set and sequence<colon><br><br>The cumulative version so myProb(x, a, b) is the probability that the sequence has at least x unique elements<colon><br><br>myProb(1, a, b) = 1<br><br>myProb(x, a, b) = myProb(x-1, a, b) - ((x-1)/a)^b-(x-1)<br><br>There<sq>s probably a way to get rid of the recurrence and find a closed form, but I can<sq>t remember how to do that.  Anyone want to help?</p>", 
                "question": "Calculating probability that a sequence won<sq>t contain a value"
            }, 
            "id": "cdp90np"
        }, 
        {
            "body": {
                "answer": "<p>Your question confuses *b* (which you defined to be a sequence) with the number of terms in it (its length, or dimension). An example of  what *b* looks like is <sq>[1,1,2]<sq>. The length of b is 3.<br><br>Let m be the number of values in the set a (what you call <sq>length(a)<sq>)<br><br>So if n is the length of b, the number of terms is m^n not a^b . If you really meant a^b that would be something like {1,2}^[1,1,2]<br></p>", 
                "question": "Calculating probability that a sequence won<sq>t contain a value"
            }, 
            "id": "cdp8jct"
        }, 
        {
            "body": {
                "answer": "<p>I<sq>d say all three of those answers are different sides of one argument, which is that the Normal distribution is important, and that variance (along with mean) naturally parameterizes it.<br><br>Answer 1 talks about how it<sq>s easier to derive about squared expectation than absolute deviation.  That<sq>s comes from the mathematical form of the Normal density.  It<sq>s also more important historically than nowadays when we have fancy computers to work things out.<br><br>Answer 2 is implicitly assuming that the data are normally distributed, which means large deviations from the mean will be less frequent than small ones, and we should have a measure of dispersion that gives them greater weight.<br><br>Answer 3 argues that Normal distributions often arise for physical reasons.  In this case they<sq>re talking about Brownian motion.  Movement occurs as the result of many small, independent, impulses.  The net displacement turns out to be have a Normal distribution with variance increasing linearly with time.<br><br>There are certainly cases where mean absolute deviation makes a better measure of dispersion.  A lot of computation math uses that as an optimization target.  But, in introductory statistics, you won<sq>t see it mentioned much.</p>", 
                "question": "Variance versus absolute deviation"
            }, 
            "id": "cdlfku4"
        }, 
        {
            "body": {
                "answer": "<p>To elaborate on answer 1 the math problems presented by the absolute deviation is that it does not have a nice derivative due to the absolute value function having a sharp peak in the middle. Since the derivative of the variance is a nice linear function it makes solving certain statistics problems easy to do by hand or with the pre-modern day computers. Now that powerful computers are plentiful people are using absolute deviation more often but you probably won<sq>t still see it in lower level statistics classes because the math is more complicated.</p>", 
                "question": "Variance versus absolute deviation"
            }, 
            "id": "cdm65g5"
        }, 
        {
            "body": {
                "answer": "<p>I<sq>m not the most mathematical statistician around, but when you study mathematical statistics, you find that variances are at the core of all of statistics.  We find that it<sq>s related to the chi-square and F distributions, and is part of the definition of the normal distribution.  It ties a lot of things together.  I don<sq>t think that you could do the same with absolute differences.  I<sq>m not sure how a covariance matrix would work, even.  I may be brainwashed with a century of statistics built on variances, but it<sq>s more <dq>natural<dq>. </p>", 
                "question": "Variance versus absolute deviation"
            }, 
            "id": "cdvmzqe"
        }, 
        {
            "body": {
                "answer": "<p>[Why square the difference instead of taking the absolute value in standard deviation?](http<colon>//stats.stackexchange.com/q/70238/805)<br><br>Also see<colon> [Why squared residuals instead of absolute residuals in OLS estimation?](http<colon>//stats.stackexchange.com/q/46019/805)</p>", 
                "question": "Variance versus absolute deviation"
            }, 
            "id": "cdlsedc"
        }, 
        {
            "body": {
                "answer": "<p>If you ignore the time split, then you have a 2 x 2 x 3 mixed factor model.  Two of your variables are between subjects and each have two levels (mouse type<colon> a or b) and chamber (w or t).  One variable is within subjects and has three levels (drug condition<colon> saline, amp, habituation).<br><br>Probably the easiest thing to do is a mixed model anova.  What software are you using to do your stats?</p>", 
                "question": "Confused about a (seemingly) simple statistical setup"
            }, 
            "id": "cdis6j4"
        }, 
        {
            "body": {
                "answer": "<p>The central limit theorem requires a large number of observations. A common rule of thumb is 30. <br><br>EDIT<colon> Please consider that this is an approximate answer, the sort of thing they are fishing for in a STAT 101 class that you are taking. Reality is more complicated.</p>", 
                "question": "When can I use z-/t-distributions?"
            }, 
            "id": "ccwk6ai"
        }, 
        {
            "body": {
                "answer": "<p>> When can I use z-/t-distributions? <br><br>You *can* use them whenever it occurs to you to do so. You press some buttons, and *voila*, you have used them.  The main issue is whether that<sq>s a good idea.<br><br>> only when the original variable is normally distributed,<br><br>With real data of the sort that people actually sit down and measure or record, the original variable is never actually normally distributed. Ever. Never, never ever.<br><br>Sometimes normality is a more-or-less adequate approximation. Don<sq>t make the mistake of thinking it<sq>s normal.<br><br>> can I use them always, because of the central limit theorem? <br><br>The central limit theorem certainly doesn<sq>t give you t-distributions!<br><br>Further, the central limit theorem alone doesn<sq>t justify saying \u221an.(\u0233 - \u03bc)/s is normal at any sample size (though with Slutsky along for the ride, yes, it<sq>s asymptotically true, as long as the conditions required for both theorems hold).<br><br>> the distribution of sample means from repeated sampling is always normal. <br><br>Definitely not so, not always, not even for truly huge sample sizes. <br><br>Consider a laser pointer (or rather a pair of them back to back, like Darth Maul<sq>s double light-saber) attached to a turntable close to an infinitely long wall. Make the point on the wall closest to the axis of the turntable <sq>0<sq>. Rotate the turntable through a random uniform angle, and put a mark at the center of the spot on the wall (one of the two pointers will leave a spot on the wall with probability 1). Record the distance to the right of the 0 mark in whatever convenient units you have (if it<sq>s to the left, that<sq>s a negative number). <br><br>The resulting measurement has a Cauchy distribution. Symmetric, bell-shaped. But the CLT doesn<sq>t apply to it! The sample mean of a sample of size $n$ has the same distribution as a single observation, no matter how large $n$ is.<br><br>And at what sample size? I can show examples where the CLT *does* hold, but it<sq>s not a good approximation until the sample size is well over 1000.<br><br>---<br><br>One is telling you that you can never use it (because even if he/she doesn<sq>t know it, you never have the normality that is being insisted is required), and the other is telling you you can always use it.<br><br>I say they<sq>re both wrong. <br><br>Here<sq>s the real deal<colon><br><br>Normality is an approximation that is sometimes reasonable. You have never seen real data that<sq>s normally distributed, though sometimes it comes really close.<br><br>The derivation of the t-distribution *does* rely on the original data being normal. The CLT doesn<sq>t save you in that derivation, because one of the properties of normality that is used to get the *t* is the independence of the sample mean and standard deviation, which characterizes the normal. No other distribution has it. If you don<sq>t have normality (and independence, and homoskedasticity, ...), your t-statistic isn<sq>t distributed as *t*.<br><br>If the conditions for the CLT and Slutsky hold and your sample sizes are sufficiently large (and that might in some cases need to be Very Large Indeed), then you can use an approximate z-test by assuming \u03c3 = s  (that is, n has to be large enough that the variance in s is effectively 0).<br><br>But you have another nice observation<colon><br><br>In many everyday situations, the t-test is pretty robust. As long as your distributions aren<sq>t too badly skewed/heavy-tailed, the t-distribution usually doesn<sq>t do all that badly as an approximation to the distribution of the test statistic under the null, and power is often reasonable in small samples.<br><br>(You probably have more to worry about with the other assumptions.)<br><br><br>If you<sq>re worried about the normality assumption, there<sq>s always resampling tests (permutation, randomization, bootstrapping etc) and there<sq>s also GLMs (which offer a richer set of distribution choices) and there<sq>s also nonparametric tests (which don<sq>t require normality).<br><br>If you like t-tests and the people you<sq>re talking to <sq>understand<sq> them, you can do a randomization test based off the t-statistic, and then you don<sq>t have to worry about the normality assumption (though low power might still be a concern if your distributions are sufficiently horrible that the statistic isn<sq>t very discriminating)<br></p>", 
                "question": "When can I use z-/t-distributions?"
            }, 
            "id": "ccwxdl7"
        }, 
        {
            "body": {
                "answer": "<p>There<sq>s no minimum as such (well, anything fewer than 3 would be a problem), nor any obvious criterion by which to pick one without more indication of what you need it to do. Are you trying to achieve some level of precision in your estimate or something?<br><br>If there<sq>s no other criterion on which to make the choice than size of sample, why would you choose anything but the largest sample size?<br><br>It<sq>s like saying <dq>If you could choose to shoot baskets from 10 feet, 20 feet or 30 feet, which would you choose?<dq> -- obviously if you<sq>re trying to make baskets, you choose the one that makes it easiest.<br><br>On the other hand, if some other criterion must also be considered, why would you hide that from us?</p>", 
                "question": "What is the minimum recommend sample size for a Pearson Correlation?"
            }, 
            "id": "cbobkb5"
        }, 
        {
            "body": {
                "answer": "<p>Bigger sample can allow you to see if a correlation is significant even if it<sq>s weak. If you are expecting weak correlations you should get a bigger sample. I don<sq>t know this by heart but lets say you have r = .20 it will not be significant at alpha = 0.05 with a sample of 100 but may be significant with a sample of 200. </p>", 
                "question": "What is the minimum recommend sample size for a Pearson Correlation?"
            }, 
            "id": "cbodzm5"
        }, 
        {
            "body": {
                "answer": "<p>Bayesian methods would give you a posterior distribution over the possible correlation coefficients... and let you be explicit about the prior assumptions rather than having them be implicit through the methods you choose.</p>", 
                "question": "What is the minimum recommend sample size for a Pearson Correlation?"
            }, 
            "id": "cbok719"
        }, 
        {
            "body": {
                "answer": "<p>The exact 95<percent> binomial confidence interval for 6/6 is 54<percent> -100<percent>. So there<sq>s room for interpretation.  A larger sample is needed before claiming perfection.</p>", 
                "question": "100<percent> efficacy in a malaria vaccine?"
            }, 
            "id": "cborm7s"
        }, 
        {
            "body": {
                "answer": "<p>I think they<sq>re just trying to say that for each of the six subjects, the vaccine prevented 100<percent> of future infections and not that the vaccine would prevent future infections in 100<percent> of the population</p>", 
                "question": "100<percent> efficacy in a malaria vaccine?"
            }, 
            "id": "cbl3yzt"
        }, 
        {
            "body": {
                "answer": "<p>Agree that the title is misleading based on n=6.  Technically correct, but still misleading.</p>", 
                "question": "100<percent> efficacy in a malaria vaccine?"
            }, 
            "id": "cbl4iat"
        }, 
        {
            "body": {
                "answer": "<p>To clarify your question, I believe you<sq>re asking what the odds are that you a.) lose the first 15 times, and b.) win on the 16th time.  <br><br>Your odds of losing on the first try are 3/4.  Your odds of losing both the first and second time would be (3/4)*(3/4) or ~0.56.  This means your odds of losing 15 times in a row would be (3/4) multiplied by itself 15 times. <br><br>We represent this would the exponent (3/4)^15.  So the probability that you would lose 15 times in a row is pretty slim, approximately 1.3<percent>.  But it gets even smaller, because you also won on the 16th time.  So the odds of your exact situation occurring is very slim, about 0.33<percent>.  <br><br>Now, you may think this means that McDonalds is screwing you, but that<sq>s not really the case for two reasons.  <br><br>First, and this is a bit tricky to explain, but this small likelihood isn<sq>t a big deal, because it is the combination of multiple events.  Lets say you flipped a coin every day for a year, and wrote down what it was each time.  Whatever specific combination of flips you had written down, the odds of that exact combination occurring would be (1/2)^365, which is an exceedingly small number.  But this is true no matter how many heads and how many tails you came up with.  Each individual event is very unlikely, yet you are guaranteed to end up with one of the may unlikely possibilities.  <br><br>The second reason is that the winning tokens probably aren<sq>t evenly distributed.  Think about if you reach into a jar of jelly beans that has green, blue, and red jelly beans in equal numbers.  If you take a handful of jelly beans, odds are you won<sq>t have exactly the same number of green, blue, and red, even though the jar has the same number of each.  But, lets say you took a million handfuls from the jar, and added up the amount of green, blue, and red from each handful (making sure you put the jelly beans back after each handful is counted).  The more times you do this, the closer your numbers should get to even.  <br><br>So your 16 tokens are a bit like taking one handful from a massive jar of jelly beans.  16 may seem like a lot to you, but McDonalds sells millions of sodas every day.  So your situation was unlikely, but theres probably some guy out there who won 10 times in a row, and maybe theres someone who played 100 times and never won. Your purchases make up a very small percentage of the total people playing the game, so you can<sq>t expect your odds to match up with the <dq>1/4<dq> very closely.  <br><br>TL;DR<colon>  it<sq>s an unlikely situation, but with your small sample relative to the total amount of tokens in play, unlikely results shouldn<sq>t be unexpected.  <br><br>Also, you may want to rethink getting a McDonalds soda every day on the way to work, that can<sq>t be good for you. </p>", 
                "question": "Odds of winning on 16th attempt when odds are 1 in 4?"
            }, 
            "id": "cb8jpjf"
        }, 
        {
            "body": {
                "answer": "<p>> .75^15  x .25<br><br>[1] 0.003340865<br><br>assuming that each token is independent, you just multiply the odds of losing the first 15 times and times 1 time winning.</p>", 
                "question": "Odds of winning on 16th attempt when odds are 1 in 4?"
            }, 
            "id": "cb8jbos"
        }, 
        {
            "body": {
                "answer": "<p>I would try posting this to r/statistics. </p>", 
                "question": "Looking for some linear algebra/MV normal tricks."
            }, 
            "id": "cb9rlz6"
        }, 
        {
            "body": {
                "answer": "<p>One of the best SPSS books you will find is <dq>Discovering Statistics using SPSS...and sex, drugs, and rock n<sq> roll<dq> by Andy Field. It is pretty much an all encompassing manual that you could use forever. It is full of a bunch of British humor, Cats, and punk rock. </p>", 
                "question": "Good SPSS book"
            }, 
            "id": "cb05uuq"
        }, 
        {
            "body": {
                "answer": "<p>Lots of beginners I know have liked the SPSS Survival Manual by Pallant. Good luck!</p>", 
                "question": "Good SPSS book"
            }, 
            "id": "cb05jea"
        }, 
        {
            "body": {
                "answer": "<p>People love mistaking a new record high (local) temperature for evidence of global warming, especially news media. And while I am happy that important global issues such as this are occasionally in the news, it misses the point somewhat.<br><br>Temperature oscillates over time. It oscillates daily, with a peak around 1pm and a trough around 1am, and it oscillates annually, with a peak around mid summer, and a trough around mid winter. Picture a sine wave, repeated end to end (365 times) which follows the shape of another, much longer, sine wave (one which has a period 365 tonnes longer).<br><br>Google sin(sin(365 x)) and then zoom out to get an idea of what hourly temperature data looks like; a big scribble.<br><br>This is the seasonal trend of typical temperature data. Random variation obscures this trend somewhat.<br><br>That graph from Google? Now picture what it would look like if you tried to trace it with a shaky hand, holding a really long pencil from on top of a ladder. Have a look at http<colon>//www.tedngai.net/wp-content/uploads/2010/08/hourly_dbt_dpt.gif to visualise this.<br><br>Evidence for global warming comes from stripping away both the seasonal oscillations in temperature, and the random <sq>noise<sq> of unexpected highs. Which, if you take into account a great many more details than I have, and use global mean temperatures, indicates a generally increasing trend over time.<br><br>To actually address your question, consider a random standing time series, random meaning that there is no correlation between data whatsoever. If our time series is one datum in length, then the chance of the datum being the highest value in the series is<colon><br><br> 1/number of data = 1/n = 1/1= 1<br><br>If our time series has two data points, then the chance is<colon><br><br>1/n = 1/2<br><br>I think an intuitive argument will suffice here. <br><br></p>", 
                "question": "What<sq>s the chance of a new daily high temperature if I only have a few years of data?"
            }, 
            "id": "cazgdv1"
        }, 
        {
            "body": {
                "answer": "<p>If your only objective is to test whether the probability of mutation is independent of the identity of the base (i.e. you are not concerned with what it mutates to), then it seems to me like a simple Chi^2 test should do the trick.<br><br>E.g. if you observe 100 mutations, your null hypothesis assuming no enzymatic bias would be that you<sq>d observe mutations at 68 of the C<colon>G pairs, and 32 of the A<colon>T pairs. You can thus calculate a chi^2 statistic with one degree of freedom as<br><br>X^2 = (W - 68)^2 / 68 + (Y - 32)^2 / 32<br><br>where W is the number of mutations at C<colon>G sites, and Y is the number of mutations observed at A<colon>T sites.<br><br>Then, you can simply look up in a table or online or in R or something what P(chi^2 > X^2 ) (i.e. the probability of getting a chi^2 statistic with one degree of freedom that is greater than or equal to X^2, assuming that the enzyme has no bias).<br><br><br>*edit<colon> I<sq>m assuming of course that you can identify for each site which allele was ancestral and which is derived, which it seems from your explanation that you can*</p>", 
                "question": "Hi AskStatistics I was hoping you could show me how to control for base pair composition with my biology research"
            }, 
            "id": "ca2pnek"
        }, 
        {
            "body": {
                "answer": "<p>Have you looked at the non-parametric Friedman two-way Anova? I believe it uses ranks rather than medians, but might be useful in your case.</p>", 
                "question": "Is there a 2-way 2-factor test that compares medians?"
            }, 
            "id": "c9qoyo7"
        }, 
        {
            "body": {
                "answer": "<p>This is kind of tricky. <br><br>Doing analysis with non-normal data isn<sq>t so much of a problem (there are a bunch of ways to handle non-normal data).<br><br>Doing multifactor analysis such that all comparisons are comparisons of *medians*? That<sq>s harder, because you don<sq>t have linearity in medians the way you do with means, yet you<sq>re constructing a model that relies on it.<br><br>(Note that median-effects remain even under monotonic transformations in a way that mean effects may not.)<br><br>If you can relax the <sq>medians<sq> part somewhat, you could maybe do an L1-regression model; it would correspond to comparing medians in a one-way model, but not necessarily all comparisons you might do in a two-way. <br><br>There are other things that might be done, depending on which aspects of what you raise you<sq>re most wedded to.<br><br>But I am curious... and the answer to this will probably guide my suggestions heavily -<br><br>*Why* are you particularly interested in interaction?<br><br>What does interaction tell you?<br><br>If you<sq>re not analyzing means, what does interaction really mean? How does one interpret it?<br><br><br></p>", 
                "question": "Is there a 2-way 2-factor test that compares medians?"
            }, 
            "id": "c9qvo5h"
        }, 
        {
            "body": {
                "answer": "<p><dq>An Introduction to Generalized Linear Models<dq> by Annette Dobson is a lovely intro book. The sampling distribution for MLE on chapter 5 is very clever!</p>", 
                "question": "What are the best books/online resources for Generalized Linear Models!?"
            }, 
            "id": "c9m71ad"
        }, 
        {
            "body": {
                "answer": "<p> I don<sq>t know if [this](https<colon>//www.khanacademy.org/science/macroeconomics/income-and-expenditure-topic/consumption-function/v/generalized-linear-consumption-function) is similar to the models described above. Generally this site is very useful.</p>", 
                "question": "What are the best books/online resources for Generalized Linear Models!?"
            }, 
            "id": "c9mbt33"
        }, 
        {
            "body": {
                "answer": "<p>OK, there are general linear models and generalized linear models.  Which GLM do you want?</p>", 
                "question": "What are the best books/online resources for Generalized Linear Models!?"
            }, 
            "id": "c9z0bps"
        }, 
        {
            "body": {
                "answer": "<p>First<colon> 50 cases for 200 factors is too few. Assuming a logistic regression, I bet that you will fail to obtain convergence. Many variables should be redundant and could be eliminated.<br>You could use easily principal component analysis to reduce the number of variables, but only if you use numerical factors. Categorical principal components analysis is a very tough beast!<br>Second<colon> I don<sq>t understand your dependent variables. There are two variables, <dq>failure<dq> and <dq>success<dq>? There is one variable, <dq>outcome<dq>, with two levels <dq>failure<dq> and <dq>success<dq>?<br>Third<colon> Assuming that your objective is <br>> When factor X is present in a case it is x<percent> more likely to fail or succeed<br><br>you need to use logistic regression. From your wording, I assume that all your factors are dichotomies.</p>", 
                "question": "So I need a hand designing a statistical model and I am unsure which measurement method is best<colon> Linear Discriminant Analysis Logistic Regression or Principal Component Analysis"
            }, 
            "id": "c8v1jvx"
        }, 
        {
            "body": {
                "answer": "<p>>How do I weight categorical data as a number without generating confirmation bias?<br><br>You appear to only have categorical data. Use multiple correspondence analysis (an extension of correspondence analysis [think of this as a chi-square PCA] designed for categorical data).<br><br><br>You can<sq>t really use LDA or another classifier, that is an extension of the SVD or an eigendecomposition, because you just end up with a line (not a plane). </p>", 
                "question": "So I need a hand designing a statistical model and I am unsure which measurement method is best<colon> Linear Discriminant Analysis Logistic Regression or Principal Component Analysis"
            }, 
            "id": "c8v2qqk"
        }, 
        {
            "body": {
                "answer": "<p>In order of increasing complexity<colon><br><br>Kernel density estimation.<br><br>One class SVM(OCSVM).<br><br>OCSVM on the lag matrix.<br><br>OCSVM on the rolling/ewma lag matrix stacked with once differenced tsa lag matrix.<br><br>Or...<br><br>Outlier detection on a time series could be cast as an adaptive filtering problem.<br><br>Residuals of a least mean squares filter would be a good start.<br><br>Or...<br><br>Gaussian hidden markov model of [smoothed Vals, differenced Vals]</p>", 
                "question": "What would be a good statistical way to determine an unusual peak in a dataset?"
            }, 
            "id": "c8smqx7"
        }, 
        {
            "body": {
                "answer": "<p>Outlier detection. </p>", 
                "question": "What would be a good statistical way to determine an unusual peak in a dataset?"
            }, 
            "id": "c8scrtg"
        }, 
        {
            "body": {
                "answer": "<p>it depends on the number of wiki articles. If there is 10 I<sq>d just plot the page views over time, look at the 10 plots and go from there. <br><br>If there is 1000 that is not feasible. I might do some plots just to get an idea. To detect the worst I might calculate (Max-median)/IQR and plot what is big there.</p>", 
                "question": "What would be a good statistical way to determine an unusual peak in a dataset?"
            }, 
            "id": "c8shzcj"
        }, 
        {
            "body": {
                "answer": "<p>Are you looking for an actual peak in distributions? You could start by doing some of the more advanced descriptives. Instead of mean and standard deviation... look at skew (which direction is heaviest in distribution) and, more importantly, kurtosis (when distributions are particularly <dq>spiky<dq> or <dq>flat<dq>).</p>", 
                "question": "What would be a good statistical way to determine an unusual peak in a dataset?"
            }, 
            "id": "c8smgsx"
        }, 
        {
            "body": {
                "answer": "<p>When you say you want to plot a correlation coefficient... you have a single number, right? <br><br>*What kind of a plot do you want to do of a single number and why would that be interesting*?<br><br><dq>pairwise exclusion<dq> is simply a way (one of several) of figuring out which observations to exclude or include in the calculation of correlation. As far as I can see, it relates to the calculation, not to plots.<br><br>The help on `cor` (?cor) indicates how to use the `use` argument to do the correlation calculation pairwise<colon><br><br>    use  <br>        an optional character string giving a method for computing   <br>        covariances in the presence of missing values. This must be  <br>        (an abbreviation of) one of the strings <dq>everything<dq>, <dq>all.obs<dq>,   <br>        <dq>complete.obs<dq>, <dq>na.or.complete<dq>, or <dq>pairwise.complete.obs<dq>.<br><br>and under **Details** it explains what the options do.<br><br>so you can just alter your call to cor to include the use argument - but be warned!! you may find that you end up with something you can<sq>t invert on the next line.<br><br><br>The Rcmdr package has this function in it<colon><br><br>    partial.cor <- function (X, ...) <br>    {<br>        R <- cor(X, ...)<br>        RI <- solve(R)<br>        D <- 1/sqrt(diag(RI))<br>        R <- -RI * (D <percent>o<percent> D)<br>        diag(R) <- 0<br>        rownames(R) <- colnames(R) <- colnames(X)<br>        R<br>    }<br><br>which looks a lot like the later part of your code.<br><br>If you run that function definition (copypaste it), you can just use the *same* `use` argument as you would with `cor` (because that function passes any additional arguments along to cor)<colon><br><br>    datmat <- as.matrix(data.frame(x=data$var1,y=data$var2,<br>                     z1=data$cntrlvar1,z2=data$cntrlvar2))<br>    partial.cor(datmat, use=<dq>pairwise.complete.obs<dq>)<br><br>if you just want the [1,2] element, you can do that in one go<colon><br><br>    partial.cor(datmat, use=<dq>pairwise.complete.obs<dq>)[1,2]<br><br>Though as the help explained we can use an abbreviation, such as<colon><br><br>    partial.cor(datmat, use=<dq>pairwise<dq>)[1,2]<br><br>As for the plot ... well you<sq>re going to have to explain more for me to say anything about that.<br><br></p>", 
                "question": "Plotting a partial corr using PAIRWISE exclusion?"
            }, 
            "id": "c8j3sx9"
        }, 
        {
            "body": {
                "answer": "<p>Yes. 1, 2, 3, 4.5, 4.5.</p>", 
                "question": "Is it possible to have a non-symmetric distribution where the mean and the median are equal?"
            }, 
            "id": "c8c0z30"
        }, 
        {
            "body": {
                "answer": "<p>Yes; discrete examples are rather easy to construct. Here<sq>s one<br><br>        x     -4     0    1     5<br>     P(X=x)   0.2   0.4  0.3   0.1<br><br>This has mean and median and also third central moment\\* zero, yet is asymmetric<br><br>Continuous examples also exist (and its possible not just to have mean = median, but even to have all odd central moments zero and to still have asymmetry).<br><br></p>", 
                "question": "Is it possible to have a non-symmetric distribution where the mean and the median are equal?"
            }, 
            "id": "c8c4s9g"
        }, 
        {
            "body": {
                "answer": "<p>Picture a distribution graph with left side uniform and right side <dq>bellish<dq> curve. Draw so area under curve same on both sides, and number of observations same on both sides. </p>", 
                "question": "Is it possible to have a non-symmetric distribution where the mean and the median are equal?"
            }, 
            "id": "c8bu1ib"
        }, 
        {
            "body": {
                "answer": "<p>It is the sum over all possible permutations of unique names of length equal to the number of students in your class<colon><br><br>  (1 - p_1) (1 - (p_1 + p_2)) ... (1 - (p_1 + p_2 + ... +p_n))<br><br>Where p_i is the probability that an individual is given the i^th name according to the current permutation.<br><br>This makes some unrealistic assumptions the biggest is probably that names are uniformly distributed across schools. In practice people with similar names go to the same schools so the real figure is likely to be higher. </p>", 
                "question": "How to calculate the probability of having 2 people in one class with the same first name."
            }, 
            "id": "c87enru"
        }, 
        {
            "body": {
                "answer": "<p>You need to consider that there are strong differences by gender (and also, ethnic and religious effects). Few females are called Matthew or Andrew. Few males are called Cheryl. Many Muslims are called Mohammed. That affects the chance (double the number of Muslims in the class, and you just about double the chance of two Mohammeds)<br><br>Because racial and religious makeup vary by region, if you don<sq>t condition on things like the makeup of the class, you can<sq>t treat the occurrence of names in the population the school draws from as independent.<br><br>Even within that P(X and Y) is not P(X)  + P(Y). It<sq>s P(X)*P(Y|X).<br><br></p>", 
                "question": "How to calculate the probability of having 2 people in one class with the same first name."
            }, 
            "id": "c87fm9m"
        }, 
        {
            "body": {
                "answer": "<p>Many more factors to include. Somebody could have moved to your country. Somebody may have been pushed up a grade. Somebody may have been pushed down a grade. People may have changed names. </p>", 
                "question": "How to calculate the probability of having 2 people in one class with the same first name."
            }, 
            "id": "c87djp8"
        }, 
        {
            "body": {
                "answer": "<p>Class size 1 <colon> 100<percent> chance of failure to get a pair<br><br>Class size 2 <colon> 1-(p1^2 + p2^2 + p3^2 ... pi^2 ) chance of failure to find a pair. You had a pi chance of drawing name i from your list of names.  The chance that this second individuals is from that group is pi^2. The sum of these is the chance that any one of these individuals was chosen twice in a row.<br><br>Class size 3 <colon> (1-(p1^2 + p2^2 + p3^2 ... pi^2 )) ^ (n choose 2) The chance of any pair of individuals failing to have the same name is given by the expression in our class size 2 example.  You raise this probability to the number of pairs within your sample to get the probability of failing to find a pair in your entire sample.<br><br>Your general formula is (1-(p1^2 + p2^2 + p3^2 ... pi^2 )) ^ (n choose 2).  (1-(p1^2 + p2^2 + p3^2 ... pi^2 )) is that chance of failing to find a pair for a given pair of randomly assigned names for a class size of n.  Your raise the chance of failure across pair observations to the number of pair observations to find the chance of failure across the whole sample</p>", 
                "question": "How to calculate the probability of having 2 people in one class with the same first name."
            }, 
            "id": "c8r8ome"
        }, 
        {
            "body": {
                "answer": "<p>This is all about model choice\u2014what concepts are we including in our universe of discourse. Since you<sq>ve invoked the distinction between <dq>all possible knowledge<dq> and <dq>your current belief<dq> (no job history, unfound lump) you<sq>re implicitly talking about the field of subjective probability as well. So, we<sq>ll dive deeply into Bayesian probability (which is a good framework for answering the kind of question in the NYT article as well)<br><br>In Bayesian formulation\u2014borrowing from Jaynes who likes to make this really explicit\u2014we carry around a term that represents <dq>all of the things that we know<dq>. For instance, in the NYT<sq>s example<colon><br><br>`P(C | +, I) = P(+ | C, I) * P(C | I) / P(+ | I)`<br><br>Where `P` is probability, `+` means <dq>positive mammogram<dq>, `C` is has cancer, `|` reads <dq>given<dq>, and `,` reads <dq>together with<dq>. The `I` represents <dq>all other knowledge<dq> which interestingly in this case must *not* contain things like <dq>family history<dq> (`F`), <dq>age<dq> (`A`), <dq>BRCA gene expression<dq> (`BRCA`), or <dq>lump<dq> (`L`). Otherwise when we calculated `P(C | I)` we<sq>d already be raising that probability for things like `I`, `A`, `BRCA`, and `L`.<br><br>Even more interestingly, the same math that gave us that last equation can be used to update it *again* with new incoming information. Each time it involves moving an observation quantity (something on the left side of `|`) to a given quantity (something on the right side) using our prior knowledge.</p>", 
                "question": "Probably simple question about Bayesian statistics and pretest probability."
            }, 
            "id": "c841jo5"
        }, 
        {
            "body": {
                "answer": "<p>It<sq>s not complicated to deal with factors you don<sq>t know the value of as long as you know the relative probabilities of taking the values.<br><br>P(x) = P(x|A).p(A) + P(x|A^c ).p(A^c )<br><br>or<br><br>P(x) = P(x|A1).p(A1) + P(x|A2).p(A2) + ...<br><br>and <br><br>P(x|B) = P(x|A1,B).p(A1|B) + P(x|A2,B).p(A2|B) + ...<br> <br>etc<br><br>You condition on what you know. </p>", 
                "question": "Probably simple question about Bayesian statistics and pretest probability."
            }, 
            "id": "c865sy9"
        }, 
        {
            "body": {
                "answer": "<p>The <dq>ideal<dq> estimate for the variance would be if you could compute the average squared distance of the sample observations from the *population mean*. Then the sample mean of the squared deviations would be an unbiased estimate the population mean squared deviation from the population mean.<br><br>The reason why there<sq>s a downward bias in the sample variance is the sample mean is the point that *minimizes* the sum of squares of distances from the observations - for continuous distributions, it is necessarily closer to the data than the population mean is. So the sample variance is always too small.<br><br>It turns out that the average downward bias caused by that is such that the expected sum of squares of deviations from the sample mean is (n-1)\u03c3^2 .<br></p>", 
                "question": "A simplified way to explain sample vs. population variance is it fully valid?"
            }, 
            "id": "c83rtin"
        }, 
        {
            "body": {
                "answer": "<p>I prefer to show it graphically, with a plot with sample size on x axis and sample variance on y axis. <br>If your plot a horizontal line on expected variance and puts the simulated variance for samples, you should note that if you use the EML, the sample variance concentrate on lower values that the expected values, but if you use the unbiased estimator, the concentrate on the expected value. </p>", 
                "question": "A simplified way to explain sample vs. population variance is it fully valid?"
            }, 
            "id": "c83fu0b"
        }, 
        {
            "body": {
                "answer": "<p>So your data looks something like this<colon><br><br>    Experiment#,Control,beginning,end<br>    0,Y,50,20<br>    0,N,50,30<br>    1,Y,60,20<br>    1,N,50,30<br>    <br>right? And you want to know the expected effect of Control (Y/N) on end/(end+beginning) ? Is that accurate?</p>", 
                "question": "Finding significant differences between two series"
            }, 
            "id": "c7r28qa"
        }, 
        {
            "body": {
                "answer": "<p>I remember Audiograbber and the half-track problem.<br><br>What you<sq>re describing sounds a lot like the coupon collector<sq>s problem.<br>http<colon>//en.wikipedia.org/wiki/Coupon_collector<sq>s_problem<br><br>> Suppose that there are n different coupons, equally likely, from which coupons are being collected with replacement. What is the probability that more than t sample trials are needed to collect all n coupons?<br><br>From the given formula, you would see all 6 faces of a die after an average of...<br>* 6/6 rolls for the first face<br>* 6/5 (1.2) rolls for the 2nd face<br>* 6/4 (1.5) rolls for the 3rd...<br>...<br>* 6/1 (6) rolls the last face, or 19.7 rolls. For large n this converges to n log(n)  (natural log)<br><br>Your Audiograbber problem has the additional twist that more than one <sq>coupon<sq> is collected at the same time. That<sq>s a lot harder and you want to look into resampling for the solution to that.  Since I<sq>m on vacation, I<sq>m going to avoid doing the math for that and give you a monte-carlo (read<colon> brute force) solution that you can use in R (which is free to download here<colon> http<colon>//cran.r-project.org/ )<br><br>I got the following results<colon><br><br> Min. 1st Qu.  Median    Mean 3rd Qu.    Max. <br><br>  3.000   5.000   6.000   5.844   7.000  20.000 <br><br>**I grabbed 10 songs each time from an album of 20. I did this 10,000 times**<br>So in 10,000 runs, I never needed to grab more than 20 times. **Half of the time I need to grab 5-7 times. On average, 5.844 grabs.**<br><br>Code used below on R 2.14.1<br><br>    set.seed(12345)  # For repeatability<br>    Ntrials = 10000  # number of time we run the sim<br>    Nsongs = 20    # songs in an ablum<br>    Ngrabbed = 10  # grabbed by audio grabber each time<br>    grabs = rep(0,Ntrials) # counter of times we need to run Audiograbber<br><br>    for(k in 1<colon>Ntrials) #  for each trial<br>    {<br>    \tsofar = NULL # start with no songs grabbed<br>    <br>    \twhile(length(sofar) < Nsongs) # while we haven<sq>t grabbed them all...<br>    \t{<br>    \t\tsofar = c(sofar,sample(1<colon>Nsongs, size=Ngrabbed)) # make a grab<br>    \t\tsofar = unique(sofar) # toss doubles<br>    \t\tgrabs[k] = grabs[k] + 1 #record that you made a grab<br>    \t}<br>    }<br><br>    # report your findings (mean-average and quantiles)<br>    summary(grabs)<br><br>    # plot your findings in a histogram<br>    hist(grabs)<br><br><br>TL;DR  5.844 grabs on average to get all 20 songs from an album. Modify the parameters in the R code above for different situations.<br>edit<colon> formatting and tl;dr</p>", 
                "question": "Sampling with replacement - how long until everything has been sampled at least once?"
            }, 
            "id": "c7odvm7"
        }, 
        {
            "body": {
                "answer": "<p>Each iteration of the Audiograbber problem can be modeled by the hypergeometric distribution, which gives the probability of drawing k successes in n draws without replacement from a population.  <br>http<colon>//en.wikipedia.org/wiki/Hypergeometric_distribution<br><br>If you want to take a more theoretical approach to the problem, you could use this method to find the expected number of successes for a certain number of draws.  </p>", 
                "question": "Sampling with replacement - how long until everything has been sampled at least once?"
            }, 
            "id": "c8r8wqn"
        }, 
        {
            "body": {
                "answer": "<p>Let<sq>s drop the numbers down significantly because it will give an answer without insane #<sq>s and help show why the answer is what it is. Instead of 49^6, lets make it 10 options and either pick 5 (different) numbers one week of 5 weeks of multiple numbers. <br><br>If we pick 5 different numbers then there<sq>s obvious a .5 probability of winning. 5 in 10 since each of the numbers has a 1 in 10 chance of winning and there<sq>s no overlap.<br><br>If we go 5 weeks in a row, each week, we have a probability of 0.9 of losing so the probability of losing all 5 is .9^5 = 0.59049, meaning you have a .40451 (rounding) probability of winning which is less than the .5 probability of winning with the 5 numbers in one week. <br><br>However, in the case where you pick 5 different numbers then you can win at maximum, once. In the case where you pick in different lotteries each week, winning or losing in one lottery doesn<sq>t change the probability of winning another lottery since each of the lotteries is independent, you can win multiple times, in an example of the [binomial distribution](http<colon>//en.wikipedia.org/wiki/Binomial_distribution). That 0.40451 is divided in winning exactly once (p=0.32805), winning exactly twice (p=0.0729), winning exactly three times (p=0.0081), winning exactly 4 times (0=0.00045), and winning all 5 times (p=0.00001).<br><br>So the answer to the generalized question is that playing multiple (different) tickets on the same lottery gives you a better chance of winning at all but (and I leave this as an exercise to the reader) if the lottery is fair, meaning the payouts are commensurate with the odds, your expected return is the same even if the distribution of the returns changes with different strategies.</p>", 
                "question": "Are your chances better by playing 100 lotteries with 1 ticket 100 times or 1 lottery with 100 tickets?"
            }, 
            "id": "c6lojpv"
        }, 
        {
            "body": {
                "answer": "<p>Either you haven<sq>t explained this in a way that makes much sense, or you may need to brush up on your understanding of how ELO works. If you mean that there are only 2 total participants, why would you even use it?</p>", 
                "question": "How do I use an elo system for a 1v1 competition? "
            }, 
            "id": "c655rdo"
        }, 
        {
            "body": {
                "answer": "<p>Interesting question!<br><br>Chance of 2 people having a birthday on a particular day are 1/(365*365)<colon> 0.0000075<br><br>If I take your numbers as good (sorry, don<sq>t know much about baseball)<colon><br><br>30 teams each playing 162 games per year = 4860 games, but you have to divide that by two for obvious reasons<colon> 2430 games played<br><br>Multiply that by 143 years, and you get 2430 * 143 = 347490 games total<br><br>That<sq>s probably an overestimate, though<colon> were there really that many games played in 1869?<br><br>One complexity here is that the chance of this event occurring is not independent. If Roy Halladay and Justin Verlander didn<sq>t have the same birthday last year, they won<sq>t have the same birthday this year. I<sq>m ignoring that for right now.<br><br>To find the probability of 0 occurrences of an event with this probability in this many trials, we can use the binomial distribution. I used Excel for this.<br><br>=BINOMDIST(number in question, number of trials, probability, FALSE to get a discrete probability instead of a cumulative one)<br><br>And here<sq>s the probability distribution, with the first column showing the number of times this event occurs in this many games, and the second showing the probability<colon><br><br>0\t7.4<percent><br><br>1\t19.2<percent><br><br>2\t25.1<percent><br><br>3\t21.8<percent><br><br>4\t14.2<percent><br><br>5\t7.4<percent><br><br>6\t3.2<percent><br><br>7\t1.2<percent><br><br>8\t0.4<percent><br><br>9\t0.1<percent><br><br>10\t0.0<percent><br><br>So there<sq>s a 19<percent> chance of having exactly one occurrence of this event. The probability would be higher if, as I suspect, the number of games is actually lower than this. And the probability will also be higher because the events aren<sq>t independent. So given that it didn<sq>t happen even once in 2010, it<sq>s less likely for it to happen in 2011. Now that it<sq>s happened once in 2012, it<sq>s more likely (much more likely!) for it to happen again, because we know that 2 such pitchers even exist in the sample space. In many years - if we knew the birthdays of all pitchers - the actual probability was probably 0<percent>.<br><br>So overall, yes, the claim is believable.</p>", 
                "question": "Chances two baseball starting pitchers are both celebrating a birthday"
            }, 
            "id": "c4jv1ko"
        }, 
        {
            "body": {
                "answer": "<p>[deleted]</p>", 
                "question": "Chances two baseball starting pitchers are both celebrating a birthday"
            }, 
            "id": "c4kaz8m"
        }, 
        {
            "body": {
                "answer": "<p>there are way too many variables in this problem to assume it should have already happened, for instance.<br><br>-For this to happen, pitchers must have birthdays during the baseball season which isn<sq>t always the case (obviously)<br><br>-You also have the fact that every pitcher doesnt face every pitcher as often as one may think<br><br>-Then the last fact is to hope the pitcher is actually pitching on his birthday. Pitchers pitch only like 35 games a year (I<sq>m not a huge baseball fan so correct me if thats wrong)<br><br><br>So what we are looking at is hoping that 2 pitchers happen to be born during the baseball season, they happen to be scheduled to pitch on their birthday (which in itself seems pretty improbable), and then on top of all that, they need to face another pitcher with the same birthday who falls into that exact same category as the first. There are plenty more variables but those stick out as the major ones to me<br><br>Edit<colon> spacing</p>", 
                "question": "Chances two baseball starting pitchers are both celebrating a birthday"
            }, 
            "id": "c4k1wte"
        }, 
        {
            "body": {
                "answer": "<p>16!/(4!)^4<br><br>edit<colon> i.e. the multinomial coefficient.</p>", 
                "question": "Choosing 4 groups of 4 from 16 people."
            }, 
            "id": "c3dqii6"
        }, 
        {
            "body": {
                "answer": "<p>It should be times, not plus, right?<br><br>Once you<sq>ve chosen 4, there are 12C4 ways of picking the next four and so on?</p>", 
                "question": "Choosing 4 groups of 4 from 16 people."
            }, 
            "id": "c3doywf"
        }, 
        {
            "body": {
                "answer": "<p>I finally found it out. I thought it would be damned easy but it is somehow a little bit complicated.<br><br>Maybe someone has the same question and googles it and gets redirected here, so here is the solution<colon><br><br>You can convert every MA(p) process into an AR(infinity) process. The AR(infinity) process can in turn be approximated.<br><br>http<colon>//fedc.wiwi.hu-berlin.de/xplore/tutorials/sfehtmlnode63.html<br>Even though it is a German page, the explanation is in English.</p>", 
                "question": "How to forecast using a MA(1) model?"
            }, 
            "id": "c2lgnu2"
        }, 
        {
            "body": {
                "answer": "<p>I<sq>m confused as to why you would take the expectation of a(t-1) since it<sq>s a known value at time t.</p>", 
                "question": "How to forecast using a MA(1) model?"
            }, 
            "id": "c3nleng"
        }, 
        {
            "body": {
                "answer": "<p>In Matlab, lsqcurvefit doesn<sq>t estimate the covariance matrix, which is what you need. <br><br>See [this page](http<colon>//www.mathworks.com/support/solutions/en/data/1-18QY1/index.html?solution=1-18QY1), it explains how to do it.</p>", 
                "question": "Error estimates on least squares fits"
            }, 
            "id": "c2bq92g"
        }, 
        {
            "body": {
                "answer": "<p>I<sq>ve never used `lsqcurvefit`, but I know that you can get some error estimates with `nls` in R if you<sq>re familiar. I don<sq>t know precisely the methodology they<sq>re using though.</p>", 
                "question": "Error estimates on least squares fits"
            }, 
            "id": "c1ybtwb"
        }, 
        {
            "body": {
                "answer": "<p>Principal Components Analysis</p>", 
                "question": "Undergrad Question - Is there a method for producing one overarching variable that accounts for the variability in my DV attributable to 3 independent variables that all significantly correlate with each other?"
            }, 
            "id": "dg1dcnn"
        }, 
        {
            "body": {
                "answer": "<p>Check the chronbach<sq>s alpha for the predictor variables that are highly correlated with each other. If it<sq>s sufficiently high (.7 or higher) then it<sq>s generally acceptable to make a composite of them (z-score them and average the z-scores, the resulting average is the composite variable).</p>", 
                "question": "Undergrad Question - Is there a method for producing one overarching variable that accounts for the variability in my DV attributable to 3 independent variables that all significantly correlate with each other?"
            }, 
            "id": "dg1n1sn"
        }, 
        {
            "body": {
                "answer": "<p>If each are independent then the probability of all happening is the product of each happening.  Since each event is presumably equivalent in properties (except from names of the people involved), then we have this simple expression<colon><br><br>p(4 couples boy-boy-girl) = (0.5^(3))^(4)<br><br>The probability of any *specific* arrangement of a set of binary outcomes does not require the binomial theorem, merely p^(x) * (1 - p)^(n-x).  And since p = 0.5 here, and n = 3, the result follows rather easily.</p>", 
                "question": "Trying to figure out the odds of 4 couples having boy boy girl in that order."
            }, 
            "id": "dfxdn01"
        }, 
        {
            "body": {
                "answer": "<p>You<sq>re probably looking to <dq>normalize<dq> the two scales on a common scale. Don<sq>t have time at the moment to give you the formulas but they are easy to find online. I should that <dq>standardize<dq> has a different meaning in statistics so be careful <colon>)</p>", 
                "question": "Comparing ratings on a 5-point scale to ratings on a 10-point scale"
            }, 
            "id": "dfsjdol"
        }, 
        {
            "body": {
                "answer": "<p>Your most straightforward option is probably to transform the raw scores into z-scores, which can be easily compared. </p>", 
                "question": "Comparing ratings on a 5-point scale to ratings on a 10-point scale"
            }, 
            "id": "dfssuwq"
        }, 
        {
            "body": {
                "answer": "<p>I would do 3 t tests for the difference In proportions. ( A /B, B/C, A/C) Testing the proportion repeating each activity. The sample for each activity is independent, because one person repeating doesn<sq>t depend somehow on whether another person repeats. <br><br>Edit<colon> I misread the sample sizes so deleted a useless comment </p>", 
                "question": "Quick question on a slightly unusual data set"
            }, 
            "id": "dfoq5hh"
        }, 
        {
            "body": {
                "answer": "<p>Forgive me if I just can<sq>t read, but....<br><br>what are you testing? Just trying to see if it<sq>s significantly different from all neutral answers?</p>", 
                "question": "What tests should I use for one-group questionnaire with Likert scale?"
            }, 
            "id": "dfluh9v"
        }, 
        {
            "body": {
                "answer": "<p>Hi again!<br><br>With this design you can<sq>t use test of significance like T or anova. You can do descriptives as others have said. <br><br>You could do confirmatory factor analysis or look at correlations between items, though. Respectively those would tell you about the structure of your scale, or what commonalities existed across the participants. <br><br>You might add an item asking for level of gaming experience (or use demographic data) and treat them as separate groups instead. This would allow for anova or regression models. </p>", 
                "question": "What tests should I use for one-group questionnaire with Likert scale?"
            }, 
            "id": "dfmea3s"
        }, 
        {
            "body": {
                "answer": "<p>You can do this in R pretty easily. (Easy if you<sq>re already an r user!)  <br>  <br>I think the package is called <dq>HMM<dq> - check out the documentation for that package, it contains a couple of worked examples. One limitation is that you can only use a single, huge run of the sequence to get the transition estimates. For my Masters I wanted to use thousands of runs, so I wrote my own function. </p>", 
                "question": "Training an HMM from data?"
            }, 
            "id": "dfhc8bi"
        }, 
        {
            "body": {
                "answer": "<p>[Here is a video where I talk about the F distribution, including how a one-way ANOVA works.](http<colon>//www.youtube.com/watch?v=tq5JfEUcFBk)</p>", 
                "question": "Need some help understanding one way ANOVA!"
            }, 
            "id": "dfgcrgg"
        }, 
        {
            "body": {
                "answer": "<p>Here is [my take](http<colon>//onlinestatbook.com/2/analysis_of_variance/one-way.html) </p>", 
                "question": "Need some help understanding one way ANOVA!"
            }, 
            "id": "dfhz80q"
        }, 
        {
            "body": {
                "answer": "<p>Sine ranges from -1 to 1, so unless you are using the absolute value of sine, I think you will run into problems using these link functions. Are you restricting the argument of sine such that it only yields values between 0 and 1? </p>", 
                "question": "Link function for sin(\u03b8) dependent variable. [GLiMM]"
            }, 
            "id": "dfbvz48"
        }, 
        {
            "body": {
                "answer": "<p>[deleted]</p>", 
                "question": "In growth curve models I have an interaction between a time-varying covariate and a time-invariant covariate (i.e. TVC*TIC). Should I also add an interaction by wave (TVC*TIC*wave) to capture changes over time or are those already captured in the first interaction?"
            }, 
            "id": "dfbbvnj"
        }, 
        {
            "body": {
                "answer": "<p>> 2) how the interaction should interpreted in either case.<br><br>Let<sq>s assume the following setup<colon> An intervention study measures blood pressure at three time points (t0, t1, t2). At each time point, the BMI of the participants is measured as well. BMI is a time-dependent covariate. Sex (0 = m, 1 = w) and group (0 = control, 1 = intervention) are time-independent covariates.<br><br>A two-way interaction of BMI\\*Sex would tell you if the blood pressure depends on the BMI and the group. As far as I can see, this is a marginal effect and does **not** capture time-dependent-effects.<br><br>A two-way interaction of BMI\\*time would tell you if the influence of the BMI on the blood pressure varies over time.<br><br>To capture time-effects, a three-way interaction BMI\\*Sex\\*time would tell you if the blood pressure depends on the BMI, the gender, as well as the time point at which blood pressure was measured.<br><br>> 1) whether the interaction by wave is necessary<br><br>The decision whether or not to include an interaction term by wave can depend on subject matter knowledge as well as statistical considerations. Variable selection is a huge field and there are multiple approaches to it. Common tools are likelihood ratio tests or information criteria (e.g. AIC).<br></p>", 
                "question": "In growth curve models I have an interaction between a time-varying covariate and a time-invariant covariate (i.e. TVC*TIC). Should I also add an interaction by wave (TVC*TIC*wave) to capture changes over time or are those already captured in the first interaction?"
            }, 
            "id": "dfbhqwb"
        }, 
        {
            "body": {
                "answer": "<p>For some perspective, I<sq>m currently studying towards a BSc in Economics and do a lot of stats. My second-year econometrics class covers everything you listed.</p>", 
                "question": "Trying to gauge the difficulty of a 3rd year regression class"
            }, 
            "id": "df66exm"
        }, 
        {
            "body": {
                "answer": "<p>A few things to do/try<colon><br>1. Plot X1 vs X2 -- what does it look like?<br>2. Standardize X1 and X2, not just center them.<br>3. Take out the interaction terms with the dummies<br>4. Add an interaction term for X1X2<br><br>Try that. It may not be the final model form, but you can build up from it.<br></p>", 
                "question": "Multiple regression analysis<colon> multicollinearity problem"
            }, 
            "id": "deu4vq3"
        }, 
        {
            "body": {
                "answer": "<p>Step back. You are comparing two models for an experiment. There are more direct ways to look at which fits better than to run a test on the correlation between observed vs. predicted values from each model under the null hypothesis that they would have equal Pearson correlation. Why is this the approach you are taking?</p>", 
                "question": "How do I know if two pearson<sq>s R correlations are significantly different?"
            }, 
            "id": "det7t6d"
        }, 
        {
            "body": {
                "answer": "<p>The information given is minimal. I assume the following model<colon><br><br>Y~ A + B + A<colon>B<br><br>Where A<colon>B is the interaction and A and B the main effects. When the interaction is significant, the main effects and their significance is usually of less importance and difficult to interpret. I recommend interpreting the interaction as usual (preferably with graphics) and to ignore the main effects and their significance.</p>", 
                "question": "One significant main effect one nonsignificant main effect and one significant interaction. How to interpret?"
            }, 
            "id": "dep9uhd"
        }, 
        {
            "body": {
                "answer": "<p>This kind of result is very common. The 3 effects are independent, so you can interpret each, but as the other responder mentioned, the interaction is probably more interesting and the main effects, or lack thereof, should be considered in light of the interaction.</p>", 
                "question": "One significant main effect one nonsignificant main effect and one significant interaction. How to interpret?"
            }, 
            "id": "depoeqr"
        }, 
        {
            "body": {
                "answer": "<p>Start by graphing the interaction with the DV for the ordinate, one IV on the absicca, and separate lines for each level of other DV. </p>", 
                "question": "One significant main effect one nonsignificant main effect and one significant interaction. How to interpret?"
            }, 
            "id": "desu5op"
        }, 
        {
            "body": {
                "answer": "<p>You<sq>ll find some answers [here](http<colon>//stats.stackexchange.com/questions/200410/is-principal-component-analysis-a-parametric-method). Other resources are [here](https<colon>//arxiv.org/pdf/1404.1100.pdf) and [here](http<colon>//webspace.ship.edu/pgmarr/Geo441/Lectures/Lec<percent>2017<percent>20-<percent>20Principal<percent>20Component<percent>20Analysis.pdf).<br><br>tl;dr Yes, PCA is considered a nonparametric procedure.</p>", 
                "question": "Is Principal Component Analysis a non-parametric statistical technique?"
            }, 
            "id": "deocr9i"
        }, 
        {
            "body": {
                "answer": "<p>[deleted]</p>", 
                "question": "What kind of mathematical analysis is done to figure out if <dq>A follows B<dq> or <dq>B follows A<dq>?"
            }, 
            "id": "deoag41"
        }, 
        {
            "body": {
                "answer": "<p>Yes, as the assumptions are about the population from whence your data came, and not about linearity of the model </p>", 
                "question": "Does polynomial regression need to satisfy the assumptions of linear regression? (x-post)"
            }, 
            "id": "denis3j"
        }, 
        {
            "body": {
                "answer": "<p>Most people are not going to have SPSS. Can you share a picture, or partial dataset, or more information about what your are doing? What do yo mean by <dq>perceived popularity<dq>? Is this yes or no, quantitative, ordinal, ??? These forms of bullying are dummy variables?  Any other variables in your data?</p>", 
                "question": "Assumption of normality violated - what to do?"
            }, 
            "id": "demu7fo"
        }, 
        {
            "body": {
                "answer": "<p>Judging from your SPSS-output, you haven<sq>t done a regression analysis. What you have done is a univariate analysis of the variables and found that they are not compatible with a normal distribution.<br><br>Multiple regression does *not* assume that the variables themselves are normally distributed. Regression assumes that the residuals are approximately normally distributed. So the next step would be to actually run the regression analysis and investigate the residuals. If you find that they deviate strongly from a normal distribution there are several options<colon> i) transformation of variables, ii) run a different model (e.g. with nonlinear trends, polynomials etc.).</p>", 
                "question": "Assumption of normality violated - what to do?"
            }, 
            "id": "dennrj6"
        }, 
        {
            "body": {
                "answer": "<p>K-means is perfectly appropriate. I wouldn<sq>t call it too complicated to explain/justify to others because it<sq>s an <sq>off the shelf<sq> analysis that has been used and tested extensively. You can check if it<sq>s appropriate for the data by looking at the R^2, a.k.a. the coefficient of determination. This statistic will tell you how much of the variation between customers is explained by the groups.<br><br>Also, you should check for unequal variance between the groups. Iirc, that<sq>s a job for the bartlett test. You can also do a histogram of your buying amounts to see if the high-level groups have more variance. A histogram will also tell you if 3 groups is appropriate.<br><br>If there IS unequal variance, do k-means on the natural log of the buying amounts, and see if you getter a better (higher) R^2 from that.</p>", 
                "question": "Segmenting Customers into 3 groups by size. Would K-means be over complicating it?"
            }, 
            "id": "den5n2z"
        }, 
        {
            "body": {
                "answer": "<p>There<sq>s nothing wrong with K-means for doing what you<sq>re doing, but I<sq>m not quite sure why you want segmentation here in the first place.  Average prices are continuous, square footage is continuous, is there some reason for not just relating these continuous things to each other directly instead of making an arbitrary split?  There might be, maybe depends on the purpose of this <dq>benchmark<dq>.</p>", 
                "question": "Segmenting Customers into 3 groups by size. Would K-means be over complicating it?"
            }, 
            "id": "denbpb7"
        }, 
        {
            "body": {
                "answer": "<p>[deleted]</p>", 
                "question": "Eli5<colon> What exactly does Two Standard Deviations from the Mean mean?"
            }, 
            "id": "delpkl4"
        }, 
        {
            "body": {
                "answer": "<p>The mean is 1504 and the standard deviation is 315. All of the other scores fall within or around one standard deviation (or from 1189 to 1819) while the 2210 is outside of two standard deviations away (874 to 2134)</p>", 
                "question": "Eli5<colon> What exactly does Two Standard Deviations from the Mean mean?"
            }, 
            "id": "delpmim"
        }, 
        {
            "body": {
                "answer": "<p>Two deviations from the mean means the interval of values that lies between (mean - 2\u00d7sd) and (mean + 2\u00d7sd). Imagine you hammer a fencepost into the ground where the mean is, and you have a piece of rope that is two standard deviations long, one end of which you tie to the post. The furthest you can make the rope stretch is the interval in question, and any points that you can<sq>t reach with the rope are being considered <dq>unusual<dq>.<br><br>The other consideration is *why* these values are being called unusual. If your data is meant to be *normally* distributed, then you should be able to reach about 95<percent> of your values with the rope. So only 1 value in 20 should lie outside that rope, and that<sq>s a slightly arbitrary value that statisticians have agreed is a decent place to go <dq>Hmm, that<sq>s a little unlikely<dq>. Obviously a 1 in 20 event will still happen occasionally, so it<sq>s not a really big deal, but it<sq>s enough to start paying attention.<br><br>If you lengthen your rope to 3 standard deviations long, then you should be able to reach 99.7<percent> of your data, meaning that anything that lies outside the rope is a 3 in 1000 chance, which is starting to get pretty unlikely, and might be a sign that there<sq>s something wrong with either the location of the fencepost, the length of the rope, the location of that point, or your assumption that the data was normally distributed in the first place. (In some scientific fields like particle physics they don<sq>t start getting excited about results until their rope is 6 or more sds long, representing a 1 in 500 million chance.)<br><br>It turns out that you can<sq>t always rely on things being normally distributed, but if your homework tells you that a certain data set is normal then obviously you can use that, and at some point you might learn about something called the Central Limit Theorem which says that there are certain things that are normally distributed or close enough to work with, and that<sq>s where the whole <dq>plus or minus n standard deviations<dq> stuff becomes quite useful.</p>", 
                "question": "Eli5<colon> What exactly does Two Standard Deviations from the Mean mean?"
            }, 
            "id": "delvtgu"
        }, 
        {
            "body": {
                "answer": "<p>This paper explains the interpretation of interaction terms pretty well<colon> Brambor, Thomas, William Roberts Clark, and Matt Golder. <dq>Understanding Interaction Models<colon> Improving Empirical Analyses.<dq> Political Analysis 14, no. 1 (2006)<colon> 63-82. http<colon>//www.jstor.org/stable/25791835.</p>", 
                "question": "Is there a method for comparing point estimates after determining an interaction effect?"
            }, 
            "id": "de6vq0q"
        }, 
        {
            "body": {
                "answer": "<p>The typical way I<sq>ve seen this done is by looking at simple effects<colon>  subset your data by lower versus higher aggression and test the effect of gender within each subset. <br><br><br>Another thing you can do, assuming you<sq>re doing a regression, is deviate your data twice, and have ine dataset where aggression now equals zero at the old +1 sd value and another where aggression now equals zero at the old -1 sd value.  If you run your model for each if thse sets, the gender coefficient (& it<sq>s significance test) will tell you the effect of gender when aggression =0 (the old +1 & -1 sd values, respectively).</p>", 
                "question": "Is there a method for comparing point estimates after determining an interaction effect?"
            }, 
            "id": "de74dfm"
        }, 
        {
            "body": {
                "answer": "<p>A test designed to do what you are doing is the [Kolmogorov-Smirnov two sample test](https<colon>//en.wikipedia.org/wiki/Kolmogorov<percent>E2<percent>80<percent>93Smirnov_test#Two-sample_Kolmogorov.E2.80.93Smirnov_test).  I am sure there are others, but this will get you started on the right path.</p>", 
                "question": "Advice? Calculating the significance of the difference between binned curves."
            }, 
            "id": "de69lsv"
        }, 
        {
            "body": {
                "answer": "<p>Multiple linear regression should work fine since it only assumes linearity in the parameters and not <dq>in the variables<dq>. Consequently, you may consider something like y = b0 +b1x1 + b2x1^2 et cetera. </p>", 
                "question": "Mixed linear & nonlinear model"
            }, 
            "id": "de2kfj3"
        }, 
        {
            "body": {
                "answer": "<p>Try logging cost<br><br>Also you should think about doing some type of variable selection like stepwise, as 16 dependent variables runs the risk of overfitting and collinearity </p>", 
                "question": "Mixed linear & nonlinear model"
            }, 
            "id": "de2k85r"
        }, 
        {
            "body": {
                "answer": "<p>Regression would work for both your binary and continuous variables. If you have a non-linear prediction (Im assuming curvilinear here) then you would need to add a squared term. Ex<colon><br><br>Y = constant + b1x1 + b2x2 + b3x3 + b3x3^2<br><br>this would be an example of capturing the bend (parabola) in your prediction for the X3 variable. To create this variable, center X3 and multiply it by itself (square it). Use it as a predictor in your model. If you have more complex relations here (cubic) then you would multiply X^3, etc.  <br></p>", 
                "question": "Mixed linear & nonlinear model"
            }, 
            "id": "de2mr9h"
        }, 
        {
            "body": {
                "answer": "<p>The best software available to you is R. Although as someone said above multiple regression in minitab is fine.</p>", 
                "question": "Mixed linear & nonlinear model"
            }, 
            "id": "de2oehs"
        }, 
        {
            "body": {
                "answer": "<p>You are interested in seeing if the change in disability scale is higher for one treatment over the over? Just create a variable that is the difference between the baseline and disability at 3 months for both treatment groups, then compare them via t-tests. Do the same for baseline to 12 months and 3 months to 12 months as needed.</p>", 
                "question": "Help with surgery statistics using SPSS"
            }, 
            "id": "ddy2dxh"
        }, 
        {
            "body": {
                "answer": "<p>I haven<sq>t given a heap of thought to this as my roast chicken is nearly ready, but I think you<sq>ll need to do a linear regression (possibly two?) which include the baseline level of disability as an independent variable. Other techniques are possible but this is the best way to handle change over time, as you<sq>ll be caught out by <dq>regression to the mean<dq> if you don<sq>t you adjust for the baseline values.   <br>  <br>Will come back to this thread tomorrow and see what I think... </p>", 
                "question": "Help with surgery statistics using SPSS"
            }, 
            "id": "ddy4mf4"
        }, 
        {
            "body": {
                "answer": "<p>Ok, chicken was good. Here we go. (pinging u/crushedbycookie) <br>  <br>When you have data that is taken from the same people multiple times this introduces structure into the data which we need to account for. We don<sq>t have independent measures anymore, because my disability at three months is going to be related to my disability at baseline.  <br>  <br>It is tempting to calculate a <sq>change score<sq> - the difference between each persons score at baseline and some future time, and analyse that. But this is not best practice because a nasty phenomenon called <dq>regression to the mean<dq> is going to pop up. What we mean by <dq>regression to the mean<dq> is that somebody measuring at the extreme of the scale at baseline - eg someone reporting the highest possible level of disability - has more <sq>disability<sq> to lose than someone who reported a <sq>medium<sq> level of disability. It is overwhelmingly more likely that the patients most disabled at baseline have large, more negative change scores, simply because they had high scores to begin with. It can be shown that *the change score is actually dependent on the baseline score* - which is a problem if you<sq>re trying to accurately estimate the effect of your treatment. <br> <br>The accepted way to get around this is to estimate an ANCOVA / linear regression model with the follow up score as the dependent variable, and treatment and baseline scores as the independent variables (plus anything else that should be controlled for). This effectively estimates the impact of treatment on follow up score *adjusted for baseline score*. You can read about this approach in [this paper](https<colon>//academic.oup.com/ije/article/34/1/215/638499/Regression-to-the-mean-what-it-is-and-how-to-deal) <br> <br>The regression equation is of the form<colon>  <br>y(follow up score) = constant + b1(baseline score) + b2(treatment)...<br>and b2 is the <dq>money parameter<dq> which will reflect the estimated impact of the treatment. <br> <br>Keep in mind the rule of thumb which is about 20 patients per variable, so I hope you have at least 40 patients worth of data. Hope that was helpful, let me know if you have other questions. </p>", 
                "question": "Help with surgery statistics using SPSS"
            }, 
            "id": "ddyd6j0"
        }, 
        {
            "body": {
                "answer": "<p>Perhaps try stata, it<sq>s a good halfway point between SPSS and R. There is also really excellent online documentation from [UCLA.](http<colon>//www.ats.ucla.edu/stat/stata/) </p>", 
                "question": "Help with surgery statistics using SPSS"
            }, 
            "id": "ddyh62z"
        }, 
        {
            "body": {
                "answer": "<p>Check out the [Dieharder site](https<colon>//www.phy.duke.edu/<percent>7Ergb/General/dieharder.php) for a discussion of this and some testing algorithms. <br><br>[Here is a stachexchange discussion](http<colon>//crypto.stackexchange.com/questions/394/what-tests-can-i-do-to-ensure-my-prng-is-working-correctly) on the topic as well. </p>", 
                "question": "Comparing <dq>true<dq> RNG and PRNG statistically?"
            }, 
            "id": "ddxtkwk"
        }, 
        {
            "body": {
                "answer": "<p>I don<sq>t think so, there is nothing to compare. <br><br>You could analyze the distribution of your prng, but you<sq>d need a bigger sample size.</p>", 
                "question": "Comparing <dq>true<dq> RNG and PRNG statistically?"
            }, 
            "id": "ddxsws4"
        }, 
        {
            "body": {
                "answer": "<p>It<sq>s Pearson Ch-squared test of independence. It<sq>s a goodness-of-fit test. You could use Pearson<sq>s chi square to compare real data to a hypothetical distribution, like the Gaussian distribution. In this case, they are calculating expected values, which represent the expected cell counts in the table if the two variables were independent. Then you use Pearson chi square to test if the actual observed counts in your table match those expected values. It<sq>s a classic test, appropriately used in this case.</p>", 
                "question": "Stats in bio textbook seems wrong... Misinterpretation of significance level? Can you say what the probability of a discrete distribution is from a chi-squared test? The conclusion in the bottom yellow box seems suspect."
            }, 
            "id": "ddu4flq"
        }, 
        {
            "body": {
                "answer": "<p>It is worded a bit awkwardly.  If p<.05 (usually, but .1 and .01 are sometimes used) we say that the difference is not due to chance, and thus, meaningful.  Anything greater than that is considered due to chance.  Appropriate [xkcd](https<colon>//xkcd.com/1478/). </p>", 
                "question": "Stats in bio textbook seems wrong... Misinterpretation of significance level? Can you say what the probability of a discrete distribution is from a chi-squared test? The conclusion in the bottom yellow box seems suspect."
            }, 
            "id": "ddu4g3k"
        }, 
        {
            "body": {
                "answer": "<p>I<sq>m gonna clarify the textbook<sq>s interpretation. (op)<br><br>For this example, (from the picture), the associated text is, <dq>A chi-square test is used to determine the probability that the difference between observed and expected values is due to chance.<dq><br><br>This seems contrary to what I know about the chi-squared test from my one semester of stats...<br><br>In another example the book says, <dq>Our chi-square value of 6.0 falls between the [chi-square] value of 5.024, associated with a probability of 0.025, and the value of 6.635, associated with a probability of 0.01. Thus, the probability associated with our chi-square value is less than 0.025 and greater than 0.01. So there is less than a 2.5<percent> probability that the deviation that we observed between the expected and the observed numbers of black and gray kittens could be due to chance.<dq><br><br>Again, this is contrary to what I understand about the chi-square test. <br><br>Thanks.<br><br>Edit<colon> grammar </p>", 
                "question": "Stats in bio textbook seems wrong... Misinterpretation of significance level? Can you say what the probability of a discrete distribution is from a chi-squared test? The conclusion in the bottom yellow box seems suspect."
            }, 
            "id": "ddufoe2"
        }, 
        {
            "body": {
                "answer": "<p>This language is why NHST gets misused so often in the published literature.</p>", 
                "question": "Stats in bio textbook seems wrong... Misinterpretation of significance level? Can you say what the probability of a discrete distribution is from a chi-squared test? The conclusion in the bottom yellow box seems suspect."
            }, 
            "id": "ddv3guz"
        }, 
        {
            "body": {
                "answer": "<p>You should be tracking the population average (i.e, mu.b1) of your specified random slope or whatever the parameter is. The summary in runjags or r2jags output will list the intervals like the other levels of the factor.</p>", 
                "question": "Confidence Intervals and Credibility Intervals"
            }, 
            "id": "ddgtkz6"
        }, 
        {
            "body": {
                "answer": "<p>I would recommend searching the Cran repository of R packages to see what else there is. A quick crtl f of <sq>generator<sq> found some stuff that could be of value to you. By default R uses Mersenne Twister, but this can be changed.<br><br><br>https<colon>//cran.r-project.org/web/packages/available_packages_by_name.html<br><br>Also, here is the R port of dieharder.<br>http<colon>//dirk.eddelbuettel.com/code/rdieharder.html<br><br>I did a project on PRNG and encryption in undergrad, and I used Diehard, the previous version of Dieharder. It was already pretty thorough and strong.</p>", 
                "question": "What is the latest suite of statistical tests for pseudorandom number generators?"
            }, 
            "id": "ddbvn53"
        }, 
        {
            "body": {
                "answer": "<p>pracrand.sourceforge.net<br>gjrand.sourgeforge.net<br>http<colon>//simul.iro.umontreal.ca/testu01/tu01.html<br>These make the assumptions<colon> not for crypto, PRNG rather than hardware, lots of pseudo-random data available for testing. Under those assumptions, these 3 are extremely tough, certainly much tougher than dieharder and NIST. All 3 expect you to be able to use command lines, C or C++ compiler, and unless you<sq>re very good or very lucky, to put in a few hours work (including to find and read the instructions). Best of luck.</p>", 
                "question": "What is the latest suite of statistical tests for pseudorandom number generators?"
            }, 
            "id": "dddzly1"
        }, 
        {
            "body": {
                "answer": "<p>Polynomial will always have a better in-sample fit. But you should be very careful when using it. You have to have very good reason for it...nonlinearity assumption. You can have to check your residuals.<br><br>Of course the parsimonious model has lower uncertainty. Fewer parameters -> higher degrees of freedom -> less uncertainty.</p>", 
                "question": "When comparing 2 regression models which results should I give preference to<colon> those from the F-test or the uncertainty on the fitted parameters?"
            }, 
            "id": "dd9421z"
        }, 
        {
            "body": {
                "answer": "<p>This is probably happens in model B because of highly correlated variables. Try to print out the VIF values or have a look at a correlation matrix. <br><br>Try remove each of the highly correlated variables at a time and refit the model.<br><br>Otherwise just use a model where the variables doesnt correlate as much.</p>", 
                "question": "When comparing 2 regression models which results should I give preference to<colon> those from the F-test or the uncertainty on the fitted parameters?"
            }, 
            "id": "dd92wkp"
        }, 
        {
            "body": {
                "answer": "<p>If you<sq>re comparing two models you want the one which will accurately predict future observations, that is data that you didn<sq>t use to fit the model. If you only look at how well the model fits the data you are vulnerable to over fitting.<br><br>Kfold cross validation is probably the best way to do this as you can compare the predictive error of the two models. Simpler statistics which are easier to implement are AIC / BIC or adjusted R squared</p>", 
                "question": "When comparing 2 regression models which results should I give preference to<colon> those from the F-test or the uncertainty on the fitted parameters?"
            }, 
            "id": "dd9ofas"
        }, 
        {
            "body": {
                "answer": "<p>You probably want to start getting you math up to par before you dive into machine learning. I would start with Calculus / Multivariable Calculus / Linear Algebra.</p>", 
                "question": "recommended MOOC and/or books to start learning stats and then machine learning with R"
            }, 
            "id": "dd8ssqd"
        }, 
        {
            "body": {
                "answer": "<p>https<colon>//www.analyticsvidhya.com/blog/2017/01/learning-plan-2017-beginners-data-science/<br></p>", 
                "question": "recommended MOOC and/or books to start learning stats and then machine learning with R"
            }, 
            "id": "dd9msc2"
        }, 
        {
            "body": {
                "answer": "<p>thank you both for your input. this is valuable.</p>", 
                "question": "recommended MOOC and/or books to start learning stats and then machine learning with R"
            }, 
            "id": "de9odpv"
        }, 
        {
            "body": {
                "answer": "<p>I think you<sq>re good to go.</p>", 
                "question": "Do I need to do anything to my data before running a GEE regression?"
            }, 
            "id": "dd6t5qi"
        }, 
        {
            "body": {
                "answer": "<p>Stack Exchange has a list free texts found [here](http<colon>//stats.stackexchange.com/questions/170/free-statistical-textbooks).<br><br>For most of these concepts, you can look up how to do them in excel via youtube.</p>", 
                "question": "Online resources for data analysis"
            }, 
            "id": "dcuslti"
        }, 
        {
            "body": {
                "answer": "<p>Assuming equal uniform likelihood for any possible score, then no, there is no optimal set of choices. Any combination is equally likely, whether you pick boxes in a row or spread out your picks. <br><br>Of course, if you believe in certain outcomes more strongly than others (i.e. Maybe strongly believe that the game score could end in 7) then you would want to play in a way that maximizes exposure to those scenarios. </p>", 
                "question": "Odds breakdown of a Superbowl Pool"
            }, 
            "id": "dculmjq"
        }, 
        {
            "body": {
                "answer": "<p>There<sq>s not an optimal strategy to winning, but there is an optimal way to <dq>invest<dq>. I<sq>m assuming payouts for 4 quarters and no rake from the organizer. If you<sq>re going to buy at least 1 square the best way to make back the most money is to buy all the squares and guarantee 100<percent> payback. There<sq>s no profitable number to buy and only buying 100 or 0 guarantees you won<sq>t lose money. Since that<sq>s no fun, the best strategy is to buy as many as possible, at least buying 36. <br><br>Buying 1 square gives you an expected return of 24.8<percent> of your buy-in. That rate of return decreases, then increases and doesn<sq>t cross the 1 square baseline until you buy at least 36 squares and increases further from there until you hit 100<percent> when buying all the squares. Since buying that many Super Bowl squares still probably won<sq>t make you friends your best bet is to just buy 1. <colon>)</p>", 
                "question": "Odds breakdown of a Superbowl Pool"
            }, 
            "id": "dcurrjn"
        }, 
        {
            "body": {
                "answer": "<p>a variety of inverse cdfs of right skewed densities look like that</p>", 
                "question": "What kind of curve is similar to this?"
            }, 
            "id": "dcqvvwa"
        }, 
        {
            "body": {
                "answer": "<p>I want to say it looks like a cumulative distribution function because it<sq>s monotonically increasing and has the characteristic <sq>wiggle<sq> burned into my head from looking at CDF<sq>s of t and normal distributions. But it can<sq>t be quite that, because the slope on the tails is non-vanishing. Not sure!</p>", 
                "question": "What kind of curve is similar to this?"
            }, 
            "id": "dcpztze"
        }, 
        {
            "body": {
                "answer": "<p>Is there no context? A quess is the cdf of a bimodal distribution... Maybe Two mixed truncated normal distributions?</p>", 
                "question": "What kind of curve is similar to this?"
            }, 
            "id": "dcq4b77"
        }, 
        {
            "body": {
                "answer": "<p>Cubic function shifted? Like (x+5)^3+8</p>", 
                "question": "What kind of curve is similar to this?"
            }, 
            "id": "dcqqm73"
        }, 
        {
            "body": {
                "answer": "<p>I think you<sq>re probably thinking of the base rate fallacy, yes.<br><br>Correct me if I<sq>m wrong, but what you<sq>re pointing out is that (a) if you are a white murder victim, the probability that your murderer was white is about 0.9, and (b) if you are a black murder victim, the probability that your murderer is black is (also) about 0.9. That is, most murders tend to occur within race.<br><br>Each of these is a conditional probability<colon><br><br>Pr(murderer of race X | victim of race X)<br><br>An important piece of information that is missing is the base rate of being a victim of race X<colon><br><br>Pr(victim of race X)<br><br>As far as I know, this number differs fairly substantially for white and black people (at least in the US). I looked at some FBI data tables a while back, and they definitely indicated that the base rates are different, but I don<sq>t recall what they were exactly.</p>", 
                "question": "What is this fallacy? <dq>White people murder white people at about ~90<percent> of their total murder rate and black people murder black people at about ~90<percent> of their total murder rate therefore they murder at the same rate.<dq>"
            }, 
            "id": "dcnsbs4"
        }, 
        {
            "body": {
                "answer": "<p>I<sq>m familiar with the statistic but it<sq>s used to rebut this kind of hysterical nonsense from the far right<colon> [Trump<sq>s Pants on Fire tweet that blacks killed 81<percent> of white homicide victims](http<colon>//www.politifact.com/truth-o-meter/statements/2015/nov/23/donald-trump/trump-tweet-blacks-white-homicide-victims/).<br><br>The claim isn<sq>t that the murder rates are the same - that would be astonishing given the demographic differences - but that claims like the above are racist lies.</p>", 
                "question": "What is this fallacy? <dq>White people murder white people at about ~90<percent> of their total murder rate and black people murder black people at about ~90<percent> of their total murder rate therefore they murder at the same rate.<dq>"
            }, 
            "id": "dco79wa"
        }, 
        {
            "body": {
                "answer": "<p>When? Where? Can you give examples? </p>", 
                "question": "why so much squaring (and rooting) all over the place?"
            }, 
            "id": "dcj01pf"
        }, 
        {
            "body": {
                "answer": "<p>(Edited in response to information in comments)<br><br>One reason to use the standard deviation (root-mean-square deviation from the population mean) rather than some other measure of scale for a distribution is that there are simple ways to get variances of sums in terms of variances of the terms in the sum, and their covariances (it also means that variances of averages are related in a simple way to the terms in teh average). There<sq>s no such general result for other measures of variability. This simplicity (which has all manner of consequences) makes standard deviations of sums (and averages) simple in turn.</p>", 
                "question": "why so much squaring (and rooting) all over the place?"
            }, 
            "id": "dcj2472"
        }, 
        {
            "body": {
                "answer": "<p>I agree that part of it is to get an easily differentiable function. With increased computing power, the use of absolute differences are now more common (and less influenced by extreme values).</p>", 
                "question": "why so much squaring (and rooting) all over the place?"
            }, 
            "id": "dckludi"
        }, 
        {
            "body": {
                "answer": "<p>You can<sq>t correlate a nominal scale so presumably your education variable is ordinal as well (e.g. high school<some college<bachelor<sq>s<phD) [if you can order the categories in a meaningful way it is ordinal]?<br><br>Provided your education values are in the right order, Spearman<sq>s rho is an appropriate statistic so if that doesn<sq>t show anything they aren<sq>t correlated. It<sq>s possible that confounding is masking a real association between the two variables but if you don<sq>t have these other variables you will not be able to tell.<br><br>As for visualising the correlation, [this stack exchange thread](https<colon>//stats.stackexchange.com/questions/56322/graph-for-relationship-between-two-ordinal-variables) has some good ideas.</p>", 
                "question": "Best methods for analysis?"
            }, 
            "id": "dchc84e"
        }, 
        {
            "body": {
                "answer": "<p>The standard error is the standard deviation of the mean. If you want to describe the population, mean and standard deviation are appropriate. If you want to indicate precision of the estimate then the standard error is appropriate. A 95<percent> confidence interval is +/- 1.96 standard errors either side of the mean.<br><br>The standard deviation doesn<sq>t change with the sample size, the standard error gets smaller as the sample size increases. The correct one to report depends entirely on the situation and you can easily derive one from the other if you know the sample size.</p>", 
                "question": "When to report standard error vs. standard deviation?"
            }, 
            "id": "dcfsf0r"
        }, 
        {
            "body": {
                "answer": "<p>They convey different information so there is no reason not to report both. As you noted, the sd is a descriptive statistic. The sem can be used in inferential statistics.  I would recommend sd<sq>s and confidence intervals, the latter obviating the need for sem<sq>s. </p>", 
                "question": "When to report standard error vs. standard deviation?"
            }, 
            "id": "dcfgrcd"
        }, 
        {
            "body": {
                "answer": "<p>I want apologize for my answer<sq>s length but I think it will help you understand what is going on better than a short answer. <br><br>First, when you are doing statistical analysis there are two goals you can have. The first is inference. One example of this would be what is the most common/likely cause or signal of default. In cases like this you have to worry about <dq>the true signal.<dq> You however, are not doing inference, you are are doing the second thing<colon> prediction. For prediction, we don<sq>t actually care about the true cause of a thing, we just want to be able to predict what is going to happen or how we should respond (see the [Waffle House Index](https<colon>//en.wikipedia.org/wiki/Waffle_House_Index) as an example). Obviously, waffle houses shutting down don<sq>t cause damages to surrounding areas, but they are an indicator of damage.<br><br>So what you are looking for are indicators of default, even if the indicator is not in itself the cause. This means we don<sq>t care about finding the <sq>true signal<sq>. It does mean that we do need to figure out how to eliminate correlated variables. <br><br>Now there are (as I<sq>m sure you know) about a million statistical tests and procedures that will give you all sorts of answers. However, they are often quite complicated and have many assumptions that are not well explained. In general, the risk you run when you use statistical procedures you do not understand is that you get an answer that is much more confident than it should be (ex. your p-value is 0.002 instead of 0.15). I will say though that with the statistical procedures you are talking about (ex ridge regression), that is not as much of an issue. I am personally of the opinion, that it is better to try to do as much model selection as you can using knowledge of the system (ex<colon> if you are trying to predict average temperature, you know latitude is going to be an important predictor; longitude though, not so much). I have also found plotting each individual predictor variable against the response variable can help.<br><br>As far as the nonnegative-garrote is concerned, it is a method for subsetting a linear regression. Basically, it is an alternative to ridge or lasso regression. All three of these methods are ways of dealing with multicollinearity. I am not personally familiar with the non-negative garrote, but I know both lasso and ridge regression are good options, especially if you have a lot (>20) of variables. I actually just browsed [an article](http<colon>//pages.stat.wisc.edu/~myuan/papers/garrote.final.pdf) that said that you could use non-negative garrote on ridge regression but I didn\u2019t read the paper that closely so there could be some caveats that I missed.<br><br>I definitely like the idea of picking one or two variables from each of your categories. However I would not hesitate to through entire groups out if they don\u2019t seem relevant. Also, I am not sure what you mean by dividing the weight for each variable among its group.<br><br>Another thing to consider is that although want good predictive power, you also want to use variables that businesses may not be monitoring as closely. So just because a ridge regression says variable A is better than variable B, if variable B is not as closely monitored It might actually be a better indicator. Hope this helps!<br><br> </p>", 
                "question": "Multicollinearity and using statistical tests I do not understand"
            }, 
            "id": "dcavxut"
        }, 
        {
            "body": {
                "answer": "<p>No coding background at all. I<sq>m learning Python for data scraping for my dissertation and have been using this to teach myself<colon> https<colon>//automatetheboringstuff.com (mobile sorry for the link). I like it so far and it<sq>s really user friendly. </p>", 
                "question": "What are some effective resources and tips in learning Python for statistics?"
            }, 
            "id": "dc6b51b"
        }, 
        {
            "body": {
                "answer": "<p>My advice is to learn the theory and algorithms themselves that are implemented and Python as a language seperately, after all these techniques are language independent. Without these skills you will almost surely be using other<sq>s code as you have with the DataCamp course.</p>", 
                "question": "What are some effective resources and tips in learning Python for statistics?"
            }, 
            "id": "dc6b78z"
        }, 
        {
            "body": {
                "answer": "<p>Econometrician. The best resource I know of to learn from scratch, assuming you<sq>re hard working, is quant-econ.org python tutorials. </p>", 
                "question": "What are some effective resources and tips in learning Python for statistics?"
            }, 
            "id": "dc6m28b"
        }, 
        {
            "body": {
                "answer": "<p>Check out<colon><br><br>1. Dataquest<br>2. Udacity<sq>s Intro to Data Anysis course<br>3. Udacity<sq>s Intro to Machine Learning Course <br>4. Udacity<sq>s MongoDB course <br>5. Coursera Applied Data Science with Python specialization <br><br>However, I think it<sq>s much better to first focus on developing your comfort with Python. I recommend the following<colon><br><br>1. First compete Team Treehouse<sq>s Learn Python track <br><br>2. Then complete Udacity<sq>s Intro to Computer Science (CS101) course, which is hard as fucking nails for a newbie but so so rewarding<br><br>Like you, I tried DataCamp first and I wasn<sq>t satisfied with it. It held your hand way too much to the point I didn<sq>t learn too much. Team Treehouse holds hand a bit but much less and is really well taught. Udacity CS101 learns to make you comfortable with being thrown into no mans land with nothing but a blank coding editor and what they want you to find. It<sq>s really overwhelming and intimidating at first but once you get through it your confidence will go through the roof by the end and you<sq>ll feel much more comfortable and knowledgeable about basic Python.<br><br>If you do these two first I can assure you you<sq>ll feel comfortable enough with your Python background to tackle the courses above. </p>", 
                "question": "What are some effective resources and tips in learning Python for statistics?"
            }, 
            "id": "dc6bg5u"
        }, 
        {
            "body": {
                "answer": "<p>You should give real life examples. For instance, risk & standard deviation relationship, corelations between economic indicators, etc. I think these sorts of examples can make the course very interesting <colon>)</p>", 
                "question": "Teaching tips for Business Statistics"
            }, 
            "id": "dbz9a7a"
        }, 
        {
            "body": {
                "answer": "<p>There is a good book about teaching statistics by Andrew Gelman and Deborah Nolan<colon> [Teaching statistics<colon> A Bag of Tricks](https<colon>//www.amazon.com/Teaching-Statistics-Tricks-Andrew-Gelman/dp/0198572247/ref=sr_1_1?s=books&ie=UTF8&qid=1483512009&sr=1-1&keywords=teaching+statistics+bag+of+tricks).</p>", 
                "question": "Teaching tips for Business Statistics"
            }, 
            "id": "dbz8q9q"
        }, 
        {
            "body": {
                "answer": "<p>Can<sq>t help with specific sources but they do exist, this looks like a good lead<colon> http<colon>//link.springer.com/chapter/10.1007/978-1-4939-0603-1_23</p>", 
                "question": "Teaching tips for Business Statistics"
            }, 
            "id": "dc19x5f"
        }, 
        {
            "body": {
                "answer": "<p>The tips i give to the new writer is that do your work with sincerity and honesty you are a writer and the writer take an important part in market point of view and second is that your English are really good for the writing and speaking also and if you want to increase your career then write then quality and a pure content because quality take an important part not a quantity. thank you https<colon>//www.skillometer.net/</p>", 
                "question": "Teaching tips for Business Statistics"
            }, 
            "id": "dem1df6"
        }, 
        {
            "body": {
                "answer": "<p>You do have yearly seasonality in your data that you need to take into account.<br><br>You probably somehow said to R that your period was a year, so your Lag 1 is in fact your Lag ~~13~~ 12 (1 year prior). You should make your period monthly, add a seasonality of <dq>12<dq> so it will compare January with January and so on.<br><br>Now, how did I see that ? Your residue (the bars) are supposed to be white noise. They are clearly correlated here. And as you can see, at <dq>Lag1<dq> (your 12th bar) it is higher than the horizontal bar, which means that Lag0 (your current event) is statisticly correlated with the event from a year prior.<br><br>Both ACF & PACF do check for correlation. I won<sq>t go into details, but all bars should both be below the horizontal bar, otherwise it means you still should add variables to your model.<br><br>EDIT <colon> Year 1 = 12 months, not 13.</p>", 
                "question": "Interpret ACF/PACF dataplots in R"
            }, 
            "id": "dbobkks"
        }, 
        {
            "body": {
                "answer": "<p>Here are some links that helped me<br>http<colon>//people.duke.edu/~rnau/411arim3.htm<br>https<colon>//www.quora.com/What-is-the-difference-among-auto-correlation-partial-auto-correlation-and-inverse-auto-correlation-while-modelling-an-ARIMA-series<br><br>If you are starting off with Forecasting, I highly recommend https<colon>//www.otexts.org/fpp for the basics. In addition, the Duke course (first link) is also good.<br></p>", 
                "question": "Interpret ACF/PACF dataplots in R"
            }, 
            "id": "dbpfxmd"
        }, 
        {
            "body": {
                "answer": "<p>Do a machine learning model like random forest or gbm or a penalized regression model (if you need interpretability) like ridge regression or lasso regression.  They all do fine when you have lots of variables. </p>", 
                "question": "Working on a project with 72 variables"
            }, 
            "id": "db4h8yo"
        }, 
        {
            "body": {
                "answer": "<p>This is very well covered in psychometric theory. Look up factor analysis, principle components analysis, item response theory, the rasch model...One or a combination of these things will probably be along the lines of what you want. There is a website called the personality project (I can<sq>t link right now) that will hopefully help you.</p>", 
                "question": "Working on a project with 72 variables"
            }, 
            "id": "db4t093"
        }, 
        {
            "body": {
                "answer": "<p>Factor analysis to create scales, and work with them, rather than variables.</p>", 
                "question": "Working on a project with 72 variables"
            }, 
            "id": "db4ig68"
        }, 
        {
            "body": {
                "answer": "<p>You<sq>re working with too many variables for talent. I recommend creating an index of the talent variables so that you are correlating the score on the personality/aptitude test with the unified measure of <dq>talent<dq> or success. </p>", 
                "question": "Working on a project with 72 variables"
            }, 
            "id": "db4fot4"
        }, 
        {
            "body": {
                "answer": "<p>Maybe you can provide some clarification or an example.<br><br>Is your goal to figure out how student cohorts are doing over time based on changing results on the tests over time by different cohorts?<br><br>Or is the goal to be able to, say, identify which students are doing particularly well based on their relative performance in a variety of science areas (e.g., <dq>Student A got a 90<percent> on the physics test but a 70<percent> on the biology test. Student B got an 80<percent> on the bio test and an 80<percent> on the physics test. Which is performing at a higher level given the relative difficulties of the physics and bio tests?<dq>)<br><br>Or are you hoping to use this information to adjust the test difficulties? Or perhaps to test the effect of different instructors in terms of their students<sq> performance on standard tests?<br><br>Or something else?<br><br>The goal is going to kind of direct how you construct a model of the results, because you need to make some simplifying assumptions and it might help direct you in figuring out which simplifying assumptions are okay to make.<br><br>There are some easy things you could do (just look at average scores on the different tests over time arranged in a table) that could help make decisions. That may be a good first step to take. There are also far more complex things you can do, like looking at the correlation of scores across different subject areas by individuals taking tests in multiple areas. But the first step is defining more clearly what you want to know.</p>", 
                "question": "Standardising Exam Marks"
            }, 
            "id": "dakvdnl"
        }, 
        {
            "body": {
                "answer": "<p>Yeah, as /u/M_Bus says, it<sq>s good to start with some clear objectives.<br><br>In the long run, you may be best served by old fashioned z-scoring (curving).  As long as you<sq>re careful with how you standardize things, you can answer a lot of reasonable questions.<br><br>If you<sq>re familiar with multivariate stuff, then you could think about modeling performance with mixed effects.  It<sq>s a powerful and flexible approach.  In a single model, you can account for differences among teachers, the influence of different subjects on performance, individual student-level differences, differences in individual aptitude across subjects, and so on.  In some policy applications, estimated random effects are actually used to generate <dq>adjusted<dq> unit-level scores.  (i.e. you could rank bus drivers by safety record after adjusting for routes, shift times, etc.)<br><br>However, the tradeoff is interpretability.  I think that most students have a tough time understanding z-scores.  When it comes to mixed effects models, even professional statisticians will argue about interpretation.  Even if you were working with a perfectly valid model, I<sq>d be reluctant to use the results in an evaluation context with students (much less parents).</p>", 
                "question": "Standardising Exam Marks"
            }, 
            "id": "dal31tu"
        }, 
        {
            "body": {
                "answer": "<p>In general, no. If they<sq>re jointly multivariate-normal, then yes.<br><br>It<sq>s perfectly possible possible to have a multivariate distribution with normal margins where at least some of the variables cannot be written as a linear combination of independent normals.<br></p>", 
                "question": "Can any N(0 1) random variables (possibly not independent) be expressed as a linear combination of independent standard normals?"
            }, 
            "id": "d9z7oqs"
        }, 
        {
            "body": {
                "answer": "<p>Does [this post](http<colon>//stats.stackexchange.com/a/56000) over on Cross Validated answer your question?</p>", 
                "question": "Help regarding standard deviation of a group of sets"
            }, 
            "id": "d9uz6w8"
        }, 
        {
            "body": {
                "answer": "<p>I think the answer is going to depend on information that is not yet provided here, unfortunatley.<br><br>What<sq>s your research question? What questions do you hope to answer using these analyses?</p>", 
                "question": "PhD student freaking out help required selecting methods/design"
            }, 
            "id": "d9u4q0b"
        }, 
        {
            "body": {
                "answer": "<p>It<sq>s super field dependent and you are the person who has to figure out what the cream of the crop is in your field. Best thing you can do now that you know the lay of the land is to bounce your ideas off other experts and see where they stand and where they fail. But to do that, you have to find the experts. Who knows if it<sq>s anybody here since you haven<sq>t said what your topic is.</p>", 
                "question": "PhD student freaking out help required selecting methods/design"
            }, 
            "id": "d9ugd5m"
        }, 
        {
            "body": {
                "answer": "<p>Also, see if the stats department at your school does consulting for other departments.  My department started a consulting center staffed by stats PhD students to let the students get experience and to offer stats consulting at very low rates/for free, depending.</p>", 
                "question": "PhD student freaking out help required selecting methods/design"
            }, 
            "id": "d9uojs5"
        }, 
        {
            "body": {
                "answer": "<p>It depends a lot on what you mean by <dq>active<dq> and what kind of granularity you are looking for. Movement data might be a good start (i.e. try to measure how many people are <dq>out and about<dq>). <br><br>* traffic <br>* public transit utilization<br>* taxi trips<br>* bike share rentals<br></p>", 
                "question": "Index<sq>s to measure how active a city is?"
            }, 
            "id": "d9qeikw"
        }, 
        {
            "body": {
                "answer": "<p>Use the stata documentation, it<sq>s pretty good. Example first session [is here](https<colon>//www.stata.com/bookstore/getting-started-windows/), more general stuff [is here](https<colon>//www.stata.com/features/documentation/) and their official question forum [is here](http<colon>//www.statalist.org/forums/)<br><br>There are of course good texts, for example see Kohler or the one by Hamilton.</p>", 
                "question": "Hi guys any recommendations for learning Stata?"
            }, 
            "id": "d9g8jwy"
        }, 
        {
            "body": {
                "answer": "<p>Checkout Udemy.com and find the best Stata course</p>", 
                "question": "Hi guys any recommendations for learning Stata?"
            }, 
            "id": "d9f27bt"
        }, 
        {
            "body": {
                "answer": "<p>I don<sq>t suppose you know Python? If so, scikit-learn has a cohen<sq>s kappa function....</p>", 
                "question": "Need to calculate inter-rater agreement kappa for a data set but not sure which one and how?"
            }, 
            "id": "d98066g"
        }, 
        {
            "body": {
                "answer": "<p>I think excel is just not suited for the job. I would use a program like spss.</p>", 
                "question": "Need to calculate inter-rater agreement kappa for a data set but not sure which one and how?"
            }, 
            "id": "d98vfdh"
        }, 
        {
            "body": {
                "answer": "<p>This seems like an abuse of the kappa statistic family. As I see it, you can do two things. First, compute a kappa for each type of code, resulting in far too many sample statistics with next to meaningless interpretation. Second, and maybe best, is report overall agreement if that<sq>s more important. In this case, you only care that 13,339 cases were concordant and the rest discordant. What are you trying to show by having high agreement? Also yes, Cohen<sq>s kappa is probably best since it works for you categories (correct/incorrect in the second case) and two raters.</p>", 
                "question": "Need to calculate inter-rater agreement kappa for a data set but not sure which one and how?"
            }, 
            "id": "d9c1db0"
        }, 
        {
            "body": {
                "answer": "<p>Intro Micro and Macro probably won<sq>t do much for you. Instead, perhaps look into Econ 2025H, the honors intro micro, that looks like it tries to combine intro and intermediate micro.  Then if you want another course, see if you can take the econometrics course- it is linear models, but with enough differences in tools and emphasis that it might be valuable.<br><br>Whether those courses would prove more valuable than more CS is a toss-up depending on your strengths and career goals. <br><br>(Econ professor here, who knows a bit about stats and computational stuff \u263a)</p>", 
                "question": "Should I take Macro and Micro Econ?"
            }, 
            "id": "d95llqe"
        }, 
        {
            "body": {
                "answer": "<p>As someone who studied Econ and math in college and now does <sq>big data<sq> work, there isn<sq>t going to be anything in micro/macro that helps you with that kind of work. That side of Econ can help greatly with understanding the world around you, and in that sense micro would be better than macro at the introductory level, but the intro courses are rarely more than common sense and weak calculus. <br><br>If I had to recommend any classes to support your big data goal, I would recommend taking math classes outside of statistics, as linear algebra, differential equations, and real analysis have all been critically important to understanding advanced statistical methods and deep learning approaches. <br><br>The punchline is that intro micro/macro are largely fluff no matter where you take them. The value parts of Econ are found in more advanced classes, so if you are looking to gain a good return, these are likely not the best classes for you to take. <br><br></p>", 
                "question": "Should I take Macro and Micro Econ?"
            }, 
            "id": "d9690am"
        }, 
        {
            "body": {
                "answer": "<p>When doing linear regression, your coefficient estimates are unbiased but inefficient if there is autocorrelation in the residuals.  This will also mean the standard errors of your coefficients will be wrong.  <br><br>I<sq>m only speculating here, but I believe the same will be true for a GLM, except that perhaps the coefficients would be biased (not sure though).   If you have low autocorrelation and WAY more observations than effects, you should be fine.  Otherwise, it<sq>s a big no no.  </p>", 
                "question": "Is the GLM effect size calculation valid if there<sq>s autoregressive error"
            }, 
            "id": "d918jjr"
        }, 
        {
            "body": {
                "answer": "<p>The mean for a truncated normal <br><br>https<colon>//en.wikipedia.org/wiki/Truncated_normal_distribution#Moments<br><br>So for the upper 1<percent> tail<colon><br><br>E(X|X>2.32635) = \u03bc+\u03c3[\u03d5(2.32635)/0.01] = \u03bc + 2.6652 \u03c3 <br><br></p>", 
                "question": "Need help trying to find the average z-score in a slice of a normal distribution"
            }, 
            "id": "d8webds"
        }, 
        {
            "body": {
                "answer": "<p>You should read the paper draft discussed [in this thread.](https<colon>//www.reddit.com/r/statistics/comments/57si4s/why_most_of_psychology_is_statistically/) They make the argument that many psychology studies are not falsifiable because when you examine the sampling distribution of differences between new and old estimated correlations, the small sample sizes used in old results make these differences too variable to ever practically detect a difference. So yes, this is a real issue, and in practice, it is widely ignored. You are still making a contribution if you improve an estimate of the correlation by improving measurement error in X ([which attenuates estimated correlations](https<colon>//en.wikipedia.org/wiki/Correction_for_attenuation)), but unfortunately your ability to say you have distinctly more correlation is hindered by the design of the original study.</p>", 
                "question": "The test for the difference between two correlation coefficients has absurdly low power. Should I abandon my study?"
            }, 
            "id": "d8uz0dt"
        }, 
        {
            "body": {
                "answer": "<p>The study can have value even if it has a relatively low probability of achieving a conclusive result (sometimes called significant). You could provide a strong hint at the difference between correlations that could be further explored  in further studies. Science would progress slowly if only studies certain to obtain the desired effect were conducted. </p>", 
                "question": "The test for the difference between two correlation coefficients has absurdly low power. Should I abandon my study?"
            }, 
            "id": "d8sd7ok"
        }, 
        {
            "body": {
                "answer": "<p>Here<sq>s a good opportunity for me to run my teaching past some people more knowledgeable than myself - I have a working understanding (practical understanding) of statistics, enough to analyze my own data, and I teach research methods within my field, teaching the bare minimum of analysis at the end of the semester (t-tests, correlation, chi-squared tests). Here<sq>s how I try to simplify it to my students (who often can<sq>t remember the difference between the mean and the median)....<br><br>There are (at least) two reasons why you may have found a difference in your sample (e.g. Men like something more than Women). The first (null hypothesis), is that chance (the randomness of the draw of people into your sample) and sampling error led you to a result in your sample that you would not expect to replicate in another sample or in the larger population. The second (alternative hypothesis), is that you found a difference in your sample because that difference really does exist in the population. The p-value then tells you the probability of finding a difference as large as the one you found in your sample due only to sampling error or chance.<br><br>How much am I fucking up this explanation?</p>", 
                "question": "Is my understanding of p-values and null hypothesis correct?"
            }, 
            "id": "d8nn513"
        }, 
        {
            "body": {
                "answer": "<p>You reject the null hypothesis for p<0.05.<br><br><br>You do not reject the null hypothesis for p>0.05. (You don<sq>t accept it, you just fail to reject it.)<br><br><br>The further away your observed values are from those expected if the null hypothesis was true, the smaller your p-value.<br><br><br>A dataset can never be statistically significant. The result of a statistical test is. <br><br>Large p-values indicate that your result has a high probability of being observed, given that the null hypothesis is true.<br>Low p-values indicate that it is very unlikely that your results happened by chance in a world where the null hypothesis is true, thus you reject it. </p>", 
                "question": "Is my understanding of p-values and null hypothesis correct?"
            }, 
            "id": "d8ne1fs"
        }, 
        {
            "body": {
                "answer": "<p>There are several versions of the central limit theorem <br><br>https<colon>//en.wikipedia.org/wiki/Central_limit_theorem<br><br>The classic theorem basically says that as the sample size goes to infinity, a standardized mean of independent identically distributed random variables  (with finite mean \u03bc and finite variance \u03c3^(2))  ... i.e. (\u0232\u2212\u03bc)/(\u03c3/\u221an) will have a distribution that converges to the standard normal distribution.<br><br>[A lot of elementary books completely mis-state the CLT and its implications.]<br><br>Other versions of the theorem generalize that in various ways (relaxing the need to be independent or identically distributed). There are still other central limit theorems that deal with convergence of sums or averages to some distribution for variables that don<sq>t fulfill those conditions.<br><br>As for *why* it holds, the real reasons lie in the proofs of the theorem. In slightly simpler terms you could say that cumulants (above the second) of standardized means will converge to zero, leaving you with a distribution that asymptotically has only the first two cumulants (which is the normal), but if you have no experience of cumulants (which are related to the moments), that probably won<sq>t help you much.<br><br>If you seek intuition, one can give a more hand-wavy explanation  as to why standardized means may tend to look approximately normally distributed in finite samples, but this isn<sq>t what the CLT talks about (that<sq>s nearer to the province of the Berry-Esseen theorem).<br></p>", 
                "question": "Can someone explain the Central Limit Theorem and why it holds?"
            }, 
            "id": "d8in2cm"
        }, 
        {
            "body": {
                "answer": "<p>[Obligatory Khan Academy video.](https<colon>//www.youtube.com/watch?v=JNm3M9cqWyc)</p>", 
                "question": "Can someone explain the Central Limit Theorem and why it holds?"
            }, 
            "id": "d8ixkoi"
        }, 
        {
            "body": {
                "answer": "<p>[Here<sq>s a great writeup of the CLT.](http<colon>//www.jeannicholashould.com/the-theorem-every-data-scientist-should-know.html)</p>", 
                "question": "Can someone explain the Central Limit Theorem and why it holds?"
            }, 
            "id": "d8irxb9"
        }, 
        {
            "body": {
                "answer": "<p>You could model each symbol presentation and response as a Bernoulli random variable with probability p, in which case the likelihood would be p^k (1-p)^(1-k) , where k is 1 if the symbol is seen and 0 if it<sq>s not.<br><br>It is convenient to model p as a [Beta](https<colon>//en.wikipedia.org/wiki/Beta_distribution) random variable (i.e., the prior for p would be a Beta density with parameters a and b).<br><br>There are various ways that this simple model could be elaborated on. You could model p as a function of, say, the time each symbol was displayed, or whatever is going on in the video when each symbol is displayed. Or if there is no within-video structure you care about, you could model the total number of observed symbols as a binomial random variable.<br><br>You could also model multiple subjects<sq> p parameters with the prior Beta distribution, in which case you would need to specify (hyper-)priors for a and b.</p>", 
                "question": "Bayesian model for estimation of counts"
            }, 
            "id": "d8ifng2"
        }, 
        {
            "body": {
                "answer": "<p>[deleted]</p>", 
                "question": "Bayesian model for estimation of counts"
            }, 
            "id": "d8i8l6j"
        }, 
        {
            "body": {
                "answer": "<p>I don<sq>t know what you seek by <dq>how the formula works<dq>.</p>", 
                "question": "Is there any book or resource where the concepts are explained in layman terms"
            }, 
            "id": "d8hph0i"
        }, 
        {
            "body": {
                "answer": "<p>Maybe the easiest way<colon> [Statistics for Dummies](https<colon>//www.amazon.com/Statistics-Dummies-Deborah-J-Rumsey/dp/1119293529/ref=sr_1_3?ie=UTF8&qid=1475841218&sr=8-3&keywords=statistics)</p>", 
                "question": "Is there any book or resource where the concepts are explained in layman terms"
            }, 
            "id": "d8hqyw3"
        }, 
        {
            "body": {
                "answer": "<p>Try <dq>Naked Statistics<dq>.</p>", 
                "question": "Is there any book or resource where the concepts are explained in layman terms"
            }, 
            "id": "d8hr865"
        }, 
        {
            "body": {
                "answer": "<p>The derivation of the formula involves some very dense math, and you<sq>re going to have to be pretty good with multivariate calculus to understand it. <br><br>For example, this is Student<sq>s (Gossett<sq>s) paper where he talks about the t-distribution, to get the standard error of a mean.  http<colon>//www.aliquote.org/cours/2012_biomed/biblio/Student1908.pdf or here<sq>s a paper on the geometry underlying the paired samples t-test (with N = 3!!)<br>http<colon>//onlinelibrary.wiley.com/doi/10.1111/1467-9884.00301/abstract<br><br>So, if I<sq>ve understood the question correctly, I think the answer is <sq>no<sq>.<br><br><br></p>", 
                "question": "Is there any book or resource where the concepts are explained in layman terms"
            }, 
            "id": "d8if4p8"
        }, 
        {
            "body": {
                "answer": "<p>If you<sq>re modeling a covariance matrix, as is necessary in quite a lot of multivariate statistics, each individual covariance value is a necessary component of the full matrix. This is probably what it<sq>s used for most often, even if each individual covariance value isn<sq>t used on its own.<br><br>With respect to each individual covariance, it<sq>s directly related to correlation. A <dq>raw<dq> covariance isn<sq>t readily interpretable, but when divided by the product of the relevant marginal variances, it gives you a (much more interpretable) correlation.</p>", 
                "question": "What is covariance specifically?"
            }, 
            "id": "d8dt458"
        }, 
        {
            "body": {
                "answer": "<p>Stop chi square. Run regressions.</p>", 
                "question": "[Help] Data set of 320 respondents completing Likert scales attempting chi-squared not working well."
            }, 
            "id": "d8ac9qy"
        }, 
        {
            "body": {
                "answer": "<p>I teach methodology and analytics courses, and the best I can say is to take a quantitative course in the social sciences.  You are right to be skeptical in some ways (e.g., was this a truly random sample, did all members of the population have a chance to participate?), but not in others (e.g., a small random sample of even 100 is sufficient to make inference about all Americans, depending on the effect measured).   <br><br>I<sq>m sure you want a satisfactory and complete answer to your questions, but it honestly takes a semester to cover this stuff.  Most stats for social sciences textbooks will cover these topics in an accessible way.</p>", 
                "question": "Do most Americans believe in supernatural healing? How reliable is this survey? How do you quantify survey certainty?"
            }, 
            "id": "d89k9eo"
        }, 
        {
            "body": {
                "answer": "<p>Hi, you asked a lot of questions and I<sq>m going to hit the points that I think are most important; although, if I had more time I<sq>d love to address all of your list in depth.  <br><br>These are survey results by the Barna research group.  I conduct research myself on religious/secular issues and I<sq>ve been following Barna<sq>s newsletter and studies for a long time.  It<sq>s important to keep in mind that Barna is very vague in regards to their analyses and polling methods.  Further, they consistently only provide single-item questions and provide percentages based on the answers.  This can be highly problematic.  I find myself shaking my head in criticism at Barna the majority of time because they often ask questions that hint at an underlying pro-Christian agenda.  Hypothesis testing is conventional in social science.  We go in with expectations a priori, and we can support that hypothesis or fail to support it.   Barna, on the other hand, goes about things by just showing us answers to polling questions all willy nilly.  Who knows all the questions they ask that never make their final newsletter.  There<sq>s a difference between a survey and a scientific study.     <br><br>One way this survey (and others from Barna) is highly problematic is that it requires the polling question to be easily interpretable and that there be a low likelihood of misinterpretation.  <dq>People can be physically healed supernaturally by God?<dq> That question has some hefty limitations.  A person might not believe in God or they may believe God exists but never intervenes in earthly affairs but a respondent could still potentially answer <dq>strongly agree<dq> because God **can** heal.  In other words, a god/God would be capable but that doesn<sq>t mean these people think God is actually doing it or has ever done it in the past.  Hell, an atheist who has no reason to think such a thing has ever happened could theoretically agree with it.  How are the survey respondents defining <dq>supernatural?<dq>  Would natural healing processes that were intelligently designed by a God essentially be considered as supernatural?  I don<sq>t think so, but that doesn<sq>t mean there weren<sq>t some responders that saw it that way.  <br><br>Scientific surveys are rarely accepted into academic journals if they rely on single-item measures.  So, Barna<sq>s percentage is flawed and highly suspect because their single question is limited and weak.  Further, look at the other polling question Barna used and note that a large majority of people admit they have never been supernaturally healed by God, they are all relying on stories they hear from others, which makes me wonder about whether people believe in supernatural healing or whether the bigger issue is people are prone to believing improbable (but uplifting) stories.  It<sq>s hard to know what<sq>s happening because we are limited to a single question about belief in supernatural healing<sq>s occurrence.  Multiple questions are needed.  Academic journals (even mediocre ones) would never allow a Barna study into their publication because their methods are too full of holes.  Usually, to arrive to a conclusion based on survey data, the survey would use a previously psychometrically validated scale that is comprised of numerous questions on a single variable (e.g., belief in supernatural healing) and the statistical analyses would be more complex than a simple percentage.   <br><br>I hope this gives you a start.  You asked many questions that your scientific training will answer for you in due time (many of which I didn<sq>t touch on).  You have a healthy dose of skepticism so I encourage you to stick with your training so that you can get involved in the scientific fray yourself.  </p>", 
                "question": "Do most Americans believe in supernatural healing? How reliable is this survey? How do you quantify survey certainty?"
            }, 
            "id": "d8a5x72"
        }, 
        {
            "body": {
                "answer": "<p>I<sq>ll try to answer the questions I<sq>ve asked in this post to reflect my understanding...<br><br>1.<br><br>2. The textbooks I<sq>ve listed here together with Internet searches.<br><br>3.<br><br>4.<br><br>5.<br><br>6.<br><br>7.</p>", 
                "question": "Do most Americans believe in supernatural healing? How reliable is this survey? How do you quantify survey certainty?"
            }, 
            "id": "d8brolu"
        }, 
        {
            "body": {
                "answer": "<p>You need the number, not the percentage, and you can do a chi-square test to get an overall test of significance, and then do some sort of follow up test. (Perhaps based on residuals.)</p>", 
                "question": "Significance Testing Among Values in a Single Variable"
            }, 
            "id": "d7xlwws"
        }, 
        {
            "body": {
                "answer": "<p>I<sq>m not sure what you<sq>re trying to do right now, but [this](https<colon>//sites.google.com/site/unmarkedinfo/home/faq) site has an example.  </p>", 
                "question": "Setting up an occupancy model in R (studio) with small mammal presence/absence data"
            }, 
            "id": "d7sku3m"
        }, 
        {
            "body": {
                "answer": "<p>You want to constrain your model to be zero at week zero because subjects should have zero weight change then, right? This could be accomplished by **not** including a term for an intercept, a term for the diet fixed effect, or a term for the subject-specific random intercept. You<sq>d still have a week effect, a diet-week interaction, and subject-specific random slopes over weeks. Each subject<sq>s weight change trajectory would be modeled as a straight line passing through the origin (0 weight loss at week 0), with individual variation (flatter or steeper slope) around a common mean line for their diet. You<sq>d be interested in the diet-week interaction term to determine whether one led to more rapid weight loss than the other. I *think* the syntax for this is `weight_change ~ 0 + week + week<colon>diet + (0 + week | subject)` but you may need to consult the lme4 documentation and adjust if you see something unexpected in the output.<br><br>For a reference on fitting and interpreting mixed effect models, see Singer and Willett<sq>s *Applied Longitudinal Data Analysis* book.</p>", 
                "question": "Interpretation of coefficient in mixed effects model"
            }, 
            "id": "d7so1fd"
        }, 
        {
            "body": {
                "answer": "<p>Glassdoor.com might be your friend here.<br><br>I know someone who graduated with a master<sq>s degree in stats (first degree something unrelated). 1 year experience, moved to a new job with total comp ~ 120k. But there are assistant professors of statistics who earn 70k (and they have PhDs, and post-docs) behind them.<br><br>It<sq>s pretty variable. It depends what you want to do, where you are prepared to go, etc. <br><br><br><br><br></p>", 
                "question": "What is the real landscape of statistics and salary job security etc...."
            }, 
            "id": "d7ouz62"
        }, 
        {
            "body": {
                "answer": "<p>There<sq>s a recruiting firm that publishes a pretty good salary report for data scientists and predictive analytics professionals each year<colon> http<colon>//www.burtchworks.com/big-data-analyst-salary/big-data-career-tips/the-burtch-works-study/.  <br><br>These are different from <dq>statistician<dq> salaries which are generally lower, though.  That<sq>s just a pathology of the market as apparently optimizing click-through rates on a site is in higher demand than being able to design a cancer study <colon>) </p>", 
                "question": "What is the real landscape of statistics and salary job security etc...."
            }, 
            "id": "d7p8vmb"
        }, 
        {
            "body": {
                "answer": "<p>Automation removes job security for all jobs. Don<sq>t ask about it. </p>", 
                "question": "What is the real landscape of statistics and salary job security etc...."
            }, 
            "id": "d7ouj9c"
        }, 
        {
            "body": {
                "answer": "<p>It<sq>s hard to get people to provide actual numbers outside of the professor role (or situations where the university has to publicly publish data) and even then there are many people who have different education level or years of experience with the same title.<br>So it<sq>s vague. From my understanding on the two fields you listed, it<sq>s difficult to land a career in sports analytics- and if you do, it<sq>s way below the average of other stats/data science fields-- since there<sq>s extreme issues of supply vs demand. Even biostats roles can greatly differ based on region, industry, or company. (e.g. are you hoping to go to california or something in the midwest? are you aiming for a children<sq>s hospital or pharma? you may easily be making double based on that alone.)<br><br>From my understanding<colon> it depends. see if you can reach out to any other previous alums too and see if they will divulge. <br><br>edit<colon> I looked at your history, and have a working theory you<sq>re in west michigan- if so do you want to stay in the area? If so you will be starting off at a lower range than what you speculated from above. If so, you don<sq>t have a ton of job opportunities even in the area that are doing stats which is why I think you hear of some people telecommute/ make a weird commute. I could talk for days about the particular area, but I may be wrong on your location.</p>", 
                "question": "What is the real landscape of statistics and salary job security etc...."
            }, 
            "id": "d7r52dd"
        }, 
        {
            "body": {
                "answer": "<p>Here<sq>s how I see it, but a more experienced statistician should jump in if I<sq>m getting anything wrong.<br><br>To be clear, your original question is figuring out if you are getting more yellow starburst out of the bowl than you would expect given the distribution of yellow, red, pink, etc. in the packs, right?<br><br>If so, the simplest way to test this is with a one-way chi-squared test. And one rule of thumb that I<sq>ve come across (I<sq>m not yet sure what the rationale is behind it, but I think I<sq>ll be learning this soon) is that you need expected frequencies of at least 5 in each cell of the one-way table. I can explain this in more depth if you want.<br><br>Well, I<sq>ll just do it anyway. =P Assuming there are 5 colors, and each is equally frequent in the packs, then before doing your chi-squared test, you<sq>ll want to sample at least 25 starburst (25 total starburst / 5 colors = 5 starburst expected per color).</p>", 
                "question": "How do I determine a reasonable sample size for starbursts?"
            }, 
            "id": "d7o7ifk"
        }, 
        {
            "body": {
                "answer": "<p>There<sq>s a [nice formula](http<colon>//www.r-tutor.com/elementary-statistics/interval-estimation/sampling-size-population-proportion) that calculates what you want.  You<sq>ll need to specify the bound of the error of your estimation and a guess for p.  Most people set p = .5 because this will give you the most conservative sample size estimate.</p>", 
                "question": "How do I determine a reasonable sample size for starbursts?"
            }, 
            "id": "d7okqo4"
        }, 
        {
            "body": {
                "answer": "<p>Hardly...<br><br>It always helps starting with a question, rather than starting with deciding which tests to use. Otherwise you will end up with an answer of <dq>statistically significant<dq> but will be left wondering what this means.<br><br>In you case if the question is<colon> <dq>does the stimulus have an affect on heart rate?<dq> then it would make sense comparing heart rate *before* and *after* stimulus.</p>", 
                "question": "T test against zero with only positive values"
            }, 
            "id": "d7ns09t"
        }, 
        {
            "body": {
                "answer": "<p>This is like gathering data on the number of days it rains and then asking whether your data are consistent with the theory it rains on 120<percent> of days. You already know the true underlying mean heart rate change subject to this stimulus cannot be zero due to its definition. It does not make sense to hypothesis test mathematical impossibilities.</p>", 
                "question": "T test against zero with only positive values"
            }, 
            "id": "d7nynom"
        }, 
        {
            "body": {
                "answer": "<p>> whether it makes any sense to do a one-sample t-test against 0 when all my values are necessarily positive (they can never be negative)<br><br>No, since the only way for the population mean to be zero would be if the variable could only be 0. For a finite population even a single non-zero member would make the population mean positive. For an infinite (notional) population any chance of a non-zero value would be enough to make the population mean positive.<br><br><br><br></p>", 
                "question": "T test against zero with only positive values"
            }, 
            "id": "d7ogsc2"
        }, 
        {
            "body": {
                "answer": "<p>I would still run a test, it<sq>s possible that though it must be positive, your value is notbSTATISTICALLY significant from 0. <br><br>That being said, you may want to consider other hypotheses that would give you more info other than <dq>it<sq>s statistically different from 0<dq> *\u00af\\_(\u30c4)_/\u00af*<br><br>Edit<colon> per our discussion its useless to do the statistical tests since we know the parameter were testing has to be more than 0.<br><br>However it may be useful to look at whether it<sq>s practically significant </p>", 
                "question": "T test against zero with only positive values"
            }, 
            "id": "d7nqnyi"
        }, 
        {
            "body": {
                "answer": "<p>The original post<colon><br>I couldn<sq>t find any easy explanation for these distributions<colon><br><br>- binomial<br>- negative binomial<br>- geometric<br>- poisson<br>- uniform<br>- normal<br>- exponential<br>- triangular<br>- gamma<br>- weibull<br>I would really like to understand them but those definitions like on wikipedia are kind of complicated and more confusing.</p>", 
                "question": "This is the best explanation of distributions I<sq>ve *ever* seen. But the poster didn<sq>t finish the list. Anyone care to tackle some of the other points?"
            }, 
            "id": "d7nor4a"
        }, 
        {
            "body": {
                "answer": "<p>* The geometric distribution answers the question <dq>How often do I have to flip a coin until the first heads/tails appears?<dq> or <dq>How often do I have to throw a die until the first <sq>6<sq> appears?<dq>.</p>", 
                "question": "This is the best explanation of distributions I<sq>ve *ever* seen. But the poster didn<sq>t finish the list. Anyone care to tackle some of the other points?"
            }, 
            "id": "d7nwztg"
        }, 
        {
            "body": {
                "answer": "<p>Multiple (linear) regression. Daylight and temperature are probably colinear (as they are strongly related with each other) so the coefficients of your model won<sq>t be particularly stable -- that won<sq>t hurt your ability to make predictions, though, you just have to be careful when interpreting the coefficients.</p>", 
                "question": "Time Series with multiple variables."
            }, 
            "id": "d7m8rxb"
        }, 
        {
            "body": {
                "answer": "<p>Arimax would work. Look up auto.arima in the forecast package. You can use xreg to include covariates. </p>", 
                "question": "Time Series with multiple variables."
            }, 
            "id": "d7ma85j"
        }, 
        {
            "body": {
                "answer": "<p>Besides ARIMA based models,  also consider gls (generalized least square). </p>", 
                "question": "Time Series with multiple variables."
            }, 
            "id": "d7mcmlu"
        }, 
        {
            "body": {
                "answer": "<p>I<sq>m not sure how rigorously they want you to understand these definitions, but the gist of it is<colon><br><br>Finite<colon> you are able to enumerate every possible outcome and it<sq>s not infinite. For example, there are 10 possible outcomes. <br><br>Countably infinite<colon> same as above, except there are infinitely many possible outcomes. For example, an event can occur 0 times, 1 time, 2 times, 3 times, ... <br><br>Continous<colon> Any real number is okay. You can restrict to a subset of real numbers (e.g. 2 to 17) but any value within that range is acceptable, including decimals, fractions, etc. <br><br>In practice, there<sq>s often not really a clear-cut right or wrong answer to questions like 3.8. People model things that are not continuous as continuous and vice versa all the time. The important thing is to understand how that affects your results and what the pros/cons are. </p>", 
                "question": "Sample Space definitions trouble (finite countably infinite and continuous)"
            }, 
            "id": "d7fa1xh"
        }, 
        {
            "body": {
                "answer": "<p>What you<sq>re looking for is a *power calculation*. How large a sample do I need to detect an effect if there is one?  <br>If this was a simple t-test (testing for difference in group means), there are functions in most stats software to help you calculate this. In `R`, this is done with `pwr<colon><colon>pwr.t.test(d, sig.level, power)` where d = observations per group, sig.level is the targeted Type I error rate, and power is the targeted Type II error rate. For instance, if you wanted to detect an effect size of .5 at a significance level of .05 and a power of .95, you<sq>d call<colon><br><br>\t> pwr<colon><colon>pwr.t.test(d=.5, sig.level=.05, power=.95)<br><br>\t     Two-sample t test power calculation <br><br>\t              n = 104.9279<br>\t              d = 0.5<br>\t      sig.level = 0.05<br>\t          power = 0.95<br>\t    alternative = two.sided<br><br>\tNOTE<colon> n is number in *each* group  <br>  <br>.. and find out you<sq>d need ~105 observations per group.  <br>  <br>HOWEVER, you data looks like something that should be analyzed with a mixed-effects (or hierachical) model, and power calculations for those are non-trivial, and it<sq>s still an actively discussed topic how to do that.  <br>  <br>I would recommend going to your local (applied?) stats department and ask for help from actual statisticians on designing your experiment, as this kind of answer often needs a lot of back-and-forth and more knowledge about the specific research question.</p>", 
                "question": "Determining sample size?"
            }, 
            "id": "d7cn83g"
        }, 
        {
            "body": {
                "answer": "<p>As this is a follow-up survey a sample size calculation isn<sq>t really required - you need to get as close to the original sample as possible. <br>  <br>What is the difference between groups 1 - 3? Is this a demographic variable you are stratifying by? </p>", 
                "question": "Determining sample size?"
            }, 
            "id": "d7cl8ar"
        }, 
        {
            "body": {
                "answer": "<p>You can<sq>t (generally) show that there is no difference. Statistical test are designed to test the null hypothesis - that is, to find the probability of the result, given that there is no difference. They are not designed to find the probability of the result, given that they are the same.<br><br>You can look for differences, and if you fail to find them, you have some (weak) evidence that there is not a difference there. Think of it as finding a needle in a haystack. If you don<sq>t find a needle, that  doesn<sq>t mean there isn<sq>t a needle there, it just means you failed to find it. The longer you look, the more confident you are that there is no needle, or that if there is a needle, it<sq>s a very small one.<br><br>The sort of thing you can do is called an equivalence test - you calculate (something like) a t-test, and you say <dq>If there is a difference, it<sq>s likely that the difference is smaller than X.<dq> (Where X is the highest confidence limit). And then you try to explain why a difference of X is so small that you don<sq>t care about it. <br><br>Here<sq>s a t-test, in R<colon><br><br>    > a <-  c(69, 74, 60, 71, 54, 68, 71, 75, 70)<br>    >  b <-  c(72, 67, 62, 71, 59, 70, 75, 70, 75)<br>    >  <br>    >  t.test(a, b, paired = TRUE)<br>    <br>    \tPaired t-test<br>    <br>    data<colon>  a and b<br>    t = -0.69749, df = 8, p-value = 0.5053<br>    alternative hypothesis<colon> true difference in means is not equal to 0<br>    95 percent confidence interval<colon><br>     -4.306166  2.306166<br>    sample estimates<colon><br>    mean of the differences <br>                     -1 <br><br>You can argue that it<sq>s unlikely you would have found this result, if the scores had not gone down by more than 4.3 points (in the population) or up by more than 2.3 points.<br><br>Finally, the great flaw in all of significance testing is that (as Cohen put it) <dq>all null hypotheses are false.<dq> There is no way that, if you had a large enough population, you wouldn<sq>t find some change in scores, if only because the change in scores was because it was a rainy day the second time you tested.<br><br><br></p>", 
                "question": "No difference in scores (Equivalence Test?) Need help"
            }, 
            "id": "d789alp"
        }, 
        {
            "body": {
                "answer": "<p>I think you<sq>ll find answers on [this post](http<colon>//stats.stackexchange.com/questions/225175/why-do-some-people-use-999-or-9999-to-replace-missing-values) over at Cross Validated.</p>", 
                "question": "Replacing NA<sq>s with a relatively large number"
            }, 
            "id": "d76kb9g"
        }, 
        {
            "body": {
                "answer": "<p>The 4/5 test is NOT a statistical test, but a rule used by the EEOC.  It basically says that if you are looking for a problem relating to a disadvantaged group being <dq>affected adversely<dq>, we are not going to say that there is a problem as long as the disadvantaged group<sq>s success rate is 80<percent> or more of the other group<sq>s success rate.  This is just a rule of thumb someone made up, not anything to do with statistics or hypothesis tests.<br><br>[Expert on stats, but not EEOC policy... Here is a link with some decent info from Google](http<colon>//annex.ipacweb.org/library/conf/08/brink.pdf) \u263a<br><br></p>", 
                "question": "Something is not right - Help me understand"
            }, 
            "id": "d71voyz"
        }, 
        {
            "body": {
                "answer": "<p>I don<sq>t know what the 4/5 test is, but using a proportion test [in r<colon> prop.test(x=c(200,20),n=c(350,50))] there is a significant difference between the two (Chisq=4.5, df=1,P=0.033). Yes there is a big difference in sample size between males and females, but 50 is an ok sample size for females. Incidentally, the test is not significant if the proportions are the same but based on numbers 200/350 and 2/5 because a female sample size of 5 would make it pretty meaningless.<br><br>In terms of whether there<sq>s discrimination, I don<sq>t see how you can possibly say from these raw numbers, seems very reductive. Perhaps the males had better qualifications or experience, or performed better at interview, or were on average older, or younger, or whatever. You<sq>d really need to control for a number of potentially confounding variables to have stronger evidence for an effect. Something like propensity score matching might work, e.g. finding pairs of similar (across a range of confounding variable) males and females and testing whether the probability of hiring still differed across the two.</p>", 
                "question": "Something is not right - Help me understand"
            }, 
            "id": "d71lgci"
        }, 
        {
            "body": {
                "answer": "<p>Simpson<sq>s paradox and confounding makes this <dq>test<dq> a joke. </p>", 
                "question": "Something is not right - Help me understand"
            }, 
            "id": "d739bpo"
        }, 
        {
            "body": {
                "answer": "<p>How much data do you have? Count data is usually modelled as a Poisson process, but you need to check a few assumptions. Read up on Poisson regression, it might be what you need. </p>", 
                "question": "Predicting likelihood of future events based on recent results?"
            }, 
            "id": "d70sl7d"
        }, 
        {
            "body": {
                "answer": "<p>Set a null and alternative hypothesis.<br>use a t or p test to to reject or accept.<br>null ; mean is <= 70<br>alt ; mean is > 70<br><br>confidence 0.05</p>", 
                "question": "Predicting likelihood of future events based on recent results?"
            }, 
            "id": "d70pgbq"
        }, 
        {
            "body": {
                "answer": "<p>> results are as follows<colon> Expected obs1 obs2 Item1 1 a b Item2 1 c d .. .. ItemN 1 x y<br><br>> how can i check if observation 1 and 2 are significantly different from the expected ratio and amogst each other?<br><br>It is very unclear what you are trying to ask. Do you want to test<colon><br><br>Hypothesis 1<colon> Expected = obs1/obs2<br><br>and<br><br>Hypothesis 2<colon> obs1 = obs2<br><br>???</p>", 
                "question": "Kindly guide me on the following problem<colon>"
            }, 
            "id": "d6zzgm8"
        }, 
        {
            "body": {
                "answer": "<p>Your chances of getting to 500 is going to be a lot worse than 50/50, because you have to more than double your stake twice, whereas to get to 1, you only need to lose ~100<percent> of your stake once. Right?<br><br>Here<sq>s some clojure code to simulate<colon><br><br>    (defn gamble [n] ((if (< (rand) 0.5) + -) n (int (Math/ceil (* 0.1 n)))))<br>    (defn play [start] (if (< (->> (iterate gamble start) (drop-while #(< 1 <percent> 500)) first) 2) <colon>lose <colon>win))<br>    (frequencies (repeatedly 100000 #(play 100))) ; => {<colon>win 19290, <colon>lose 80710}<br>    (frequencies (repeatedly 100000 #(play 25))) ; => {<colon>win 4634, <colon>lose 95366}<br><br>So the chances of making it to 500 before hitting 1 is around 19<percent> if you start with 100 or 4.6<percent> if you start with 25.<br><br>Placing bigger bets slightly hurts your chances of winning, e.g. betting 100<percent> of your money instead of 10<percent> gives you ~12.6<percent> chance of winning starting with 100.<br></p>", 
                "question": "Statistics question about 50/50 odds"
            }, 
            "id": "d6vkmta"
        }, 
        {
            "body": {
                "answer": "<p>I mean I think it would make more sense they way you describe, but this seems like it<sq>s just a descriptive report, not a rigorous, scientific study, and isn<sq>t going to change the conclusions too much. Also, if the number of people with zero hour contracts is small relative to the total (2924), then it wouldn<sq>t make much difference anyway. </p>", 
                "question": "Why would a report about a differing attitudes of a subset of respondents use the whole set of the respondents for comparison in results?"
            }, 
            "id": "d6wn3vv"
        }, 
        {
            "body": {
                "answer": "<p>Ordinal regression</p>", 
                "question": "I gave 40 runners a random amount of Super Speed Potion ranging from 0L to 2L. They then raced against each other. How can I analyze the effect of the amount of SSP on what place they came in? (1st 2nd 3rd etc...)"
            }, 
            "id": "d6uscq0"
        }, 
        {
            "body": {
                "answer": "<p>Spearman<sq>s rank sum correlation.</p>", 
                "question": "I gave 40 runners a random amount of Super Speed Potion ranging from 0L to 2L. They then raced against each other. How can I analyze the effect of the amount of SSP on what place they came in? (1st 2nd 3rd etc...)"
            }, 
            "id": "d6uu0mi"
        }, 
        {
            "body": {
                "answer": "<p>Skewness is a mathematical concept- the <dq>third central moment<dq>, whereas the mean and variance are the first and second.  While I won<sq>t type the entire formula here, let<sq>s look at the important bit in the numerator<colon> Sum(xi-mean)^3 . A <dq>tail<dq> of outliers on the right will add a lot of large, positive numbers to the sum, making the overall value positive.  A tail of outliers on the left (less than the mean) will add in a lot of large negative numbers to the sum, making the overall sum negative.  Left skewed corresponds with  negative value for skewness, and right skewed the opposite. </p>", 
                "question": "Why does skewness refer to the tail?"
            }, 
            "id": "d6qjll6"
        }, 
        {
            "body": {
                "answer": "<p>Not a silly question. I think we say <dq>right skewed<dq> because we mean that there are outliers on the *right* pulling the mean to the *right* - but it<sq>s probably just convention.... </p>", 
                "question": "Why does skewness refer to the tail?"
            }, 
            "id": "d6qj9q3"
        }, 
        {
            "body": {
                "answer": "<p>It<sq>s a convention that dates back at least to Karl Pearson (writing about 1895, if I recall correctly). He died some time ago so you can<sq>t ask him, but you can read some of what he wrote about it<br><br> Pearson, K. (1895),<br><dq>Contributions to the Mathematical Theory of Evolution, II<colon> Skew Variation in Homogeneous Material,<dq><br>Philosophical Transactions of the Royal Society, Series A, 186, 343-414<br><br>The term <dq>skewness<dq> as applied to a probability distribution seems to originate there (but the notion of using the third moment to measure something like this predates this; it seems to be due to Thiele).<br><br>(To my recollection he just seems to jump in and start using the word in a particular way. The use of <dq>postive skewness<dq> and <dq>negative skewness<dq> seems to be his, but I don<sq>t recall him using <sq>right skew<sq> and <sq>left skew<sq> in that work</p>", 
                "question": "Why does skewness refer to the tail?"
            }, 
            "id": "d6qtisv"
        }, 
        {
            "body": {
                "answer": "<p>>So for instance if I get p = 0.04 can I say there<sq>s a 96<percent> chance that the 2 questions correlate? But then if I get p = 0.3 which indicates that the results are not statistically significant saying that there<sq>s a 70<percent> chance seems dishonest. <br><br>That is not the correct interpretation of the complement of your p value. Say I set up a silly experiment and my null hypothesis is that my dog cannot differentiate between a monkey and a gorilla and my alternate is that he can. I put 2 pictures in front of him(of a monkey and of a gorilla) and I tell him to bring me the gorilla.  I repeat this experiment 1000 times and he only brings me the gorilla 510 times which gives me a p value of .5271. Well there isn<sq>t enough evidence to reject my null so I conclude that he indeed cannot differentiate between a monkey and a gorilla. Would you say there is a almost a 48<percent> chance that my dog knows the difference between a monkey and a gorilla? This is a stupid example but it<sq>s all I could think of right now. Let me refer you to some reading on interpreting and reporting results of your tests<colon><br><br>http<colon>//onlinelibrary.wiley.com/journal/10.1111/(ISSN)1476-5381/homepage/statistical_reporting.htm<br></p>", 
                "question": "How to present p-values to lay people"
            }, 
            "id": "d6nfljx"
        }, 
        {
            "body": {
                "answer": "<p>How about saying <dq>A correlation (or whatever) this large or larger would only occur 3<percent> of the time if the true correlation were 0.<dq></p>", 
                "question": "How to present p-values to lay people"
            }, 
            "id": "d6nfj6l"
        }, 
        {
            "body": {
                "answer": "<p>> if I get p = 0.04 can I say there<sq>s a 96<percent> chance that the 2 questions correlate? <br><br>No, that<sq>s not what it means at all. Personally I<sq>d pick a good measure of association and report that (and maybe give a CI for it) ... leaving out the p-value altogether.<br><br>> Right now I<sq>m using <dq>highly likely<dq> for p <= 0.05, <dq>possible<dq> for p <= 0.1 and <dq>no correlation<dq> for p > 0.1 but I don<sq>t feel like this gives a nice gradual gradient indicating how close things are. <br><br>It<sq>s also an incorrect interpretation of p-values.<br><br>Your categories are ordered; the chi-square ignores that. There are several measures of association suitable for ordered categories.<br><br>You might consider reporting Kendall<sq>s tau-b for example, which is interpretable as a measure of correlation on square ordered tables.<br><br>e.g. see<br><br>https<colon>//en.wikipedia.org/wiki/Kendall_rank_correlation_coefficient#Tau-b</p>", 
                "question": "How to present p-values to lay people"
            }, 
            "id": "d6nlyd2"
        }, 
        {
            "body": {
                "answer": "<p><dq>Here are some semi-arbitrary numbers which I<sq>ve hacked into supporting my original assertion, give me grant money<dq> </p>", 
                "question": "How to present p-values to lay people"
            }, 
            "id": "d6nm5y4"
        }, 
        {
            "body": {
                "answer": "<p>If you know the expected finish times of all swimmers in the heat, then you would simply need to rank them and you would be correct, on average. Without a measure of dispersion for finish times, inferential statistics do not enter into this question. <br><br>I think what you might want to ask instead is along the lines of<colon> given some distribution of finish times for each swimmer, what is the probability that swimmer X wins?<br></p>", 
                "question": "Not sure if I<sq>m asking the right place but how do I model the probability of winning a race?"
            }, 
            "id": "d6445yk"
        }, 
        {
            "body": {
                "answer": "<p>This sounds like a version of the [Coupon Collector](https<colon>//en.wikipedia.org/wiki/Coupon_collector<percent>27s_problem) problem.  I<sq>d start there and try to tailor the solution to your scenario.</p>", 
                "question": "Forming a probability distribution of independent trials with a pool of unique results"
            }, 
            "id": "d62kaea"
        }, 
        {
            "body": {
                "answer": "<p>No, absolutely not. <br><br>It might be possible theoretically, but not in any practical sense. The number of permutations of a 40-card deck is 40! (40 factorial). We would reach the heat death of the universe long before you could produce that many permutations in a Magic<colon> The Gathering game.  </p>", 
                "question": "Given infinite shuffles could I guarantee a certain deck order?"
            }, 
            "id": "d5z9qfc"
        }, 
        {
            "body": {
                "answer": "<p>With infinite time, there will be an infinite number of specific deck arrangements. As a similar analog<colon> this is why the theory that if the universe is infinite (not just the observable portion) there is an exact replica of earth within 10^10^115 meters away. The number of permutations is less than that size.</p>", 
                "question": "Given infinite shuffles could I guarantee a certain deck order?"
            }, 
            "id": "d5znbqd"
        }, 
        {
            "body": {
                "answer": "<p>Just out of interest, why are you asking? Are you making a deck which uses a specific order of cards? I play magic and am intrigued.</p>", 
                "question": "Given infinite shuffles could I guarantee a certain deck order?"
            }, 
            "id": "d606e1u"
        }, 
        {
            "body": {
                "answer": "<p>You would be able to guarantee your deck order *almost surely*. Which is a term that in statistics/probability means that while the probability of it happening is 1, the event that it doesn<sq>t happen (which has probability 0) is still a valid possible outcome. A simpler case is flipping a coin - the probability of a never-ending run of heads becomes less and less likely the more flips you make, but it<sq>s still a possible outcome, as are all other possible outcomes.<br><br>That said, while I haven<sq>t actively played Magic in ages, I<sq>m surprised they<sq>d let such an ability be triggered endlessly at no cost, but if you have managed to set it up so that you it<sq>s possible then you might be able to use the rule about infinite loops.</p>", 
                "question": "Given infinite shuffles could I guarantee a certain deck order?"
            }, 
            "id": "d60ax42"
        }, 
        {
            "body": {
                "answer": "<p>I<sq>m a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit<colon><br><br>- [/r/mlquestions] [Unbalanced dataset <colon> does switching from accuracy to AUC impact training ?](https<colon>//np.reddit.com/r/MLQuestions/comments/4uo0om/unbalanced_dataset_does_switching_from_accuracy/)<br><br>[](#footer)*^(If you follow any of the above links, please respect the rules of reddit and don<sq>t vote in the other threads.) ^\\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*<br><br>[](#bot)</p>", 
                "question": "Unbalanced dataset <colon> does switching from accuracy to AUC impact training ?"
            }, 
            "id": "d5rayhq"
        }, 
        {
            "body": {
                "answer": "<p>Correct.</p>", 
                "question": "Khan Video<colon> Q about Expected value property Sal mentiones."
            }, 
            "id": "d5lr0hy"
        }, 
        {
            "body": {
                "answer": "<p>The most likely outcome is the mode. Not even in all symmetric distributions does the mean = median = mode (but that is true of the Normal). As you point out, a distribution that peaks at 1 and 10 and is low but equal everywhere in between is symmetric but the mean isn<sq>t the most likely.<br><br>Imagine a rod of varying density. The median splits the rod into two sections of equal weight. The mean is the center of mass. The mode is the heaviest point. </p>", 
                "question": "Khan Video<colon> Q about Expected value property Sal mentiones."
            }, 
            "id": "d5luq46"
        }, 
        {
            "body": {
                "answer": "<p>>  in passing that the expected value is also the most likely outcome<br><br>It<sq>s not true in general, as you suppose.<br><br>Consider rolling a fair die with faces numbered 1,2,3,6,6,6<br><br>The expected outcome is 4, but a roll of 4 has probability 0. The most likely outcome is 6.<br></p>", 
                "question": "Khan Video<colon> Q about Expected value property Sal mentiones."
            }, 
            "id": "d5lvk0a"
        }, 
        {
            "body": {
                "answer": "<p>Depending on the situation at hand, you can decide between two methods. I<sq>m assuming you are applying your scenario to something other than fishing boats. Look into Repeated Measure(RM) ANOVA, and Dependent t-test. <br><br>If you have any other questions after looking into which of those would work for you, feel free to post and I<sq>ll see if I can answer.</p>", 
                "question": "Comparing two groups over time"
            }, 
            "id": "d5l6j7m"
        }, 
        {
            "body": {
                "answer": "<p>Paired t-test is the most powerful choice in this case. Compares daily differences and ignores trends over time.</p>", 
                "question": "Comparing two groups over time"
            }, 
            "id": "d5n4vxf"
        }, 
        {
            "body": {
                "answer": "<p>What exactly do you mean by <sq>add *s<sq>. Do you mean add a standard error? Are you trying to get a confidence interval?<br><br>Getting a confidence this way doesn<sq>t work for odds ratios. Among other reasons, if your standard error is too large, your confidence interval will include an odds ratio of less than zero, which is nonsense.<br><br>Are you using PROC FREQ with the <sq>chisq<sq> tables option? If so, try also including <sq>cl<sq> for the confidence limits.<br><br>If you have small samples, use the <sq>fisher<sq> or the <sq>exact<sq> option instead.<br><br>Modified from<colon> <br>http<colon>//support.sas.com/documentation/cdl/en/statug/63347/HTML/default/viewer.htm#statug_freq_sect010.htm#statug.freq.freqchisqopt<br><br>    proc freq;<br>        tables treatment*response / chisq cl fisher;</p>", 
                "question": "Is there a preferred significance statistic/confidence interval to be used with odds rations?"
            }, 
            "id": "d5j0wyd"
        }, 
        {
            "body": {
                "answer": "<p>A good first step would be a quick Chi squared to see if either group is favouring *any* method. If the Chi2 tells you preference is not spread equally across the 5 options then you have a reason to go after a more descriptive test which will confirm *which* methods are favoured. It will also be a good learning experience for you!</p>", 
                "question": "I<sq>m a PhD student and am building an experiment."
            }, 
            "id": "d5hq0wz"
        }, 
        {
            "body": {
                "answer": "<p>One of your independent variables is a between-subjects variable (group) and the other is a within subjects variable (audience) your outcome isn<sq>t truly interval ratio, so that may create some problems with the choice of ANOVA.  I suppose you could give each choice a value with a 5 going to the first choice and 4 to the next highest, and that could probably be jammed into an ANOVA design.<br><br>Twenty participants would only be sufficient if you have really large, and probably obvious effects.  50 may work.  There is something called a power analysis (sample size calculation) that can help you get an idea of how many participants you may need to detect the size that you expect. <br><br>I strongly suggest that you read a good chapter or two on the sort of design you end up using.  </p>", 
                "question": "I<sq>m a PhD student and am building an experiment."
            }, 
            "id": "d5hpn0e"
        }, 
        {
            "body": {
                "answer": "<p>This isn<sq>t a repeated measure.  What are you repeating?</p>", 
                "question": "I<sq>m a PhD student and am building an experiment."
            }, 
            "id": "d5hpy6i"
        }, 
        {
            "body": {
                "answer": "<p>https<colon>//www.mathsisfun.com/combinatorics/combinations-permutations-calculator.html</p>", 
                "question": "Number of Potential Item combinations"
            }, 
            "id": "d5h0sjp"
        }, 
        {
            "body": {
                "answer": "<p>You<sq>ll want to use combination for this since order doesn<sq>t matter. The easiest way to calculate this might be just to use Google. Type in 100 choose #. Then, that will give you the number of combinations for that number out of the 100 products. e.g. If you want to know how many combination of 2 items there are. 100 choose 2 = 4950<br><br>Edit<colon> /u/apc0243 wrote python code to get the exact number you<sq>re looking for. The number gets really big really fast. It might help to know what you want to use this info for and maybe we can come up with something else. </p>", 
                "question": "Number of Potential Item combinations"
            }, 
            "id": "d5h3bt0"
        }, 
        {
            "body": {
                "answer": "<p>What are your explanatory and outcomes variables and how are they measured? What do you mean by <dq>there is no significance but need to test it further<dq>? </p>", 
                "question": "Unsure on which statistical test to use for my data"
            }, 
            "id": "d5ep2jy"
        }, 
        {
            "body": {
                "answer": "<p>How much the name matters depends on where you want to work. <br><br>Top tier places want top tier people. In one position, I was told that you<sq>ll be treated as a second class citizen if you didn<sq>t go to Harvard, Stanford, or MIT for your Ph.D.<br><br>Industry seems to care quite a bit less than academia and in the public sector. <br><br>In general, look at the people that are at the employer already. Those with big name schools are going to want big name school people. </p>", 
                "question": "Graduate statistics programs - How much do rankings matter?"
            }, 
            "id": "d5a7wct"
        }, 
        {
            "body": {
                "answer": "<p><dq>Name recognition<dq> helps considerably, but it<sq>s only one factor made in a hiring process for graduates. I<sq>ve seen search committees throw out resumes from people graduating from top schools if they didn<sq>t have publications, if they didn<sq>t think the person would be a good colleague, if they didn<sq>t like or know the person<sq>s advisor, or if the person wouldn<sq>t be a good <dq>fit<dq> for the institution<sq>s research. The biggest turnoff, however, is ego/rudeness. It<sq>s fairly easy to detect and if the committee senses it, you<sq>re done. <br><br>Not that it<sq>s likely to happen, but if you<sq>re coughing out Nature, Science, and The Lancet publications coming out of grad school, most research oriented schools will overlook where you went to school.  <br><br>As fortyninerbruin mentions, industry doesn<sq>t care so much. Many companies are hiring <dq>blind<dq> now. HR personnel can<sq>t see your name or even your institution when they review resumes. A handful of major companies (not many, so don<sq>t get your hopes up) have stopped recruiting ivy league graduates. </p>", 
                "question": "Graduate statistics programs - How much do rankings matter?"
            }, 
            "id": "d5affbx"
        }, 
        {
            "body": {
                "answer": "<p>After the first year employers didn<sq>t seem to care. I got jobs based on what I had done. Note this is in industry with MS.</p>", 
                "question": "Graduate statistics programs - How much do rankings matter?"
            }, 
            "id": "d5am662"
        }, 
        {
            "body": {
                "answer": "<p>Tufte<sq>s books and Wainer<sq>s would be a great start. You might also consider M. McCrudden, G. Schraw, and C Buckendahl (Eds.) Use of Visual Displays in Research and Testing<colon> Coding, Interpreting, and Reporting Data., Information Age Publishing, Charlotte, NC. .<br></p>", 
                "question": "Seminal works on the theory/design side of data visualization?"
            }, 
            "id": "d4ut3sm"
        }, 
        {
            "body": {
                "answer": "<p>[William Cleveland](http<colon>//priceonomics.com/how-william-cleveland-turned-data-visualization/) is a prominent researcher in this area.</p>", 
                "question": "Seminal works on the theory/design side of data visualization?"
            }, 
            "id": "d4uvne5"
        }, 
        {
            "body": {
                "answer": "<p>I<sq>ve heard this one<sq>s a classic, but I haven<sq>t read it, so I can<sq>t comment personally<colon><br><br>https<colon>//smile.amazon.com/Grammar-Graphics-Statistics-Computing/dp/0387245448?sa-no-redirect=1</p>", 
                "question": "Seminal works on the theory/design side of data visualization?"
            }, 
            "id": "d4uvyat"
        }, 
        {
            "body": {
                "answer": "<p>I<sq>m not a dataviz expert, but check out #dataviz on twitter, I often see discussion about influential authors in that hashtag. </p>", 
                "question": "Seminal works on the theory/design side of data visualization?"
            }, 
            "id": "d4ukwfq"
        }, 
        {
            "body": {
                "answer": "<p>I<sq>m not sure what the latter part of your question is.<br><br>For the first part - Statistics deals with how data is **distributed**. If you give me one data point with no context, as a Statistician I have nothing interesting to say about it. <br><br>But if you give me a set of data points (a.k.a. a dataset) and then show me a new data point that is supposed to be from the same distribution, I can tell you how this data point compares to the distribution of data points. Is it typical? Rare? Weird in any way?<br><br>I hope that helps.</p>", 
                "question": "How do statistics and statistical models that are used to understand datasets apply to individual data points?"
            }, 
            "id": "d4srogn"
        }, 
        {
            "body": {
                "answer": "<p>My stats is limited to that one lower division class I took freshman year, so I am by no means an authority. I<sq>m on this sub to get help too. That disclaimer aside-..<br><br>My understanding is that, if you have a large data set that has a neat linear regression and a high r^2 then that means if you know one aspect of an individual (the x variable) you can likely predict another aspect of that person (the y variable). </p>", 
                "question": "How do statistics and statistical models that are used to understand datasets apply to individual data points?"
            }, 
            "id": "d4ssbdz"
        }, 
        {
            "body": {
                "answer": "<p>Probably the closest you could get to what you seem to want to do is to conceptualize those who voted as a random sample of the entire population of eligible voters and use a z-test to estimate whether or not the proportion of those supporting <dq>Leave<dq> in the entire population was greater than half. Then you<sq>d basically be treating the official vote as a very, very large poll for the entire population<sq>s opinion. However 1) this feels a little dicey because the sample is already 72<percent> of the population, 2) the sample size is so large that the outcome of all but the closest races could be declared as indicating that whatever received the majority vote was also preferred in the population, and 3) those who voted were almost certainly not a random sample from the population.  All that said, the calculation would be as follows<colon><br><br>z = (.519 - .5)/sqrt(.5(1-.5)/(17,410,742 + 16,141,241)) = 220.1<br>P(z > 220.1) = approx. 0<br><br>Therefore the probability of Brexit winning if the population didn<sq>t truly support Brexit is essentially zero.<br><br>A binomial test would be overkill here because of the extremely large sample.</p>", 
                "question": "Whats the probability that the Brexit result was due to chance?"
            }, 
            "id": "d4mrlqn"
        }, 
        {
            "body": {
                "answer": "<p>[deleted]</p>", 
                "question": "Whats the probability that the Brexit result was due to chance?"
            }, 
            "id": "d4mluft"
        }, 
        {
            "body": {
                "answer": "<p>If you<sq>re testing proportions you could use a chi-squared test!<br><br>That<sq>s probably the best way. Which test did you use?</p>", 
                "question": "Help with hypothesis testing"
            }, 
            "id": "d4mfhpq"
        }, 
        {
            "body": {
                "answer": "<p>I just found the critical values from a normal table. Figured out the 99<percent> confidence interval around a probability of 50<percent> and saw that my 57<percent> was way out side those bounds. I was sorta bummed I am extremely skeptical about this report I<sq>m reading and wanted to find something wrong with it. </p>", 
                "question": "Help with hypothesis testing"
            }, 
            "id": "d4mibh6"
        }, 
        {
            "body": {
                "answer": "<p>Well I got frustrated and drew a picture so I saw what was really happening and had to admit I was wrong. I just get confused with one tail tests so set up the hypotheses wrong. </p>", 
                "question": "Help with hypothesis testing"
            }, 
            "id": "d4n19zr"
        }, 
        {
            "body": {
                "answer": "<p>My hypothesis is that when a random person hears the word average, they don<sq>t think of a distribution at all. I think most people have no concept of a distribution, and only think in terms of point values.</p>", 
                "question": "When people unfamiliar with statistical distributions are given an <dq>average<dq> as part of marketing material or somewhere else in life do they subconsciously picture a normal distribution in their mind without even knowing what a normal distribution is?"
            }, 
            "id": "d4k9ls1"
        }, 
        {
            "body": {
                "answer": "<p>> do they subconsciously picture a normal distribution in their mind without even knowing what a normal distribution is? <br><br>a few might, but I doubt it<sq>s the case in general.<br><br></p>", 
                "question": "When people unfamiliar with statistical distributions are given an <dq>average<dq> as part of marketing material or somewhere else in life do they subconsciously picture a normal distribution in their mind without even knowing what a normal distribution is?"
            }, 
            "id": "d4k6j4s"
        }, 
        {
            "body": {
                "answer": "<p>My experience from reading online comments is that they think an average means everyone. <br>The number of times i have read <dq>well i<sq>m a this and i am not that so this study is bunkum<dq>. <br><br>It drives me mad. </p>", 
                "question": "When people unfamiliar with statistical distributions are given an <dq>average<dq> as part of marketing material or somewhere else in life do they subconsciously picture a normal distribution in their mind without even knowing what a normal distribution is?"
            }, 
            "id": "d4kok4y"
        }, 
        {
            "body": {
                "answer": "<p>A lot of things (including wage distribution) don<sq>t follow a normal distribution. Is your hypothesis that people should or should not picture a normal distribution? Or that people are biased because they visualize a normal distribution event if it is inappropriate?</p>", 
                "question": "When people unfamiliar with statistical distributions are given an <dq>average<dq> as part of marketing material or somewhere else in life do they subconsciously picture a normal distribution in their mind without even knowing what a normal distribution is?"
            }, 
            "id": "d4k91cn"
        }, 
        {
            "body": {
                "answer": "<p>You have a log-normal distribution on your hands.  Your data is probably generated by components that have a multiplicative effect rather than an additive one (adding logs is the same as multiplying).</p>", 
                "question": "Why does my histogram becomes normally distributed when I apply log natural to it?"
            }, 
            "id": "d4j2jo6"
        }, 
        {
            "body": {
                "answer": "<p>No, it almost certainly doesn<sq>t have anything to do with *e*. Take a sample from a normal distribution (X(i), i=1,2,...,n), raise *any* number to the power of those numbers (i.e. 2^X(i) , 10^X(i) , ... pi^X(i) - take your pick). Now take natural logs ... and you have something that looks normal again. So nothing to do with $e$ -- indeed any other base of logs would give the same appearance.<br><br>Also, having a histogram that looks normal doesn<sq>t mean it is -- lots of fairly symmetric distributions look sort of normal and a histogram is a very blunt instrument. You can bet it *isn<sq>t* actually normal.<br><br>What having a normalish-looking histogram after taking logs tells you is that the original distribution was somewhat close to [lognormal](https<colon>//en.wikipedia.org/wiki/Lognormal_distribution). But not actually lognormal.<br><br>So that<sq>s not necessarily especially meaningful -- lots of skew distributions are more-or-less lognormal-ish. It<sq>s reasonably common to have something log-normal-like with various kinds of financial and economic data for example.<br><br>Something lognormal-ish can arise if you have lots of small not-too-depedendent multiplicative effects but it can (at least approximately) arise in all manner of other ways.<br></p>", 
                "question": "Why does my histogram becomes normally distributed when I apply log natural to it?"
            }, 
            "id": "d4j74c8"
        }, 
        {
            "body": {
                "answer": "<p>Log normal distributions typically arise from summary statistics which use ratios, eg the odds ratio, relative risk, hazard ratio. By convention you put control on the bottom of the calculation so that values over 1 indicate an increase in risk and values below mean a reduction. But you can calculate them both ways up and that gives you the typical feature of a log normal distribution<colon><br><br><measure> = 0.5 is equivalent in magnitude to<br><measure> = 2<br><br>So rather than reversing the sign of the difference you take the inverse<colon> a multiplicative rather than additive effect.<br><br>That should give you enough to consider your data and the way it is generated and see if you can make the link.</p>", 
                "question": "Why does my histogram becomes normally distributed when I apply log natural to it?"
            }, 
            "id": "d4jatbo"
        }, 
        {
            "body": {
                "answer": "<p>This is doable. It requires assuming a distribution type -- such as Pareto -- and then finding the parameters that best fit the observed data. Then you can do what you like and say, <dq>If we are willing to assume that the underlying data follows a Pareto distribution with parameters *a* and *b*, then the *n*th percentile would be ___.<dq> You can also do tests to compare how well, say, the Pareto model fits compares with something like an exponential distribution, if you are interested in checking your initial assumption. Can you share what the percentiles are?</p>", 
                "question": "Estimating percentage of population with income >x"
            }, 
            "id": "d4ivpgt"
        }, 
        {
            "body": {
                "answer": "<p>Have you looked at the IRS data here? This is a direct count of number of returns per zip code and other data. <br><br>https<colon>//www.irs.gov/uac/soi-tax-stats-individual-income-tax-statistics-2013-zip-code-data-soi<br><br>https<colon>//www.irs.gov/uac/soi-tax-stats-individual-income-tax-statistics-zip-code-data-soi<br><br>You can use the number of returns in each category to infer a distribution, but I suspect you<sq>ll need to adjust for single, and married households returns etc. <br><br><br><br></p>", 
                "question": "Estimating percentage of population with income >x"
            }, 
            "id": "d4ixwh1"
        }, 
        {
            "body": {
                "answer": "<p>Is there a least squares regression for a Pareto curve on excel?</p>", 
                "question": "Estimating percentage of population with income >x"
            }, 
            "id": "d4li865"
        }, 
        {
            "body": {
                "answer": "<p>Don<sq>t take a class on R programming. If you<sq>re already familiar R, you can teach yourself whatever tools you need as you go.</p>", 
                "question": "Which grad class to take?"
            }, 
            "id": "d4h9g75"
        }, 
        {
            "body": {
                "answer": "<p><dq>Analysis of medical data<dq> could be anything. What<sq>s on the syllabus?<br><br>If you<sq>re serious about going for a PhD and you don<sq>t have much programming background, this is the time to take care of that. The better stat/biostat programs all use R. If you end up in a program that prefers SAS or stata, you<sq>ll find that your knowledge of R will remain useful. At the end of the day, a for-loop looks pretty much the same in any language. It doesn<sq>t matter if you learned about them in R, Java, python, or whatever.</p>", 
                "question": "Which grad class to take?"
            }, 
            "id": "d4hcfgm"
        }, 
        {
            "body": {
                "answer": "<p>Also DC.  The feds hire a lot of statisticians.</p>", 
                "question": "What are the best areas in the United States for Statisticians?"
            }, 
            "id": "d4g5soc"
        }, 
        {
            "body": {
                "answer": "<p>NYC, Chicago, Philadelphia, research triangle NC.  Depends on your industry.</p>", 
                "question": "What are the best areas in the United States for Statisticians?"
            }, 
            "id": "d4fnkuc"
        }, 
        {
            "body": {
                "answer": "<p>Normal, Illinois<br></p>", 
                "question": "What are the best areas in the United States for Statisticians?"
            }, 
            "id": "d4fru1f"
        }, 
        {
            "body": {
                "answer": "<p>Get experience. Start applying statistical methods in you area of interest, and add it to your project portfolio. <br><br>Become adept with the preferred software and programming languages of the industry you are interested in.<br><br>Network. Attend workshops, meetups, events. Get to know your community **and collaborate**.<br><br>Anything that starts giving you a lead on understanding more your subject area of interest will keep you in the industry game.</p>", 
                "question": "How does someone make sure that getting a Masters in Statistics doesn<sq>t leave them in the similar position as getting a liberal arts degree (lots of potential but no hard skills?)"
            }, 
            "id": "d492hf0"
        }, 
        {
            "body": {
                "answer": "<p>Internships, TA/RAships. Especially research assistantships where you will really be in charge of a project. Interviewers will want to hear about the experience you have (dataset sizes, data cleaning, statistics methodology applied), and working on a specific task for a research project is a great way to get that exposure and experience. </p>", 
                "question": "How does someone make sure that getting a Masters in Statistics doesn<sq>t leave them in the similar position as getting a liberal arts degree (lots of potential but no hard skills?)"
            }, 
            "id": "d49bg12"
        }, 
        {
            "body": {
                "answer": "<p>All great suggestions so far. Consider volunteering for a charity organisation, many big charities have research/data support roles. If you are an R user there is probably a vibrant R community in your city - attend their meetings, try to understand what is presented and get your face out there. </p>", 
                "question": "How does someone make sure that getting a Masters in Statistics doesn<sq>t leave them in the similar position as getting a liberal arts degree (lots of potential but no hard skills?)"
            }, 
            "id": "d49nui9"
        }, 
        {
            "body": {
                "answer": "<p>Sure sounds like a stinkpile at first glance. What<sq>s the citation?</p>", 
                "question": "appropriate use of MLM for missing data?"
            }, 
            "id": "d43m957"
        }, 
        {
            "body": {
                "answer": "<p>Probably outside your reach right now but I highly recommend you start using R, because Rob Hyndman<sq>s [forecast package](http<colon>//robjhyndman.com/software/forecast/) is going to be your best friend. If it works for google and yahoo it can probably work for you, too! </p>", 
                "question": "Building a rudimentary model to forecast transaction volume? Please help!"
            }, 
            "id": "d3tsizg"
        }, 
        {
            "body": {
                "answer": "<p>This is a regression problem. There are also time series elements to this. Pick up Elements of Statistical Learning (free online textbook) to get started <colon>)</p>", 
                "question": "Building a rudimentary model to forecast transaction volume? Please help!"
            }, 
            "id": "d3t8c4i"
        }, 
        {
            "body": {
                "answer": "<p>Install anaconda,<br>Launch Jupyter notebooks,<br>Read a primer on how to code in Python.<br>Install a database connector,<br>Hook that up to your data store,<br>Get the data into a Pandas data frame for further manipulation<br>Employ one of the many, many utilities available under numpy, SciPy, sci kit-learn, etc that suits your need for this type of problem.<br><br>I<sq>m sure excel has some rudimentary regression analysis tools, but you likely want something like a Bayesian time series regression or a Gaussian process regression.<br><br>Unless you are using something like SPSS etc you will need to learn a little bit of code. That<sq>s just the way it is, there<sq>s no one size fits all situation for statistical modeling, so you will need to learn how to adapt them according to theory and data. There are however many libraries at your disposal.</p>", 
                "question": "Building a rudimentary model to forecast transaction volume? Please help!"
            }, 
            "id": "d3tjobk"
        }, 
        {
            "body": {
                "answer": "<p>In this case, your <dq>group<dq> would be the number of people with whom you enter a relationship.  Let<sq>s say it<sq>s a group of four.  You would also have to determine the number of dates in the span you deem appropriate for you to enter into a relationship.  For example, let<sq>s say that span is Jan 1, 1960 to Jan 1, 1990, in which there are 10,958 dates.  Then the likelihood of sharing the same birthdate is 1- (10957/10958)*(10956/10957)*(10955/10956)*(10954/10955)=2/5479.  This is just 1 minus the probability that you do NOT share the same birthdate with any of the four.  </p>", 
                "question": "Curious about the birthday problem - any help?"
            }, 
            "id": "d3ro2d9"
        }, 
        {
            "body": {
                "answer": "<p>DiffEq isn<sq>t really strictly needed for stats, but it might help a bit more in biostatistics depending on your focus. For example, differential equations can model a lot of things in physiology. For instance, diffusion rates are often a function of concentrations, so you can use differential equations to model things like O2 levels, hormone levels, etc.<br><br>As far as pure stats, I<sq>d say linear algebra and vector calculus are more important. And if you have a chance to take a machine learning course, I<sq>d do that.</p>", 
                "question": "Should I take Differential Equations?"
            }, 
            "id": "d3qusuc"
        }, 
        {
            "body": {
                "answer": "<p>Differential equations are used much more in physics than in stats, but I really enjoyed learning them.  Since it sounds like you<sq>ve got an extra class you can take, try asking some of your stats professors what they recommend for a good, useful, not-required math class.  And remember that it<sq>s good to take some classes just because they<sq>re enjoyable.  You<sq>re in college to learn, so consider taking a completely random class that it unrelated to your major.</p>", 
                "question": "Should I take Differential Equations?"
            }, 
            "id": "d3qijjq"
        }, 
        {
            "body": {
                "answer": "<p>I enjoyed my differential calculus as part of a biostats Masters. Being able to get from a given probability density function to the CDF to the likelihood with a pen really gets you to the heart of the thing... </p>", 
                "question": "Should I take Differential Equations?"
            }, 
            "id": "d3r7f77"
        }, 
        {
            "body": {
                "answer": "<p>If the datasets are independent, you can sum the chi-squareds and the df. Whether that<sq>s really meaningful in your case depends on specifics of your problem<br></p>", 
                "question": "A stupid chi squared question but I<sq>m a noob at this"
            }, 
            "id": "d3jti5d"
        }, 
        {
            "body": {
                "answer": "<p>R and Python are both free and good resources. Specifically, there is a package in R called gmodels with the function [CrossTable](http<colon>//www.inside-r.org/node/89238) that performs cross tables, is easy to use, and provides nicely formatted results, including basic statistics, such as Chi-Square test. </p>", 
                "question": "Open-source or cheap statistics programmes for cross-tabulation."
            }, 
            "id": "d3j5el8"
        }, 
        {
            "body": {
                "answer": "<p>JMP, R are two of my favorites. Clearly R is the cheapest (being free...), JMP may be a little cheaper (it is at the academic level) but not sure about how much it costs in private companies. <br><br>Pythons getting pretty good as well<br><br><br>Good luck!</p>", 
                "question": "Open-source or cheap statistics programmes for cross-tabulation."
            }, 
            "id": "d3j0zqq"
        }, 
        {
            "body": {
                "answer": "<p>KNIME or RapidMiner maybe.</p>", 
                "question": "Open-source or cheap statistics programmes for cross-tabulation."
            }, 
            "id": "d3j0rg8"
        }, 
        {
            "body": {
                "answer": "<p>R is going to be your best option here. The start-up effort is moderate to high (depending on your familiarity with coding), but the flexibility, number of packages, and the huge community of supporters makes it one of the best to use. Python is another good option, however I<sq>m not familiar with the family of statistical packages available.</p>", 
                "question": "Open-source or cheap statistics programmes for cross-tabulation."
            }, 
            "id": "d3j9oxn"
        }, 
        {
            "body": {
                "answer": "<p>They are using a nonsignificant p-value as support of the null hypothesis.  This practice is somewhat frowned upon. There are better methods that have been devised to attempt to support a null hypothesis. Equivalency testing comes to mind if they are comparing means. </p>", 
                "question": "Understanding P-Value for Two-Sided Test."
            }, 
            "id": "d3fyxkw"
        }, 
        {
            "body": {
                "answer": "<p>As a scientist, it<sq>s very important that you understand what statistical significance is. An arbitrary cutoff for the p-value like <dq>.05<dq> is not universally well suited to all situations. You need to calibrate this cutoff based on your acceptable error rate for a given experiment. A significance threshold of .05 means that you are allowing for no more than a 1 in 20 random error rate. Consider for example the [multiple comparisons problem](https<colon>//en.wikipedia.org/wiki/Multiple_comparisons_problem#Example)</p>", 
                "question": "Understanding P-Value for Two-Sided Test."
            }, 
            "id": "d3ga603"
        }, 
        {
            "body": {
                "answer": "<p>> I had always been told that P>0.05 is statistically significant,<br><br>This is backwards. a p-value \u2264 0.05 would imply significance at the 5<percent> level.<br><br><dq>This is leaving me very confused<dq> isn<sq>t really explaining what information you need, though. </p>", 
                "question": "Understanding P-Value for Two-Sided Test."
            }, 
            "id": "d3g349h"
        }, 
        {
            "body": {
                "answer": "<p>You have 1 df for each effect and 40 df for error. Some would drop the interaction from the model but that is a judgment call. If you do, your dfe would increase by 1. <br></p>", 
                "question": "Which df do I report for an ANCOVA? [JMP]"
            }, 
            "id": "d3cg5z3"
        }, 
        {
            "body": {
                "answer": "<p>Usually in papers you would report that as F(3,40) = xx.xx p = 0.xx.<br><br>I believe Your model has degrees k-1 (groups -1) and your error has df n-k (data points - groups). <br><br>Ps love that you<sq>re using JMP! It<sq>s my absolute favorite. Have you tried looking at the JMP community website?</p>", 
                "question": "Which df do I report for an ANCOVA? [JMP]"
            }, 
            "id": "d3cvye1"
        }, 
        {
            "body": {
                "answer": "<p>Interesting problem. I guess the forst question is can you identify what the failure modes are and then assign one stressor per failure mode to test? </p>", 
                "question": "Using failure analysis to produce guidelines"
            }, 
            "id": "d387gh3"
        }, 
        {
            "body": {
                "answer": "<p>Generally people refer to them as the same thing. However some researchers do make a distinction [1](http<colon>//seis.bris.ac.uk/~frwjb/materials/nhmm.pdf).<br><br>I prefer the term multilevel.</p>", 
                "question": "Multilevel vs. Hierarchical"
            }, 
            "id": "d341m06"
        }, 
        {
            "body": {
                "answer": "<p>Hierarchical can refer to Hierarchical Linear Modeling, which can be used interchangeably with Multilevel Modeling; it can also mean hierarchical regression which simply assess the contributions of predictors in a hierarchy of steps or blocks.</p>", 
                "question": "Multilevel vs. Hierarchical"
            }, 
            "id": "d35w4u6"
        }, 
        {
            "body": {
                "answer": "<p>Calculus by James Stewart is the best introductory Calculus book that I used in college - I definitely recommend it. It will get you through both single-variable calculus, as well as most of multi-variable calculus that you will need for for master<sq>s level probability and statistical theory. In particular, if you plan to use the book, you should focus on chapters 1-7 (for single variable calculus), chapter 11 (infinite sequences and series) and chapters 14 and 15 (partial derivatives and multiple integrals). These chapter numbers are based on the 7th edition.<br><br>If you have previously taken calculus, you might consider looking at Khan Academy for an overview instead.<br><br>If you have not previously taken linear algebra, or it has been awhile, you will definitely need to work through a linear algebra textbook (don<sq>t have any particular recommendations here) or visit Khan academy.<br><br>Finally, a book such as [Stephen Abbott<sq>s Understanding Analysis](http<colon>//www.amazon.com/Understanding-Analysis-Undergraduate-Texts-Mathematics/dp/1493927116/ref=sr_1_1?s=books&ie=UTF8&qid=1462846156&sr=1-1&keywords=stephen+abbott+understanding+analysis) is not necessary for master<sq>s level statistics, but could be helpful for getting into the mindset of calculus-based proofs.<br><br>I<sq>m not sure what level of math you have previously completed, and what level of rigor the MS in Statistics program is, but you will likely need be very familiar with single- and multi-variable calculus as well as linear algebra to be successful in probability and statistical theory. It<sq>s certainly possible, just pointing out that there could be a lot of work! If you have any other questions, I<sq>m happy to answer them.</p>", 
                "question": "Calculus for Statistics<colon> where to begin?"
            }, 
            "id": "d2zf50s"
        }, 
        {
            "body": {
                "answer": "<p>It depends a bit on the MSc, some are more mathematical than others, but statistics isn<sq>t really math. Maths is the only language precise enough to express some of its results but that is not the same thing at all.<br><br>You will certainly need some calculus but I<sq>d focus on the calculus you need to derive relevant results and get a deeper understanding of what is going on. I<sq>d find a text on statistical distributions which walks you through the calculus instead of just listing the results. Unless you want to become a theoretical statistician instead of an applied statistician, you don<sq>t need to go any deeper than that.<br><br>An MSc which political science PhDs are being invited to take is unlikely to be very mathematical in content. They want you to be able to understand evidence and assess its reliability and not misuse it in your own research. You don<sq>t need to be able to reproduce the underlying mathematics yourself, you need to know how to apply it without fooling yourself or anyone else.<br><br>If you<sq>re uncertain about the course content, ask around. The maths you need is not that difficult if you<sq>re not scared of maths but it would certainly be useful to make sure that this course is pitched at a suitable level for candidates like you.</p>", 
                "question": "Calculus for Statistics<colon> where to begin?"
            }, 
            "id": "d2zivy2"
        }, 
        {
            "body": {
                "answer": "<p>Lots of intro calc books have sections on application to stats. I would download some textbook PDFs and search for these sections so you have an idea of your end goal. There are lots of good calc intro videos on youtube, I would browse through them and find what teaching style fits you best. </p>", 
                "question": "Calculus for Statistics<colon> where to begin?"
            }, 
            "id": "d2zk2gu"
        }, 
        {
            "body": {
                "answer": "<p>Hi there! Cool homework assignment! So you<sq>d want to compare the chi square value to the P=0.05. You do this because, as long as the chi square value from your test is above (greater than) the value for p=0.05, you know that your results are statistically different with a probability of 95<percent> being truly different (but there<sq>s a 5<percent> chance that they appear to be different just due to error). I<sq>d think in biology, especially medicine, scientists would use lower p-values normally, like p=0.01 or 0.001, but the 0.05 is standard in social sciences. </p>", 
                "question": "Question on Chi-squared test in schoolwork"
            }, 
            "id": "d2z2zop"
        }, 
        {
            "body": {
                "answer": "<p>You cannot prove there is no difference, you can merely fail to detect a difference.<br><br>The idea of checking p=0.95 isn<sq>t saying there is a 95<percent> chance that there is no difference, it is saying that you will flag a difference even if this method would result in false flags 95<percent> of the time. It<sq>s the idea of <dq>false alarms<dq> that we want to keep to 5<percent>, not our ability to detect differences where one exists.</p>", 
                "question": "Question on Chi-squared test in schoolwork"
            }, 
            "id": "d2z5wti"
        }, 
        {
            "body": {
                "answer": "<p>>  I learnt that the null hypothesis of the chi-squared test is that the observed data fit a certain distribution.<br><br>There are many chi-squared tests. This is the chi-squared (multinomial) goodness of fit test.<br><br>[The distribution to which the goodness of fit is being assessed is effectively the multinomial, with a set of specified probabilties of being in various categories; the distribution of the statistic used to test that fit is approximately chi-squared.<br><br>> We were then asked to compare the chi-squared value to P=0.05.<br><br>Actually, as phrased that<sq>s wrong in at least two ways (the intent is probably correct, but the way it<sq>s put is wrong), but the 0.05 part was actually correct. <br><br>You can either compare the test statistic to the chi-square critical value at significance level \u03b1 (alpha; \u03b1 is 0.05 here, and the corresponding critical value is found under it in the little table in the question) or you can compare the p-value (which can be computed from the test statistic) to the significance level (if you have a way of computing p-values). You<sq>d normally do the first thing working by hand and you<sq>d normally do the second thing if working with a stats package.<br><br>If the test statistic is larger than the critical value you<sq>d reject the null hypothesis (while for the comparison of p-value to significance level you<sq>d reject when the p-value is smaller). <br><br>> This way, if the chi-squared is lower than this value, we can say with a high confidence level that the differences are not significant<br><br>No, that<sq>s not how it works. You can<sq>t make such a statement with any hypothesis test (and if you<sq>ve been taught such a wording the interpretation is not actually correct).<br><br>You compare the test statistic with the critical value from the table. If the test statistic is at least as extreme than the critical value (i.e. in the direction consistent with the alternative hypothesis, in this case, larger) then you reject the null hypothesis. Failure to reject doesn<sq>t allow you to say the null hypothesis is true with any confidence whatever (maybe your sample size is simply too small to pick up a small effect); you simply fail to detect any deviation from the null that couldn<sq>t be explained by ordinary random variation. <br><br>[If you want a test that lets you say something like <dq>something pretty close to the null must be going on<dq> you<sq>d be looking at something more like equivalence testing which will likely not be covered in your course. Or you might avoid tests altogether and look at confidence intervals.]<br><br></p>", 
                "question": "Question on Chi-squared test in schoolwork"
            }, 
            "id": "d2z926k"
        }, 
        {
            "body": {
                "answer": "<p>You do know you can interact two continuous variables?<br><br>In Stata it<sq>s just c.x1##c.x2</p>", 
                "question": "multiple regression or something else interaction?"
            }, 
            "id": "d2uz8pl"
        }, 
        {
            "body": {
                "answer": "<p>Are you adjusting for other variables which may be confounders (age, sex, ethnicity, education, saturated fat intake?</p>", 
                "question": "multiple regression or something else interaction?"
            }, 
            "id": "d2vyrub"
        }, 
        {
            "body": {
                "answer": "<p>Careful when interpreting model coefficients with significant interaction terms. A quick google will describe it better than I can at midnight. Predictive  model building is complicated, but if you want to keep things simple you could do the stepwise thing like you mentioned in the comments, but make sure to keep predictors in the model with p<0.15 or so instead of p<0.05, Google this too if you<sq>re interested.</p>", 
                "question": "multiple regression or something else interaction?"
            }, 
            "id": "d2x4g2d"
        }, 
        {
            "body": {
                "answer": "<p>Assuming your binary variable is a predictor (independent) variable, there<sq>s some good information [on Wikipedia](https<colon>//en.m.wikipedia.org/wiki/Categorical_variable). In the absence of other information I<sq>d code as (0, 1) and go from there. <br><br>If the binary variable is an outcome (dependent) variable, i.e., you want to model the probability of saying <dq>yes<dq> dependent on other factors, you should be looking at [logistic regression](https<colon>//en.m.wikipedia.org/wiki/Logistic_regression). </p>", 
                "question": "Running a Regression with a yes/no coded variable."
            }, 
            "id": "d2emghf"
        }, 
        {
            "body": {
                "answer": "<p>It<sq>s fine to code it 1 or 0 but most stat programs allow you to indicate that it is a categorical variable in which case you can code it any way you want such as <dq>y<dq> and <dq>n<dq>. </p>", 
                "question": "Running a Regression with a yes/no coded variable."
            }, 
            "id": "d2eode5"
        }, 
        {
            "body": {
                "answer": "<p>Part of what you need to think about is what it would mean in your regression and what the other independent variables are. In the most basic aspect you can code it as (0,1) to get going. You could also try to use multi-level modeling if you suspect that the intercepts or slopes of the regression may differ significantly on the basis of that variable.</p>", 
                "question": "Running a Regression with a yes/no coded variable."
            }, 
            "id": "d2erd31"
        }, 
        {
            "body": {
                "answer": "<p>As everyone said, this is a binary variable. You should consider whether or not you should fully interact it with the rest of your model as well.</p>", 
                "question": "Running a Regression with a yes/no coded variable."
            }, 
            "id": "d2f8al0"
        }, 
        {
            "body": {
                "answer": "<p>Look at the residual plot to more easily tell how bad it is. It looks you need a squared term or perform some sort of transformation (possibly using ratio of black residents instead of percent). </p>", 
                "question": "Should I test this data for heteroskedasticity?"
            }, 
            "id": "d2art4h"
        }, 
        {
            "body": {
                "answer": "<p>could someone ELI5 to a stats noob like me why this matters?</p>", 
                "question": "Should I test this data for heteroskedasticity?"
            }, 
            "id": "d2awxdg"
        }, 
        {
            "body": {
                "answer": "<p>When in doubt use robust errors, nbd. </p>", 
                "question": "Should I test this data for heteroskedasticity?"
            }, 
            "id": "d2b40f5"
        }, 
        {
            "body": {
                "answer": "<p>It doesn<sq>t like like the variance isn<sq>t constant here. Could be, but I don<sq>t see any evidence from the image. Remember that the variance is straight up and down and isn<sq>t perpendicular to the fit line. From that point of view, I don<sq>t see a funnel and the little peaks and valleys look random to me.</p>", 
                "question": "Should I test this data for heteroskedasticity?"
            }, 
            "id": "d2amfyz"
        }, 
        {
            "body": {
                "answer": "<p>If you<sq>re looking at many different associations or comparisons then you should control the family-wise error rate using some correction such as Bonferroni,  Bonferroni-holm, or others. For example, if you<sq>re investigating a bunch of factors and their association with substance abuse, or looking at a big correlation matrix, you<sq>re probably going to want to control the family-wise error rate. That said, Bonferroni is considered pretty conservative, so you<sq>re more likely to fail to reject the null. I prefer Bonferroni-Holm, although there are other approaches that may be better, but I<sq>m not familiar with them. If you<sq>re only looking at one relationship then you don<sq>t need to do it. You should refer to the recent literature in your field though, look at similar analyses, if they did then you should too, and vice versa.</p>", 
                "question": "bonferroni question"
            }, 
            "id": "d255i94"
        }, 
        {
            "body": {
                "answer": "<p>I think it depends on if you have selected factors of interest a priori. If you suspect through intuition that factor x will be significant and do an ANOVA with factor x, then there is no need. However, if you would like to examine all combinations of factors and their significance, then a bonferroni correction is a conservative approach. Others have mentioned alternative methods as well which can be used depending how you lean on the conservative approach scale to say. </p>", 
                "question": "bonferroni question"
            }, 
            "id": "d25dpoy"
        }, 
        {
            "body": {
                "answer": "<p>I think you can do this by fitting a Poisson log-linear model.  Say Mobile is an indicator variable, 0 if baseline and 1 if mobile.  And Rank is an indicator of whether you<sq>re in rank 2 (0) or rank 3 (1).  Then fit something like this<colon><br><br>log(click-through-rate) = B0 + B1 x Mobile + B2 x Rank + B3 x Rank x Mobile<br><br>So the CTR drop in mobile devices will be<colon><br>exp(B0 + B1 x 1 + B2 x 1 + B3 x 1 x 1) / exp(B0 + B1 x 1 + B2 x 0 + B3 x 0 x 1) = exp(B2 + B3)<br><br>And the CTR drop in baseline devices will be<colon><br>exp(B0 + B1 x 0 + B2 x 1 + B3 x 1 x 0) / exp(B0 + B1 x 0 + B2 x 0 + B3 x 0 x 0) = exp(B2)<br><br>So to test equality of the rate drops, you just test whether B3 = 0.  Which you can do with a likelihood ratio test or whatever.<br><br>Usually it<sq>s preferable to fit this model instead<colon><br><br>log(click-through-count) = B0 + B1 x Mobile + B2 x Rank + B3 x Rank x Mobile + log(offset)<br><br>Where offset is something like number of views, but in the end it won<sq>t make much difference.</p>", 
                "question": "Comparing two click-through rates for significance"
            }, 
            "id": "d1xrwvm"
        }, 
        {
            "body": {
                "answer": "<p>You could try a [zero-inflated model](https<colon>//en.wikipedia.org/wiki/Zero-inflated_model), and have you taken a look at your [receiver-operating characteristic curve](https<colon>//en.wikipedia.org/wiki/Receiver_operating_characteristic)? That could help you decide on the threshold value.</p>", 
                "question": "Predicting probability of an event when the sample taken has rare occurrence of the event"
            }, 
            "id": "d1vzxkz"
        }, 
        {
            "body": {
                "answer": "<p>To be more precise with what I *think* you are asking<colon> You think it looks wrong because the upper limit of the error bar goes beyond 5, and numbers above 5 are simply not possible.  So yes, in this way, it is not correct... (Saying something about a confidence interval for the mean response that includes numbers above 5 is nonsensical) but this does happen from time to time since the formula you are using does not take into account that you are working with discrete response data bounded between 1 and 5.  Similarly, you have **many** error bars that dip below 1, which is equally impossible- the true value cannot be below 1. <br><br>I am sure there is a decent way to correct for this- but I don<sq>t commonly work with this kind of data.  I Googled a bit, and didn<sq>t see anything very useful aside from the suggestion of bootstrapping. </p>", 
                "question": "I made a simple graph with standard deviation error bars. One bar goes outside the scale (pic inside). Does this mean it<sq>s incorrect? It doesn<sq>t <sq>look right<sq>."
            }, 
            "id": "d1tid2o"
        }, 
        {
            "body": {
                "answer": "<p>there<sq>s nothing wrong with the SD bars (would it help if there was a <sq>6<sq> under the rightmost line?)</p>", 
                "question": "I made a simple graph with standard deviation error bars. One bar goes outside the scale (pic inside). Does this mean it<sq>s incorrect? It doesn<sq>t <sq>look right<sq>."
            }, 
            "id": "d1tg8v0"
        }, 
        {
            "body": {
                "answer": "<p>I agree that what you have is probably not a big deal for the kind of information you are presenting. <br><br>However, I always think it<sq>s worth thinking about what kind of data you have (which is bounded and takes only discrete values, not normally distributed), what you are trying to convey, and the best way to convey it, rather than just producing bar graphs with SE bars b/c that<sq>s what everyone else does. <br><br>For example, you might consider other measures, such as the mode response rather than mean, and you could produce histograms, box plots, tables with counts and percentages, etc. </p>", 
                "question": "I made a simple graph with standard deviation error bars. One bar goes outside the scale (pic inside). Does this mean it<sq>s incorrect? It doesn<sq>t <sq>look right<sq>."
            }, 
            "id": "d1ubvau"
        }, 
        {
            "body": {
                "answer": "<p>Have you looked into Amazon Web Services?  It lets you run R on their supercomputers.<br><br>Here<sq>s a tutorial for how to do this.  It<sq>s less complicated than it looks<colon><br>https<colon>//www.youtube.com/watch?v=NQu3ugUkYTk<br><br>Pricing is pretty reasonable<colon><br>https<colon>//aws.amazon.com/ec2/pricing/</p>", 
                "question": "Unusual Question<colon> Does anyone have a (very) powerful computer to share? (X-Post from /r/rstats)"
            }, 
            "id": "d1sm9b4"
        }, 
        {
            "body": {
                "answer": "<p>+1 for AWS, but two follow-up questions<colon><br><br>1. How much does your data scale?<br>2. How much does your code scale?<br><br>If you<sq>re dealing with something that<sq>s double-digit GB of data, maybe you should consider using other technology than R. The in-memory model is just intractable without oodles of RAM (which is the expensive factor for AWS instances).<br><br>As for the second question, if it<sq>s a very short snippet of code, maybe we could help you clean it up and attack the problem in a more efficient way. Even more ideally, if we could devise a solution which shards up the dataset into smaller parts *a la* MapReduce, that would also significantly cut down on your time/costs.</p>", 
                "question": "Unusual Question<colon> Does anyone have a (very) powerful computer to share? (X-Post from /r/rstats)"
            }, 
            "id": "d1swtan"
        }, 
        {
            "body": {
                "answer": "<p>After going over this a few more times I think I understand. Could anyone confirm that this is true<colon><br><br>Let P(|Z|<w) = k<br><br>P(Z<w) = (k/2) + 0.5</p>", 
                "question": "Central Limit Theorem question"
            }, 
            "id": "d1ptdvs"
        }, 
        {
            "body": {
                "answer": "<p>My working this problem quickly gave me a number somewhat smaller than I initially expected, but I think it<sq>s reasonable. Simulating it on a computer would help convince me, but I<sq>m just on my phone now.<br><br>Consider the problem this way<colon> On your first roll, you<sq>re guaranteed to get a unique value. On your second, it<sq>s practically guaranteed, with probability 103/104, and so on for each new roll, with decreasing probability of success (104-*k*)/104 after *k* unique numbers are obtained. Each pursuit of a unique number can be considered as its own trial, with an expected number of rolls before success given by the reciprocal of the probability, 104/(104-*k*). To get 89 successes, you would have to roll, in expectation, Sum[104/(104-*k*),{*k*,0,88}] = 198.45 times, so **between 198 and 199, on average**.<br><br>(Edited to make one statement more precise.)</p>", 
                "question": "Probability question tricky for me but simple for you all"
            }, 
            "id": "d1p39mi"
        }, 
        {
            "body": {
                "answer": "<p>This sounds like [The Coupon Collector<sq>s Problem](https<colon>//en.wikipedia.org/wiki/Coupon_collector<percent>27s_problem), but with a rephrase.<br><br>You can use that formulation to build a table of expected number of rolls to get 1 to 104 out of 104 and then just use the table in reverse to get the best estimate of how many times you rolled.<br><br></p>", 
                "question": "Probability question tricky for me but simple for you all"
            }, 
            "id": "d1p8q2n"
        }, 
        {
            "body": {
                "answer": "<p>> simple for you all <br><br>On what basis do you so confidently assert that this is simple? <br><br>One thing I find very frustrating about such assertions is it dramatically belittles the effort of people that try to help, making it sound like what is often a fairly difficult problem required no effort; frequently that<sq>s just not the case.<br><br>Please don<sq>t do that to people. Ask for help, sure, but don<sq>t tell people how little you value their effort. </p>", 
                "question": "Probability question tricky for me but simple for you all"
            }, 
            "id": "d1p5sfw"
        }, 
        {
            "body": {
                "answer": "<p>On what basis do you so confidently assert that this is simple? <br><br>One thing I find very frustrating about such assertions is it dramatically belittles the effort of people that try to help, making it sound like what is often a fairly difficult problem required no effort.<br><br>Please don<sq>t do that to people. Ask for help, sure, but don<sq>t tell people how little you value their effort. </p>", 
                "question": "Probability question tricky for me but simple for you all"
            }, 
            "id": "d1p5s7f"
        }, 
        {
            "body": {
                "answer": "<p>1) The test statistic will be asymptotically chi-squared, but its true distribution will depend on the specifics of the data distribution.  So there is no general analytic approach to characterizing the discrepancy.  In practice, people use simulations that depend on a proposed data distribution.  That<sq>s the origin of the suggestions (not really requirements) for when to use this statistic.<br><br>2) Edit-- my explanation here was completely wrong.  My apologies.  It is NOT true that a chi-square_1 density will look like OP<sq>s first picture.  /u/efrique<sq>s explanation is correct.  <br><br>Regarding your last question<colon>  With non-degenerate random variables, some deviation is always more probable than no deviation.  Intuitively, the probability of drawing a sample whose average exactly matches the distribution<sq>s mean is vanishingly small.  It<sq>s much more likely that your sample<sq>s average will deviate slightly from the true parameter.<br><br>More theoretically, the expected (or <dq>average<dq>) squared deviation from the mean is known as the variance of the distribution.  Any random variable will have a non-zero variance (otherwise it would be a constant).  In other words, we *expect* to see deviations when we analyze samples. </p>", 
                "question": "Two questions about the Chi-squared distribution"
            }, 
            "id": "d1cuazd"
        }, 
        {
            "body": {
                "answer": "<p>>  Is there a way to quantify the uncertainty/discrepancy between the sampling distribution of the Chi-squared test statistic under the null hypothesis and the theoretical Chi-squared distribution, which is an approximation of this?<br><br>Certainly. You can use simulation for example; specify the exact conditions you<sq>re assuming and you can calculate a distribution for a the chi-squared statistic. For example, recently in a discussion of the chi-squared goodness of fit test I generated a comparison between the statistic and the underlying distribution<br><br>For a chi-squared goodness of fit test for 6 equally likely categories and expected value of 7.5 in each cell, this is the comparison between cdfs<colon><br><br>http<colon>//i.imgur.com/agmxIl1.png<br><br> -- the black is the actual (discrete) distribution, and the red is the chi-squared approximation. Even at that relatively moderate size, the approximation is excellent.<br><br>But reduce that expected just a little, or make the probabilities unequal, and the deviation between the two grows quickly.<br><br>> Or is this difference so small as to be negligible and undeserving of attention and consideration, <br><br>No -- even within the boundaries of the rule you quote (one of many sets of rules that exist), the distribution can be much less good than that example I gave. You should beware such rules of thumb without investigating cases like the one you<sq>re faced with - your tolerance of the deviations may be less than that of the person making the recommendation (or indeed they may not have even looked at the behavior in your specific case).<br><br>>  Why does the Chi-squared distribution start low and spike up?<br><br>That depends on which chi-square you look at. With 1 or 2 df the peak is at 0.<br><br>> Would closer to 0\u2014less deviation\u2014not have the highest probability of occurring?<br><br>No; if you add together enough chi-square(1) random variates, you naturally get some being large (in general they won<sq>t all be small at the same time - it becomes very unlikely to get *only* very small deviations - compared with the median, say). As a result, the peak moves right. Indeed the CLT should at least suggest to you that the shape of the chi-square should become more normal-ish as the df becomes large (more formally, a standardized chi-square should converge to a standard normal in the limit as the df go to infinity, and since location shift and scaling don<sq>t change the shape, you would expect the shape to become more normal with increasing df)<br><br>> How is some deviation more probable than no deviation, given that a sampling distribution represents the values we<sq>d get if we could sample an infinite amount of times?<br><br>I don<sq>t quite get what you<sq>re saying here. For an individual cell your intuition holds up -- but not for lots of cells at once. <br></p>", 
                "question": "Two questions about the Chi-squared distribution"
            }, 
            "id": "d1cuzpa"
        }, 
        {
            "body": {
                "answer": "<p>Essentially what I want to know is if the r^(th) success is always matched with the probability of success p; and the number of x failures is always matched up with q?<br><br>Edit<colon> I might be answer my own question, but this function is difficult to study because there are many variations of it and the symbols used are not standard from text to text. </p>", 
                "question": "I need help understanding Negative Binomial Distribution Function."
            }, 
            "id": "d1cgxo0"
        }, 
        {
            "body": {
                "answer": "<p>The p-value depends on sample size. So, no matter how tiny is your effect size, with  enough sample you will find (almost) always statistical significant results. <br><br>Your intuition about being more confident on larger samples is correct. Maintaining your level of significance (reject H_0 when p<alpha), with larger samples your power to reject false H_0 on the population is higher. Remember<colon> no rejection of H_0 doesn<sq>t imply that the relation/differences between groups doesn<sq>t exists on the population. You just don<sq>t have enough information to differentiate the results from that hypothesis. <br>  <br>Please, read the classic [<dq>The earth is round (p<0.05)<dq>](http<colon>//ist-socrates.berkeley.edu/~maccoun/PP279_Cohen1.pdf). </p>", 
                "question": "Does the P-value take into account the sample size?"
            }, 
            "id": "d18rtfk"
        }, 
        {
            "body": {
                "answer": "<p>> Does the P-value take into account the sample size?<br><br>Certainly, in that the sample size affects the distribution of (notably, the variance of) the test statistic.<br><br>> In study 2, the sample size is 20, and the P-value is found to be 0.03. Aside from the sample size, everything else in the study is exactly the same.<br><br>No, you cannot have everything else the same. To get the same p-value from a smaller sample size, the *effect size* must have been larger (with some edge-case exceptions that are not really relevant here that relate to doing tests nobody would do in practice).<br><br>>  Another closely-related question is<colon> Should how we interpret the P-value vary depending upon the sample size?<br><br>Possibly, it depends on what you mean by <dq>interpret<dq>. In the sense of whether or not to reject the null, the interpretation would only vary as compared with our significance level (and that probably should change with sample size, in that if you<sq>re sensible, you should tend to use smaller significance levels as your samples sizes get large). However, given that most people (including many editors and referees) sphexishly fixate on some arbitrary significance level, in practice it usually makes no difference there.<br><br>As far as <dq>what does it really mean<dq> considerations, a small p-value at a small sample size (when your power is low) is in some sense more likely to have been random variation making a very small/null effect large than it would have been at a large sample size, so some additional caution in making strong conclusions from small samples is called for. (You would avoid this issue by not doing a low-power study in the first place; if for some reason you don<sq>t, you can<sq>t take much satisfaction from a significant result because it<sq>s probably not reproducible.)<br><br>Outside of those kinds of considerations, a p-value is a p-value, and you understand what it means from its definition.</p>", 
                "question": "Does the P-value take into account the sample size?"
            }, 
            "id": "d18vvbd"
        }, 
        {
            "body": {
                "answer": "<p>The p-value is based on the sampling distribution under the null hypothesis, and the variance of that distribution is proportional to 1/n with respect to the sample size n. If the true value of the parameter of interest is not equal to its value under the null hypothesis and you perform two otherwise identical experiments around that parameter with unequal sample sizes, the expected p-value under the experiment with the larger sample size is smaller than the expected p-value under the experiment with the smaller sample size.<br><br>Heuristically, if two experiments with different sample sizes (and different parameters of interest) give the same p-value, the effect size is likely larger for the experiment with the smaller sample. It<sq>s relatively easy to show statistical significance with a large sample even if the effect size is small, while experiments with small sample sizes tend to require larger deviations from the null hypothesis for results to be statistically significant.</p>", 
                "question": "Does the P-value take into account the sample size?"
            }, 
            "id": "d18s34n"
        }, 
        {
            "body": {
                "answer": "<p>It does make a difference for two reasons. One trivial and one important.<br><br>The trivial observation is that the p-value essentially measures the difference between your estimate and no difference in units of standard errors. The standard error is the standard deviation over sqrt sample size, so the larger the sample size the smaller the difference you can detect.<br><br>That effect on the power of the study to detect a difference leads to an important difference in interpretation. The p-value is a conditional probability, so its interpretation depends heavily on how likely it was that a difference might be found in the first place and also on the power to detect a difference. [This is explained in detail in this paper.](http<colon>//rsos.royalsocietypublishing.org/content/1/3/140216)</p>", 
                "question": "Does the P-value take into account the sample size?"
            }, 
            "id": "d18ub1h"
        }, 
        {
            "body": {
                "answer": "<p>Since you want to use Excel, here<sq>s a very simple model. I<sq>m assuming you have a data table with columns being contributors and the final column the view count, and entries in each column are 0/1 (contributor in episode or not) and the number of views. <br><br>Then, using the [regression features](http<colon>//www.wikihow.com/Run-Regression-Analysis-in-Microsoft-Excel) of excel, run a linear regression where the columns of contributors are the X<sq>s, and the view count is your Y vector.<br><br>That will fit a model where the intercept is the average number of video views, and the coefficient of each person is their contribution beyond average (can be plus or minus).<br><br>To get fancier, you can also add columns that are 1 when groups of contributors are together. These are called interaction terms. They will help control for the value of teaming up. In some cases you might have contributors that have a great synergy that promotes video views, but that don<sq>t do all that well on their own.<br><br>Finally, add in more data like <dq>day of the week<dq> or <dq>time of day<dq> to the model to see if there are any effects of publishing time of the video that affect views. <br><br>Watch out for the ratio of contributors to the number of videos, though. If you have a lot of contributors but not much data, then your regression will be poor and the coefficients will have wide uncertainty bounds. Also, it<sq>s possible in these kinds of regressions (discrete yes/no inputs) to have highly correlated inputs (in this case, many contributors work together a lot). Correlated inputs make estimating coefficients difficult because they could take on many values and still achieve a good fit. If that<sq>s the cases, it might be best to treat the two as one contributor. </p>", 
                "question": "How to calculate member contribution to team result (VORP-type stat)"
            }, 
            "id": "d185i8y"
        }, 
        {
            "body": {
                "answer": "<p>Look up logistic regression--that should answer both of your questions <colon>) instead of a general linear regression, you<sq>ll fit a <dq>sigmoid<dq>/logistic function to the data, with an output that can be interpreted as a probability. I<sq>ll leave it up to you to find out the details; it<sq>s a pretty fun tool/concept. <br><br>For #2, the factors which you find to be significant (if you find any) are the factors with regression coefficients that are different from at least one of the others. In order to find specifically (pairwise) which coefficients are different from each other, you could apply Tukey<sq>s HSD Test or the Fisher LSD test. I believe they work for binomial responses, but be sure to check the assumptions.<br><br>Hope this helps!</p>", 
                "question": "Correct method for analyzing a 2x2x2 factorial design with Binary response data and 1 categorical independent variable?"
            }, 
            "id": "d12uve9"
        }, 
        {
            "body": {
                "answer": "<p>I thought I replied to this already, sorry for such a long delay. <br><br>If a x=1 is a working product and x=0 is a defective one, you<sq>re looking for P(1/n * sum(x) >= .99) with 95<percent> confidence. 1/n * sum(x) is the sample mean with distribution of a normal with mean mu (what you<sq>re asking us to solve for) and variance of the population of x over the sample size you drew for the sample mean.  <br><br>This leaves us with three unknowns now. The population average and either the population variation or sample variation, and the sample size. You can calculate the sample variation as 1/(n-1) sum(x-mu) ** 2, leaving us with just mu and sample size as unknown. <br><br>From there, once you have the samples you want and know their sizes, you can use a single tailed student T distribution to calculate what mu needs to be I order to get the output you desire. If that last step is an issue let me know and I<sq>ll make up some numbers and give you an example. </p>", 
                "question": "Can you figure out total error rate from customer error rate?"
            }, 
            "id": "d18nf2n"
        }, 
        {
            "body": {
                "answer": "<p>I<sq>ve used Bayesian estimation to evaluate ordinal data a few times. [Here<sq>s a blog post](http<colon>//eamoncaddigan.net/psych/bayes/2015/09/03/antivax-attitudes/) describing one of my analyses. <br><br>As /u/D-Juice mentioned, this method does involve mapping the ordinal responses to a more intuitive distribution. </p>", 
                "question": "Smallest worthwhile change for Ordinal Data"
            }, 
            "id": "d0z9r0l"
        }, 
        {
            "body": {
                "answer": "<p>It<sq>s not particularly easy regardless of the type of data. You can<sq>t answer this question without understanding the subject matter well enough to define the smallest important difference, it doesn<sq>t just fall out of the numbers. You<sq>d normally consult subject experts (eg medics) to help define this.<br><br>As far as the type of data being more or less easy for human beings to work with, that<sq>s a translation issue. If the answer is most intuitively expressed in a different form, then you need to find out how your scale relates to this more intuitive schema. There isn<sq>t an off the shelf answer to this one.</p>", 
                "question": "Smallest worthwhile change for Ordinal Data"
            }, 
            "id": "d0z8tkl"
        }, 
        {
            "body": {
                "answer": "<p>Likert scale data aren<sq>t numbers in the same sense that, say, heights are numbers, so it<sq>s not clear to me that computing the usual statistics is meaningful. Computing the standard deviation seems completely spurious to me; maybe the mean is too.<br><br>My advice is to construct a histogram.</p>", 
                "question": "I<sq>m a teacher. I did a survey of my student<sq>s motivation to learn at school using Likert scale. I<sq>ve got a mean but I<sq>m not sure how to analyse the St Dev stats I have generated. What does it mean?"
            }, 
            "id": "d0u0nc8"
        }, 
        {
            "body": {
                "answer": "<p>You don<sq>t want to use mean for likert scale questions... It isn<sq>t a continuous measure, so if the mean isn<sq>t a whole number, how do you interpret it, you know? You<sq>re best off using a median. </p>", 
                "question": "I<sq>m a teacher. I did a survey of my student<sq>s motivation to learn at school using Likert scale. I<sq>ve got a mean but I<sq>m not sure how to analyse the St Dev stats I have generated. What does it mean?"
            }, 
            "id": "d0u178s"
        }, 
        {
            "body": {
                "answer": "<p>Need more information. How many students do you have? 5 or 7 pt likert scale? Can you give your means and SDs for a couple variables?</p>", 
                "question": "I<sq>m a teacher. I did a survey of my student<sq>s motivation to learn at school using Likert scale. I<sq>ve got a mean but I<sq>m not sure how to analyse the St Dev stats I have generated. What does it mean?"
            }, 
            "id": "d0ttqyz"
        }, 
        {
            "body": {
                "answer": "<p>I agree with /u/ignorantveil r.e. it being representative of a population. In which case just some descriptive stats are all you need. Either a mean and SD or possibly the median if values are skewed. Histograms would be a good place to start though.</p>", 
                "question": "I<sq>m a teacher. I did a survey of my student<sq>s motivation to learn at school using Likert scale. I<sq>ve got a mean but I<sq>m not sure how to analyse the St Dev stats I have generated. What does it mean?"
            }, 
            "id": "d0tvvsa"
        }, 
        {
            "body": {
                "answer": "<p>Python is great for handling CSVs and other text files. In particular, the pandas package gives you an R-like syntax for handling tabular data, as well as the pandasql package letting you do SQL queries on it. It all works very well (there is an extension or feature of pandas which lets you work with it on disk if it<sq>s too big for your RAM) and python has a lot of other packages which come in handy for data parsing and analysis - scipy, numpy, and scikit-learn (which all play very nicely together and with pandas). All free and open. And of course it<sq>s a fully-featured programming language so you can do lots of other things once you know python, including very easily incorporating your lower-level stuff as libraries. </p>", 
                "question": "Software/Programming advice for data parsing"
            }, 
            "id": "d0tzask"
        }, 
        {
            "body": {
                "answer": "<p>Python is a good choice, but take a look at R (http<colon>//www.r-project.org) as well. R also has lots of tools for parsing data, as well as lots of statistical and graphical functions.</p>", 
                "question": "Software/Programming advice for data parsing"
            }, 
            "id": "d0u0dq0"
        }, 
        {
            "body": {
                "answer": "<p>I would say explore the possibility of migrating away from CSVs and having your logging process log directly to a database. PostgreSQL is a good and free database that doesn<sq>t take much time to setup for simple tasks. Once it<sq>s in a database, you don<sq>t have to worry about parsing. Just write a query to get what you want. Also, if you decide down the road to change the way you are selecting your data, it<sq>s much easier to change a query than a parser.<br><br>Once you have it in a database, you could use R (r-project.org) to analyze your data. You can use the RPostgreSQL package to pull your data directly into your session by running your query in R.</p>", 
                "question": "Software/Programming advice for data parsing"
            }, 
            "id": "d0u97kx"
        }, 
        {
            "body": {
                "answer": "<p>For k >= 1 the bound can be attained. X = -1, 0, or 1. X = -1 or 1 with probability 1/2k^2 each and 0 ow. </p>", 
                "question": "Chebyshev<sq>s Inequality"
            }, 
            "id": "d0q0ipl"
        }, 
        {
            "body": {
                "answer": "<p>Yes, what<sq>s great about Chebyshev<sq>s inequality is that it works for any distribution!</p>", 
                "question": "Chebyshev<sq>s Inequality"
            }, 
            "id": "d0rqeoe"
        }, 
        {
            "body": {
                "answer": "<p>/r/dataisbeautiful<br><br>/r/economics ?<br></p>", 
                "question": "How to get real data for forecasting project?"
            }, 
            "id": "d0nyfjs"
        }, 
        {
            "body": {
                "answer": "<p>I was once able to find the data set for this<colon>  http<colon>//pubsonline.informs.org/doi/abs/10.1287/msom.1070.0176<br><br>*This data set describes 38 multiechelon supply chains that have been implemented in practice. These chains exhibit special structure that can be used to inform and test analytical models. Although the data were not collected with the intention of econometric analysis, they may be useful in an empirical study. The data described in this paper are publicly available at the journal<sq>s website (http<colon>//msom.pubs.informs.org/ecompanion.html).*<br><br>It might be what you<sq>re looking for.<br><br>Update<colon>  and now I<sq>ve found where I managed to download it from (and it still seems to be there)<colon><br>http<colon>//pubsonline.informs.org/doi/suppl/10.1287/msom.1070.0176<br></p>", 
                "question": "How to get real data for forecasting project?"
            }, 
            "id": "d0ofjyj"
        }, 
        {
            "body": {
                "answer": "<p>He<sq>s taking the variance of both sides. Var(y_t-1) and Var(y_t-2) , these are both gamma_0.<br><br>It<sq>s kinda like if I say y = 5x + 3z<br><br>assuming the rhs terms are independent you would have var(y) = 5^2 var(x) + 3^2 var(z)<br><br>If there was some covariance between the two rhs terms here, you would then also add on a 2x(5)(3)x cov(x,z)<br><br>In the AR(2), the cov(y_t-1, y_t-2) is just the lag 1 covariance.<br><br>Are you using the old Cryer book he wrote himself or his newer book with Chan? What software does your class use?</p>", 
                "question": "Time Series AR(2) Variance Derivation"
            }, 
            "id": "d0o932b"
        }, 
        {
            "body": {
                "answer": "<p>One simple method would be to create an <dq>adjustment factor<dq> for each grader by finding the average across all of the applications and dividing that number by the average for each grader. For example, if the average across all of the graders is 80 and a particular grader gives an average score of 60, then the adjustment factor would be 80/60 = 1.33. Then simply multiply all of that grader<sq>s scores by 1.33 to make them equivalent to the sample average. Does that make sense?<br><br>The biggest assumption here is that the total pool of applicants for each grader is roughly similar. If one grader happened to draw a really crummy pool (and he was therefore grading them badly for good reason) then you would be inflating his grades unfairly. </p>", 
                "question": "Making grading more cohesive between graders?"
            }, 
            "id": "d0kn1e6"
        }, 
        {
            "body": {
                "answer": "<p>Are the applications being assigned to the assessors entirely randomly? Have to make some non-trivial assumptions to do this kind of normalization when you have only 1 assessor rating each application.</p>", 
                "question": "Making grading more cohesive between graders?"
            }, 
            "id": "d0khzhe"
        }, 
        {
            "body": {
                "answer": "<p>If you have many assessors (4) swimming ignores the highest and lowest score. </p>", 
                "question": "Making grading more cohesive between graders?"
            }, 
            "id": "d0kkton"
        }, 
        {
            "body": {
                "answer": "<p>Give a part of each application to an assessor and have them evaluate 1/12 of the criteria.  After 1/12 of the papers are done, reassign what criteria each grader uses.</p>", 
                "question": "Making grading more cohesive between graders?"
            }, 
            "id": "d0kn9y1"
        }, 
        {
            "body": {
                "answer": "<p>You working with small-ish data? Maybe burning df with estimating a slope variance has you on the brink of power.<br>You need to figure out with the relationship between A and the outcome varies across level two units.<br>You can do this by running a model with and without it, and seeing which model fits better, using the -2LL (deviance) and you and also do this by looking at the variance estimate and calculating a 95<percent> plausible values range of the slopes....do those seem meaningfully different to you?<br><br>SAS is going to give you a z-test of the variance, which isn<sq>t great because Z tests don<sq>t work well for variances, if you went in the HLM software you could get a chi-square test of the significance of the slope variance...but with model fit and some looking at the data, you don<sq>t really need a p-value.<br><br>If this is all not making sense, let me know and I can give more details.  Seeing this right before I teach stats, about to be late for class, 150 ugrads very excited to learn about t-tests today <colon>)<br></p>", 
                "question": "When should I be letting slopes vary in multilevel regression? Is there a standard procedure?"
            }, 
            "id": "d0md9ru"
        }, 
        {
            "body": {
                "answer": "<p>Random slopes or intercepts (or both)? I<sq>m not familiar with proc mixed SAS code well enough to know if the model is actually configured for random slopes here - it looks like it might be just random intercepts. <br><br>Are you familiar with the basis for allowing random intercepts and slopes in models? Essentially it better accounts for variance at the subject level by considering how differences in intercepts may be creating noise in the baseline of the model - partitioning this variance using a random effect this can help better account for explained variance by fixed effects and error in the model. Random slopes are useful where there are multiple measurements for each subject that might vary according to a fixed independent variable in the model. If there are large differences in intercepts or slopes, this may reduce or increase the error terms, depending on how that variance is allocated by model terms. In your case, it is increasing variance attributed to error, reducing the <dq>significance<dq> of the fixed effects.<br><br>If you include a random effect where none is needed, this may needlessly increase error terms. So firstly, you need to assess if allowing a random effect in the model significantly accounts for variance or not. If not, then it is unlikely to be improving model fit. Start with intercepts only, then consider random slopes if your data is suitable.<br><br>Secondly, you need to be assessing model quality via AIC or BIC scores (lower values are better). When you allow intercepts and slopes to vary by subject (i.e. a random slope model) you increase the complexity of the model in an effort to produce a better fit. But this might not be worth the trade-off if a simpler model term (or covariance structure) can explain this nearly as effectively. This is referred to as parsimony.<br><br>Lastly, I see you<sq>re also using an unstructured covariance. While this produces the most <dq>accurate<dq> fit, it is also the most complex, and might not be the best model to explain the data. If you<sq>re using random slopes and expect a linear change with time or some other variable then an autoregressive structure might be a better and simpler model. If just intercepts, a simple identity variance component structure might be best.<br></p>", 
                "question": "When should I be letting slopes vary in multilevel regression? Is there a standard procedure?"
            }, 
            "id": "d0mu16h"
        }, 
        {
            "body": {
                "answer": "<p>https<colon>//www.ai-therapy.com/psychology-statistics/sample-size-calculator<br><br>I calculate you<sq>d need 620 total (at least 310 in the first email promotion and 310 in the second)<br><br>t-test would be used to compare these independent groups (different people would be emailed)<br><br>I used a single tail test, as your hypothesis was improvement (a single direction of effect) not difference (two-tailed)<br><br>The effect size was .2 (20<percent> as you stated)<br><br>Alpha was set at .05 and Beta was set at 80<percent><br><br></p>", 
                "question": "How to determine sample size"
            }, 
            "id": "d0iwmxt"
        }, 
        {
            "body": {
                "answer": "<p>The first question that comes to mind is what the basis of the ambitious expectation of 20<percent>? There is no legitimate route of conducting a power analysis (for the determination of sample size) here because you<sq>re not answering the question of how two variables differ, or are related; or the like. </p>", 
                "question": "How to determine sample size"
            }, 
            "id": "d0jkc7a"
        }, 
        {
            "body": {
                "answer": "<p>One problem you haven<sq>t thought of is the nesting structure of your data. That is, people nested within groups tend you have more similar values than people in other groups (the classic example is children nested in classrooms). By ignoring this you<sq>re biasing your standard errors down. Multilevel models help account for this, and if you start introducing individual and group level variables you<sq>re definitely going want to know how much of the variance is explained at the group level versus the individual level.<br><br>Sorry if this isn<sq>t the exact answer you wanted! Good luck!</p>", 
                "question": "What do I need to watch out for when adding a country-level variable to a regression with individual-level (independent and dependent) variables?"
            }, 
            "id": "d0h30xx"
        }, 
        {
            "body": {
                "answer": "<p>Multicollinearity is only a concern when one variable follows another near directly. This shouldn<sq>t be a problem - unless it is, which would be an interesting result. It would imply that one of your measured variables perfectly predicts and follows this binary variable you<sq>ve created.<br><br>There shouldn<sq>t be much of an issue here with the inclusion of the variable. You may want to consider what you are trying to measure - are you trying to determine whether or not the two groups have a different marginal effects? <br><br>In this case, you might split the sample into the 2 groups and run the regression on bot - then compare the cross-group marginal effects. It may turn out that having 1 year of additional education from the mean results in a 5<percent> increased trust in the institution when not in a crisis, and a 5<percent> decreased trust when in a crisis. <br><br>By splitting the sample you are effectively interacting your binary variable with all your other explanatory variables. </p>", 
                "question": "What do I need to watch out for when adding a country-level variable to a regression with individual-level (independent and dependent) variables?"
            }, 
            "id": "d0h6y7p"
        }, 
        {
            "body": {
                "answer": "<p>This is the full abstract for the first article<colon> <br><br><dq>We evaluate air Pb emissions and latent aggravated assault behavior at the scale of the city. We accomplish this by regressing annual Federal Bureau of Investigation aggravated assault rate records against the rise and fall of annual vehicle Pb emissions in Chicago (Illinois), Indianapolis (Indiana), Minneapolis (Minnesota), San Diego (California), Atlanta (Georgia), and New Orleans (Louisiana). Other things held equal, a 1<percent> increase in tonnages of air Pb released 22years prior raises the present period aggravated assault rate by 0.46<percent> (95<percent> CI, 0.28 to 0.64). Overall our model explains 90<percent> of the variation in aggravated assault across the cities examined. In the case of New Orleans, 85<percent> of temporal variation in the aggravated assault rate is explained by the annual rise and fall of air Pb (total=10,179metric tons) released on the population of New Orleans 22years earlier. For every metric ton of Pb released 22years prior, a latent increase of 1.59 (95<percent> CI, 1.36 to 1.83, p<0.001) aggravated assaults per 100,000 were reported. Vehicles consuming fuel containing Pb additives contributed much larger quantities of Pb dust than generally recognized. Our findings along with others predict that prevention of children<sq><sq>s lead exposure from lead dust now will realize numerous societal benefits two decades into the future, including lower rates of aggravated assault.<dq><br><br><br>I don<sq>t have access to the articles themselves, but 90<percent> and 85<percent> of variability explained is insane. Period. Is it possible that it<sq>s accurate? Sure. Likely? Not at all. Without seeing their control variables and so forth, what initially popped into mind is that they may have tapped into a spurious correlation via seasonality<colon> crime increases in the summer, and people are generally out and about more when the weather<sq>s nice (via foot and car), which can facilitate assaults. Crime often decreases in winter (though this depends on geography), fewer people are out and about (via foot and car), so opportunities for assaults/etc decrease in kind. Just spit-balling though.</p>", 
                "question": "Appropriate use of statistical tool in 3 studies linking lead to criminality mentioned in r/askscience"
            }, 
            "id": "d0e0g6h"
        }, 
        {
            "body": {
                "answer": "<p>Those effect sizes sure sound crazy<colon> <dq>In the case of New Orleans, 85<percent> of temporal variation in the aggravated assault rate is explained by the annual rise and fall of air Pb (total = 10,179 metric tons) released on the population of New Orleans 22 years earlier.<dq> (Mielke & Zahran, 2012). No time to read through that right now, though.</p>", 
                "question": "Appropriate use of statistical tool in 3 studies linking lead to criminality mentioned in r/askscience"
            }, 
            "id": "d0dx0x2"
        }, 
        {
            "body": {
                "answer": "<p>Consider the opposite interpretation. The likelihood of never getting 100 heads over infinity trials is (1- p(at least one sequence of 100 heads| infinity trials))<br><br>Spoiler alert it<sq>s (1-1)=0</p>", 
                "question": "If you flip a coin repeatedly for all of infinity how likely is it you will never flip heads 100 times in a row?"
            }, 
            "id": "d0anpbv"
        }, 
        {
            "body": {
                "answer": "<p>The probability is 1.<br><br>In fact, you<sq>ll do it an infinite number of times. </p>", 
                "question": "If you flip a coin repeatedly for all of infinity how likely is it you will never flip heads 100 times in a row?"
            }, 
            "id": "d0b1bas"
        }, 
        {
            "body": {
                "answer": "<p>If you flip a coin repeatedly for all of infinity then you have an infinite number of 100 flips in a row, which means that every possibility for those flips will happen. One of those possibilities is 100 heads in a row, another is 100 tails in a row, and others are every other combination. The probability of all possibilities not occurring is 0, but the probabilities of different possibilities are still different. There<sq>s no way that 100 heads won<sq>t be flipped in a row, but taking 100 flip stretches you<sq>re more likely to find a ratio closer to 50 heads-50 tails than 100 heads-0 tails.  </p>", 
                "question": "If you flip a coin repeatedly for all of infinity how likely is it you will never flip heads 100 times in a row?"
            }, 
            "id": "d0avqwd"
        }, 
        {
            "body": {
                "answer": "<p>The expected number of tosses until you get 100 consecutive heads is 2,535,301,200,456,458,802,993,406,410,750. That<sq>s incredibly small compared to infinity.<br><br>[The general formula is<colon>](https<colon>//math.stackexchange.com/questions/364038/expected-number-of-coin-tosses-to-get-five-consecutive-heads) 2*(2^n - 1)<br><br>[The general formula for an arbitrary *p* is<colon>](https<colon>//math.stackexchange.com/questions/95396/expected-number-of-tosses-for-two-coins-to-achieve-the-same-outcome-for-five-con/95404) (p^-n - 1)/(1 - p)</p>", 
                "question": "If you flip a coin repeatedly for all of infinity how likely is it you will never flip heads 100 times in a row?"
            }, 
            "id": "d0b0idu"
        }, 
        {
            "body": {
                "answer": "<p>I<sq>m not an expert on all of those methods, but I know that Seasonal ARIMA would fit your requirements and it<sq>s not too conceptually hard to understand.<br><br>>My boss wants me to have chosen a model before I<sq>m going to test it on my data.<br><br>If time isn<sq>t the issue, then why not just go ahead and test multiple models anyway? Is he going to fire you for doing your job too well? </p>", 
                "question": "Choosing forecasting models"
            }, 
            "id": "d052mm7"
        }, 
        {
            "body": {
                "answer": "<p>If you have the time, this book is a great book on forecasting<colon><br><br>https<colon>//www.otexts.org/fpp<br><br>It<sq>s available for free online, and it<sq>s about $40 on Amazon.  The authors do a great job of breaking things down and making them very easy to understand.  he also includes sample data and code (in R).</p>", 
                "question": "Choosing forecasting models"
            }, 
            "id": "d05bu1a"
        }, 
        {
            "body": {
                "answer": "<p>https<colon>//www.khanacademy.org/math/probability <br><br>Would probably be a decent place to start</p>", 
                "question": "Resources for the layman"
            }, 
            "id": "d052xi7"
        }, 
        {
            "body": {
                "answer": "<p>I think you could approach this several ways, but keep in mind a plot won<sq>t really tell you if a sample is representative or not. The plot may show differences but it<sq>s up to you to argue any differences may not be representative differences based on your knowledge of the underlying sample and population. Also, I think if the three samples weren<sq>t randomly chosen you can stop right there and argue the lack of randomness biased the samples. Or you could argue the small sample sizes could lead to a non-representative samples.<br><br>As for plotting I think calculating the averages of the samples and the sub groups and plotting histograms of each would show a difference, if one was existed. You could also maybe plot boxplots of the sample side by side with the subgroup it was sampled from to.<br><br>I<sq>m not totally sure what that trend-line plot you mentioned looks like, and I don<sq>t think that approach will do you any good in this case. You might be over thinking it a bit, some side by side boxplots along with an explanation would probably do the trick. <br></p>", 
                "question": "How can I show that a small sample within a group does not well represent the entire group?"
            }, 
            "id": "d04j91d"
        }, 
        {
            "body": {
                "answer": "<p>If I understand your question correctly, you want to determine if sample statistics are significantly different from population parameters.  This is a fairly straightforward statistics problem with the null hypotheses being the population mean and the alternative hypothesis that the sample is not equal to the population.  Do you know how to conduct a two sided hypothesis test?  You<sq>ll also want to consider the central limit theorem because your <dq>sub-groups<dq> have small sample sizes.  This means that you<sq>ll lose the confidence that your sample means will approximate a normal distribution.  A normal distribution of sample means for the <dq>sub-groups<dq> of 78 and 80 should be an okay assumption but not for your <dq>sub-groups<dq> of 16 and under.  If you<sq>re using income data then outliers will have a massive effect on means because of your small sample size.  Consider the example that there are 10 people in a room and 9 of them have an income of 0 dollars per year; the 10th person is Bill Gates.  The average income of the people in the room is now millions of dollars.</p>", 
                "question": "How can I show that a small sample within a group does not well represent the entire group?"
            }, 
            "id": "d04jik6"
        }, 
        {
            "body": {
                "answer": "<p>If you want a simple and fast way to compare, you could compare within-group variation to between-group and total variation (variance).<br><br>It<sq>s expected that your within-group variation will be less than total variation hence only using the one group will not capture the population as a whole. </p>", 
                "question": "How can I show that a small sample within a group does not well represent the entire group?"
            }, 
            "id": "d08hf8a"
        }, 
        {
            "body": {
                "answer": "<p>I hope I<sq>m understanding your problem correctly. <br><br>I<sq>m thinking [conjugate priors](https<colon>//en.wikipedia.org/wiki/Conjugate_prior#Table_of_conjugate_distributions). Since you<sq>re supposed to use the gamma distribution as a prior for both lambda and theta and exponential/poisson likelihood. This, if I<sq>m not mistaken, will yield a gamma posterior. Because of conjugacy. Then you have to do some algebra to figure out the functional form of the (hyper) parameters. </p>", 
                "question": "Finding Joint Posterior Density for Poisson & Exponential"
            }, 
            "id": "d02ozgd"
        }, 
        {
            "body": {
                "answer": "<p>Unless I<sq>m misunderstanding the problem, the size of each individual claim (each y_i) follows an Exponential distribution with rate \u03b8, whereas the value you are given is the total size of all n claims, or y=y_1 + y_2 + ... y_n. So, with this, the distribution of y is not Exponential with rate \u03b8, but Gamma with shape n and rate \u03b8.</p>", 
                "question": "Finding Joint Posterior Density for Poisson & Exponential"
            }, 
            "id": "d03jztz"
        }, 
        {
            "body": {
                "answer": "<p>You can<sq>t legitimately use an ANOVA since the data is binomially distributed. Assuming you have a large number of samples, a [Chi-square test of independence](http<colon>//www.biostathandbook.com/chiind.html) can evaluate binomial results amongst multiple multiple groups. If you only have a few hundred samples between all these groups (i.e. less than 1000 total), I<sq>d recommend [Fisher<sq>s exact test](http<colon>//www.biostathandbook.com/fishers.html) instead as it is more accurate. Don<sq>t fish for which one gives you the better results.<br><br>Most important part of this is understanding how to interpret the results in the context of the sampled groups and the populations they represent.  How were these people sampled? Think about sampling bias here and whether you can extrapolate your results to broader populations or only those that met the criteria for being sampled (e.g. owning a computer/smart phone, age demographics, etc).</p>", 
                "question": "Analysing A/B/C.../n tests - how best to do it"
            }, 
            "id": "d00sw8r"
        }, 
        {
            "body": {
                "answer": "<p>I am going to assume you have a basic understanding of linear algebra, statistics, and econometrics. If anything is unclear I will try my best to clarify. <br><br>We have some outcome variable Y and a set of independent variables X. Assume we can write the relation between these as Y = XB + u, where B is a set of regression coefficients we want to estimate and u is a random error term. Our goal is to derive a consistent estimator of B, if we can do this we get a causal interpretation (this is because we specified the exact relation between Y and X). <br><br>To get our estimator premultiply our equation by X<sq> and take the expected value<br><br>E(X<sq>Y) = E(X<sq>X)B + E(X<sq>u)<br><br>If X and u are uncorrelated then E(X<sq>u) = 0. This is the key assumption for OLS, when it fails we need an instrument, but more on this later. Solving our equation for B we get <br><br>B = E[(X<sq>X)]^(-1)E(X<sq>Y)<br><br>This can be estimated using method of moments (or maximum likelihood) and can be shown to be consistent, thus we have a causal interpretation for our estimated regression coefficients. <br><br>Note that if X and u are not uncorrelated, so that E(X<sq>u) != 0 then our estimator of B can be written as <br><br>B = E[(X<sq>X)]^(-1)E(X<sq>Y) - E[(X<sq>X)]^(-1)E(X<sq>u)<br><br>This means that if we try to estimate B if X and u are correlated we get an inconsistent estimator and we are not really sure what it means. <br><br>For an instrumental variable approach we require a set of instruments Z such that E(Z<sq>u) = 0, that is the instruments are uncorrelated with the error term. Given the assumptions that E(Z<sq>u) = 0 can we construct a consistent estimator for B? Premultiply our original equation by Z<sq> and take expectations to get <br><br>E(Z<sq>Y) = E(Z<sq>X)B + E(Z<sq>u)<br><br>Note that the last term drops out and we can write B as<br><br>B = E[(Z<sq>X)]^(-1)E(Z<sq>Y)    <br><br>which can be shown to be consistent, giving us a causal interpretation of our estimated coefficients. <br><br>&nbsp;<br><br>For a concrete example lets examine the link between years of education and and wage. Write the equation as <br><br>wage = B0 + B1*educ + u<br><br>where u contains everything that impacts a persons wage other than education. If we estimate this equation as is we only can claim causality if educ and u are  uncorrelated, is this true? Probably not, for example people with a higher natural ability are likely to have a higher wage and they are also more likely to have more education. Thus we are not likely to get a consistent estimate of B. For an instrument we can use the quarter a person was born in. See [Angrist and Krueger (1991)](http<colon>//masteringmetrics.com/wp-content/uploads/2015/02/Angrist_Krueger_1991.pdf) for an explanation of why this instrument is likely to be uncorrelated with a persons ability. <br><br>Just using OLS gets an estimated coefficient of 0.071, while the instrumental variable approach yields 0.074. Not much of a difference in this case, but that is not always true. <br><br>&nbsp;<br><br>This turned out to be slightly more technical than what I had originally planed, if anything is unclear I would be more than happy to take more about it. </p>", 
                "question": "How is an instrumental variable helpful?"
            }, 
            "id": "czxwaez"
        }, 
        {
            "body": {
                "answer": "<p>I would love to hear an economist<sq>s take on this, because as far as I can tell the steps seem to be<colon><br>1. Select a proxy variable<br>2. Bump out standard error<br>3. Conclude causality due to IV magic</p>", 
                "question": "How is an instrumental variable helpful?"
            }, 
            "id": "czxhs68"
        }, 
        {
            "body": {
                "answer": "<p>A fairer way to make things even is to give the teams that didn<sq>t play one game at least 5 points each (because that is the minimum amount of points that they would receive). The adjustment almost up giving C more points than they could have earned otherwise.<br><br>When you do that, you see that Team F might not be as good as E. If F won, then F would be tied with E in round 2. You could start looking into estimating the probability of winning between F and C and using that to get an expected score. <br><br>A better way would probably be to skip the points system and use a ranking system. There<sq>s a whole bunch of ways to do that, including things like [RPI](https<colon>//en.wikipedia.org/wiki/Rating_Percentage_Index), [Glicko 2](http<colon>//www.glicko.net/glicko.html), [TrueSkill](http<colon>//trueskill.org/), and a bunch more that are out there. The ways I linked are either easily implemented on your own, or are TrueSkill (but I linked to a Python library that does it all for you).<br><br>The advantage to a ratings system is that they all work to take into account strength of schedule and help answer questions like <dq>If E beat F, but F beat D and D beat E, is E better than F?<dq>. Since you<sq>re doing a round robin, you<sq>ll have lots of good matchup info that will help the ratings systems out a lot.<br><br>Edit<colon><br><br>I ran your games through TrueSkill, and it ranked the teams (using conservative skill estimates as recommended by Microsoft research) as follows (higher is better)<colon><br><br>    Team<colon> Conservative Skill<br>    C<colon> 19.92<br>    G<colon> 18.05<br>    E<colon> 18.00<br>    F<colon> 16.93<br>    B<colon> 15.13<br>    A<colon> 12.73<br>    D<colon> 11.34<br>    H<colon> 11.18<br><br>Using Glicko2 got the same ranking. I used [this python module](https<colon>//github.com/sublee/glicko2) to do the calculations. </p>", 
                "question": "Can I adjust for one fewer game played?"
            }, 
            "id": "czu9nqd"
        }, 
        {
            "body": {
                "answer": "<p>There is. It is called cluster analysis. </p>", 
                "question": "How to build survey participants profiles based on their responses?"
            }, 
            "id": "czudu13"
        }, 
        {
            "body": {
                "answer": "<p>If anyone reads this, I found a free software to do cluster analyses<colon> it<sq>s called Cluster 3.0 and was written for Human Genome Analysis. You can find it here<colon> http<colon>//bonsai.hgc.jp/~mdehoon/software/cluster/software.htm</p>", 
                "question": "How to build survey participants profiles based on their responses?"
            }, 
            "id": "czxa6vr"
        }, 
        {
            "body": {
                "answer": "<p>From a statistical perspective, I can only say that the sampling of the polls is inherently biased.  People choose to not vote for a number of reasons<colon> a lack of representation, a feeling of futility with the system, an understanding of the statistical impossibility of your vote mattering, ethical qualms, or.basic laziness  Some countries, like Australia, make voting mandatory to ensure that polls are ideally more representative.<br><br>For a more complete treatment of your question (and not just from a statistical perspective), I<sq>d recommend reading [The Myth of the Rational Voter<colon> Why Democracies Choose Bad Policies](http<colon>//www.amazon.com/gp/product/0691138737?keywords=voting<percent>20bryan<percent>20caplan&qid=1454802909&ref_=sr_1_1&sr=8-1).  The author analyzes American democracy from an economic and game theoretical angle that<sq>s both accessible to non-experts and, in my view at least, plausible.</p>", 
                "question": "This question probably doesn<sq>t have a single satisfactory answer but does the American presidential election do a good job of reflecting what America actually thinks and wants?"
            }, 
            "id": "czqhwnd"
        }, 
        {
            "body": {
                "answer": "<p>What could the phrase <sq>what America wants<sq> possibly mean?</p>", 
                "question": "This question probably doesn<sq>t have a single satisfactory answer but does the American presidential election do a good job of reflecting what America actually thinks and wants?"
            }, 
            "id": "czqg80u"
        }, 
        {
            "body": {
                "answer": "<p>/r/asksocialscience</p>", 
                "question": "This question probably doesn<sq>t have a single satisfactory answer but does the American presidential election do a good job of reflecting what America actually thinks and wants?"
            }, 
            "id": "czr59c2"
        }, 
        {
            "body": {
                "answer": "<p>What are you trying to achieve with this analysis?<br><br>For data that don<sq>t satisfy the assumptions of the t-test, there are nonparametric alternatives. Namely the Wilcoxon test. These tests don<sq>t make assumptions about the distribution of the data.</p>", 
                "question": "High kurtosis (leptokurtic) how to transform to normal distribution?"
            }, 
            "id": "czotmwq"
        }, 
        {
            "body": {
                "answer": "<p>what are you measuring/how do these value arise? <br><br>What made you decide to perform t-tests (/what is the underlying question of interest)?<br><br>Note that if you<sq>re interested in testing the mean, after you transform you won<sq>t be testing the mean any more (and if you<sq>re not interested in testing the mean, why do you need the t-test at all?)<br></p>", 
                "question": "High kurtosis (leptokurtic) how to transform to normal distribution?"
            }, 
            "id": "czpo90m"
        }, 
        {
            "body": {
                "answer": "<p>what are you measuring/how do these value arise? <br><br>What made you decide to perform t-tests (/what is the underlying question of interest)?<br><br>Note that if you<sq>re interested in testing the mean, after you transform you won<sq>t be testing the mean any more (and if you<sq>re not interested in testing the mean, why do you need the t-test at all?)<br></p>", 
                "question": "High kurtosis (leptokurtic) how to transform to normal distribution?"
            }, 
            "id": "czpoit9"
        }, 
        {
            "body": {
                "answer": "<p>You have so much wrong, it would be difficult to know where to start. You need to find a statistician at your school and talk to them, possibly add them to your thesis committee. Briefly<colon><br><br>(1) non-parametric tests rarely test hypotheses about the mean of a sample. Are you even sure that you need a nonparametric test? Why?<br><br>(2) Wilcoxon tests rank your data from highest to lowest, then compare the sums of the ranks between two groups. Under most conditions, this is equivalent to a comparison of the medians between two samples. Wilcoxon tests are among the most basic and popular nonparametric tests.<br><br>The Kolmogorov-Smirnov test is used to compare one <dq>empirical cumulative distribution function<dq> (eCDF) against either a theoretical CDF like the normal distribution or against another eCDF. This test essentially compares two entire distributions without focusing on any single parameter (mean, variance) or metric (median, geometric mean). Unfortunately, the KS is notoriously underpowered, meaning it often does not find significant p-values even when groups are truly different. The KS test is unpopular and rarely used<br><br>(3) both tests above are used to compare two groups of samples (eg male vs female, young vs old). No test is really designed to compare single data points against the rest of a sample. That doesn<sq>t make sense at all. There is no statistical test for your first question. Then best you can do is to report descriptive statistics<colon> mean, std dev, median, ...<br><br>(4) your second question makes some sense. You want to compute the correlation among your three traits<colon> sociability, curiosity and boldness. Use Pearson, Spearman or Kendall correlation. Pearson is the parametric method (assumes your data is bivariate normal). Spearman is a nonparametric method that basically just ranks your data and computes Pearson correlation on the ranks. Kendall correlation is a different nonparametric correlation that uses <dq>concordance and discordance<dq>. Both Spearman and Kendall yield similar results.<br></p>", 
                "question": "Help! Graduate student needs help with non-parametric SPSS tests"
            }, 
            "id": "czmlb0d"
        }, 
        {
            "body": {
                "answer": "<p>This might help<colon><br><br>http<colon>//www.mayo.edu/mayo-edu-docs/center-for-translational-science-activities-documents/berd-5-6.pdf</p>", 
                "question": "Help! Graduate student needs help with non-parametric SPSS tests"
            }, 
            "id": "cznb590"
        }, 
        {
            "body": {
                "answer": "<p>This is a fraud detection problem.  <br><br>The ideal approach here would be machine learning. But before you can apply any algorithm, you need (1) enough data, at least 200+ employees and (2) examples of cases where you KNOW that someone cheated (or that someone did not).  <br><br>Before you have this, you have to look for cheaters manually. You need to find patterns <dq>intuitively<dq>. Best possible advice is to have a human approach, without scaling or stats in mind. Log everything. Once you have enough supervised data (i.e. examples where you know if that cas cheating or not) you can use machine learning.</p>", 
                "question": "Catching Time Cheaters."
            }, 
            "id": "czjw1e4"
        }, 
        {
            "body": {
                "answer": "<p>A non stats comment<colon> I love the thought that goes into this. Create an arbitrary deadline for someone to report on so someone else can then analyze those reports for who knows what. (probably to give shit to the people who do work). Then give an assignment to someone unqualified, to report on who is meeting said arbitrary deadline. Those who are not can then be giving shit by someone else. I imagine the reason they adjust their time later is because this is not a critical activity and just some sort of internal control. <br><br>Unless this is government compliance or directly revenue related this is a shit assignment, from a shit company to do shit things. <br><br>Rant over </p>", 
                "question": "Catching Time Cheaters."
            }, 
            "id": "czk4ukf"
        }, 
        {
            "body": {
                "answer": "<p>If you have a lot (100s-1000s) of examples of cheating and non-cheating, you can pose this as a classification problem. There are many different methods to solve classification problems; which one works best will depend on your problem and the amount of training data you have. <br><br>The first thing to try should be a linear model<colon> find a linear combination of the inputs that separates positive from negative examples (known cases of cheating from known cases of non-cheating). Lecture 3 from http<colon>//work.caltech.edu/previous.html will explain more.<br><br>If you pursue the machine learning approach, I should tell you that it is tremendously important what the input features are. And it is even more important that you protect yourself against overfitting, by leaving out a test set.<br><br>If you do not have many examples and thus cannot pose this as a learning problem, you can try another approach<colon> try to define <dq>cheating<dq> in a quantitative way. Cheating is when someone records hours that are different from the hours they actually worked. <br><br>To make this quantitative you need a probabilistic model of the hours people actually work. Then you can ask the question<colon> What is the likelihood that this person<sq>s data was generated by this model? (As a starting point, look up Bayesian methods).<br><br>In this framework you turn your intuitions about normal behavior into probabilistic models. For example, maybe you think that most of the time people work the same number of hours every day, and that each person has some <dq>steadiness level<dq><colon> some people are extremely regular, while others are wild. This is a kind of iid assumption and would predict that the hours worked over bigger and bigger chunks of time look more and more Gaussian.<br><br>Moreover, work goes in phases, so if someone worked 12 hours on Monday they<sq>re probably going to work a lot on Tuesday too. You can model this with a Markov chain.<br><br>(You can combine the second approach with machine learning by expressing your intuitions as a prior that is combined with training examples. Look up Bayesian learning.)<br><br>A completely different way to solve the problem is to disincentivize cheating instead of trying to detect it. Make it costly to move hours later. Remove the reward for reporting 40 hours a week by moving to a salary system and anonymizing the timekeeping.</p>", 
                "question": "Catching Time Cheaters."
            }, 
            "id": "czlomic"
        }, 
        {
            "body": {
                "answer": "<p>Yep.  He is assuming that race and liberality are independent, when they are almost certainly not. </p>", 
                "question": "Question about a graphic on fivethirtyeight"
            }, 
            "id": "cziouyt"
        }, 
        {
            "body": {
                "answer": "<p>Yes, it is a naive assumption. He has used this same chart many times and [discussed its use](http<colon>//fivethirtyeight.com/datalab/bernie-sanders-could-win-iowa-and-new-hampshire-then-lose-everywhere-else/) earlier<colon><br><br>> It would be better if the exit polls directly listed the number of white liberals. Unfortunately, the exit polls do not provide this data, so we have to live with an estimate instead.<br><br>I agree, in the sense that it is sometimes okay to use a simplified model, including an assumption of independence even when we know that two variables are not independent. Justified use of such <dq>improper<dq> models is a complex and interesting topic. For the moment, I will say that they are sometimes correctly used professionally. For the purposes of these articles (roughly ranking states on this combined metric), it<sq>s probably ok. <br> <br>I do think it is fair to demand a better, more explicit disclaimer and to demand that the disclaimer be included every time this or a similar chart is used. </p>", 
                "question": "Question about a graphic on fivethirtyeight"
            }, 
            "id": "czj0tvq"
        }, 
        {
            "body": {
                "answer": "<p>Try throwing them in a random forest. If there<sq>s any predictive power to those columns, the variable importance metrics of the random forest will help you see it. <br><br>Honestly though, just because SMEs care doesn<sq>t mean the variables are actually valuable. One trick we use at work is we build two models<colon> one model that is actually predictive, and another model that contains all the shitty variables that <dq>experts<dq> have forced on us. Then we ensemble the two models with a weighted average and calculate some metric of interest (accuracy, lift, F1, whatever) across varying weight values to demonstrate the effect that the crappy model has on the accuracy. We then take this back to the client and make a recommendation for how tolerant our predictions are to their shitty variables (e.g. <dq>Our analysis shows that we can go to about 75/25 without significantly affecting model accuracy, but any more than that and accuracy will be significantly affected<dq>) and let them decide how accurate the model needs to be based on their risk appetite.<br><br>This approach has the added benefit that if their confidence in you grows, you can easily improve the model in the future by just dropping the ensemblification step.</p>", 
                "question": "How to use near zero variance predictors in machine learning ?"
            }, 
            "id": "czciz7f"
        }, 
        {
            "body": {
                "answer": "<p>The problem you list here reminds me about <dq>sparsity<dq>, which describes matrix that have many entries of zero in comparison to the total number of entries possible. The 100 columns you suggested seems to fall into this category so you may want to look into method regarding sparsity. <br><br>I couldn<sq>t give you a concrete answer on how to approach this type of model but I would advise you on caution of interpreting any predictive power/pattern arise from those 100 variables. Since the effect is extremely sparse (in other word, very rare), you maybe able to find some significant effect even if there isn<sq>t. Just consider this example, suppose you want to test on person<sq>s income related to their attributes, if someone who has 11 fingers for whatever reason, if their income is significantly higher than other people, the model you build may interpret it as <dq>have 11 fingers -> higher likelihood of high income<dq>, which is a very silly conclusion.<br><br>So it will be more desirable if you can have a large sample size just that these rare events (having value 1 in some of those columns) still make up enough of a sample to make inference. Or you may want to do some transformation on the variables to make them less <dq>rare<dq> (like column A OR column B OR column C) but you should have a good justification of doing that.<br><br>For keyword searching, I would suggest looking for terms like Sparsity, Robustness towards rare event. From my experience I know text mining may have literature on that regard since the data involved can be pretty sparse at time. But that will delve into more on machine learning category.</p>", 
                "question": "How to use near zero variance predictors in machine learning ?"
            }, 
            "id": "czex6cs"
        }, 
        {
            "body": {
                "answer": "<p>Example <dq>interesting<dq> answer <colon> [from here](http<colon>//www.r-bloggers.com/near-zero-variance-predictors-should-we-remove-them/)<br>> Try not to throw your data away<br><br>Think for a moment, the solution above is easy and \u201csolves the problem\u201d, but we are assuming that all those predictors are non-informative, which is not necessarily true, specially for the near-zero variance ones. Those near-variance predictors can in fact turn out to be very informative.<br><br>For example, assume that a binary predictor in a classification problem has lots of zeroes and few ones (near-variance predictor). Every time this predictor is equal to one we know exactly what is the class of the target variable, while a value of zero for this predictor can be associated with either one the classes. This is a valuable predictor that would be thrown away by the method above.<br><br>This is somewhat related to the separation problem that can happen in logistic regression, where a predictor (or combination of predictors) can perfectly predicts (separate) the data. The common approach not long ago was to exclude those predictors from the analysis, but better solutions were discussed by [2], which proposed a penalized likelihood solution, and [3], that suggested the use of weekly informative priors for the regression coefficients of the logistic model.<br><br>Personally, I prefer to use a well designed bayesian model whenever possible, more like the solution provided by [3] for the separation problem mentioned above. One solution for the near-variance predictor is to collect more data, and although this is not always possible, there is a lot of applications where you know you will receive more data from time to time. It is then important to keep in mind that such well designed model would still give you sensible solutions while you still don\u2019t have enough data but would naturally adapt as more data arrives for your application.</p>", 
                "question": "How to use near zero variance predictors in machine learning ?"
            }, 
            "id": "czcgq3x"
        }, 
        {
            "body": {
                "answer": "<p>You may want look at LDA. If the difference are in the mean values, it can help reducing the number of features.</p>", 
                "question": "How to use near zero variance predictors in machine learning ?"
            }, 
            "id": "czdnw4b"
        }, 
        {
            "body": {
                "answer": "<p>Interesting question. For the sake of clarifying your use of ranges--should we assume that you have normally distributed random error in each thermometer and that the ranges you give are 95<percent> confidence intervals? This is an important assumption--we could alternatively (although not very realistically) assume that the error was uniformly distributed with the ranges you give with a hard cutoff (e.g. never an error of more than \u00b12 for the camera). </p>", 
                "question": "How do I combine asymmetric systematic errors with statistical errors?"
            }, 
            "id": "czc25ar"
        }, 
        {
            "body": {
                "answer": "<p>Easy way<colon> [assume the systematic errors and the random errors are independent of each other and that all are normally distributed](https<colon>//en.wikipedia.org/wiki/Sum_of_normally_distributed_random_variables).<br><br>Hard way<colon> check your assumptions and (if necessary) modify the analysis accordingly</p>", 
                "question": "How do I combine asymmetric systematic errors with statistical errors?"
            }, 
            "id": "czc4l0z"
        }, 
        {
            "body": {
                "answer": "<p>The best advice I can offer is to take math classes that interest you<colon> you<sq>ll learn the most when you<sq>re enjoying yourself, and a really valuable part of a math major is the intuition into problem solving you gain<br><br>That said<colon> multi variable calculus is taken for granted as something you are fluent in. Real analysis is a very good course to learn how to write proofs, and is a basic benchmark for whether a student has begun doing <sq>serious<sq> math. Linear algebra is the bread and butter of modeling, so a class on that should also be considered essential. If you can take a mathematical statistics course---I would be very surprised if your university doesn<sq>t have that---it<sq>s great to receive exposure to the basics as soon as possible, so you know what you<sq>re getting into and because you<sq>ll need to think about even basic concepts a LOT and OFTEN to really master them, eventually, at a graduate level.<br><br>Measure theory is the foundation for probability, and experience with that (and/or stochastic processes) will be serious icing on the cake---but are by no means essential before grad school.<br><br>I would also see if there are any applied classes you can take, things that would give you exposure to data analysis and regression and the like.</p>", 
                "question": "Math Major interested in Statistics PhD"
            }, 
            "id": "cz8ccnj"
        }, 
        {
            "body": {
                "answer": "<p>> If I am interested in statistics, would it still be helpful for me to learn algebra or topology even if I am not required to take those kinds of classes?<br><br>Definitely.  First off, you should know algebra if you<sq>re going to be doing theoretical grad-level statistics.  Second off, you don<sq>t know if you<sq>re always going to be working in statistics.  Learn everything you can.<br><br>> My school also has graduate courses on measure theory and stochastic processes; are those the kinds of classes that I should try to take?<br><br>Most grad students in statistics weren<sq>t stats majors, and most grad-level stats programs are working under the assumption that their incoming students have not had courses in things like measure theory.  Take these classes if you want to (and to make sure that you<sq>re interested in statistics), but don<sq>t skip the normal math major classes to take stats classes.<br><br>> If you have any suggestions on which kinds of courses I should be looking for, or any other advice on how to be a good candidate for graduate school, I<sq>m all ears.<br><br>Take the classes that are considered to be standard or normal for your major so that any place you apply to doesn<sq>t have an admissions committee going <dq>she<sq>s a math major but never took algebra?! what?!!!<dq>.  Then take classes in anything you<sq>re interested in.<br><br>I double-majored in math and physics and ended up doing a minor in East Asian Art History.  It didn<sq>t help my career or grad school prospects at all but what it did do was make me happy, which made me more relaxed, which meant that I worked harder on my math and physics classes.  Don<sq>t underestimate how bad stress can be for you.  I did something similar in grad school - I took Old Norse (Icelandic) which was really interesting and fun but everybody in my program made fun of me for it.  Didn<sq>t matter, made my life more enjoyable.</p>", 
                "question": "Math Major interested in Statistics PhD"
            }, 
            "id": "cz8c6yo"
        }, 
        {
            "body": {
                "answer": "<p>IMHO<colon><br><br>Abstract algebra - a little bit. It<sq>s required for the theory behind [Sigma algebras](https<colon>//en.wikipedia.org/wiki/Sigma-algebra), which is one of the  foundation of more advanced probability theory. Topology would only be helpful in as  much as it would help with real analysis, which in turn is needed for most measure theory stuff, which is the other foundation of probability theory. (However, there is this awesome new approach called [Topological data analysis](http<colon>//www.nature.com/articles/srep01236?message-global=remove&WT.ec_id=SREP-639-20130301), so I might take a topology course anyhow). <br><br>Basically, look at the prerequisites for the graduate courses on inference theory and probability theory (either in the course plan or in the text book), and figure out what math would be useful. FWIW, looking through my degree document,  the only math courses of the many that I<sq>ve taken that have never come to use are complex analysis and geometry. And those were still kind of fun courses.</p>", 
                "question": "Math Major interested in Statistics PhD"
            }, 
            "id": "cz8cueo"
        }, 
        {
            "body": {
                "answer": "<p>[deleted]</p>", 
                "question": "Math Major interested in Statistics PhD"
            }, 
            "id": "cz8drnm"
        }, 
        {
            "body": {
                "answer": "<p>This is the hypergeometric distribution formula.</p>", 
                "question": "Combinatorics<colon> Is there a name for this formula?"
            }, 
            "id": "cz7pvss"
        }, 
        {
            "body": {
                "answer": "<p>Its the pmf for the hypergeometric distribution<br><br>https<colon>//en.wikipedia.org/wiki/Hypergeometric_distribution<br></p>", 
                "question": "Combinatorics<colon> Is there a name for this formula?"
            }, 
            "id": "cz8svbs"
        }, 
        {
            "body": {
                "answer": "<p>The joint distribution just describes the actual data you have and then the graphical model encodes your assumptions about the relationships between the different variables contained in the data.  Using your assumptions, you can try to learn the distribution of the [conditional] probabilities you wrote (each of the components of the factorized joint distribution)<br><br>It<sq>s not really clear what you actually are trying to do here, though.  If you are trying to estimate the value of the cost function you mentioned, you can take the integral of p(A) * c(A) after you fit p(A) to your data?</p>", 
                "question": "Graphical models/ Bayesian statistics - what is the value in the joint distribution?"
            }, 
            "id": "cyysicj"
        }, 
        {
            "body": {
                "answer": "<p>I don<sq>t know what you<sq>re aiming for either. Do you have any data for any of the variables A B C D? If so then you want to fit the model to your data.</p>", 
                "question": "Graphical models/ Bayesian statistics - what is the value in the joint distribution?"
            }, 
            "id": "cz2rty0"
        }, 
        {
            "body": {
                "answer": "<p>Suppose X and Y are two random variables. You want to know if <br><br>E[X/Y] = E[X]/E[Y]<br><br>This is not true in general, especially if the random variables X and Y are not independent. Here is a pdf discussing the non-independence case<colon><br> <br>http<colon>//www.faculty.biol.ttu.edu/Rice/ratio-derive.pdf<br><br>In the easier independent case (which you do not seem to be in) then what can be said is the following<colon><br><br>E[X/Y] = E[X] * E[1/Y]<br><br>But note that E[1/Y] is not equal to 1/E[Y]. Here is a wiki article regarding this fact<colon><br><br>https<colon>//en.wikipedia.org/wiki/Inverse_distribution</p>", 
                "question": "ratio of non-normal distributions"
            }, 
            "id": "cyqx3b8"
        }, 
        {
            "body": {
                "answer": "<p>What are your hypotheses about the differences between the groups and what number of observations do you have for each of the groups? General advice follows<colon><br><br>You can use a [one-way between subjects Analysis of Variance (ANOVA)](https<colon>//en.wikipedia.org/wiki/One-way_analysis_of_variance) with (virtually) any number of levels for your grouping factor. This test extends the t-test to multiple groups. It will test the <sq>omnibus<sq> hypothesis that *all* your groups have equal means. It is likely that you have a more sophisticated hypothesis regarding your groups, you should read about [post-hoc tests](https<colon>//en.wikipedia.org/wiki/Post_hoc_analysis) and consider carefully what hypotheses you want to test ***before*** running any analysis (or even looking at the group means). You will in any case need to perform appropriate correction for [multiple comparisons](https<colon>//en.wikipedia.org/wiki/Multiple_comparisons_problem).<br><br> If you want to extend Mann-Whitney U (aka Wilcoxon rank sum) instead of t-test then you could consider the ordinal/ranked equivalent to ANOVA which is called the [Kruskal-Wallis one way analysis of variance](https<colon>//en.wikipedia.org/wiki/Kruskal<percent>E2<percent>80<percent>93Wallis_one-way_analysis_of_variance). There are post-hoc tests specific for the KW-ANOVA, e.g. [Dunn<sq>s test](http<colon>//stats.stackexchange.com/tags/dunn-test/info).<br><br>ANOVA isn<sq>t nearly as simple as the t-test and you may want to do a bit of reading around to make sure your analysis is correct.</p>", 
                "question": "Evaluating Likert scale data across multiple groups?"
            }, 
            "id": "cyqi2ir"
        }, 
        {
            "body": {
                "answer": "<p>You would describe them just as you<sq>ve done. For your question on success rate, I<sq>m assuming that N/A are people who refuse to give a response or for whom it does not apply, so then you would exclude then in the proportion. So success proportion is 30/70.</p>", 
                "question": "N/A in a Binary Scoring Scheme"
            }, 
            "id": "cypxu0s"
        }, 
        {
            "body": {
                "answer": "<p>I think it depends on the meaning of NA. Does it mean you have no information? Or does it describe a variety of 3rd outcomes that are of no interest? <br><br>Suppose you are testing some widget (eg lightbulbs) and NA means that the desired batch was missing or unavailable for testing. Then your total sample size is 70 and your success rate is 30/70.<br><br>Now suppose you are evaluating some biological assay (eg pregnancy test), where the NAs represent subjects with no need to be tested (eg pre-pubescent girls, post-menopausal women, historectomies, trans-women). Those NAs contain real information, so they should be included. Then your success rate might be 30/100.  It<sq>s all context.</p>", 
                "question": "N/A in a Binary Scoring Scheme"
            }, 
            "id": "cyq0frz"
        }, 
        {
            "body": {
                "answer": "<p>>How do I use PCA to compare two categories?<br><br>To be somewhat blunt<colon> you don<sq>t. PCA isn<sq>t for comparing categories.<br><br>>I have a matrix of values that fall in two categories A and B.<br><br>I assume this means you have a matrix of variables (columns), wherein the rows happen to be observations that come from two groups.<br><br><br>>I would like to use PCA to know if the categories are large sources of variability.<br><br>PCA won<sq>t be able to tell you that except for (1) visualization of effect, and (2) post-hoc tests analogous to t-tests.<br><br><br>PCA is for understanding how a whole bunch of variables are related to one another, with the optimization criterion of maximum variance (per component, where each component is orthogonal to the previous).<br><br>However, you can use PCA as a visualization technique to see if, for example, the first component is driven by distinctly different structures of the two groups. However, it<sq>s not a real (statistical) comparison.<br><br><br>>How do I do that in R?<br><br>There are two packages in R that make this kind of easy. They allow you to input a matrix for a PCA, and with that a way to color the observations based on *a priori* groups. I<sq>m the author of one of those packages (ExPosition).<br><br><br>So, you could use ExPosition<sq>s epPCA() function with the DESIGN parameter, or, in a similar fashion, use FactoMineR<sq>s PCA() function with the <dq>habillage<dq> parameter (which, loosely, is French for <dq>dressed<dq> -- you<sq>re <dq>dressing<dq> your observations in different colors).</p>", 
                "question": "How do I use PCA to compare two categories?"
            }, 
            "id": "cyomba7"
        }, 
        {
            "body": {
                "answer": "<p>PCA doesn<sq>t work well with categorical data. Check out [multiple correspondence analysis](https<colon>//en.wikipedia.org/wiki/Multiple_correspondence_analysis) or [multiple factor analysis](https<colon>//en.wikipedia.org/wiki/Multiple_factor_analysis) as an alternative.<br><br>http<colon>//stats.stackexchange.com/a/5777/8451</p>", 
                "question": "How do I use PCA to compare two categories?"
            }, 
            "id": "cyot8sq"
        }, 
        {
            "body": {
                "answer": "<p>There isn<sq>t a <dq>the<dq> here unless you specify a criterion. <br><br>There are many ways to measure how similar the two are. One example is to look at a two-sample Kolmogorov-Smirnov statistic (attempting to find an allocation to teams that minimizes it). <br><br>However, I imagine you could do fairly well for your simply by making the mean and standard deviation of the scores as similar as possible. Equivalently, the sum and sum of squares of scores would need to be made close. [Perhaps you want the sum as close as possible and then within the allocations that satisfy that you<sq>d like to make the sum of squares as close together as you can.]</p>", 
                "question": "How can I split a pool of game players into two teams that have not just similar mean skills but also similarly distributed skills?"
            }, 
            "id": "cyjvi6w"
        }, 
        {
            "body": {
                "answer": "<p>One way is to choose the team which minimized some goodness of fit statistic.<br><br>In your case, you could minimize<br><br>(Best on team A) - (Best on team B) squared +<br><br>(2nd best A) - (2nd best B) squared + <br><br>...<br><br>(Weakest on A) - (Weakest on B) squared.<br><br>This is a quantile-quantile approach. If you are looking for something fancier, there are lots of goodness of fit statistics and distribution comparison statistics. Look up Anderson-Darling for one.<br><br>You can find such a minimum with the optim() function in R. Here is a blog post that may help with that<colon> http<colon>//factotumjack.blogspot.ca/2015/07/using-optim-to-get-quick-and-dirty.html</p>", 
                "question": "How can I split a pool of game players into two teams that have not just similar mean skills but also similarly distributed skills?"
            }, 
            "id": "cyjm9n5"
        }, 
        {
            "body": {
                "answer": "<p>If you have 2 separate draws with .5 chance to win, then the odds of getting at least 1 success is .75.  Found by 1 minus chance of no success which is 1-(.5)^2 =.75 so a is better </p>", 
                "question": "What alternative is best?"
            }, 
            "id": "cyisflj"
        }, 
        {
            "body": {
                "answer": "<p>The expected value of the Nheads/N is 1/2, and according to the [law of large numbers](https<colon>//en.wikipedia.org/wiki/Law_of_large_numbers) the mean for increasing N should approach the expected value.<br><br>However this does not indicate that the probability of getting exactly 1/2 approaches 1. In fact I think it approaches zero. Consider by analogy a continuous probability density where the probability of getting some exact value is zero.<br><br>Another way to think of this is the probability that an independent increment random walk process is exactly 0 at some N.(where you randomly add -1 or 1) Clearly the probability that the output is exactly zero does not approach 1, but the output of the process divided by N should approach zero.</p>", 
                "question": "Intuitively I expect the probability of getting n/2 heads to approach 1 as n the number of coin flips increases but that<sq>s not what I<sq>m seeing"
            }, 
            "id": "cyhviej"
        }, 
        {
            "body": {
                "answer": "<p>Yep, /u/datanaut is exactly right about the probability of exactly n/2 approaching zero because the probability of any single outcome is dropping as the number of possible outcomes increases.<br><br>You can see this in action by looking at all the options for small values of n. For example, at n=2 there are 4 possibilities<colon> HH, HT, TH, TT. So 2/4 (50<percent>) get exactly n/2. <br><br>For n=4<colon> HHHH, HHHT, HHTH, HTHH, THHH, HHTT, HTHT, HTTH, THHT, THTH, TTHH, TTTH, TTHT, THTT, HTTT, TTTT. That<sq>s 16 possible outcomes, all equally likely, of which 6 (38<percent>) are exactly n/2. <br><br>For n=6, I<sq>m not going to list them all but it<sq>s 64 possible outcomes of which 20 (31<percent>) are exactly n/2. At n=8 it<sq>s 70 of 256 outcomes (27<percent>), at n=10 it<sq>s 252/1024 (25<percent>), etc.</p>", 
                "question": "Intuitively I expect the probability of getting n/2 heads to approach 1 as n the number of coin flips increases but that<sq>s not what I<sq>m seeing"
            }, 
            "id": "cyi7sab"
        }, 
        {
            "body": {
                "answer": "<p>For n even, it actually approaches 0 (slowly), since the probability is asymptotically proportional to 1/sqrt(n)<br><br>Some of the discussion here might be useful<br><br>http<colon>//stats.stackexchange.com/questions/136870/does-10-heads-in-a-row-increase-the-chance-of-the-next-toss-being-a-tail<br><br><br><br></p>", 
                "question": "Intuitively I expect the probability of getting n/2 heads to approach 1 as n the number of coin flips increases but that<sq>s not what I<sq>m seeing"
            }, 
            "id": "cyiavmw"
        }, 
        {
            "body": {
                "answer": "<p>Change the <dq>Rewards Member<dq> variable to take values of 1 or 0 (depending on yes or no) and run a simple regression. That<sq>s called making it a <dq>dummy variable<dq>.<br><br>Straightforward regression output<colon><br><br>Variable | Coefficient | SE | t  | P>t | 95<percent> | 95<percent><br>---|---|----|----|----|----|----<br>const | 19.6 | 3.1 | 6.3 | 0.00 | 13.5 | 25.9<br>Distance from Hub | -0.48 | 0.15 | -3.3 | 0.00 | -0.77 | -0.19<br>Rewards Member | 16.9 | 3.9 | 4.4 | 0.00 | 9.2 | 24.6<br><br>We can<sq>t do too much more with the data you gave, sadly. I would expect being a rewards member to correlate with other factors, so the coefficient we have is probably significantly biased upwards from omitted variables.</p>", 
                "question": "Finding Impact of Rewards Program on Sales"
            }, 
            "id": "cygxgta"
        }, 
        {
            "body": {
                "answer": "<p>I have never heard of the domain of a variable. Can you give us some clues? </p>", 
                "question": "What is the domain of this statistical variable?"
            }, 
            "id": "cyg6oak"
        }, 
        {
            "body": {
                "answer": "<p>1. [Wikipedia](https<colon>//en.wikipedia.org/wiki/Bernoulli_distribution) gives a pretty good explanation. A Bernoulli experiment has two possible outcomes<colon> heads vs. tails, yes vs. no, however you want to think of it. That<sq>s the domain of the variable<colon> the possible values it can take. The probabilities of the two outcomes have to add up to 1.<br><br>2. Now we<sq>ve moved from the specific case of a Bernoulli distribution to a binomial distribution (a binomial distribution is what you get when you repeat a Bernoulli experiment more than once). The domain has changed - do you see what the possible outcomes are? The probability mass function (PMF) is going to look like a skewed bell-curve. That may be difficult to visualize, but a good place to start is to think of where the PMF would be greatest. What<sq>s the most likely outcome?</p>", 
                "question": "What is the domain of this statistical variable?"
            }, 
            "id": "cyg939r"
        }, 
        {
            "body": {
                "answer": "<p>The domain of variables are the outcomes of the experiments.</p>", 
                "question": "What is the domain of this statistical variable?"
            }, 
            "id": "cygd6ad"
        }, 
        {
            "body": {
                "answer": "<p>Only to remind that a random variable is a function on S, the sample space, that set values to, usually, the real line.<br><br>So, the domain for 1) is Success-Failure, that maps to the image 0-1<br>2) 0 to 10 <dq>6<dq> on the dice, that maps to 0-10.<br></p>", 
                "question": "What is the domain of this statistical variable?"
            }, 
            "id": "cygeoq1"
        }, 
        {
            "body": {
                "answer": "<p>Dont rerun the model.</p>", 
                "question": "Split-Plot Designs are a cruel form of torture"
            }, 
            "id": "cygbwkp"
        }, 
        {
            "body": {
                "answer": "<p>You can use ztests on binary data. So you could just perform it 3x to find if there are differences between all of the groups. I<sq>m not sure if kruskal Wallis works. Id assume not since it requires the cdf to be continuous and don<sq>t think we would have that here. Maybe someone can correct me if I<sq>m wrong though. <br><br>Edit<colon> changed ttest to ztest</p>", 
                "question": "Kruskal-Wallis for binary data?"
            }, 
            "id": "cyfmukr"
        }, 
        {
            "body": {
                "answer": "<p>If you roll a dice 6 times, you will not have observations of 1 2 3 4 5 6 (most likely). If you roll the dice 100000000 times you will have about 1/6 of each number. <br><br>The smaller dataset is poor because of few observations. Another example, if you pick out 10 people on the street, you might get 5 kids and 5 adults. But if you pick out 10000 people on the street, you will find more adults than kids (because thats how it is right?).<br><br><br>Use this link to toy around, https<colon>//academo.org/demos/dice-roll-statistics/<br><br>Try <dq>roll automatically<dq></p>", 
                "question": "Higher Sample Size =>More concentration at the middle?"
            }, 
            "id": "cyeb69f"
        }, 
        {
            "body": {
                "answer": "<p>You can<sq>t do it if your reason for suspecting fraud was the observed result. All you<sq>re proving is that rare events happen. If you predicted that fraud would occur and you also predicted that the nature of the fraud would involve just making up the numbers instead of stuffing ballot boxes, you can test your hypothesis when the result comes in.<br><br>There have been a number of high profile cases in the UK where women have been convicted of child murder after a second cot death because prosecutors used this fallacy (known as the prosecutor<sq>s fallacy) to present ridiculous statistics to juries. There<sq>s been a lot written about it, the RSS produced some important work which now informs prosecutorial practice. Sally Clarke and Trupti Patel were two of the women wrongfully convicted on this basis. Useful reading.<br><br>There is a body of literature detailing fraud detection methods where such tests are valid, mostly based on humans not being very good at generating random numbers. Not terribly likely in an electoral situation where fraud is usually done by other means but it is used as a check on data integrity in a variety of high risk situations.</p>", 
                "question": "Determine probability of election fraud from strange results"
            }, 
            "id": "cycki7k"
        }, 
        {
            "body": {
                "answer": "<p>The issue of detecting electoral fraud with Bayesian methods has been recently addressed in [Political Analysis](http<colon>//pan.oxfordjournals.org/content/23/4/488.full.pdf+html) (ungated). Not my field, but I<sq>d give that paper and its lit review a close look to answer your question. </p>", 
                "question": "Determine probability of election fraud from strange results"
            }, 
            "id": "cycv9bo"
        }, 
        {
            "body": {
                "answer": "<p>The bottom line for Bayesian analysis is, as you note in your post, computing the likelihood of the hypotheses given the evidence.  This is why I hate using Bayesian for anything other than actual statistics--because for questions like this, you just have to make your likelihood up, which is definitely going to be biased by what you thought in the first place (the prior).  <br><br>Even if you could use this point to adjust your posterior belief in fraud, you would need to process all the other available <dq>evidence<dq> under both hypotheses.  </p>", 
                "question": "Determine probability of election fraud from strange results"
            }, 
            "id": "cydffvk"
        }, 
        {
            "body": {
                "answer": "<p>Your concern is understandable but you are misunderstanding with this statement<colon><br>*<dq>it is a list of neither concordant nor discordant pairs<dq>*<br><br><br>Tau(X,X) is not a list of neither concordant nor discordant pairs.  Consider this data<colon><br><br>(1,1)<br>(2,2)<br>(3,3)<br><br>n=3<br><br>ND = 0<br>NC = 3<br><br>So Tau(X,X) = (3 - 0)/3 = 1.  That is correct, of course.  Perfect concordance. <br><br>But now consider this data<colon><br>(1.5,1.5)<br>(1.5,1.5)<br>(3   ,3   )<br><br>n=3<br>ND=0<br>NC=2<br><br>So Tau(X,X) = (2 - 0)/3 = 0.667.  The ties have screwed up the computation.  Tau(X,X) is not required to be 1 even though the data is identical.  So Somer<sq>s D normalizes to this issue.</p>", 
                "question": "I don<sq>t understand Somers<sq> D how does Kendall<sq>s tau(XX) make any sense?"
            }, 
            "id": "cy6o9f1"
        }, 
        {
            "body": {
                "answer": "<p>Sensitivity is the true positive rate. It says<colon> out of everyone tested who had cancer, how many did the test identify as having cancer? In this case, 100<percent>. So sensitivity of 1 means that if you have cancer the test will always come back positive. <br><br>Specificity is the true negative rate. This means, of all the people tested who don<sq>t have cancer, 67<percent> of those got a negative test, and 33<percent> of them are incorrectly given a positive test result.<br><br>So, now the question is<colon> if 1/5000 people have this kind of cancer and you get a positive test result, what are the odds that you are actually positive?</p>", 
                "question": "Interpreting sensitivity and specificity"
            }, 
            "id": "cy4n0xk"
        }, 
        {
            "body": {
                "answer": "<p>This seems like an interesting alternative to power analysis for uncontrolled observational and modeling studies. My uneducated guess is that q would equal the number of arrows you have. Although, why would anyone do this as opposed to running a Monte Carlo simulation?</p>", 
                "question": "SEM and sample size (do ALL relationships residuals and error terms qualify as <sq>model parameters that require statistical estimates<sq>?)"
            }, 
            "id": "cy4c21j"
        }, 
        {
            "body": {
                "answer": "<p>You would need to build a model, IMO, because it would depend on so many factors. Primarily, you would want to control for height or position (eg 1 = PG, ..., 5 = C) and distance from the basket. Generally you would assume that taller PFs and Cs will defend best close to the basket, while smaller PGs, SGs and SFs will defend best on the perimeter. You also might expect to find a problematic relationship between offensive and defensive efficiency. Ie the best defenders will get the toughest defensive assignments, reducing their defensive efficiency numbers.  </p>", 
                "question": "Basketball question"
            }, 
            "id": "cxyc0ug"
        }, 
        {
            "body": {
                "answer": "<p>I think the simplest way to think about this is to estimate how much the defensive player usually manages to reduce someone<sq>s scores. To estimate this, you would ideally get a list of who they were guarding in all their previous games and the number of points allowed for each of those players. You could then compare that to the average ppg for each of those players. <br><br>So, for example, if you know that he<sq>s guarding a player that usually gets 10 ppg and he keeps them to 6, the effect of the guard is a 4 point reduction. Averaged over all his match-ups, you should get a good sense for how much he reduces player<sq>s scores. This could also tell you whether he does better at reducing people<sq>s scores when they are a very good player or only when they are a poor player. <br><br>Does that make sense?</p>", 
                "question": "Basketball question"
            }, 
            "id": "cxyxakz"
        }, 
        {
            "body": {
                "answer": "<p>It depends on whether 10 ppg is good or bad, based on who he has defended. Chances are it will be somewhere in between.  <br>Source<colon> I work in basketball. </p>", 
                "question": "Basketball question"
            }, 
            "id": "cxza024"
        }, 
        {
            "body": {
                "answer": "<p>You are correct, and other posters so far are off base.  Using p=0.5 gives a <dq>worst case scenario<dq>, giving the maximum standard error, and therefore maximum (most conservative value<dq> for margin of error.  This is because the standard error *sp=sqrt(p(1-p)/n)* is maximized when p=0.5.<br><br>That does NOT mean that one **always** wants to use  p=0.5, or that we use it because is is agreed upon or that Ho is equality (do we assume it equally likely that someone has lymphoma or does not?).  It might make sense when doing political polls, for example, of one candidate versus another.  However, it would make no sense whatsoever when estimating confidence intervals or margins of error for, say, the lifetime rate of breast cancer in the US, since we know this number to be nowhere near 0.5 (closer to 0.12).  Using 0.5 vs. 0.12 in this case will overstate the margin of error by over 50<percent>, falsely reducing the power of studies and/or increasing the necessary sample size in studies, wasting a lot of funds. </p>", 
                "question": "Why do we use p = 0.5 for proportion estimators?"
            }, 
            "id": "cxq4rzq"
        }, 
        {
            "body": {
                "answer": "<p>I think the reasoning stems from discussions among some of the <sq>great ones<sq> in statistics. I think it<sq>s called <sq><sq>the principle of insufficient reason<sq><sq> which was later renamed as <sq><sq>principle of indifference<sq><sq>. Stigler<sq>s work *the history of statistics* (1986) covers this. There<sq>s a [wiki](https<colon>//en.wikipedia.org/wiki/Principle_of_indifference) page too.</p>", 
                "question": "Why do we use p = 0.5 for proportion estimators?"
            }, 
            "id": "cxqgtuh"
        }, 
        {
            "body": {
                "answer": "<p>Its what we all agree upon. A convention. </p>", 
                "question": "Why do we use p = 0.5 for proportion estimators?"
            }, 
            "id": "cxq2qmd"
        }, 
        {
            "body": {
                "answer": "<p>It<sq>s hard to say without seeing the data. What<sq>s the enrollment split like for the only people who inquired universe? What<sq>s the split for everyone in the data? <br><br>In your <sq>everyone<sq> model, what<sq>s the prediction quality like when you predict only on the <sq>inquired<sq> subset? Is it about the same as when you fit only to the <sq>inquired<sq> data?</p>", 
                "question": "Question on random forest modeling"
            }, 
            "id": "cxlveob"
        }, 
        {
            "body": {
                "answer": "<p>Interesting situation! One thing before offering ideas<colon> both the sample size (i.e., the number of students) and the number of items (i.e., quiz questions) are far too small to run any rigorous analyses.<br><br>That said, I<sq>m not sure if there<sq>s anything you can do to objectively pinpoint cheating. If all students had assigned seating and sat in the same seats every time there was a test (and all students attended each test session), that<sq>d be a start. You<sq>d also need to show longitudinal data to test for outliers within individuals. Problem is that if the student has cheated (or you suspected him of cheating), then that would confound the issue and make it more difficult for <dq>cheating outlier score<dq> to stand out. In other words, that one student<sq>s test scores may be comprised of multiple instances of cheating which, on average, will negate outliers from emerging. <br><br>Long story short<colon> I don<sq>t think there<sq>s anything you can do currently, given the limitations mentioned above. However, keeping track of other data in future classes may help weed out future cheaters.</p>", 
                "question": "Help a clueless teacher catch a cheater"
            }, 
            "id": "cxesgo2"
        }, 
        {
            "body": {
                "answer": "<p>You won<sq>t be able to definitively prove someone cheated, but if you look at the number of *incorrect* answers in common--assuming each incorrect answer is equally unlikely--you can estimate the probability of choosing the same wrong answers at random. However, multiple choice questions often have some answers that are more obviously incorrect than others, so this may not be at all fair to the student. And, again, a series of the same incorrect answers may be highly improbable, but is not proof of anything.<br><br>The second set of questions is trickier. There are obviously more possible answers, but the context is going to narrow it down. If a student can tell they need an article, a preposition, etc., it<sq>s very possible for students to pick the same wrong answers here.</p>", 
                "question": "Help a clueless teacher catch a cheater"
            }, 
            "id": "cxf39y6"
        }, 
        {
            "body": {
                "answer": "<p>What **were** the right answers?  As the others have said, there<sq>s not enough data to provide a truly rigorous answer, but it might be possible to get a rough idea of the <dq>maybe<dq>, as it were.</p>", 
                "question": "Help a clueless teacher catch a cheater"
            }, 
            "id": "cxf6bkb"
        }, 
        {
            "body": {
                "answer": "<p>I want to suggest building a seating chart for future tests that prevents such events from happening. I don<sq>t think this is university, so you should have a small enough class size that this is manageable, no? Maybe involve administration to reserve a second room if need be for space?</p>", 
                "question": "Help a clueless teacher catch a cheater"
            }, 
            "id": "cxf9hx3"
        }, 
        {
            "body": {
                "answer": "<p>You<sq>ve got to partial sort.  Essentially each player has a latent rating and you<sq>re sorting by that, but only need to find the top 20<percent>.  https<colon>//en.m.wikipedia.org/wiki/Partial_sorting</p>", 
                "question": "How can I design a tournament which eliminates the bottom 80<percent>?"
            }, 
            "id": "cxe9jsq"
        }, 
        {
            "body": {
                "answer": "<p>Binomial will give an approximate answer, hypergeometric is actually the correct model. A better question to ask/answer is<colon> How many do I have to pick to have 95<percent> probability of finding *at least* one bad one.<br><br>In Excel to calculate the probability of getting zero ~~bed~~ bad ones when picking a sample of x from a population of 10,000 with 1,000 faulty items... the formula is <br><br>     =hypgeomdist(0,x,1000,10000)<br><br>Vary x until you find that the probability of getting zero is under .05.  In this case the answer is 29, with a probability of 0.0468889.</p>", 
                "question": "Looking for a formula that solves<colon> If 10<percent> of 10000 objects are bad how many do you have to pick out in order to have 95<percent> certainty of finding 1 bad one?"
            }, 
            "id": "cxd9oof"
        }, 
        {
            "body": {
                "answer": "<p>Sounds like you want the binomial probability. </p>", 
                "question": "Looking for a formula that solves<colon> If 10<percent> of 10000 objects are bad how many do you have to pick out in order to have 95<percent> certainty of finding 1 bad one?"
            }, 
            "id": "cxd8hx6"
        }, 
        {
            "body": {
                "answer": "<p>You should make a list of advantages and disadvantages. If you do not fully understand multilevel models, you should NOT use it.<br><br>That said, I would propably use multilevel in your case. But do the above advantage/disadvantage list first!</p>", 
                "question": "Do I need a multilevel/nested model?"
            }, 
            "id": "cxa9taf"
        }, 
        {
            "body": {
                "answer": "<p>I think this is what you mean but just confirming. The point of a nested model is to account for the fact that nesting might create noise, influencing the precision of results and diffusing the signal. You<sq>re not actually interested in differences by class/school, correct? Otherwise if yopu were then these should be fixed factors in your model.<br><br>If not, then you<sq>d be better off with them considered as random factors, which would be best accomplished by a mixed model. I<sq>m assuming your DV isn<sq>t continuous and unbounded, which means you<sq>d want a generalised linear mixed model. But as noted by /u/tasashinae, they are complicated and unless you plant to take some time to learn them not suitable for those without some background first. </p>", 
                "question": "Do I need a multilevel/nested model?"
            }, 
            "id": "cxabvqe"
        }, 
        {
            "body": {
                "answer": "<p>How many is <dq>several<dq>? It sounds like you could just include them as fixed effects in the model (usually using dummy variables). <br></p>", 
                "question": "Do I need a multilevel/nested model?"
            }, 
            "id": "cxai6sm"
        }, 
        {
            "body": {
                "answer": "<p>I would not run analysis on the percentage change data. [This paper](http<colon>//www.ncbi.nlm.nih.gov/pubmed/11459516) gives some reasons why. Heteroskedasticity (aka homogeneity/heterogeneity of variance) won<sq>t be the only problem. A technical point, when you deal with repeated measures ANOVA, homogeneity of variance assumptions are better termed [<sq>sphericity<sq>](https<colon>//en.wikipedia.org/wiki/Mauchly<percent>27s_sphericity_test) - short for sphericity of the covariance matrix. <br><br>I believe your best option would be to run & report descriptives and analyses on the raw data, but then have an illustrative figure displaying the percentage change.<br><br>My trusty [ANOVA textbook](http<colon>//www.amazon.co.uk/Behavioral-Sciences-Researcher-Rudolf-Cardinal/dp/0805855858) suggests that you can correct for violations of sphericity assumption by applying the Huynh-Feldt correction to your degrees of freedom. This should really be done routinely whether you suspect sphericity or not as performing a statistical test (e.g. Mauchley<sq>s) for sphericity and only applying the correction if this reaches significance is not defensible given performance at large or small sample sizes.<br><br>Another way is to reformat your whole statistical model as a MANOVA, but this sounds more complicated that you would require.</p>", 
                "question": "Repeated measures ANOVA for percent change?"
            }, 
            "id": "cxa02ru"
        }, 
        {
            "body": {
                "answer": "<p>1) Why use percent change in the model? Just use the raw cell size data. That<sq>s what ANOVA is designed to tackle and there would be no model assumption issues with this. Then report your effect sizes as percent change as you like.<br><br>2) Yes - tentatively. You can simply show the change in standard deviations. If you prefer, you can calculate cohen<sq>s d on the difference scores, though be aware that the error terms might not be correct. <br><br>3) No. But, but depending on HOW it<sq>s heteroscedastic, you could transform your data. Also, it is only the residuals that need hoogeneity of variance. Show us a plot of the residuals plotted against predicted values from the analysis, a histogram of the residuals, ans QQ plot. <br><br>Lastly - your concerns over messy baseline data are circumvented by use of a mixed linear model. By including a random intercept for subjects, the model accounts these subject based dependent differences, and is able to better fit changes over time (i.e. repeated measures). Worth looking into but more challenging to configure than a standard repeated measures (worth it IMO).</p>", 
                "question": "Repeated measures ANOVA for percent change?"
            }, 
            "id": "cxa5kli"
        }, 
        {
            "body": {
                "answer": "<p>>So does this mean that each hippo has an 80-100<percent> chance of killing a human during its lifetime? 2<percent> chance per year * 40-50 years?<br><br>The correct calculation would be to say that if each hippo has a 2<percent> chance to kill each year, then some hippos will kill multiple people in their lifetimes, and some will not kill at all. IF we assume that hippos can only kill one person per year, then we could use a binomial distribution to describe how many kills each hippo will have over their lifetimes if they all lived 50 years.<br><br>| #Kills  | Probability |<br>|<colon>-----------<colon>|<colon>------------<colon>|<br>| 0 | 0.36416968<br>| 1 | 0.371601714<br>| 2 | 0.185800857<br>| 3 | 0.060669668<br>| 4 | 0.014548339<br>| 5 | 0.002731525<br>| 6 | 0.000418091<br>| 7 | 0.00005363<br><br>and so on up to a maximum of 50, with decreasing probability.<br><br>If we go with the more realistic idea that Hippos could kill more than one person per year, then we should use the Poisson distribution with a rate of 1 kill over a lifetime of 50 years<br><br>| #Kills  | Probability |<br>|<colon>-----------<colon>|<colon>------------<colon>|<br>| 0 | 0.367879441<br>| 1 | 0.367879441<br>| 2 | 0.183939721<br>| 3 | 0.06131324<br>| 4 | 0.01532831<br>| 5 | 0.003065662<br>| 6 | 0.000510944<br>| 7 | 0.000072992<br><br>and so on up to infinity theoretically, but with decreasing probability.  The numbers are pretty similar to the binomial, with around 36-37<percent> of hippos killing no one, and around 6<percent> killing 3 people.</p>", 
                "question": "Hippopotamus Question"
            }, 
            "id": "cx4v3k2"
        }, 
        {
            "body": {
                "answer": "<p>You<sq>re making a very common mistake, and it<sq>s worth mentioning<colon> you can<sq>t add probabilities like this<colon><br><br>>So does this mean that each hippo has an 80-100<percent> chance of killing a human during its lifetime? 2<percent> chance per year * 40-50 years?<br><br>Like you said, you<sq>d end up with a probability greater than 100<percent>, which is nonsensical (I like your interpretation that 200<percent> means the hippo will kill two people. It<sq>s not correct, but it<sq>s creative).<br><br>The way to do it is to take the probability that a hippo *doesn<sq>t* kill in a given year (98<percent>, or .98) and multiply it for as many years as you<sq>re interested in. Because you *can* multiply probabilities when the events are connected by <dq>AND<dq> (no kills in year 1 AND no kills in year 2 AND...). So you get .98^40 = .45, or a 45<percent> chance that a hippo never kills in 40 years. Subtract from 1 if you want the probability that he *does* kill during that time.</p>", 
                "question": "Hippopotamus Question"
            }, 
            "id": "cx53dwo"
        }, 
        {
            "body": {
                "answer": "<p>Well, first of all, the hippo killing link seems to imply (at least to me) that it<sq>s 2,900 deaths just in Africa. Also, there<sq>s nothing saying those deaths are from aggressive behavior or where the hippo attacked someone. They could be accidental too, like if someone is sleeping out in the prairie and inadvertently gets trampled.  <br>Secondly, consider that death toll versus the entire human population. Rather than calculate the probability of a hippo killing, instead consider the probability of dying from a hippo. Granted this is a very rough approach and doesn<sq>t take things into account, but if we assume 10 billion people in the world, that<sq>s a 3-in-10-million chance of dying from a hippo. Even if we just use the world death rate of ~55 million per year, that<sq>s only 0.0055<percent> of all deaths. </p>", 
                "question": "Hippopotamus Question"
            }, 
            "id": "cx4tv0v"
        }, 
        {
            "body": {
                "answer": "<p>> then this means each individual hippo has a 2<percent> chance of killing a human, per year.<br><br>No it doesn<sq>t. 0.02 is the average humans killed per year by each hippo. By your logic, you are saying that if there were 300.000 human deaths because of hippos, the probability of a hippo killing a human is 2, which obviously doesn<sq>t make sense. </p>", 
                "question": "Hippopotamus Question"
            }, 
            "id": "cx4u128"
        }, 
        {
            "body": {
                "answer": "<p>In brief, all p >.05 means is that there is not enough evidence to reject the null. That<sq>s all. We never prove the null true - we either reject it as highly unlikely to be true given the data we<sq>ve collected, or we don<sq>t.<br><br>The p value is a number representing the probability of (in this case) a difference as great as the one between the groups in your sample, assuming the null hypothesis is true in the population.<br><br>With regards to your statement, yes, that<sq>s something you might see in the literature, because the literature typically discusses H1, even though we<sq>re testing H0.<br><br>As for a reference, try [this great piece](http<colon>//ist-socrates.berkeley.edu/~maccoun/PP279_Cohen1.pdf) from Jacob Cohen, one of the elder gods of social science statistics. It might be a bit complicated, but it will greatly help your understanding of p values if you make an effort to work through it.</p>", 
                "question": "Question concerning p-value >.05"
            }, 
            "id": "cx3jof8"
        }, 
        {
            "body": {
                "answer": "<p>Here are [two](http<colon>//stats.stackexchange.com/questions/60670/accept-null-hypothesis-or-fail-to-reject-the-null-hypothesis) decent [answers](http<colon>//stats.stackexchange.com/questions/85903/why-do-statisticians-say-a-non-significant-result-means-you-cant-reject-the-nu) on stackexchange.<br><br>And [from here](https<colon>//liesandstats.wordpress.com/2008/09/08/accept-the-null-hypothesis-or-fail-to-reject-it/)<colon><br><br>> Null hypotheses are never accepted. We either reject them or fail to reject them. The distinction between \u201cacceptance\u201d and \u201cfailure to reject\u201d is best understood in terms of confidence intervals. Failing to reject a hypothesis means a confidence interval contains a value of \u201cno difference\u201d. However, the data may also be consistent with differences of practical importance. Hence, failing to reject H0 does not mean that we have shown that there is no difference (accept H0).<br><br>What I would say in direct response to your last question there is a simple test case to dispel your intuition<colon> you can have large differences in means, but if the sample size is small you will get large p-values. Does this mean that the null is <dq>true<dq>? No. It just means you don<sq>t have enough data to reject the null.</p>", 
                "question": "Question concerning p-value >.05"
            }, 
            "id": "cx3jsah"
        }, 
        {
            "body": {
                "answer": "<p>A p-value over 0.05, or whatever <dq>alpha<dq> you chose, simply means that the data you saw is reasonable enough if the null is true that you don<sq>t see evidence against your null. <br><br>If you use different hypotheses are your null, you may fail to reject many of them. All that means is that there is not sufficient evidence to say that those nulls were wrong. Only one of them can be <dq>true<dq>, but all of them can be <dq>reasonably consistent with the observed data<dq>.<br><br>As stated by /u/stich09 the duality between confidence intervals and hypothesis testing can be useful here. If you have a 95<percent> confidence interval that ranges from [-1.2, 6], then 0 would be in your confidence interval, and you would fail to reject the null that there was no effect. That doesn<sq>t mean you should accept that there is no effect necessarily, as you would also fail to reject if the null was that the effect was -1 or 3, or 4.5 or any other value in between -1 and 6. All that it is saying is that there is not sufficient evidence to say that the those nulls are wrong, they can<sq>t all be the true value.<br><br>In practice, people tend to use the null as the default, or status quo, and make decisions assuming it is true unless there is sufficient evidence to say it is not true. For instance, assuming a drug has no treatment effect unless there is sufficient evidence to say it<sq>s effect is non-zero, etc.</p>", 
                "question": "Question concerning p-value >.05"
            }, 
            "id": "cx3kj3p"
        }, 
        {
            "body": {
                "answer": "<p>I don<sq>t like saying <dq>Group A and B did not differ<dq> because they did differ (otherwise t would be 0). Preferable is <dq>These data provide no evidence for a difference  between Groups A and B, t(22) = -0.65, p = 0.52.<dq> This would be understood  to mean no evidence for a difference in the population. In my view, it is always better to give the p value than a statement of whether or not it is below an arbitrary cutoff. Of course you shouldn<sq>t accept the null hypothesis because it is not the only hypothesis consistent with the data. For example, the hypothesis that the difference is 0.0001 is also likely consistent with the data. </p>", 
                "question": "Question concerning p-value >.05"
            }, 
            "id": "cx3z4hg"
        }, 
        {
            "body": {
                "answer": "<p>Hey there. I actually study metanalysis of observational studies, and yes, you can meta-analyze that data.<br><br>Please see if [this article]( http<colon>//www.biomedcentral.com/1756-0500/5/52) helps you to get the basic formulas. It does not discuss metaregression, but it has the steps for the metanalysis itself detailed.<br><br>I can get R instructions to you later on, if you want.<br><br>EDIT<colon> just read about no variance, sorry. Then no, you<sq>ll need more than just the mean... Raw data is better, but a measure of variance will do. You can always write to authors asking for information if you really have nothing.<br></p>", 
                "question": "Performing a Meta-Analysis with no control or treatment groups."
            }, 
            "id": "cwvktw0"
        }, 
        {
            "body": {
                "answer": "<p>No answer? <colon>-(</p>", 
                "question": "Kruskal-Wallis-Test with small populations"
            }, 
            "id": "cx4fydk"
        }, 
        {
            "body": {
                "answer": "<p>What kind of effect size are you looking for? Any difference between your means is an effect size. You can select EM means under the dialogue box and save your means for different factors. These are adjustment for model parameters and assume equal variances, so you don<sq>t get proper standard errors of the mean for them. Alternatively simply use the Compare Means function under Analyse to find the differences in means between groups and report as such, or calculate proportional differences. If you mean some type of standardised effect size, I think you need to <dq>manually<dq> calculate cohen<sq>s d (simple to do) from means, standard deviations, and n.</p>", 
                "question": "Effect Size in a Mixed Model"
            }, 
            "id": "cwqvo4c"
        }, 
        {
            "body": {
                "answer": "<p>IRT models generally focus on items with right/wrong answers rather than a graded scale like you have here so they probably aren<sq>t the best choice. What issues are you seeing in the ANOVA that you think IRT would solve?</p>", 
                "question": "Item Response Theory<colon> Can I use it on my data?"
            }, 
            "id": "cwnkiwh"
        }, 
        {
            "body": {
                "answer": "<p>If you are posting from a problem set, could you make sure you didn<sq>t miss any words. This seems similar to what I<sq>ve done before, but I think the wording may be off?</p>", 
                "question": "Sample size to establish minimum average of Poisson Variable?"
            }, 
            "id": "cwk9q7b"
        }, 
        {
            "body": {
                "answer": "<p>A random regression model is a mixed-model -- composed of fixed and random effects.<br><br>It is more commonly known as a random coefficients model and you may find more information there.<br><br>To place it into better context, let<sq>s take a growth model where we assume that <dq>growth<dq> can be modeled as linear.<br><br>So we go out and, for each animal, we assign them fixed treatment groups and then we measure body weight once a week for 10 weeks.<br><br>A standard model for this data is a repeated-measures model with AR(1) correlation between observations on a given animal (this is a classic mixed-model).  This view considers that we have 10 treated observations (lets skip the baseline for now).<br><br>It asks whether treatment group affects body weights averaged over all times and subjects in the treatment group. This is the Treatment main effect.<br><br>It asks whether the time point affects body weights averaged over all treatment groups and subjects at the time point.  This is the Time main effect.<br><br>It asks whether the combination of treatment group and time affects body weights averaged over subjects in that combination of treatment and time AFTER removing the effects that may be explicable from the main effects. This is the Time*Treatment interaction.<br><br>In this model, you view the data as a bunch of snapshots in time of each animal -- correlated but otherwise not connected.  A <dq>sample<dq> is 10 possibly-correlated body weight observations on a single animal.<br><br><br>Let<sq>s look at this from a different point of view.  <br>The 10 observations are just a crude representation of the ONE FUNDAMENTAL thing observed -- that animal<sq>s growth curve.  If the growth is linear, I now have two parameters and not 10 -- the body weight (usually at the center time point so this parameter is uncorrelated with the growth rate) and the estimated growth rate.  And it doesn<sq>t matter if I have 10 points or 1000 points.  It doesn<sq>t matter if the times are regular or irregular.<br><br>This is also a mixed-effects model but is specified differently.  And yes, [SPSS can do it](http<colon>//www.spss.ch/upload/1126184451_Linear<percent>20Mixed<percent>20Effects<percent>20Modeling<percent>20in<percent>20SPSS.pdf).<br><br></p>", 
                "question": "Questions about Random Regression"
            }, 
            "id": "cwh8jak"
        }, 
        {
            "body": {
                "answer": "<p>random regression model is not a population averaged model, you are coming up with regression lines for each sampling unit.<br><br>y= (b0+b0i)+(b1+b1i)+e <br><br>a random effects model, in my mind, is an experimental design in which you have certains effects you are wanting to see the variability in, but not directly producing regression lines for each effect.<br><br>In summary, I usually assume random effects models aren<sq>t looking at effects over some continuous covariate, while random regressions are looking at effects over some continuous covariate.<br></p>", 
                "question": "Questions about Random Regression"
            }, 
            "id": "cwgu21c"
        }, 
        {
            "body": {
                "answer": "<p>Your concern is correct; your study is almost exactly the same as [the statistical power example](http<colon>//www.statisticsdonewrong.com/power.html#the-power-of-being-underpowered) I used to discuss this issue on my website. You<sq>re right that you may not have found a difference just because your sample size was too small, and I<sq>m a bit worried about your stats person.<br><br>The type of test you<sq>re looking for is an equivalence test or a non-inferiority test; here<sq>s an [open-access review article explaining how to use them](http<colon>//www.ncbi.nlm.nih.gov/pmc/articles/PMC3019319/). Notice the big heading labeled <dq>No difference does not imply equivalence.<dq><br><br>edit<colon> [here<sq>s another article on non-inferiority](http<colon>//www.trialsjournal.com/content/12/1/106) if that helps</p>", 
                "question": "statistical befuddlement"
            }, 
            "id": "cwfss8t"
        }, 
        {
            "body": {
                "answer": "<p>Some great answers here, just adding my two cents. Inferential statistics do not actually test for differences; they test the probability that differences are due to random variation alone. Your results do not indicate that there is or is not a difference. Instead, it is as you intuitively understood - that the test was unable to distinguish a significant effect but it was probably underpowered. Your idea to consider it a pilot test is wise.</p>", 
                "question": "statistical befuddlement"
            }, 
            "id": "cwg2mzn"
        }, 
        {
            "body": {
                "answer": "<p>Start with the questions you want to answer, not your measures. <br><br>Is your first question something like <dq>Are stress test scores different for this single group of people from pre-test to post-test?<dq>  If so, you could do something like a paired-samples t-test.<br><br>Write out all of your research questions; that will guide your analytic strategy since those are the questions you want to test!</p>", 
                "question": "Having trouble determining appropriate statistical test(s)"
            }, 
            "id": "cwe1aoy"
        }, 
        {
            "body": {
                "answer": "<p>This isn<sq>t my area of expertise but seems interesting. Could you point me to some references for the stuff in your second paragraph?</p>", 
                "question": "response surface methodology?"
            }, 
            "id": "cw9pduo"
        }, 
        {
            "body": {
                "answer": "<p>>I don<sq>t understand how if you have two <dq>independent<dq> vars. you can have a mean / SD for them together, other than if you just add<br><br>I think the problem wants to know what the mean / SD is for the total kickbacks received when the kickbacks are independent. In that case, you can use [the sum of normally distributed variables](https<colon>//en.wikipedia.org/wiki/Sum_of_normally_distributed_random_variables). For aluminum, the kickback for $10,000 will have a mean of $400 and a std. dev. of $100. <br><br>Using that distribution (and the one for steel kickbacks), you can add the means and the variances to get the new normal distribution.<br><br>> I can<sq>t conceptually understand how events like <dq>kickbacks<dq> could be correlated<br><br>The actual mechanism isn<sq>t important for the problem, but I understand the desire for it to make sense. Think of it this way, perhaps. On days where the kickback for aluminum is higher, the steel one is likely to be higher as well because all metals are doing well in the market (perhaps).<br><br>See the same link for how to add correlated normal variables.</p>", 
                "question": "Need Help With Joint Random Variable Problem"
            }, 
            "id": "cwabnc6"
        }, 
        {
            "body": {
                "answer": "<p>Sometimes I wonder why people don<sq>t think before they collect data.<br><br>You wanted to have<colon><br><br>Participant # | tot time | time1stfind | time2ndfind | time3rdfind | finda (1, 2, 3 - turn into indicators) | findb | findc <br><br>And for each participant fill that out.<br><br>Is all you have is three different <percent>s?</p>", 
                "question": "I<sq>m having problems analysing this sort of data who knows what to do? <colon>)"
            }, 
            "id": "cw2vg59"
        }, 
        {
            "body": {
                "answer": "<p>Chi square is used to compare a proportion to an <dq>expected<dq> proportion. If you are simply comparing the proportion of two groups then use fisher<sq>s exact test. <br><br>This is a useful chart<br><br>http<colon>//www.graphpad.com/support/faqid/1790/<br><br>The p value you got suggests that the two groups are not significantly different from each other with regards to proportion with disease. <br><br>There is really no way to establish through statistics if the two groups are <dq>equivalent<dq> or the same. This would require a sample size of infinity. This is a subtle difference than what I wrote above. </p>", 
                "question": "Does Chi Square test tell me if two groups are the same?"
            }, 
            "id": "cw2mtdg"
        }, 
        {
            "body": {
                "answer": "<p>No; in fact it should be obvious you can<sq>t distinguish identical proportions from different but sufficiently close together proportions. At any given sample size, there<sq>s a pair of distinct proportions you can<sq>t tell apart.<br><br>If you can define what would be <dq>close enough<dq> to the same you might be able to do an equivalence test.<br></p>", 
                "question": "Does Chi Square test tell me if two groups are the same?"
            }, 
            "id": "cw2snt7"
        }, 
        {
            "body": {
                "answer": "<p>I feel that this is a classic question. I think that this is largely about how you set up your hypothesis and what is your H0. Could you provide more details about your study? I wonder if there<sq>s another way to set up the hypothesis.</p>", 
                "question": "Does Chi Square test tell me if two groups are the same?"
            }, 
            "id": "cw2r74k"
        }, 
        {
            "body": {
                "answer": "<p>The general subject area goes by the names<colon><br><br>1. Bioequivalence testing<br>1. Equivalence Testing<br>1. Non-Inferiority (Superiority) Testing<br><br>The populations are presumed to be different  and the test rejects if they are not different enough.<br><br>You have to specify how different DIFFERENT must be to decide that they are the same.</p>", 
                "question": "Does Chi Square test tell me if two groups are the same?"
            }, 
            "id": "cw58ri4"
        }, 
        {
            "body": {
                "answer": "<p>I wouldn<sq>t *test* any assumptions. I might make some assessment of some. </p>", 
                "question": "Multiple regression assumptions"
            }, 
            "id": "cw1oyps"
        }, 
        {
            "body": {
                "answer": "<p>Most assumptions have to do with the error term, assuming you want to use ordinary least squares regression. The OLS assumptions are, more or less<colon><br><br>* There is a linear relationship between dependent and independent variables<br>* The expected value of the error terms is 0<br>* The errors are independent<br>* The errors are normally distributed<br>* The errors have constant variance<br><br>You can<sq>t really be sure if these assumptions are violated until after you fit the model, but once you have, you can use residual analysis to check them.</p>", 
                "question": "Multiple regression assumptions"
            }, 
            "id": "cw1wtr5"
        }, 
        {
            "body": {
                "answer": "<p>It depends on what model you<sq>re using</p>", 
                "question": "Multiple regression assumptions"
            }, 
            "id": "cw1ovs1"
        }, 
        {
            "body": {
                "answer": "<p>[deleted]</p>", 
                "question": "Significant mediation without significant c or c<sq>?"
            }, 
            "id": "cvxvgkw"
        }, 
        {
            "body": {
                "answer": "<p>Not sure how to calculate it in Matlab (only ever use R) but look up variance inflation factor (VIF), it<sq>s an index for collinearity</p>", 
                "question": "Multiple regression collinearity problem"
            }, 
            "id": "cvw7hby"
        }, 
        {
            "body": {
                "answer": "<p>I think it<sq>s an interpretational problem not an an analysis  problem. It is not uncommon for a simple slope to be positive but a corresponding partial slope to be negative. The challenge is to figure out what it means. </p>", 
                "question": "Multiple regression collinearity problem"
            }, 
            "id": "cvwzo8p"
        }, 
        {
            "body": {
                "answer": "<p>Can you explain briefly why no. of illness and no. of deaths are not independent? Is it because they can be on the same person?</p>", 
                "question": "Multiple regression collinearity problem"
            }, 
            "id": "cvx5qhb"
        }, 
        {
            "body": {
                "answer": "<p>Have a play with a binomial calculator. The binomial distribution describes the probability of each number of <dq>wins<dq> over a certain number of trials (or games) when there<sq>s a fixed probability of <dq>winning<dq><br><br>http<colon>//stattrek.com/online-calculator/binomial.aspx</p>", 
                "question": "In terms of picking a winner between two teams."
            }, 
            "id": "cvvmcxl"
        }, 
        {
            "body": {
                "answer": "<p>*I<sq>m going to be in Las Vegas for a weekend and would like to know what to expect. <br><br>Thank you.  </p>", 
                "question": "In terms of picking a winner between two teams."
            }, 
            "id": "cvv4ie8"
        }, 
        {
            "body": {
                "answer": "<p>I think I found the answer.    <br><br>So it<sq>s as simple as taking the probability * 3.<br><br>So, the chances of me losing 3 times in a row (assuming overall win rate of 40,45,50,55,60) is<colon><br><br>40<percent> = 21.6<percent> <br><br>45<percent> = 16.63<percent><br><br>50<percent> = 12.5<percent><br><br>55<percent> = 9.11<percent><br><br>60<percent> = 6.4<percent><br><br>If I<sq>m wrong, please correct me! </p>", 
                "question": "In terms of picking a winner between two teams."
            }, 
            "id": "cvv5gt6"
        }, 
        {
            "body": {
                "answer": "<p>Can you past the text? It<sq>s hard to see. </p>", 
                "question": "Help with SPSS syntax for RECODE function"
            }, 
            "id": "cvruhga"
        }, 
        {
            "body": {
                "answer": "<p>Yeah, please paste the text. I<sq>m guessing there<sq>s an unclosed parenthesis, or a stray apostrophe somewhere between lines 59 and 1173.</p>", 
                "question": "Help with SPSS syntax for RECODE function"
            }, 
            "id": "cvs34qj"
        }, 
        {
            "body": {
                "answer": "<p>Sorry.  That is not correct.<br><br>There are two simple regression lines<colon> <br><br>1.  The one that predicts y given x<br>2.  The one that predicts x given y<br><br>They will, in general, **NOT** obey the simple inversion formula.  And that is what the authors are saying.<br><br>Take a very simple dataset<colon><br><br><br>x | y<br>---|---<br>1 | 1<br>2 | 3<br>3 | 3<br><br>Regress x on y and you get the line<colon>  x = 3/4 y + 1/4.<br>This inverts to the line<colon> y = 4/3 x - 1/3<br><br>Regress y on x and you get the line<colon>  y = x + 1/3<br><br>These are two distinct lines!  It matters whether you regress <dq>x on y<dq> or <dq>y on x<dq>.  This does not affect the global <dq>significance<dq> but it does affect the significance of the parameters.  Try this dataset<colon><br><br>X|Y<br>---|---<br>1|\t1<br>2|\t1<br>3|\t155<br>4|\t1<br>5|\t1<br>6|\t1<br><br>A little thought tells you that they must be different because the error is being measured along the direction of the response...not the predicted.  So if you switch them around, the lines almost must change.  The shocking part is what does NOT change.<br><br>A common <dq>third<dq> line is often discussed<colon>  The line of least distance -- that is, the line that minimizes the distance from the line to the data points (the principle components line).  This will fall between the two above.</p>", 
                "question": "Question about data set having <dq>two regressions<dq>"
            }, 
            "id": "cvp9luv"
        }, 
        {
            "body": {
                "answer": "<p>Edit<colon> As Statnoodles rightly points out the inverted formula is not necessarily similar to the one you get from swapping x and y variables in the regression. And i completely misjudged the question. <br><br>TLDR Read statnoodles answer instead.<br><br>deleted mine to reduce confusion.</p>", 
                "question": "Question about data set having <dq>two regressions<dq>"
            }, 
            "id": "cvp66wp"
        }, 
        {
            "body": {
                "answer": "<p>> Is the slope of the other regression line - y predicting X - simply r*SD X / SD Y<br><br>This exactly correct; I suspect several readers have simply misunderstood what you were asking! They<sq>re talking about taking the *equation* with y in terms of x and rewriting it as x in terms of y (which isn<sq>t the regression of x on y, but it<sq>s also NOT what you asked).<br><br>So yes, that<sq>s true. In fact, this is obviously true since the labels <dq>x<dq> and <dq>y<dq> were arbitrary (correlation doesn<sq>t care which is x and which is y).<br><br>Consider *standardized* variates, x* and y*. Then the regression of y* on x* has slope r and the regression of x* on y* has slope r (and both pass through the origin). [Of course if you try to draw them both on the same plot, in the second case your axes are <dq>flipped<dq> and so that line will be drawn with slope 1/r.] <br></p>", 
                "question": "Question about data set having <dq>two regressions<dq>"
            }, 
            "id": "cvpkbee"
        }, 
        {
            "body": {
                "answer": "<p>You should calculate what sample size you need to say something useful. If you<sq>re stuck with a fixed sample size for practical/resource reasons, you should be clear about the limitations. Absence of evidence is not evidence of absence.<br><br>Look up the literature on equivalence trials. In theory proving equivalence requires an infinite sample size, but it is possible to define the smallest difference that would be considered of practical importance and aim to have a good chance of detecting a difference that small.</p>", 
                "question": "Is there a minimum sample size needed to sufficiently argue probabilistic equivalence?"
            }, 
            "id": "cvi1cwu"
        }, 
        {
            "body": {
                "answer": "<p>That<sq>s kind of harsh -  you can dream up a question and fail to find a suitable dataset. The big public datasets are the way to go. I<sq>ve done SEM using the National Longitudinal Survey of Youth (very easy to get), the American Communities Survey (can<sq>t remember, think it was easy), the National Comorbidity Survey (I think that one was easy, not sure now, someone else did it), the California Health Interview Survey (no, don<sq>t even think about it), and maybe some others that I<sq>ve forgotten about.<br><br>Some journals now encourage authors to upload their data with the article. If you can find one that did that, you have a dataset.<br><br> <br><br><br><br></p>", 
                "question": "Where are datasets useful for Structural Equation Modeling?"
            }, 
            "id": "cvdb8qi"
        }, 
        {
            "body": {
                "answer": "<p>I don<sq>t know of anything off hand. It seems like, if you have the <sq>no<sq> locations, you could just create an equal probability of a hit anywhere on the board, and then decrement that at the <sq>no<sq> locations. That might be more of a heuristic way, and not exact.<br><br>I know this isn<sq>t a direct answer, but if the issue is computational time, then you could reduce effort by doing a structured search for the largest vehicle first. Once you find it, a quick probability model for the next hit considering only that vehicle would be fast.<br><br>From there, you could do more grid or use your current method if the sample space is low enough.<br><br>I<sq>m wondering if you could pick the next guess based on the spot that maximizes the amount incompatible locations. That might act as a proxy to probability. It might not give you a hit, but it should reduce the search effort of the compatible problem. If you imagine a small board, the spot that makes everything incompatible must contain a hit, because a ship apparently always exists there.</p>", 
                "question": "How to count the outcomes of five interconnected events"
            }, 
            "id": "cvd0v6d"
        }, 
        {
            "body": {
                "answer": "<p>What if I did something like<colon><br><br>Lowest tier (say tier 4) gets x<br>Next tier higher (tier 3) gets x(1.25) so 25<percent> higher than tier 4<br>Tier 2 gets 25<percent> higher than tier 3<br>Tier 1 gets 25<percent> higher than tier 2<br><br>If I know how many artists are in each tier, I should be able to write it out as a formula where the whole thing equals 100 to get a percentage breakdown, right?</p>", 
                "question": "Formula for revenue distribution"
            }, 
            "id": "cvc2egt"
        }, 
        {
            "body": {
                "answer": "<p>Sure.  You are looking for an upper-bound on the estimate of his Emmy-winning rate.  One possibility would be an estimate of the upper limit on it.  As the sample size goes up without winning, this rate will keep falling towards zero but never be zero.<br><br>The <dq>usual<dq> statistic would be the 95<percent> confidence interval (Clopper-Pearson).  This is the same as the non-inferiority test.  (Google it).<br><br>The second-most <dq>usual<dq> statistic would be the upper-most of one of the Bayes<sq> credible bounds (Google it or see any book or website on Bayes credible regions).<br><br>The simplest non-approximate interval to compute is the weak-evidence, upper-support-bound (relative likelihood of ~3 -- see Edwards<sq> book *Support* or Royall<sq>s book *Statistical Evidence*).<br><br>But in your case, the simplest is the [rule-of-three interval](https<colon>//en.wikipedia.org/wiki/Rule_of_three_\\(statistics\\)) which, for your case of 0/15, is <dq>ratio<dq> <= 3/15 = 1/5 = 0.2 and which, I<sq>ll wager, is rather higher than you expected...but maybe not.  For n=15, the actual value is slightly lower than this (it becomes a **very** accurate approximation for n>=30 but it is pretty accurate for n=15.</p>", 
                "question": "Alternative to ratio when numerator is zero"
            }, 
            "id": "cv9dnvc"
        }, 
        {
            "body": {
                "answer": "<p>Look into the binomial distribution. You could use that (or other means) to put a CI around the probability of winning. </p>", 
                "question": "Alternative to ratio when numerator is zero"
            }, 
            "id": "cv9dk5t"
        }, 
        {
            "body": {
                "answer": "<p>You want a way to capture both the proportion itself and the scale with a single number. But what are you trying to accomplish? Do you want an estimate for a given actor, or do you want to be able to visualize or otherwise distinguish between actors with no/few wins and no/few nominations (less good actors) and with no/few wins and many nominations (probably more good actors)? <br><br>A good visualization (though using two variables, wins and nominations) is a scatterplot (in which you can designate regions (e.g., near the origin, poorer actors, near the Y axis, under-rewarded actors, etc.).<br><br>Another thing to think about is that there is another variable<colon> while a nomination represents a chance to win, you have to be in a show in order to have a chance at a nomination. If being in a show becomes somehow part of the denominator, there are no divide-by-zero problems.<br><br>There<sq>s also Tukey<sq>s folded logs, which helps to deal with the problem of identical proportions with different scales. http<colon>//www.sumsar.net/blog/2013/09/going-to-plot-some-proportions/</p>", 
                "question": "Alternative to ratio when numerator is zero"
            }, 
            "id": "cv9g0g2"
        }, 
        {
            "body": {
                "answer": "<p>1) Creating histogram is actually a viable way for comparing population, especially for population with vastly different characteristics. Still, it is not particularly good when you are doing analysis on large dataset for obvious reason. And using human judgement can be quite inaccurate in reaching any kind of statistical conclusion.<br><br>2) The varying population size is not really a problem. Instead of looking at raw frequency of each rank of category, you can look at the proportion instead. There will be some information lost but that is not the major reason why histogram is not accurate (see point 1).<br><br>3) Standard deviation measures the variability of data from mean. Whether the distribution is flat or not cannot be inferred from it alone. To see this, consider three sets of data<colon> The first set has all the score at 5 and 6 equally; The second set has equal proportion of all scores in 1 to 10; The third set has all the score at 1 and 10. Variance of group 1<group 2<group 3 but group 2 is actually most even. So looking at SD alone does not help.<br><br>4) This will also not work for the same reason in point 3. Also it makes no sense in dividing the SD by population size because by definition **standard deviation is the (square root) of average squared distance of observations from mean.** SD has already standardized for population size so dividing it by population size is just incorrect.<br><br>You may want to look up Rank data as scores are a type of ranking data. Theoretically speaking you can also check the flatness of data by considering the likelihood of observing each population under the assumption that your underlying distributuon is a multinomial with equal probability (e.g. Likelihood ratio test).</p>", 
                "question": "I can<sq>t do this can I?"
            }, 
            "id": "cv6nxeg"
        }, 
        {
            "body": {
                "answer": "<p>What you might want is to use one of the many diversity indices.  A very simple one is the Hirfindahl-Hisrchman Index of concentration. Say, for example you are looking at income.  Divide ALL of your populations into the same 10 income groups, like you are doing for histograms.  Then, calculate this for each population you are looking at<colon><br><br>H= sum([proportion in each group]^2 )<br><br>For example, in county A the proportions in each histogram bin are .1, .2, .5, .2, and 0. In county B the proportions are .2, .2, .2, .2, .2.  H for county A will be (.01 + .04 + .25+ .04 +0) = 0.34. H for county B will be (.04 \u2022 5) = 0.2.  A bigger H means more <dq>concentration<dq> and less <dq>flatness<dq>. If all were in the same category, the limit is 1^2 = 1.  <br><br>Other possibilities are things like Gini indexes, entropy indexes, etc.  There are lots- one of them might fit your needs.</p>", 
                "question": "I can<sq>t do this can I?"
            }, 
            "id": "cv6z35v"
        }, 
        {
            "body": {
                "answer": "<p>Insignificance of the kolmogorov test means normal distribution. Just run a z-test (you can even do it online). SPSS is not bad with - tests, whatever that should mean anyway. </p>", 
                "question": "Which test do I run in SPSS?"
            }, 
            "id": "cv52z2l"
        }, 
        {
            "body": {
                "answer": "<p>What is your research question? You want to see whether temperature is uniform across the heather? Assuming particular distribution of heat and validity of one probe, you want to see whether second probe is valid as well? Or do you have some data and are looking for ways to do something cool with it?<br><br>> If so, what test should I run for a z-test?<br><br>You can run t-tests to compare means. I am more than sure that there is ANOVA design for exactly what you have (two units, but repeated measures). I guess that there is some time-series analysis technique that will see whether series are converging or not.<br><br>> since I have a population (over 50,000 samples)<br><br>What makes population different from sample is not size (number of units), but measurement methodology. You have sliced continuous variable (time) into discrete variable and you have data from this year only (presumably heather was running before that), so what you have is totally a sample. Quite big one, probably not biased one, but still a sample.</p>", 
                "question": "Which test do I run in SPSS?"
            }, 
            "id": "cv540x2"
        }, 
        {
            "body": {
                "answer": "<p>Why choose between those three and not some other?<br><br>Why should you use any skewness coefficient at all?<br></p>", 
                "question": "Which Skewness coefficient should I use and why?"
            }, 
            "id": "cv262i5"
        }, 
        {
            "body": {
                "answer": "<p>You could just use a binomial test where<colon><br><br>    P(Observed(B+D)>=(13+11)|N=(3+13+7+11),Expected((B+D)/N)=0.5)<br><br>In other words, how likely is one to get 24 or more B and D shifts if someone is truly assigning them randomly.<br><br>    > binom.test( 13+11-1, 3+13+7+11, 0.5, alternative=<dq>greater<dq> )<br>    <br>            Exact binomial test<br>    <br>    data<colon>  13 + 11 -1 and 3 + 13 + 7 + 11<br>    number of successes = 23, number of trials = 34, p-value = 0.02881<br>    alternative hypothesis<colon> true probability of success is greater than 0.5<br>    95 percent confidence interval<colon><br>     0.5523891 1.0000000<br>    sample estimates<colon><br>    probability of success<br>                 0.7058824<br><br>The ***p*-value = 0.02881** tells you this extreme an outcome in favour of B and D would only occur ~2.9<percent> of the time if the shifts are fairly assigned.  You can then decide whether that<sq>s a sufficient number to conclude that the assignment isn<sq>t fair.</p>", 
                "question": "Can someone help me determine if the results of my <dq>randomly assigned<dq> shifts are statistically significant?"
            }, 
            "id": "cuzcozq"
        }, 
        {
            "body": {
                "answer": "<p>This is called stratified sampling since you divided the population into groups and took samples within each group. Your computation for the overall point estimate accounting for the known population weights is correct<colon> est_overall = 0.7 * est_men + 0.3 * est_women.<br><br>To obtain standard errors, assuming infinite populations within each stratum, you observe that Var(est_overall) = Var(0.7 * est_men + 0.3 * est_women) = 0.7^2 * Var(est_men) + 0.3^2 * Var(est_women) using independence of the results from your two samples. Then plug in estimates of the variances for each separate sample, e.g. Var(est_men) = est_men * (1-est_men) / n_men, and likewise for women. Then your SE is just the square root of Var(est_overall) and you can compute confidence intervals as usual.</p>", 
                "question": "Confidence intervals around an averaged point estimate"
            }, 
            "id": "cuwe6ar"
        }, 
        {
            "body": {
                "answer": "<p>[deleted]</p>", 
                "question": "Confidence intervals around an averaged point estimate"
            }, 
            "id": "cuwm94p"
        }, 
        {
            "body": {
                "answer": "<p>What are you trying to analyse?<br><br>Multiple regression would be one logical approach. You<sq>d need to find a transformation that made the data approximately linear. You can then fit a function of the form y=F(x<sq>, <other variables of interest>)<br><br>Some useful discussion and links here<colon> http<colon>//www.researchgate.net/post/Why_do_we_do_transformation_before_data_analysis </p>", 
                "question": "Help with normalizing naturally skewed data?"
            }, 
            "id": "cuspc0r"
        }, 
        {
            "body": {
                "answer": "<p>>maybe in units of standard deviation from the mean<br><br>These would be z-values. However, the relative difference between these and the raw values are the same, so converting to z-values would make no difference.<br><br>This skew isn<sq>t specifically a problem, but it depends on your questions. What exactly are you trying to determine? If you<sq>re<br>trying to find differences between (unmentioned) categorical groups, and know that text legnth is a confounding factor, you may wish to include text length as a covariate in the analysis. Same for other regressions. However, as you note you<sq>d probably need transform the data first. Since the skew is positive, you might wish to log-transform the dependent variable first - this may improve linear fit.</p>", 
                "question": "Help with normalizing naturally skewed data?"
            }, 
            "id": "cut0e1c"
        }, 
        {
            "body": {
                "answer": "<p>Hi fellow linguist! <colon>)<br><br>Is there a reason why using text length as a covariate is undesirable in this case? i.e., this simple linear mixed model in R<colon><br><br>     model <- lmer(type.token.ratio ~ var.of.interest * text.length + (var.of.interest * text.length | document), data)<br><br>Then if you get a main effect of your variable of interest, it<sq>s over and above that of text length alone. </p>", 
                "question": "Help with normalizing naturally skewed data?"
            }, 
            "id": "cuttusz"
        }, 
        {
            "body": {
                "answer": "<p>Your data doesn<sq>t need to be normally distributed. Your residuals do. More importantly, they need to have homogeneity of variance between groups. Chances are if your DV is skewed your residuals are too though. From what your data is showing the log-transform will only exaggerate your distribution skew and probably isn<sq>t the right transformation but we<sq>d need to see the results of the residuals first.<br><br>Run the ANOVA and save the residuals. Using SPSS, under <dq>Analysis<dq> select <dq>descriptive statistics<dq> then <dq>explore<dq>, and add the residuals to the dependent variable list and your categorical factors under <dq>factor list<dq>. Under <dq>plots<dq> unselect <dq>stem and leaf<dq> and select histogram. Also select <dq>normality plots with tests<dq> and spread vs. level with levene test, checking <dq>untransformed<dq>.<br><br>This will show you the results for normality tests, levene<sq>s test for homogeneity of variance, histograms, QQ plots, and detrended QQ plots for each factor level as well as box plots for each factor.<br><br>When you<sq>re done post screenshots of the QQ plots, detrended QQ-plots, shapiro wilks and levene<sq>s test results.</p>", 
                "question": "Advice for transforming data to normal distribution?"
            }, 
            "id": "cund7mi"
        }, 
        {
            "body": {
                "answer": "<p>Just so it<sq>s clear, you need to test the residuals, not the DV for normality. BTW modest departures from normality isn<sq>t that critical for mixed procedures (or for ANOVA either). Homogeneity of variance is.<br><br>Test each level of the within measures and between subjects separately. However I<sq>m concerned that you have so many variables. At the end of the day you<sq>re essentially asking for a 4-way interaction effect which is not realistically interpretable IMO.<br><br>[Here<sq>s some info on how to check this properly in SPSS.](https<colon>//www.reddit.com/r/AskStatistics/comments/3j8b2x/advice_for_transforming_data_to_normal/cund7mi)</p>", 
                "question": "mixed design ANOVA normality assumption"
            }, 
            "id": "cundeh5"
        }, 
        {
            "body": {
                "answer": "<p>Pick out a topic that **interests you** and perform **some analysis**, as a recent graduate I assume you<sq>re looking to be in the market to purchase a house soon? I<sq>d suggest maybe testing a few things you think might impact the future price of houses... or the prices of bananas, price of rice in China etc... etc...<br><br>Also as far as data sources<colon> GOOGLE, GOOGLE, GOOGLE, Check government statistical offices etc..., GOOGLE, Check sources of academic literature, GOOGLE oh and GOOGLE<br><br>in saying that... some of the better sources for data IMO<colon><br><br>World<colon><br>**OCED**<br>IMF<br>Worldbank <br><br>USA<br>**data.gov**<br>census.gov<br><br><br>AUS<br>**data.gov.au**<br>census.gov.au<br><br><br>Good-luck! and remember to have fun!<br>  </p>", 
                "question": "Stats projects to fill out a portfolio?"
            }, 
            "id": "cuo1ytx"
        }, 
        {
            "body": {
                "answer": "<p>It<sq>s valid in the sense of comparing proportional differences between two time periods. But it means little about the overall rate. Perhaps 2009 was a particularly low year relative to the past few decades; in fact, 2013 could be a completely average year for cancer deaths, or even low. Additionally, as pointed out by /u/itastelikecaramel, it should probably be scaled by some other measure, such as #deaths per 1000 people, in order to be standardised. There are many other factors that might contribute to this, including changes to the number of oncologists on staff, changes to funding structures for hospitals, etc. By itself the number doesn<sq>t tell us very much.</p>", 
                "question": "Cancer incident and Statistical test question"
            }, 
            "id": "cugi3n7"
        }, 
        {
            "body": {
                "answer": "<p>I<sq>m sure people with more knowledge will comment but just a question - what about population increase/decrease and the deaths relative to that.</p>", 
                "question": "Cancer incident and Statistical test question"
            }, 
            "id": "cuggi36"
        }, 
        {
            "body": {
                "answer": "<p>Usually cancer deaths are scaled by population and corrected for the age distribution of the population.  <br><br>This page describes how to do it<colon><br>https<colon>//www.health.ny.gov/statistics/cancer/registry/age.htm</p>", 
                "question": "Cancer incident and Statistical test question"
            }, 
            "id": "cugl224"
        }, 
        {
            "body": {
                "answer": "<p>Well in a regression~~you<sq>re not comparing means, so it<sq>s not that.~~ You<sq>re saying Beta is 0, which is closer to saying there is no association with x and y. <br><br>edit<colon> see below, i was wrong! that<sq>s what you get for reddit<sq>ing too early.</p>", 
                "question": "In a regression does null hypothesis mean no association or that means are the same?"
            }, 
            "id": "cucp25w"
        }, 
        {
            "body": {
                "answer": "<p>You can define your null hypothesis as you like, either <dq>my null hypothesis is that GDP growth doesnt affect unemployment rate<dq> versus <dq>my null hypothesis is that GDP growth affect unemployment rate<dq><br><br><br>A general null hypothesis to see if a estimated variable is statistically significant means <dq>no association<dq>. Thus if you have a 1.96 value (5<percent> level) you<sq>ll have association with the dependent variable</p>", 
                "question": "In a regression does null hypothesis mean no association or that means are the same?"
            }, 
            "id": "cuegyol"
        }, 
        {
            "body": {
                "answer": "<p>Here is the joint probability table given no information about anyone just leaving. You are eliminating the possibility of a line forming outside, I guess.   <br><br>                B      ~B      Total<br>        A     .04      .16      .2<br>       ~A     .16      .64      .8<br>      Total   .2        .8       1.0<br><br>So, when the person walks out, **we know that either one stall or two stalls are open**, correct? Well then, this eliminates one option<colon> (A and B). Everything else is on the table. So, what we want is P(~B|At least one stall open).  P(at least one stall open)=1-P(A and B)=.96.<br><br>P(~B|At least one stall open)=P(~B and At least one open)/P(at least one open) <br><br>P(~B and At least one open)=P(~B)=.8, so<br><br>.8/.96=.833333.<br><br>------------------------<br><br>Alternatively, <br><br>P(B|At least one stall open)=P(B and At least one open)/P(at least one open) <br><br>P(B and At least one open)=P(B and ~A)=.16, so<br><br>.16/.96=.166666667.<br><br>----------------------------<br><br>> I believe P(W|B) refers to the event where someone has used Stall A and walked out, which should be 0.20<br><br>No, this is the probability someone walked out GIVEN they used B, not AND they walked out. Focus on what the fact that someone just walked out tells you-- they are not both occupied.<br></p>", 
                "question": "Conditional Probability of Bathroom Stall Availability"
            }, 
            "id": "cuayvdx"
        }, 
        {
            "body": {
                "answer": "<p>My take on it is as follows<colon><br><br>At any random time, there Pr(A) = Pr(B) = 0.2. We can divide the bathroom into three states<colon> times when both A and B are open, times when A or B but not both are open and times when both are closed. <br><br>Pr(A and B) is easiest<colon> 0.2 * 0.2 = 0.04<br><br>Pr(A xor B) is a bit more complex but it simply the sum of the probabilities that A (or B) is occupied less the probability that both are<colon> (0.2 - 0.04) + (0.2 - 0.04) = 0.32<br><br>And then Pr(not A and not B) = 1 - 0.32 - 0.04 = 0.64<br><br>At any given time, there is a 0.64 * 1 + 0.32 * 0.5 + 0.04 * 0 = 0.8 probability that B is open (assuming the choice between A and B for everyone else is uniform). <br><br>We take the prior probability of the states (0.64, 0.32, 0.04) and multiply them by the likelihood of that state after seeing someone leave the bathroom we<sq>d have the probability of each state. However, we don<sq>t like the likelihood of seeing someone leave given each state right before entering the bathroom. But we do know that if two people are in the bathroom, we are twice as likely to see someone leave relative to the case where just one person is in the bathroom. Using the proportional likelihoods of (0, 1, 2) and after normalizing the probabilities to one we arrive at the probability of the bathroom being in each state (before the person left) after seeing the walkout is (0, 0.8, 0.2). <br><br>Now, the first case (both A and B were empty) is impossible if someone left. So we can ignore that. <br><br>If it was in the state that A xor B were occupied, then there is Pr(B) = 0 after the walkout. So we can also ignore this. <br><br>If it was in the state that A and B were occupied, there is a 50/50 chance that B is open. <br><br>So Pr(B | W) = 0.2 * 0.5 = 0.1. <br><br></p>", 
                "question": "Conditional Probability of Bathroom Stall Availability"
            }, 
            "id": "cuar0ir"
        }, 
        {
            "body": {
                "answer": "<p>I<sq>m just going to write down my thought process for this one.<br><br>Let<sq>s call the dice, in order of highest to smallest<colon> A, B, and C. The pmf needs to answer P(A,B). To get that, we basically consider all possible values of C that give us A and B, along with how many different ways we could have rolled A, B, and C. Let<sq>s say that A and B and 5 and 3. So we want P(5, 3).<br><br>C could be 1, 2, or 3. The probability of rolling a 5, a 3, and a (1, 2, or 3) is given by the probabilities of those independent events times the number of different ways of arranging those events. We can just shortcut that by realizing we want any one of the three independent scenarios<colon><br><br>    5, 3, 1<br>    5, 3, 2<br>    5, 3, 3<br><br>We can get the probability of each of those from a [multinomial pmf](https<colon>//en.wikipedia.org/wiki/Multinomial_distribution). We get to simplify the multinomial pmf, because no matter what, getting any 3 dice rolls in one particular order has a chance of (1/6)^3.<br><br>That gives us the following solution<colon><br><br>    P(5,3) = Sum (1/6)^3 * 3! / (n_5! * n_3! * n_i!)  for i in [1,2,3]<br>    P(5,3) = (1/6)^3 * 3! * Sum 1 / (n_5! * n_3! * n_i!)  for i in [1,2,3]<br>    P(5,3) = 1/36 * Sum 1 / (n_5! * n_3! * n_i!)  for i in [1,2,3]<br><br>When i is 3 in that sum, you can<sq>t have n_i, you have to make n_3 equal 2. There isn<sq>t really a good closed solution to that, but it<sq>s really easy for a computer to do. Here<sq>s the best closed form I could come up with<colon><br><br>    P(A, B) = 1/36 * ( 1 / (2 if A == B else 1)! * (B-1) + 1 / (3 if A == B else 2)!)<br><br>Kinda messy, so maybe someone can fix it up. I tested it against a Monte Carlo in Python and I am getting the same answers, though, so that<sq>s a plus.<br><br>EDIT<colon> <br><br>Here<sq>s a python function that calculates that probability<colon><br><br>    from scipy import misc<br>    <br>    def prob(A,B)<colon><br>        a = [1,2][A == B]<br>        p = (1/misc.factorial(a,exact=1)*(B-1) + 1/misc.factorial(a+1,exact=1))<br>        p *= 1/36<br>        return p<br><br>You could also just loop over each value of C (1 through B inclusive) and get it the more explicit way. These are the analytical answers I got, compared to 500,000 monte carlo runs<colon><br><br>      Roll   Eqn   Monte<br>     (1, 1) 0.0046 0.0045<br>     (2, 1) 0.0139 0.0138<br>     (2, 2) 0.0185 0.0188<br>     (3, 1) 0.0139 0.0139<br>     (3, 2) 0.0417 0.0417<br>     (3, 3) 0.0324 0.0322<br>     (4, 1) 0.0139 0.0136<br>     (4, 2) 0.0417 0.0417<br>     (4, 3) 0.0694 0.0694<br>     (4, 4) 0.0463 0.0464<br>     (5, 1) 0.0139 0.0141<br>     (5, 2) 0.0417 0.0413<br>     (5, 3) 0.0694 0.0704<br>     (5, 4) 0.0972 0.0969<br>     (5, 5) 0.0602 0.0604<br>     (6, 1) 0.0139 0.0139<br>     (6, 2) 0.0417 0.0416<br>     (6, 3) 0.0694 0.0700<br>     (6, 4) 0.0972 0.0971<br>     (6, 5) 0.1250 0.1239<br>     (6, 6) 0.0741 0.0745</p>", 
                "question": "pmf of 3D6 (discard lowest dice)?"
            }, 
            "id": "cu7tq43"
        }, 
        {
            "body": {
                "answer": "<p>Sounds like you have two independent variables to me - seeing the item or not (i.e. control or tested - two levels) and training (3 levels). You<sq>d look for an interaction between these in a repeated measures. You could use no training as a covariate, but you need to determine what question you are asking first. If you want to simply understand if there is a difference between trained groups, not if there is a change with training, then you could use no train as a covariate. If you want to determine if the DV changes with different levels of training, then you would keep it as a separate level (3 level repeated measures). Sounds like the latter to me.</p>", 
                "question": "Comparing change from baseline in experiment w/ only 1 factor"
            }, 
            "id": "cu7cqno"
        }, 
        {
            "body": {
                "answer": "<p>So if I<sq>m understanding this correctly, you have an encoding/training phase and a test phase.  You won<sq>t be analyzing response time for each item would you?  You would be analyzing mean response time of all tested items?  If that<sq>s so it is an independent anova and will be set up differently in spss.  There would be three columns<colon> participant I<sq>d (don<sq>t have to but I suggest it), training, and response time.   Each participant has one measurement per row...because it isn<sq>t repeated measures if you don<sq>t have multiple measurements per participant.  You just set labels (blanking on the actual term) as 0 for control, 1 for training 1, and 2 for training 2.  As somewhat of an aside, independent analyses have less statistical power so keep sample size in mind.</p>", 
                "question": "Comparing change from baseline in experiment w/ only 1 factor"
            }, 
            "id": "cu7lyrm"
        }, 
        {
            "body": {
                "answer": "<p>* Social incentives to over- and under-report. Men are told that they<sq>re supposed to be up for it all the time, women that they<sq>re supposed to be virginal.<br><br>* Homosexual encounters. More women identify as gay or bisexual than men, but there may be more men who have sex with men (without necessarily identifying as gay or bisexual).<br><br>* Outliers not being captured by the data. Sex-workers will have a much higher average than the general population but may be less likely to be included in such surveys and, if they are, they may not report professional encounters as sexual encounters.</p>", 
                "question": "How is it that the average man has more sexual partners than women?"
            }, 
            "id": "cu28hvb"
        }, 
        {
            "body": {
                "answer": "<p>I unfortunately don<sq>t have a reference, but there was one paper that claimed to have found the majority cause for this<colon> A difference in the way of finding the number of partners.  Women will count the partners one by one, which  underestimate or break even.  Men will take a guess, which tends to overestimate.</p>", 
                "question": "How is it that the average man has more sexual partners than women?"
            }, 
            "id": "cu29280"
        }, 
        {
            "body": {
                "answer": "<p>I think your math is wrong, and you don<sq>t even have to dip into reporting inconsistencies to show it. Consider a situation like the following<colon><br><br>https<colon>//imgur.com/NVTKBmt<br><br>Let<sq>s look at the degree distributions (number of partners per person)<br><br>For the red circles, the number of partners looks like [1, 1, 1, 1, 8].<br>For the green circles, the number of partners looks like [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1].<br><br>In general, the SUMS of these lists will always be equal (due to the symmetry argument you make) but there<sq>s no reason that their means have to be equal, or any other distribution statistics.<br><br>Clearly, the mean number of partners for the red circles is 2.4.<br><br>The mean number of partners for green circles is 1.0.<br><br>You can construct arbitrary examples where the means are different because Euler<sq>s Formula for graphs has a <dq>free parameter<dq> in the number of faces.</p>", 
                "question": "How is it that the average man has more sexual partners than women?"
            }, 
            "id": "cu2xpdk"
        }, 
        {
            "body": {
                "answer": "<p>You are absolutely correct. A new partner by definition requires +1 for both the male and female. It<sq>s not so much an experimental failure as it is a cognitive failure by the researchers. Statistics are qualitative not quantitative and they have confused calculated averages as experimental measurements. Surveys rely on faith in the responder since their is no way to verify the data. </p>", 
                "question": "How is it that the average man has more sexual partners than women?"
            }, 
            "id": "cu307id"
        }, 
        {
            "body": {
                "answer": "<p>I<sq>m not entirely sure how to solve this problem. However, simply using the percentages you listed and assuming you buy 50 packs (250 cards), you can assume you will get the following cards<colon><br>3 Legendary (out of 22) <br>12 Epic (out of 30)<br>57 Rare (out of 39)<br>179 common (out of 39)<br><br>I<sq>m not sure exactly how to take into account duplicates on this next part but just looking at the numbers above logically, it would mean having probably all of the common cards, half of the rare cards, close to a third of the epic cards and a small percentage of the legendary cards. Hope that helps!<br></p>", 
                "question": "Question about Hearthstone game"
            }, 
            "id": "ctz94jg"
        }, 
        {
            "body": {
                "answer": "<p>This question doesn<sq>t make a ton of sense in my head, but I just got back from the International and am a bit fuzzy.<br><br>It would be easier to answer a question like <dq>What is the probability that I get at least half the deck<dq> It<sq>s entirely possible to only get one card albeit this is a very rare. YOu can get anywhere from 1 to 250 distinct cards.</p>", 
                "question": "Question about Hearthstone game"
            }, 
            "id": "ctzga3u"
        }, 
        {
            "body": {
                "answer": "<p>The thing you want to think about is if your data/distribution is symmetric.  What does the SD measure?  It<sq>s the square root of the variance, which is the average of the squared distance of each data point to the mean.  What does the MAD measure?  It<sq>s the mean of the distance of each data point to the mean.<br><br>Notice that each of these measures of distance of data to the mean (deviations) uses the mean.  And the mean is a good measure of center when the distribution is symmetric, but it gets pulled towards the tail of a non-symmetric distribution.<br><br>You<sq>ve got a non-normal distribution.  Is it symmetric or asymmetric?  The variation of the data (aka measure of spread) needs to be measured with respect to some value that is about at the middle (aka measure of center).  When the data is symmetric, the measure of center/measure of spread combination must be appropriate for symmetric data, and the mean and SD or MAD work fine.  When the data is not symmetric, the mean is not a good measure of center so you want to use a different measure of center/measure of spread combination.  The most commonly used one is median and inter-quartile range.  <br><br>Also note that whatever you<sq>re using as the measure of center is what you need to use when you<sq>re calculating the measure of spread.  So if you use the median as the measure of center, you can<sq>t use SD or MAD as the measure of spread because those formulas have the mean in them, not the median.</p>", 
                "question": "Standard deviation vs. mean absolute deviation with non-normal distributions"
            }, 
            "id": "ctygimo"
        }, 
        {
            "body": {
                "answer": "<p>I generally throw caution to the wind and ignore the voice in my head that tells me to be 100<percent> statistically correct at all times.  I don<sq>t give coefficients or p-values or anything.  It<sq>s usually just <dq>An increase in X will result in Y increase in Z, independent of A, B, and C<dq>.  That<sq>s it.  <br><br>Sometimes I<sq>ll throw in <dq>the model explains X<percent> of the variation in Y<dq> (R^(2)) but usually I just rely on their trust that I know what I<sq>m doing.  People generally don<sq>t care about the details, they want the results.  They want actionable information.  It<sq>s my job to worry about the details and make sure the information I give is actually correct.<br><br>One thing I<sq>ve found very helpful is trying to find an example in the data that shows it doing exactly what your model says it<sq>s doing.  So you show them an instance where X went up by 5 units and at the same time Y went down by 3 units, while A, B, C were relatively constant.  This isn<sq>t always possible, but it really helps if you can find such an example.</p>", 
                "question": "Do you know a clever way to return the result of a regression ?"
            }, 
            "id": "cttkssa"
        }, 
        {
            "body": {
                "answer": "<p>Lovely post! My contribution will be a very non specific one. <br><br>I think that when communicating results to the general public one should give clear cut interpretations of the effects of X on Y. Concrete examples are almost always welcome (which works for both lin. and log. models).  <br><br>Try to communicate what conclusions *can*, and perhaps more importantly, *cannot* be drawn, and the implications.<br><br>I prefer intervals over point estimates since the effects are (more or less) sophisticated guesses. Make a real effort to cut the technical lingo and don<sq>t flood the report with numbers (if possible).<br><br></p>", 
                "question": "Do you know a clever way to return the result of a regression ?"
            }, 
            "id": "cttkztl"
        }, 
        {
            "body": {
                "answer": "<p>>Question<colon> Are my claims valid? Any thoughts on this?<br><br>>a) Method1 shows a bias in one direction,whereas Method2 doesn<sq>t.<br><br>Correct. Method 1 also shows much less variance.<br><br>>b) They cannot claim equal variance, because the methods/instruments are different.<br><br>They cannot claim equal variance because the variance is quite obviously unequal. Should<sq>ve used a non-parametric test.<br><br>>c) They cannot use the absolute values. They should use the actual values.<br><br>Exactly. Otherwise you miss the bias and variance in the measurements.</p>", 
                "question": "Two sample T-test<colon> Question on using absolute/actual values"
            }, 
            "id": "ctqhz6n"
        }, 
        {
            "body": {
                "answer": "<p>> Am I correct in thinking the Wilcoxon Signed Rank Test is the best option for these data?<br>> Our N is quite large (500+)<br><br>Since you<sq>ve got a large sample size, you should use the large sample approximation of the test.  It<sq>s a statistic that is calculated using W and has a N(0,1) distribution.  [here](http<colon>//courses.wcupa.edu/rbove/Berenson/CD-ROM<percent>20Topics/topice-10_5.pdf) and [here](https<colon>//onlinecourses.science.psu.edu/stat414/node/319) are a couple of links that discuss it.  Scroll down to find a description of the large sample approximation.<br><br>Note that the link that you supplied to the page with direction for excel has the large sample approximation on it.<br><br>> does Excel have the puff to be able to handle it or will I need specialist statistical software?<br><br>If you<sq>re good at excel, you could probably either make a macro yourself or do a search for one.  Otherwise I<sq>d suggest using R.</p>", 
                "question": "Questions About Wilcoxon Signed-Rank Test w/ Large N"
            }, 
            "id": "ctqc1v3"
        }, 
        {
            "body": {
                "answer": "<p>Power calculator is the term to Google. The sample size depends on what sort of <dq>not random<dq> and how big of an effect you care about.</p>", 
                "question": "Determining the required sample size for a test of randomness"
            }, 
            "id": "ctow52m"
        }, 
        {
            "body": {
                "answer": "<p>>   be 90<percent> confident that my sample size is sufficiently significant<br><br>Please don<sq>t use any statistical jargon; you<sq>re using it wrong and it makes it impossible to understand what you actually want. <br><br>It<sq>s like walking into emergency and saying <dq>I<sq>ll be needing 15 quarts of sutures and a fully-charged crash-cart *stat*<dq>.... it<sq>s kind of got some jargon words in there, a few of which might kind of relate to what you want, but it<sq>s pretty much nonsense. If you said <dq>I have a really bad cut ...<dq>, you<sq>d probably get what you want faster.<br><br>Where does the RNG come from? What purposes do you want to use it for? What do you mean by <sq>test<sq> exactly and why do you think that will solve the underlying problem?<br><br>There are some standard suites of RNG tests, but unless you<sq>re building a RNG into a language you may not quite need that level of effort.</p>", 
                "question": "Determining the required sample size for a test of randomness"
            }, 
            "id": "ctoyfwb"
        }, 
        {
            "body": {
                "answer": "<p>I suppose you are doing a hypothesis testing. If it is not then disregard what I said below.<br><br>Now, I see that you are confusing a few things here. There is no such thing as <sq>required sample size for the test to be significant<sq>. Sample size governs the standard error of your test and thus the width of your confidence interval. So regardless what sample size you choose (of course larger than 2), you can still reach conclusion whether the result is signiciant or not.<br><br>The next step is to transform what you hypothesize into an actual statistical statement. If we define the event of favorable outcome (i.e. 3 lands out of 7 opening hand) as A. Then you can calculate the exact probability P(A) under a fair system (note that an unfair system can still be random, e.g. P(A)=0.99 is still a random system, albeit unfair). After that you test with the null hypothesis that the system is fair, i.e. P(A)=0.2836 and alternative being P(A)!=0.2836.<br><br>Next step is collect data. Simply record the number of times A happen and mark it as 1 and 0 otherwise. With N trials the data follows a binomial distribution with parameters N and p=P(A) (Assuming independence between each trial, which I suppose is fair assumption, if not then you need to assume other model). <br><br>At this point you may realize the thing you are testing is no different then testing the probability of flipping a coin being actually p (in the case of fair coin, H0<colon> p=0.5). You can do a normal approximation for binomial distribution who has mean as Np and variance Np(1-p) or you can do an exact calculation. The confidence level is determined by you which will affect the width of confidence interval, whether you want it to be 0.9, 0.95, 0.99 or something else.<br><br>The more sample you collect, the smaller the interval becomes and you can be more confident about the result. Remember that failure to reject the null does not prove the system is fair.</p>", 
                "question": "Determining the required sample size for a test of randomness"
            }, 
            "id": "ctpnkpc"
        }, 
        {
            "body": {
                "answer": "<p>The Mann-Whitney U test is an alternative name for the Wilcoxon rank-sum test.  This is not the same test as the Wilcoxon signed-rank test. <br><br>    Mann-Whitney U test = Wilcoxon two-sample test = Wilcoxon rank-sum test =/= Wilcoxon signed-rank test<br><br>* Wilcoxon Two-Sample Test aka Mann-Whitney U test aka Wilcoxon two-sample test<colon> non-parametric test to see if two independent samples come from the same distribution (or if their distributions have the same median).  You can think of this as the non-parametric version of the two-sample t-test.  This is a good test to use as an alternative to the t-test since it does not need to satisfy the assumption that the samples were drawn from a normal distribution.<br><br>* Wilcoxon signed-rank test<colon> non-parametric test of paired/correlated samples (e.g. repeated measures, before-and-after measures, matched pairs) to see if the difference between the pairs is centered around 0.  </p>", 
                "question": "Nomenclature<colon> Wilcoxian Two-Sample Test / Mann-Whitney U-test?"
            }, 
            "id": "ctmk2ja"
        }, 
        {
            "body": {
                "answer": "<p>It<sq>s the same as a Mann-Whitney U test; Wilcoxon<sq>s original paper did both a one- and two-sample rank test; the original Wilcoxon rank-sum test is a Mann-Whitney U simply shifted by a constant (though in a later paper with tables -- still pre Mann&Whitney -- the statistic was shifted and was then effectively the same as U)<br><br>(Once you standardize it, of course it doesn<sq>t matter which of the various definitions are used.)</p>", 
                "question": "Nomenclature<colon> Wilcoxian Two-Sample Test / Mann-Whitney U-test?"
            }, 
            "id": "ctnnkdo"
        }, 
        {
            "body": {
                "answer": "<p>A weighted fit seems like a reasonable solution to your problem. Is deviation from the model equally  at all points on the curve (measured in degrees)? <br><br>An ad-hoc idea is to divide the curve into segments and weight the points in each segment by the inverse of the number of points in that segment. If some areas on the curve are more or less important, this also gives you a mechanisms to express that by over or underweighting those sectors on a relative basis.</p>", 
                "question": "Fitting a curve to measurements of varying density"
            }, 
            "id": "ctlyq6z"
        }, 
        {
            "body": {
                "answer": "<p>I would think a logistic regression, with 1 predictor variable (original price) and a binary outcome variable (sold/unsold).  I<sq>m hoping these are all the same type of item?</p>", 
                "question": "how can i best show this statistically?"
            }, 
            "id": "cti6a4j"
        }, 
        {
            "body": {
                "answer": "<p>also bonus question!<br><br>Is there anyway to show how setting a high original price will increase profitability? like it has to some how factor in things not selling lol</p>", 
                "question": "how can i best show this statistically?"
            }, 
            "id": "cti6gti"
        }, 
        {
            "body": {
                "answer": "<p>You must be interviewing. I recognize this question. </p>", 
                "question": "how can i best show this statistically?"
            }, 
            "id": "ctikxza"
        }, 
        {
            "body": {
                "answer": "<p><br>>  Why can<sq>t I use 3 separate paired t-tests instead to compare P-LO, P-HI, and LO-HI separately? <br><br>Who said you can<sq>t. You certainly can, though of course the chances of making at least one type I error if the drug is not effective at the low dose or if a higher dose makes no difference (or both) will be higher. Correspondingly, if you reduce your significance level to control the type I error rate across your three tests, the power to find differences will tend to be lower. <br><br>> this is likely to be the first in a series of questions about stats in general and statistical testing.<br><br>You may find a lot of your questions already answered at stats.stackexchange.com (well, they may already be answered here too, but harder to find).<br><br></p>", 
                "question": "ANOVAs and t-tests - why use one over the other?"
            }, 
            "id": "cte2xxr"
        }, 
        {
            "body": {
                "answer": "<p>I will try to take a stab at this and anyone else please correct me if I am wrong. <br><br>An ANOVA or an analysis of variance is a statistical test that compares the means of several groups to determine if they are equal. Generalizing the t-test to more than two groups. You could run multiple t-tests to compare all three groups but you would have to run around 6 t-tests for all comparisons, increasing your likelihood to commit a type I error (rejecting the null hypothesis when the null hypothesis is true). </p>", 
                "question": "ANOVAs and t-tests - why use one over the other?"
            }, 
            "id": "cte1v2a"
        }, 
        {
            "body": {
                "answer": "<p>Seems like most of the main stuff is covered, but just wanted to add a little bit to flush out some of the ideas<colon> <br><br>- If you run multiple t-tests, in addition to increasing error, you are essentially underutilizing your data.  The first t-test doesn<sq>t know anything about the second or third. A RM ANOVA builds the fact that the same group is getting the same treatment multiple times into the design and giving you results more matched to your full data as a result<br><br>- One thing I don<sq>t believe anyone has mentioned is that ANOVA and t-tests have different assumptions. My recollection is that the latter is more robust (has fewer assumptions) than the former.  Amongst other assumptions, RM ANOVA requires sphericity of the data (as I understand it, equal variances); if sphericity is violated, you need to correct df.   </p>", 
                "question": "ANOVAs and t-tests - why use one over the other?"
            }, 
            "id": "ctgy6mr"
        }, 
        {
            "body": {
                "answer": "<p>Having each participant act as their own control sounds like a single-subject design. In addition to the suggestion /u/normee made, you could try an ABA design, where you measure visual acuity a third time with the glasses off again. Then you would have an idea of how much improvement was time-based (measurement 1 vs. measurement 3) and how much was due to the glasses (measurement 2 vs. 1 and 3). </p>", 
                "question": "What kind of study design is this?"
            }, 
            "id": "ctdws8k"
        }, 
        {
            "body": {
                "answer": "<p>If you have one participant, then /u/adlalking is right. This would be considered a single subject design, or n=1 design. An A-B-A design as he suggested would be good, or an A-B-A-B design could also be used to increase the validity of the study, where the A phases are the baselines (no glasses) and the B phases are the treatment (glasses). If the level of visual acuity returns to the baseline level during the second A phase, then this looks very good, and even better if the visual acuity increases again during the second B phase. So you would want the results to look something like [this] (http<colon>//www.csus.edu/indiv/w/wickelgren/psyc008/ABAB.gif).</p>", 
                "question": "What kind of study design is this?"
            }, 
            "id": "ctebnmt"
        }, 
        {
            "body": {
                "answer": "<p>I asked a very similar question here, hopefully it helps!<br><br>https<colon>//www.reddit.com/r/AskStatistics/comments/3c1xeh/how_to_interpret_interaction_between_2/</p>", 
                "question": "Interactions for dummy variables?"
            }, 
            "id": "ctc4tw1"
        }, 
        {
            "body": {
                "answer": "<p>>Because what I want to know is, are bisexual females, more likely than bisexual males to be the victims of sexual abuse and so on. Would the interaction terms be interpretable that way,<br><br>Sounds like you<sq>re dummy coding (reference group departure) instead of effect coding (grand mean departure). So, yes and no--you can say that BSF have 4x the risk of reference group and BSM have 2x the risk of reference group and so on, or whatever the real numbers are, but your betas will not directly compare two groups together when neither is the reference group. More generally, you might check out Agresti<sq>s *Categorical Data Analysis* for more in depth description of how to treat these data, including various approaches for looking at risk. </p>", 
                "question": "Interactions for dummy variables?"
            }, 
            "id": "ctcee08"
        }, 
        {
            "body": {
                "answer": "<p>FYI<colon> You can use backslashes to escape characters in reddit<sq>s markdown.<br><br>Example<colon> \\\\\\* renders as \\*</p>", 
                "question": "Interactions for dummy variables?"
            }, 
            "id": "ctcyi4s"
        }, 
        {
            "body": {
                "answer": "<p>[deleted]</p>", 
                "question": "How would you statistics folks advise going about calculating the number of Esperanto-speakers in the world?"
            }, 
            "id": "ct85c7w"
        }, 
        {
            "body": {
                "answer": "<p>Assuming that the elements of dist_A and dist_E are correctly paired, yes. In other words, the first element of dist_A and dist_E should each correspond to the first sample from the joint posterior, the second elements to the second sample, etc.<br><br>This ought to be the case (i.e. there<sq>s no reason their orders should have been scrambled), but it<sq>s worth mentioning to make sure.</p>", 
                "question": "Computing posterior X_e - X_a if we know the posterior distribution for X_e and X_a?"
            }, 
            "id": "ct7jd2p"
        }, 
        {
            "body": {
                "answer": "<p>In one sense, they<sq>re *not* different. If you know probability, you can calculate odds -- and vice versa. So you<sq>re totally right, they<sq>re two different representations of the same concept. Each number is useful for different applications. Probability was always a more natural concept for me. Odds come up a lot in gambling, obviously, but pops up in other contexts too. </p>", 
                "question": "I understand the difference in how probability and odds are calculated. But what is the difference conceptually?"
            }, 
            "id": "ct6pv2s"
        }, 
        {
            "body": {
                "answer": "<p>I always think of probability as a percentage (E.G. 25<percent> is 1/4) where as odds are a comparison of how likely an event is to happen vs. the event not to happen. (E.G. 1 vs 3). Always keeps it straight in my head.</p>", 
                "question": "I understand the difference in how probability and odds are calculated. But what is the difference conceptually?"
            }, 
            "id": "ct70gk8"
        }, 
        {
            "body": {
                "answer": "<p>They<sq>re not really different but non-statisticians find odds easier to understand than probabilities. </p>", 
                "question": "I understand the difference in how probability and odds are calculated. But what is the difference conceptually?"
            }, 
            "id": "ct72u4q"
        }, 
        {
            "body": {
                "answer": "<p>The only way in which they are different is that 0 and 1 are probabilities but you cannot make proper odds of those.</p>", 
                "question": "I understand the difference in how probability and odds are calculated. But what is the difference conceptually?"
            }, 
            "id": "ct73ocd"
        }, 
        {
            "body": {
                "answer": "<p>Using bounds from the [Coupon collector<sq>s problem](https<colon>//en.wikipedia.org/wiki/Coupon_collector<percent>27s_problem#Extensions_and_generalizations), the number will be something less than 29.93e9  (since that would be the bound if there were a large number of each item).<br><br>I don<sq>t expect it will necessarily be very much less though.<br></p>", 
                "question": "Minimum sample size to fully sample large population (without replacement)"
            }, 
            "id": "ct1topv"
        }, 
        {
            "body": {
                "answer": "<p>Well, assuming that you tested them all in the same room then the external conditions should be homogeneous across all specimens within the same day. <br><br>Typically you<sq>d want to start with a linear reg model since it is so simple and provides an interesting starting grounds for non-parametric testing (if you so choose). <br><br>I don<sq>t believe you should use a random effects model. All you should need is an indicator for each day. So all those plants tested on day 1 have the day1 flag set to 1, and the rest set to 0, day 2 has day 1 set to 0, day2 set to 1, and the rest set to 0, so on and so forth. <br><br>This should capture the variances between testing days that should be constant across all specimens within the same day.<br><br>Good luck!<br><br>edit<colon> Also, you mention adding a differing slope, that would mean that you interact the day indicator with the variable of interest, suggesting that the efficacy of the chemical differs with the conditions of the external environment. By only adding an binary to the equation (without interaction) then you<sq>re implying that the marginal effect of the chemical is constant but that there is an inherent and constant change to the outcome based on the environmental conditions. You need to decide which makes more plausible sense and move forward from there. I can<sq>t comment on which is correct because I am unfamiliar with the literature and the overall experiment. Consult your literature for more information. But at face value, I would not interact them.</p>", 
                "question": "random effects model vs linear regression with factor variable"
            }, 
            "id": "csw87vv"
        }, 
        {
            "body": {
                "answer": "<p>[deleted]</p>", 
                "question": "random effects model vs linear regression with factor variable"
            }, 
            "id": "csw8ai2"
        }, 
        {
            "body": {
                "answer": "<p>Ecological statistician checking in. Where is your advisor and why are they letting you even contemplate this? There are a number of problems with this approach as u/normee has appropriately pointed out.<br><br>I would only add that what you are doing is not resource selection. This is a well established sub-discipline and there are appropriate study design and statistical models to answer your real question<colon> What kind of sites to turtle prefer for nesting? See [Manly and company, Resource Selection by Animals](https<colon>//books.google.com/books/about/Resource_Selection_by_Animals.html?id=hNy8aM8HmrwC)<br><br>This comes down to doing a sample of the environment and cataloging the availability of different kinds of sites and comparing that to the utlization of these different kinds of sites.<br><br><br><br>I read the other thread were you explain a little bit about your study design. </p>", 
                "question": "How to make a for loop to find interactions between several variables in R?"
            }, 
            "id": "csrefm1"
        }, 
        {
            "body": {
                "answer": "<p>Do you want all of the interactions in a single glm, or each interaction in a separate glm?<br><br>If the former try<colon><br><br><br>    glm(selected ~(south + sqrt(sand))^2, ...)<br><br>If the latter the easiest way I can think of is to use the form <br><br>    glm(y ~ x + x[,i]*x[,j], ...) <br><br>where x is a matrix of covariates.<br></p>", 
                "question": "How to make a for loop to find interactions between several variables in R?"
            }, 
            "id": "csrb0k6"
        }, 
        {
            "body": {
                "answer": "<p>It seems to me that you<sq>re effectively doing subset selection / regularization to find the best model. There are packages in R that do this automatically without using a for loop.<br><br>Check out chapter 6 from _Introduction to Statistical Learning_ (free pdf available online from the authors) and check the _regsubsets_ method from the leaps package.</p>", 
                "question": "How to make a for loop to find interactions between several variables in R?"
            }, 
            "id": "csrrfbf"
        }, 
        {
            "body": {
                "answer": "<p>I use Cryer and Chan<sq>s book on time series, and I have the solutions manual to the odd questions, it isn<sq>t just the solutions but gives you a certain amount of steps to solve the problems as well.<br>[here is the link to the solutions manual](http<colon>//www.docstoc.com/docs/151886741/Solutions-to-Time-Series-Analysis-With-Applications-in-R_-second) <br><br>I am sure you can find the text itself online as well, although I think the problems are in the solutions manual, but the book helps for context.  Hope this helps, it helped me pass my time series analysis class (even though our problems were not out of the book)</p>", 
                "question": "Teaching myself Time Series Analysis using Hamiton<sq>s textbook"
            }, 
            "id": "csla7lg"
        }, 
        {
            "body": {
                "answer": "<p>Check out  [Forecasting<colon> principles and practice](https<colon>//www.otexts.org/fpp)<br><br>Haven<sq>t actually gone through this one, but <dq>open-access<dq> textbooks are always a plus for me. The language is approachable and it<sq>s light on math and derivations, so I don<sq>t know if that<sq>s exactly what you<sq>re looking for.</p>", 
                "question": "Teaching myself Time Series Analysis using Hamiton<sq>s textbook"
            }, 
            "id": "csllp4c"
        }, 
        {
            "body": {
                "answer": "<p>Hamilton<sq>s book is more like an encyclopedia; it<sq>s fine if you need to reference a topic that you are already familiar with, but, in my opinion, it<sq>s a bit too dense to learn from directly.<br><br>I would recommend supplementing with something like Shumway and Stoffer that can help with the intuition.  I<sq>ve also heard that Enders Applied Time Series text is an accessible reference, but I<sq>m not personally familiar with it.   </p>", 
                "question": "Teaching myself Time Series Analysis using Hamiton<sq>s textbook"
            }, 
            "id": "csn3jgv"
        }, 
        {
            "body": {
                "answer": "<p>I have both Hamilton and Montgomery et al<sq>s Forecasting & Time Series Analysis and the later is a bit more approachable.</p>", 
                "question": "Teaching myself Time Series Analysis using Hamiton<sq>s textbook"
            }, 
            "id": "csp70ro"
        }, 
        {
            "body": {
                "answer": "<p>Smaller population. Samples are by definition representative of the population, which your subpopulation is not.</p>", 
                "question": "If a new data set is a section of a larger population should it be considered a smaller population or a sample?"
            }, 
            "id": "csieaw8"
        }, 
        {
            "body": {
                "answer": "<p>[deleted]</p>", 
                "question": "Getting confidence intervals on performance metrics from validation set?"
            }, 
            "id": "cshnxl6"
        }, 
        {
            "body": {
                "answer": "<p>Assuming that both groups<sq> data are ~normally distributed and your CI<sq>s don<sq>t include 0, you can use [Fieller<sq>s](https<colon>//en.wikipedia.org/wiki/Fieller<percent>27s_theorem) </p>", 
                "question": "Dear Reddit I<sq>m in a rut - If you have two means (with their own confidence intervals) and want to represent them as a ratio how do calculate the confidence interval for the ratio?"
            }, 
            "id": "csft4me"
        }, 
        {
            "body": {
                "answer": "<p>More generally, you can draw bootstrap samples from the original confidence intervals, divide them and then compute an empirical confidence interval for the ratio.</p>", 
                "question": "Dear Reddit I<sq>m in a rut - If you have two means (with their own confidence intervals) and want to represent them as a ratio how do calculate the confidence interval for the ratio?"
            }, 
            "id": "csftldy"
        }, 
        {
            "body": {
                "answer": "<p>[This page](http<colon>//www.jerrydallal.com/lhsp/ci_logs.htm) shows how to get a confidence interval on the ratio of geometric means. </p>", 
                "question": "Dear Reddit I<sq>m in a rut - If you have two means (with their own confidence intervals) and want to represent them as a ratio how do calculate the confidence interval for the ratio?"
            }, 
            "id": "csfy00c"
        }, 
        {
            "body": {
                "answer": "<p>A couple questions, what are the variables? I presume they are the same regardless of site? <br><br>Do the species occupy different habitats? In other words, how do you know the sites you selected are exclusive to 1 species or the other. Also, did you measure abundance of the rabbits on the site?<br><br>I was thinking if there was actual overlap and abundance you could have done something like ordination which can tell you (sort of like regression) which habitat variables correlate with each species. </p>", 
                "question": "Help choosing analysis for ecology study"
            }, 
            "id": "cses3ns"
        }, 
        {
            "body": {
                "answer": "<p>Not sure if you caught my answer in /r/SPSS or not so reposting this here.<br><br>Generally speaking if you<sq>re including multiple factors you probably don<sq>t want to use a univariate analysis to account for random factors and include a large number of covariates. It<sq>s just not well built for that purpose. What you want to use is a mixed model which properly accounts for random factors.<br><br>Regardless, it sounds like you are possibly including too many parameters in your model. Model selection is complex and there are reams and reams of books written on the topic and probably more disagreement than agreement about methods. Without more information about the type of data and the goals of your analysis it is hard to provide any more info as to whether factors should be included or not, let alone if they are fixed or random.<br></p>", 
                "question": "fixed/random factors in univariate analysis of variance"
            }, 
            "id": "csf14qr"
        }, 
        {
            "body": {
                "answer": "<p>A correlation between X and Y means that, on average, people with higher levels of X have higher levels of Y (and vice versa - correlation is non-directional).  That doesn<sq>t imply that every *individual* with high levels of one will have high levels of the other.  A typical regression model will enable you to estimate a <dq>spread<dq> for individuals around the conditional mean (RMSE in an OLS model).<br><br>> In some cases, wouldn<sq>t the individual be better off smoking?<br><br>That<sq>s potentially a lot more than a statistical question.  But if you wanted to argue that smoking was good for individuals with traits A, B, and C, that can be answered empirically by sampling from the <dq>slice<dq> of the population with traits A, B, and C.  In any case, the <dq>best estimate<dq> for any particular individual (from a statistical standpoint) is the population estimate.</p>", 
                "question": "When we analyse a sample space and we find a correlation between 2 variables or better yet a causation link is it correct to apply the knowledge to individuals?"
            }, 
            "id": "cset4w7"
        }, 
        {
            "body": {
                "answer": "<p>You could use Google N-Grams and try to develop a rule perhaps on the relative scores of the full word vs. the separated one. Or go machine learning, get some data and train a regression. </p>", 
                "question": "A statistics-based answer to a simple question?"
            }, 
            "id": "csdt466"
        }, 
        {
            "body": {
                "answer": "<p>[deleted]</p>", 
                "question": "Simple rearrangement using Bayes<sq> Theorem"
            }, 
            "id": "cs42gcz"
        }, 
        {
            "body": {
                "answer": "<p>It<sq>s easy to use, I think. I made use of probably the more basic functionality (relating to an ANOVA) and the workflow was not unlike that of SPSS, except it was rather cleaner. I used R for some of the stuff that went outside the program<sq>s expectations though (using omega squared for effect size, for instance). I<sq>m a psychology undergraduate so I reckon you would do just fine if you just went in with the data you have and see what you can do with it over a weekend or so.</p>", 
                "question": "need to learn minitab."
            }, 
            "id": "cs382od"
        }, 
        {
            "body": {
                "answer": "<p>If your data is qualitative, then I would do a Chi-Squared analysis.</p>", 
                "question": "Biostatistics help?"
            }, 
            "id": "crzmdck"
        }, 
        {
            "body": {
                "answer": "<p>Is logistic regression appropriate here?</p>", 
                "question": "Biostatistics help?"
            }, 
            "id": "crzmz8d"
        }, 
        {
            "body": {
                "answer": "<p>Do you intend control for multiple testing. You will probably find 2 or 3 variables statistically significant if none of them are.</p>", 
                "question": "Biostatistics help?"
            }, 
            "id": "crzqwxy"
        }, 
        {
            "body": {
                "answer": "<p>I would urge you to get a handle on your data before worrying about analysis.  I would suggest correspondence analysis for the discrete endpoints.  This would <dq>show<dq> you which groups are near to one-another.<br><br>But frankly, you will not get very far with Excel.  You need to get a statistics package.<br></p>", 
                "question": "Biostatistics help?"
            }, 
            "id": "cs275ih"
        }, 
        {
            "body": {
                "answer": "<p>Not necessarily, it depends on the reason you are transforming to begin with. Transformation might improve model fit but if the main reason for doing so was to contain some rowdy residuals rather than try to find the best fitting line to the data then you should seek a transformation that accomplishes this task best rather than focus on R2. If the reason for transformation was that the best fit line better suits the data/residual  distribution, this will also tame residuals, but since your transformation goal was to find the best model fit, it makes sense to consider r2 as well.<br><br>Sometimes you need to use your best judgement. Extreme values, even after transformation, might disproportionately increase r2 yet fit more poorly. This is why it<sq>s best to use a variety of methods to test for data homoschedasticity, outliers (e.g., cooks distance), and simply plotting raw data values and residuals.</p>", 
                "question": "R^2 & MSE"
            }, 
            "id": "crwczt6"
        }, 
        {
            "body": {
                "answer": "<p>See, when you transform your response variable, the <dq>context<dq> is already different. For the model with log transformation, high r2 or low mse means the model fits well to the log transformed data. Likewise, without transformation, a high r2 or low mse means the model fits well to the untransformed data. Directly comparing  them is somewhat like asking the following<colon> based on taste, which is better - coffee or tea? We cant really compare because they are meant / supposed to taste different<br><br>Now to answer your question directly<colon> transformation of data is typically used when the distribution of your data and the assumptions of your model dont quite match. So if you are thinking of transformation, then it is likely that the model with the untransformed data is no good in the first place</p>", 
                "question": "R^2 & MSE"
            }, 
            "id": "crwfp1z"
        }, 
        {
            "body": {
                "answer": "<p>The usual purpose of transformation is to make the distribution of the residuals fit the model.  The model assumption is that the residuals are normally distributed.  Look at a qq plot of residuals for each transformation and use the one that makes that makes the residuals most normal.<br><br></p>", 
                "question": "R^2 & MSE"
            }, 
            "id": "crwgbun"
        }, 
        {
            "body": {
                "answer": "<p>Sounds like you want to do an Association Rules analysis. Here<sq>s some quick results from Google.<br><br>[wiki](http<colon>//en.wikipedia.org/wiki/Association_rule_learning)<br><br><br>[R-example](http<colon>//www.rdatamining.com/examples/association-rules)<br><br><br>[R-blog](http<colon>//www.r-bloggers.com/examples-and-resources-on-association-rule-mining-with-r/)<br><br><br>If you<sq>ve never done this sort of thing before, the [rattle](http<colon>//rattle.togaware.com/) package in R has this functionality. It might be a good place to start.<br></p>", 
                "question": "Sales Trends"
            }, 
            "id": "crsms8i"
        }, 
        {
            "body": {
                "answer": "<p>Jaccard similarity is a useful metric for this.  Just take the number who bought A and B divided by the number who bought A or B.  This is really easy in SQL by joining a transaction table on itself.  Just make sure you have a high enough denominator when you are filtering results.</p>", 
                "question": "Sales Trends"
            }, 
            "id": "crtrreb"
        }, 
        {
            "body": {
                "answer": "<p>I presume you<sq>re going to be using some kind of model-based clustering to produce your probabilities. <br><br>Is there a particular context in which you want to consider these multiple cluster assignments? I presume subject-matter and the reasons you want to look at multiple class assignments at all would govern what kind of cutoffs you<sq>re interested in. But, lacking that context, I<sq>d first want to look at, for a k-class problem, anything that has probability > 1/k (or 1/k - \u025b for some small \u025b). </p>", 
                "question": "help with probabilistic clustering and multi-label classification"
            }, 
            "id": "crtfsbt"
        }, 
        {
            "body": {
                "answer": "<p>There are a variety of multivariate techniques which may help you better understand the interrelationships between sold items. Canonical relationships, cluster analysis, factor, principle component; there are many ways to explore your data set without a traditional response.</p>", 
                "question": "Statistics on Data Sets with no Dependent Variables"
            }, 
            "id": "crscww3"
        }, 
        {
            "body": {
                "answer": "<p>You seem to be attempting to solve a classification problem. Basically, you have a table of values, and the end result is either <dq>sold<dq> or <dq>not sold<dq>, the two classifications. There are many ways to solve these kinds of problems. Try searching for <dq>Machine Learning Classification<dq>  I would first try to approach the problem with Decision Tree Learning and see where that got me.<br><br>However, I<sq>m not sure if you will be able to solve this problem without having some <dq>not sold<dq> rows in your table. </p>", 
                "question": "Statistics on Data Sets with no Dependent Variables"
            }, 
            "id": "crrzb6h"
        }, 
        {
            "body": {
                "answer": "<p>Sounds like you<sq>re in the <dq>Dimensionality Reduction<dq> part of [this helpful flowchart](http<colon>//scikit-learn.org/stable/tutorial/machine_learning_map/). Does that sound right?</p>", 
                "question": "Statistics on Data Sets with no Dependent Variables"
            }, 
            "id": "crs00b1"
        }, 
        {
            "body": {
                "answer": "<p>1 in 10^7 ? If you assume each number from 0-9 is equally likely for all 7 digits.</p>", 
                "question": "I am asked for my phone number for a raffle. I don<sq>t like giving it out so I give my first 3 home digits and my last 4 cell digits..."
            }, 
            "id": "crrtlrm"
        }, 
        {
            "body": {
                "answer": "<p>I agree with you.  Normally pain is rated on a scale of 0-10, but these represent words (e.g. Wong-Baker scale, no pain, mild, moderate, etc.)  Don<sq>t worry, there are MANY, many wrong answers in the back of stats books.</p>", 
                "question": "Simple stats question"
            }, 
            "id": "crqdjty"
        }, 
        {
            "body": {
                "answer": "<p>I<sq>m not in medicine - I<sq>m in the social sciences - but one of the most commonly ignored problems with measurement that we see over here is how a single Likert-type item is clearly measured as ordinal, but that multiple Likert-type items magically can be treated as if they were interval.<br><br>Could the homework be referring to something like that?</p>", 
                "question": "Simple stats question"
            }, 
            "id": "crqtn81"
        }, 
        {
            "body": {
                "answer": "<p>From the sidebar over to the right<colon><br><br>> If it<sq>s homework, send it to r/homeworkhelp </p>", 
                "question": "Simple stats question"
            }, 
            "id": "crrc4ay"
        }, 
        {
            "body": {
                "answer": "<p>Maybe, just maybe, the researchers in this study used some form of quantifiable biomarker that is known to be representative of pain in arthritis, e.g. some form of protein or hormone etc. But yes, typically pain is measured on an ordinal scale</p>", 
                "question": "Simple stats question"
            }, 
            "id": "crqg6qi"
        }, 
        {
            "body": {
                "answer": "<p>The head-to-head comparison isn<sq>t that impressive. For everything that a frequentist can do, there<sq>s a Bayesian equivalent that<sq>s maybe slightly better or more intuitive (depending on your epistemology).<br><br>Where Bayesian stats really shines in the things it can do intuitively that standard frequentism doesn<sq>t really approach. Things like<colon><br><br>* how do you include reliable a priori information about the parameters<br>* stabilizing estimates when you have relatively small amounts of data (I mean, you could technically consider penalized regression a <dq>frequentist<dq> technique but the Bayesian interpretation is so much more honest)<br>* how do you combine complex models and update them as new information comes in<br>* causal learning<br>* non-standard hypothesis tests (e.g. is my expected loss from this financial model less than $X 90<percent> of the time, how certain am I that regression coefficient B_1 is greater than *both* B_2 and B_3)<br>* intuitive weighting of classification costs <br>* multi-model inference</p>", 
                "question": "What does Bayesian statistics do that frequentists can<sq>t?"
            }, 
            "id": "crq9aiu"
        }, 
        {
            "body": {
                "answer": "<p>There are some problems out there that are simply intractable using frequentist methods, but are pretty easy with Bayesian estimation.  The field where most of my empirical work is now is in Spatial Econometrics. The basics are ok, done with GMM or maximum likelihood.  But, estimating a hierarchical probit with two weights matrices... Bayes is the way.<br><br>I am no expert in the WHY or HOW yet, but am working on it. </p>", 
                "question": "What does Bayesian statistics do that frequentists can<sq>t?"
            }, 
            "id": "crqh8zh"
        }, 
        {
            "body": {
                "answer": "<p>There<sq>s an example in this talk by one of the pymc3 authors that really blew me away<colon> a model with a changing slope and intercept.  Maybe there<sq>s a way to estimate models like this in a non-Bayesian way, but this is just really eloquent.    <br><br>Edit<colon> maybe there<sq>s actually no way in hell to do this in a frequent its way<br><br>http<colon>//twiecki.github.io/blog/2013/12/12/bayesian-data-analysis-pymc3/<br></p>", 
                "question": "What does Bayesian statistics do that frequentists can<sq>t?"
            }, 
            "id": "crtryib"
        }, 
        {
            "body": {
                "answer": "<p>wow. You guys were all incredibly helpful. Thanks!</p>", 
                "question": "What does Bayesian statistics do that frequentists can<sq>t?"
            }, 
            "id": "crvq7av"
        }, 
        {
            "body": {
                "answer": "<p>It depends a lot of 2 things <colon> <br><br>* Do you want to be a generalist or a specialist ?<br>* Eventually, do you want to do research or to work in industry ?<br><br>Usually, industry likes generalists when research loves specialists (with exceptions in both groups).<br><br>So if you go generalist, take a course about something missing in your academic resum\u00e9, like **Time Series analysis** or **Game Theory**. <br><br>On the other hand, if you are looking for a master degree or a Ph.D., then start looking already for a thesis director and go ask him/her what would be beneficial for you if you were to be his/her student.</p>", 
                "question": "Seeking advice on what courses will be best for doing stats work upon graduation."
            }, 
            "id": "crooy34"
        }, 
        {
            "body": {
                "answer": "<p>A problem is that you<sq>ve only collected data on Emmy winners. If you had some random sample of comedy performers, or maybe all nominated individuals with the full data set, then you could perform a logistic regression. Then you could predict who, among top contenders, might win. But your dataset only includes winners so you have no variation in your expected value.</p>", 
                "question": "I need help with a statistics exploration I<sq>m doing"
            }, 
            "id": "crm54k1"
        }, 
        {
            "body": {
                "answer": "<p>That<sq>s actually a good question. Disclaimer<colon> I haven<sq>t really worked with this sort of data much. <br><br>Narrowly, the correct way to choose bin size (and hyperpameters in general) is to use cross-validation. Pick some performance metric, estimate that metric under a variety of bin sizes using cross-validation, select the one that does best.<br><br>More broadly, you probably don<sq>t need to bin your events at all and should be able to estimate r(t) directly. Flexible nonparametric fits are probably ideal<colon> local regression and penalized splines should work well.</p>", 
                "question": "Question about Poisson statistics"
            }, 
            "id": "crjsq10"
        }, 
        {
            "body": {
                "answer": "<p>I<sq>m having a very similar problem/question! I<sq>m using poisson events in a dynamic programming optimization. It is a little disconcerting that I<sq>m getting somewhat different results depending on the bin size. I wish I had a good answer for you!</p>", 
                "question": "Question about Poisson statistics"
            }, 
            "id": "cro49rv"
        }, 
        {
            "body": {
                "answer": "<p>There are a couple different common ways of assessing mediation. By your description of stages, I<sq>m guessing you are trying to follow the [Baron and Kenny approach](http<colon>//en.wikipedia.org/wiki/Mediation_<percent>28statistics<percent>29#Baron_and_Kenny.27s_.281986.29_Steps_for_Mediation)? Under that framework, it sounds like aggressive humor is failing to meet Step 2 (following the wiki numbering of steps). For self-defeating, you are probably *very* borderline since it sounds like beta_32 is only significant if you keep aggressive in the model as a covariate, and from your description of the partial correlations it sounds like the change from beta_11 to beta_31 is marginal at best. The change in overall model R-square isn<sq>t particularly relevant to the question of mediation.<br><br>If you want a more direct statistical test of the mediation, I<sq>d suggest the Preacher & Hayes method described further down the wiki page. Wiki even has a link to some SPSS macros to help run that test.</p>", 
                "question": "Dropping variables from a hierarchical regression"
            }, 
            "id": "crh0kej"
        }, 
        {
            "body": {
                "answer": "<p>If you think the classical assumptions are violated, which they probably are quite frequently, then by all means use GLS. But always using GLS would mean sacrificing some power in cases where the OLS assumptions hold. <dq>Reduces to OLS<dq> is the not the same as <dq>becomes BLU like OLS<dq>. Hence why you might consider using OLS if you don<sq>t having convincing evidence (from heteroskedasticity tests, etc) that the OLS assumptions are violated.<br><br>There<sq>s also the pragmatic reason that OLS is firmly established as a <dq>default<dq> analysis. It<sq>s simple, it<sq>s commonly used, it<sq>s failure mode vs. GLS is not entirely horrific. By comparison, flipping the switch to GLS is not necessarily trivial. It means finding the implementation (hint<colon> it<sq>s not in base R), making some more complex modeling decisions (what<sq>s the right structure for the variance term? Leave it too loose and the extra df may eat any advantage over the violated OLS, too strict and your back to non-BLUE), and explaining to everyone else on the project why you<sq>re making things more complicated.<br><br>Of course none of this means GLS isn<sq>t still the right answer for many analyses, but it<sq>s sufficient to prevent a switch to always using GLS over OLS. </p>", 
                "question": "Why not always use GLS over OLS?"
            }, 
            "id": "crft19t"
        }, 
        {
            "body": {
                "answer": "<p>One reason is that GLS estimators typically require additional parametric assumptions. E.g. to correct heteroskedasticity, we may need to estimate weights based on some model for error variance. To correct autocorrelation, we may need to assume that residuals follow AR(1) process. If those assumptions are violated, optimal properties of GLS no longer hold. On the other hand, OLS is still consistent and robust standard errors can be obtained easily. For an argument in favor of OLS along similar lines, see e.g. this [note](http<colon>//faculty.chicagobooth.edu/john.cochrane/research/papers/overdifferencing.pdf) by John Cochrane.</p>", 
                "question": "Why not always use GLS over OLS?"
            }, 
            "id": "crg7ub6"
        }, 
        {
            "body": {
                "answer": "<p>I have modeled similar networks using the igraph package in R. Each graph had its own edge density, so I would end up just looking at the conversations of the chattiest people. The best looking graphs are undirected graphs simplified to only one edge at most connecting two vertices. I can send you code if you want.</p>", 
                "question": "Help with choosing a Social Network Analysis (SNA) method"
            }, 
            "id": "cre3ui7"
        }, 
        {
            "body": {
                "answer": "<p>I can<sq>t answer your inquiry, but I did want to share an R tutorial for SNA that I found very useful when auditing a course a few years back<colon><br><br>http<colon>//sna.stanford.edu/rlabs.php<br><br></p>", 
                "question": "Help with choosing a Social Network Analysis (SNA) method"
            }, 
            "id": "cre7kwp"
        }, 
        {
            "body": {
                "answer": "<p>Your understanding is correct, but the lecturer<sq>s definition isn<sq>t contradicting it. Just another way of looking at it. Maybe an example will clear it up<colon> <br><br>Say you<sq>re doing a statistical test and you<sq>ve decided on a significance level of 0.10. If the p-value is 0.07 you<sq>d reject the null hypothesis. However, if you<sq>d decided on a significance level of 0.05, you wouldn<sq>t be able to reject the null hypothesis. In other words, the minimum significance level at which you<sq>d reject the null hypothesis would be 0.07, i.e. the p-value.<br><br>Hope that makes sense.</p>", 
                "question": "Definition of the p-value of a hypothesis test."
            }, 
            "id": "crbfjs5"
        }, 
        {
            "body": {
                "answer": "<p>You<sq>re right to be confused, the first definition is to do with alpha, the significance threshold, which is arbitrarily set at 0.05. <br><br>Whenever you<sq>re doing a test, you<sq>re computing a test statistic that is a function of your data. The null distribution is probability distribution constructed such that it gives high probability to values of the test statistic that are <dq>unremarkable<dq> - like if you<sq>re testing if two groups are the same, your test statistic could be the difference in means of the groups. If the groups are the same, the difference is going to be 0 (or closeish to zero, allowing some random variation), and these values have the highest probability under the null distribution. If the groups are truly different, the test statistic is going to have some nonzero value, and the probability of seeing this value under the null is low. <br><br>P-values are probabilities of test statistic values under the null distribution. Alpha is a threshold set by the analyst, and it says that test statistic values with alpha probability (or lower) under the null are unlikely enough that we believe the alternate hypothesis instead of the null. <br><br>(I<sq>m on mobile, sorry for not going into detail)<br></p>", 
                "question": "Definition of the p-value of a hypothesis test."
            }, 
            "id": "crbfg22"
        }, 
        {
            "body": {
                "answer": "<p>Thank you for the comments guys, I think you<sq>ve cleared it up for me <colon>)</p>", 
                "question": "Definition of the p-value of a hypothesis test."
            }, 
            "id": "crbhczj"
        }, 
        {
            "body": {
                "answer": "<p>How did you specify <dq>nonlinearity<dq> in your model? Take a look at using the mgcv package in R (or generalized additive models in the stats program you<sq>re using) instead.</p>", 
                "question": "Applying a moderator to non-linear OLS regression"
            }, 
            "id": "craj9et"
        }, 
        {
            "body": {
                "answer": "<p>[deleted]</p>", 
                "question": "Applying a moderator to non-linear OLS regression"
            }, 
            "id": "crc0c70"
        }, 
        {
            "body": {
                "answer": "<p>The information you provided isn<sq>t enough (Also, we<sq>re going to use 25<percent> instead of 75<percent> because you want to be able to compare the probabilities). Team A has a 25<percent> chance of allowing the event again generic teams and Team B has a 60<percent> chance of causing the event against generic teams. <br><br>Suppose the event occurs 25<percent> of the time in general, then team A can be considered generic and we<sq>d expect the event 60<percent> of the time. Instead, suppose the event occurs 60<percent> of the time, then team B can be considered generic and we<sq>d expect the event 25<percent> of the time. For other cases, we need to know more about the joint distributions of the events... is the likelihood of an event occurring more of a function of the strength of the side causing the event or the side trying to prevent it? <br><br>The one thing to note when trying to figure this out is to use common sense. Assuming that when two generic teams play, the chance of the event occurring is between 25<percent> and 60<percent>, you should expect your answer to also fall between these numbers. If it is above 60<percent> then team B is worse than the generic team and team A is exceptionally good so we should expect the chance to be below 25<percent> and vice versa for if the generic chance is below 25<percent>.</p>", 
                "question": "Joint probability of oppositional events"
            }, 
            "id": "cr8t8i0"
        }, 
        {
            "body": {
                "answer": "<p>Could you say (1-0.7)*(0.6) = 0.18? </p>", 
                "question": "Joint probability of oppositional events"
            }, 
            "id": "cr8e30c"
        }, 
        {
            "body": {
                "answer": "<p>Is the idea that, for example, team b will be able to make the event occur 60<percent> of the time, but then team a can stop the event, once triggered, 75<percent> of the time? Then yes, you can say that Pr(event X occurs) = Pr(team b causes event X)\u00d7Pr(team a fails to block event X) = 0.6\u00d7(1-0.75) = 0.15.</p>", 
                "question": "Joint probability of oppositional events"
            }, 
            "id": "cr8kbkk"
        }, 
        {
            "body": {
                "answer": "<p>It will work fine.  The penalty you pay for the ties is that they are essentially null observations (information-less observations).  But 44 should still get reasonable power.</p>", 
                "question": "Wilcoxon signed rank test question"
            }, 
            "id": "cr84pi1"
        }, 
        {
            "body": {
                "answer": "<p>A key missing piece is how did you generate this set of random numbers.</p>", 
                "question": "Why would the standard dev for a random set of numbers from 0-100 tend towards 28.8?"
            }, 
            "id": "cr5t4ri"
        }, 
        {
            "body": {
                "answer": "<p>I assume you mean <dq>random integers<dq> there <br> <br>The variance of a discrete uniform is r(r+2)/12, where r is the range (largest possible value minus smallest possible value).<br><br>Or if you define m=largest-smallest + 1, then variance is (m^(2)-1)/12<br><br>so for 0-100, the variance is 10200/12 = 850 and the s.d. is 29.155<br><br>for large r, the standard deviation will tend to 0.2887(r+1)<br><br>If you meant you generated *continuous* random numbers on 0-100, then the variance is r^(2)/12 and the sd will tend to  0.2887r<br></p>", 
                "question": "Why would the standard dev for a random set of numbers from 0-100 tend towards 28.8?"
            }, 
            "id": "cr6asfx"
        }, 
        {
            "body": {
                "answer": "<p>In this case I think you are right, you can treat each specimen as independent. </p>", 
                "question": "Looking for the appropriate statistic test to run on unique data"
            }, 
            "id": "cr4r2dk"
        }, 
        {
            "body": {
                "answer": "<p>To add to what /u/dodgy-stats said, it sounds like you have something like this. Subject A, tests 1, 2, 3. Subject B, tests 1, 2, 3. Your question is whether A1 tests the same using two different technologies, B1 tests the same, B2, etc. Is that right? In that case, the correlations between subjects are probably irrelevant, assuming that there are no individuals level differences in fecal cortisol types, such that subject A<sq>s might be detected better via test 1, but subject B<sq>s are systematically better detected using test 2.</p>", 
                "question": "Looking for the appropriate statistic test to run on unique data"
            }, 
            "id": "cr52bhg"
        }, 
        {
            "body": {
                "answer": "<p>Let me apply reductio ad absurdum.<br><br>Why not just take a sample from yourself and test it 20 times?  Are these not independent by your reasoning?  Why bother with multiple individuals at all?<br><br>Besides, your problem is much more complicated than you think.  Your problem is actually a measurement agreement problem desirous of testing equivalence.<br><br>A t-test assumes equivalence and rejects non-equivalence.  Failure to reject does not imply equivalence!!!!<br><br>Your test must assume non-equivalence and reject for equivalence.  Failure to reject does not imply non-equivelence but rejecting implies equivalence.<br><br>**Edit<colon>**  I was not as clear as I ought to have been about your original question (because it is not really that applicable) and it did deserve more addressing than I gave it.  The question is quite complex.<br><br>Suppose you could prepare fecal samples (as a matter of fact... ).  You would have gone out and CREATED the samples throughout the cortisol concentration range you wanted to test.  Right?  That<sq>s what any assay would do...run a test against prepared and KNOWN standards.  Your difficulty is that the fecal part (rather than the cortisol part) **may** affect the test, too.  Because it is almost certainly true that the diet that produced the fecal matter may affect your ability to accurately measure the cortisol.<br><br>Let<sq>s assume that the cortisol MEASUREMENT error is unaffected by the fecal matter.<br><br>So, in principle, the subject/animal should not matter and you mainly need the different samples to flesh out the range of cortisol concentrations.  In this case, the observations are independent.  But the downside of using possibly correlated measurements from the same individual is that you are not exploring the range of cortisol concerntrations very efficiently.  Potentially a minor consideration.  This would be like preparing 100 cortisol samples and putting 75 of them in the middle and scattering the other 25 over the rest of the range.  It is not wrong and the observations are independent; but the design is inefficient.<br><br>But if you think that the cortisol measurement error is not associated with the fecal matter then why bother with fecal matter at all.  Why not just make up the standard cortisol concentrations and do it like you would do a standard assay?<br><br>If you think the cortisol ME **IS** associated with the fecal matter then the composition of that fecal matter will be correlated by subject and so the data **IS NOT** independent.<br><br>I am not foisting one conception or the other.  I have no idea what your assay is or how it might be affected.  This is a science question for you.  I can only explain the statistical implications of your choice.  But as I say, this question is secondary to what your actual null hypothesis is, in the first place...not a simple t-test.</p>", 
                "question": "Looking for the appropriate statistic test to run on unique data"
            }, 
            "id": "cr5np3w"
        }, 
        {
            "body": {
                "answer": "<p>> because I am only testing the technology and not interested in the cortisol levels per individual, in my opinion each specimen is independent<br><br>It doesn<sq>t matter a damn what you<sq>re interested in, that<sq>s dependent data. </p>", 
                "question": "Looking for the appropriate statistic test to run on unique data"
            }, 
            "id": "cr5jedr"
        }, 
        {
            "body": {
                "answer": "<p>A paired t test is appropriate.  You might also look at a [Bland-Altman plot](http<colon>//en.wikipedia.org/wiki/Bland<percent>E2<percent>80<percent>93Altman_plot) for a more detailed picture of the relationship.</p>", 
                "question": "What type of t-test should I use?"
            }, 
            "id": "cr1ad6c"
        }, 
        {
            "body": {
                "answer": "<p>If you want to test if two populations have different means when you don<sq>t want to assume that they have the same variance, you need to use a Welch<sq>s t-test.<br><br>http<colon>//en.wikipedia.org/wiki/Welch<percent>27s_t_test#Calculations<br><br>Note, that this only tests whether the means are different, not whether the variances are different.</p>", 
                "question": "What type of t-test should I use?"
            }, 
            "id": "cr1734n"
        }, 
        {
            "body": {
                "answer": "<p>Hmm.<br><br>The paired t-test is appropriate *if* you want to assume there is no difference and then reject if there is a difference.<br><br>Your hypothesis in your second sentence, however, is that you want to assume there is a difference and prove that there is not.<br><br>But then your third sentence restates the standard null hypothesis<colon> Assume no difference and prove there is.  And a paired t-test is appropriate for this.<br><br>Based on what I believe you are doing, I think your second sentence hypothesis really **is** what you want to do and that you really want to do something closer to an EQUIVALENCE (or non-inferiority) study rather than a REJECTION TRIAL.<br><br>You want to show that your prototype is *at least as good as* the commercial device (a non-inferiority study).  You might start [here](http<colon>//ncss.wpengine.netdna-cdn.com/wp-content/themes/ncss/pdf/Procedures/PASS/Non-Inferiority_Tests_for_Two_Means_using_Differences.pdf).<br><br>However, there is one other caveat that I would add.  This will test whether, on average, your device is at least as good as the other.  But it will only test the average performance.  Your device could be be <dq>not inferior<dq> to the other device and still be objectively better or worse based on its variability.  That is, it could display much wilder swings and still average out the same or it could display much more contained swings and still average out the same.  <br><br>So I<sq>m tempted to point you to this [book](http<colon>//www.amazon.com/Statistical-Tools-Measuring-Agreement-Lawrence/dp/1461405610/ref=sr_1_3?ie=UTF8&qid=1431015547&sr=8-3&keywords=measuring+agreement)<colon> Statistical Tools for Measuring Agreement by Lin, Hedayat, and Wu.  This is a fairly technical book though and I find the style somewhat opaque -- an interesting read but not a fun read.</p>", 
                "question": "What type of t-test should I use?"
            }, 
            "id": "cr1iqox"
        }, 
        {
            "body": {
                "answer": "<p>0) The normal distribution is used a lot because a) it does turn up quite frequently, and b) it is convenient to work with in many cases.<br><br>1) Before you assume that this data or any data follows a normal distribution (or something similar), see if it does.  Make histogram, see if it looks like a bell.  Check the skewness and kurtosis (should both be close to 0 if it is normal-ish).<br><br>2) If it is close to a normal distribution (looks like a bell, not skewed, kurtosis close to zero), then how you are interpreting it it close enough to right.  Don<sq>t use =stdev.p to calculate the std. dev., just use =stdev since you have a sample.  This will not change your answer by much though.<br><br>3) Think of a <dq>standard deviation<dq> as a <dq>common distance that things are from the average<dq>.  The standard deviation of men<sq>s heights in the U.S. is around 3 inches, the mean is around 5<sq>9<dq>. SO it is very **common** to find people that are between 5<sq>6<dq> and 6<sq> tall-- around 68.26<percent> of people. A <dq>two standard deviation event<dq> is something much less common- seeing a guy walk by who is 5<sq>3<dq> or 6<sq>3<dq>.  Since 95.44<percent> of men are *between* these two heights, it is fairly unusual to see someone not between these two heights.  A three standard deviation event is **extremely** rare, but they do happen (men under 5<sq> or over 6<sq>6<dq>).<br><br>4) Please elaborate on exactly what you would like to plot in Excel, and I can help.  When you say <dq>plot *this*<dq>, what is *this*?</p>", 
                "question": "Understanding standard deviation as it relates to stock returns"
            }, 
            "id": "cr0dh7c"
        }, 
        {
            "body": {
                "answer": "<p>Just a comment about interpreting stock returns and standard deviation.<br><br>Standard deviation of returns is often used as a measure of risk.<br><br>The goal of investing is to maximize average return, while minimizing the risk (standard deviation).<br><br>You can do this by finding uncorrelated returns.</p>", 
                "question": "Understanding standard deviation as it relates to stock returns"
            }, 
            "id": "cr0p2bk"
        }, 
        {
            "body": {
                "answer": "<p>Note that in the bootstrap, sampling is with replacement.<br><br>You can sample more (or fewer) than $n$ values, so if n is 100 and you need 150 values, just sample 150 values from the residuals.<br><br>If you need a prediction interval, it<sq>s a little more complex.<br><br></p>", 
                "question": "Residual bootstrap for non-linear least squares model in R w/o response data"
            }, 
            "id": "cqxni7l"
        }, 
        {
            "body": {
                "answer": "<p>First, let me state that I am not certain from your description that your <dq>8 chronological evaluations<dq> are on the same egg.  I<sq>m going to assume that it is.  If it is, this is a repeated measures study.  If it is not, that is another factor in your study.<br><br>There are two possible analyses that come to mind.  These are not trivial analyses but they are not too complicated...the most complicated part is that your design itself is complicated (repeated measures) even if you didn<sq>t have to deal with the ordinal response.<br><br>The first suggestion is a generalized linear mixed effects model.  The second suggestion is a relative treatment effects model.  I would recommend the second but either could be very good.<br><br>Both models will correctly incorporate the ordinal response.<br>Both models can incorporate the repeated measures design.<br><br>For the GLMMIX model, I do not know enough to suggest whether you should use cumulative logits or continuation ratios.  If an egg tends to progress from class 1 to class 4, continuation ratios would seem preferable...and probably forward continuation ratios with a complementary-log-log link function.  But I do not know enough about the egg classes and you leave it open that this may change.  There are many good books on GLMMIX models and they can be fit in SAS or R).  This model would be fit with GLMMIX in SAS.  For information on continuation ratios versus conditional logits, I would recommend Ann A. O<sq>Connell<sq>s book <dq>Logistic Regression Models for Ordinal Response Variables, Issue 146<dq> from Sage.  It is very inexpensive and very good.<br><br>For the RTE model, this is fit in SAS using PROC RANK and PROC MIXED.  In R, this can be fit with the nparLD package.  I would strongly urge you to get a copy of <dq>Nonparametric Analysis of Longitudinal Data in Factorial Experiments<dq> by Edgar Brunner, Sebastian Domhof, and Frank Langer, 2002, Wiley. This is a wonderfully written book and explains the technical concepts pretty well.</p>", 
                "question": "Need help on a Statistical Test for analyzing fruit fly eggs"
            }, 
            "id": "cqy62cv"
        }, 
        {
            "body": {
                "answer": "<p>You would want to look into learning econometrics. What data do you have? <br><br>Without knowing the data you have on hand, you would want to learn (quick models off the top of my head) GARCH (to adjust for periodic heteroskedasticity), ADL (which holt winters looks like), differenced ADL (in case of unit roots), and ECM (if you have two cointegrated variables). You also probably want to test forecasts with out of sample forecasting methods.<br><br>All of these have good academic intro papers easily found on google! Lastly, remember to keep black swans in you mind at all times; it<sq>s the bane of any economic forecasting.</p>", 
                "question": "Forecasting Deposits at a Bamk"
            }, 
            "id": "cqulx3o"
        }, 
        {
            "body": {
                "answer": "<p>> Kinda small, I know,<br><br>Kinda? You<sq>re trying to assess *distributional shape* from 4 values. <br><br>> but it has to be done.<br><br>Why?<br><br>If you must assess normality, a QQ plot is about the best way, but with four data values? ridiculous.<br><br>Don<sq>t *test* it. <br><br>> Is it best to say that if its within 2 standard deviations, then its within 95<percent> of distribution?<br><br>What? If *what* is within two standard deviations of *what*? The final clause even makes less sense to me, and I fail to see how that tells you much of anything about normality.<br><br>Can you explain what you<sq>re trying to say here, and how it tells you about normality?</p>", 
                "question": "What<sq>s the best way of checking for normal distribution across 4 values?"
            }, 
            "id": "cqtb78t"
        }, 
        {
            "body": {
                "answer": "<p>n=4 is too small for either a normality test or for whatever you<sq>re doing that you think needs a normality test.</p>", 
                "question": "What<sq>s the best way of checking for normal distribution across 4 values?"
            }, 
            "id": "cqtannd"
        }, 
        {
            "body": {
                "answer": "<p>Do you mean that you have 4 observations and want to test the hypothesis that your data come from a normal distribution? Because the best way to do that would be an appeal to either theory or prior research.</p>", 
                "question": "What<sq>s the best way of checking for normal distribution across 4 values?"
            }, 
            "id": "cqt7y17"
        }, 
        {
            "body": {
                "answer": "<p>You can probably conclude that the null hypothesis that the distribution is exactly normal is false without doing any analysis. It<sq>s hard to imagine a real-world distribution being exactly normal? In agreement with Dr_Underdunk, prior research should give you an indication of the degree of non-normality. </p>", 
                "question": "What<sq>s the best way of checking for normal distribution across 4 values?"
            }, 
            "id": "cqu0xr1"
        }, 
        {
            "body": {
                "answer": "<p>Any loss function is somewhat arbitrary.  K=2 yields Euclidean distances (it is the usual distance metric).  This metric tends to yield the most simple mathematics and we understand it well.  Choosing k=1, for instance, will apply a city-block metric and will yield very unhandy mathematics.  [Wikipedia ](http<colon>//en.wikipedia.org/wiki/Norm_<percent>28mathematics<percent>29#Euclidean_norm) has a pretty good overview.<br><br>The L2 norm tends to produce the physics of springs and rubber bands...the data tends to add to the loss like springs add to the potential energy.  It is easy to show that the natural estimators are the mean and standard deviation.<br><br>The L1 norm tends to produce the physics of suspended weights...the data tends to add to the loss like suspended weights add to potential energy.  It is easy to show that the natural estimator is the median.<br><br>If one allows the mathematics to be somewhat less than rigorous, the L0 norm generates the mode as its natural estimator.<br><br>Etc., etc.</p>", 
                "question": "In the loss function |\u03b8hat-\u03b8|^(k) is k=2 intrinsically optimal in some sense?"
            }, 
            "id": "cqprsaa"
        }, 
        {
            "body": {
                "answer": "<p>Evidently, 30 * 100 billion<br>/r/badstats</p>", 
                "question": "When I need to read a chart and the Y axis says the value is <dq>in 100 billions $<dq> and say the value in Y axis is 30 what is it?"
            }, 
            "id": "cqk35s8"
        }, 
        {
            "body": {
                "answer": "<p>3 trillion dollars. \\end{dr evil}<br><br>What<sq>s the chart of? Nominal gdp? Can<sq>t think of anything else you<sq>d want in 100sB. </p>", 
                "question": "When I need to read a chart and the Y axis says the value is <dq>in 100 billions $<dq> and say the value in Y axis is 30 what is it?"
            }, 
            "id": "cqk9il2"
        }, 
        {
            "body": {
                "answer": "<p>hopefully, 30 lots of 100 billion dollars, otherwise something has gone wrong.</p>", 
                "question": "When I need to read a chart and the Y axis says the value is <dq>in 100 billions $<dq> and say the value in Y axis is 30 what is it?"
            }, 
            "id": "cqkectz"
        }, 
        {
            "body": {
                "answer": "<p>I know it may be seem stupid of me to ask a question like that but really I was confused by the scale in Y axis. Thank you for your help!</p>", 
                "question": "When I need to read a chart and the Y axis says the value is <dq>in 100 billions $<dq> and say the value in Y axis is 30 what is it?"
            }, 
            "id": "cqlapcq"
        }, 
        {
            "body": {
                "answer": "<p>Normalizing the data places the variables on a common scale, allowing each to be considered equally. I see no real reason that what you<sq>ve done is wrong. My only concern would be if one of the variables had considerable skew. Imagine you had 10 products and 9 were $50 and the 10th was worth $1000 - in that example, the 9 would bunch up near one score, the 10th would bunch up near the other end of the spectrum, and all you<sq>ve basically done is dichotomized your data rather than continuously scaled it.</p>", 
                "question": "Is normalizing data ever wrong when analyzing a dataset for a trade study?"
            }, 
            "id": "cqj3zpr"
        }, 
        {
            "body": {
                "answer": "<p>[removed]</p>", 
                "question": "Is normalizing data ever wrong when analyzing a dataset for a trade study?"
            }, 
            "id": "cqj2z4j"
        }, 
        {
            "body": {
                "answer": "<p>I don<sq>t think you do, but could you expand on the type of data you<sq>re working with and what you<sq>re investigating?</p>", 
                "question": "Homogeneity of variance and nonparametric tests"
            }, 
            "id": "cq86wu7"
        }, 
        {
            "body": {
                "answer": "<p>> Do you need homogeneity of variances in order to conduct nonparametric statistical tests? <br><br>That depends on the test, but in many cases (even ones where you regularly find people claiming you do) you really don<sq>t. homogeneity of variance may make for much simpler interpretations.<br><br>What tests are you doing?<br><br>> I<sq>m getting mixed messages from the resources I<sq>m using.<br><br>Which resources, and what do they say, exactly? (but put it in your question as well, please)<br><br><br></p>", 
                "question": "Homogeneity of variance and nonparametric tests"
            }, 
            "id": "cq8g29z"
        }, 
        {
            "body": {
                "answer": "<p>You might try Berger (different Berger than Casella and Berger) - Statistical Decision Theory and Bayesian Analysis<br><br>It is on the same level as Casella and Berger in terms of rigor and math level.</p>", 
                "question": "Recommendation Requested<colon> Introduction to Bayesian Statistics (graduate level). Somewhere between BDA and Schervish."
            }, 
            "id": "cq6j8rc"
        }, 
        {
            "body": {
                "answer": "<p>>and since standard deviation for a binomial variable is found by p(1-p) I get +/- Za/2 * sqrt{[p1(1-p1)2 /n1]+[p2(1-p2)2 /n2]}<br><br>No, the **variance** of a binomial is p(1-p).  Do not square it. The formulas are really the same thing.</p>", 
                "question": "Calculating CI for the difference with binomial data"
            }, 
            "id": "cq5bfqx"
        }, 
        {
            "body": {
                "answer": "<p>> standard deviation for a binomial variable is found by p(1-p)<br><br>Nope, that<sq>s **variance** for a Bernoulli (i.e. binomial with n=1). If you do it right, they look very, very similar. <br><br>However, the proportion CI you mention only applies asymptotically (i.e. it doesn<sq>t work so well if np(1-p) is small; you<sq>d probably want that to be at least 10 or so, and probably a good deal more, though if p is near 0.5 a continuity correction could make it work pretty well  down smaller than that.)</p>", 
                "question": "Calculating CI for the difference with binomial data"
            }, 
            "id": "cq5z6si"
        }, 
        {
            "body": {
                "answer": "<p>So, the intention behind MIC is that it<sq>s just like correlation, except with much (much!) weaker assumptions. Basically, it<sq>s a transformation of mutual information that ranges between 0 and 1. 0 means that you can<sq>t tell anything about x if you know y, 1 means you can always guess it perfectly.<br><br>The major benefit is that it<sq>s (ideally) agnostic to the form that the relationship takes (and which order you plug the variables in). Linear, exponential, sinusoidal etc. relationships all give about the same number if they<sq>re about equally noisy. The idea is to use MINE to find strong (and thus hopefully meaningful) relationships between variables and then use that to inform a more structured modelling approach.<br><br>That said, just using mutual information content might work better (although you<sq>d have to code something up). [Kinney and Atwal 2014](http<colon>//www.pnas.org/content/111/9/3354.long) poke some fairly convincing holes in MIC and argue that you should just be using k-nearest neighbours estimates of mutual information instead. I suspect that there will be a correctly normalized version of mutual information eventually but MIC isn<sq>t it.</p>", 
                "question": "Can someone provide me some insight on interpreting results when using this? - MINE<colon> Maximal Information-based Nonparametric Exploration"
            }, 
            "id": "cq5q938"
        }, 
        {
            "body": {
                "answer": "<p>I can run MINE on data sets but I can<sq>t seem to make sense of the results that I get.  I basically need to understand what the numbers mean.  I don<sq>t have any background in statistics so I started reading all that I could find.  I can<sq>t find anything on MIC.  Any help would be appreciated, even if it<sq>s just pointing me in the direction of a good book that will help me learn it.</p>", 
                "question": "Can someone provide me some insight on interpreting results when using this? - MINE<colon> Maximal Information-based Nonparametric Exploration"
            }, 
            "id": "cq5ak1n"
        }, 
        {
            "body": {
                "answer": "<p>There are a lot of possible approaches that would fit this general category of pooling outcomes into a <dq>mega-hypothesis<dq>. For instance<colon><br><br>1) Pick a more general outcome measure. For instance, consider the impact of your miracle elixir on <dq>all cause mortality<dq>. If it reduces cancer, diabetes, heart disease, etc, each by a bit, that should add up to a meaningful impact on general mortality that you can test as a single effect.<br><br>2) Factor analysis, SEM, etc. Statistically pool your outcome measures into one (or a few) latent factors, then test for association of the elixir with the factor. Works best if you have some conceptual framework to link your different outcome measures (common in psychology with latent factors like depression, personality, or intelligence).<br><br>3) Multivariate regression (i.e. with 2+ dependent variables, not to be confused with multiple regression). Do a single omnibus test of whether to full beta vector is zero. Saves the multiple testing burden, and takes advantage of any correlation between the dependent variables.<br><br>Tests of the distribution of p-values are also possible, but generally require you to be doing a *lot* of tests to get much power.</p>", 
                "question": "(x-post from /r/AskScienceDiscussion) Family-wise error corrections and multiple moderately significant effects?"
            }, 
            "id": "cq4vrpn"
        }, 
        {
            "body": {
                "answer": "<p>I<sq>m only an interested amateur so take this with a grain of salt <colon>)<br><br>One of the principles of good study design is you should have a clear research question, preferably with a single outcome measure It seems like what you have done in the example above is exploratory analysis which you could use to generate hypotheses to test, with further studies looking at particular effects you want to find out about. (Dallal G., *The Little Handbook of Statistical Practice*).<br><br>As part of those further studies you would choose an appropriate sample size so that if there<sq>s a useful effect you can claim significance.<br><br>So instead of testing your hypothesis <dq>Mountain Dew is bad for you<dq> - which might be too vague to have many practical applications anyway - you could find out if <dq>Mountain Dew increases tooth decay<dq>, <dq>Mountain Dew elevates risk of metabolic syndrome<dq> etc etc, you wouldn<sq>t have to worry about controlling for family wise error rate and you will get more useful data.</p>", 
                "question": "(x-post from /r/AskScienceDiscussion) Family-wise error corrections and multiple moderately significant effects?"
            }, 
            "id": "cq61hzp"
        }, 
        {
            "body": {
                "answer": "<p>So, the numbers for the means are clearly different, right? But those numbers are just estimates. And the variances of those estimates reflect uncertainty about the true value. Say you were asked if one rope was longer than another. If you said one was 20 feet, plus or minus 5 feet, you wouldn<sq>t say for sure that it is longer than 18 feet. But if you said it was 20 feet, plus or minus 6 inches, then you<sq>d confidently say it is longer than 18 feet. It<sq>s the variance (relative to the mean) that determines significance, not the estimate of the mean itself.</p>", 
                "question": "I don<sq>t understand why certain values are significantly different and why others are not"
            }, 
            "id": "cq1m4h5"
        }, 
        {
            "body": {
                "answer": "<p>This looks suspiciously like a systematic, technical error in the handling of Flask 1.  I<sq>d want to rule that possibly out from the <dq>wet<dq> side before I started in too hard with the stats.</p>", 
                "question": "How to detect an outlier in one of my biological samples (4 samples 7 unique measurements of each)?"
            }, 
            "id": "cpzzxxe"
        }, 
        {
            "body": {
                "answer": "<p>They are certainly not paired -- too many uncontrolled variables.  <br><br>You<sq>ve given no reason whatsoever for us to think that observation #1 in the first sequence is more like observation #1 in the second sequence than it is like observation #i in the second sequence, etc.<br><br>Kolmolgorov-Smirnov (or some other distribution test) would test whether the distributions are the same<colon> mean, variance, shape, etc....and that<sq>s what you care about.  If it rejects, plot them and figure out whether it is a location shift, variability shift, etc.</p>", 
                "question": "Paired or two sample t-test?"
            }, 
            "id": "cpzqnq0"
        }, 
        {
            "body": {
                "answer": "<p>Two sample.</p>", 
                "question": "Paired or two sample t-test?"
            }, 
            "id": "cpy82op"
        }, 
        {
            "body": {
                "answer": "<p>I just read a paper that did this comparison and if I remember correctly they used what was called an r to z transformation. I<sq>m not familiar with the procedure so it may be best to wait for more experienced users to chime in but that might get you going  in the right direction. </p>", 
                "question": "How can I compare two r^2 values"
            }, 
            "id": "cpxb7xq"
        }, 
        {
            "body": {
                "answer": "<p>go to a general linear model and put in it dummy coded variable to indicate whether its treatment or control group that will interact with your predictor variable, if the correlation is significantly different between the treatment and control groups you will get a significant interaction</p>", 
                "question": "How can I compare two r^2 values"
            }, 
            "id": "cpxpxyf"
        }, 
        {
            "body": {
                "answer": "<p>It<sq>s really hard to do statistics on something like this. If you see the coworker 100<percent> of the time then we have no way to estimate his or her off time. It<sq>s unlikely that the assumptions apply, because, as Obers_Herzog mentioned, you have to conclude that the coworker works 24 hours per day. Impossible scenario.<br><br>That said, we can do math. In the scenario you describe, you know that the coworker works at least as much as you.</p>", 
                "question": "I do shift work. If 100<percent> of the time that I am there a specific co-worker is there can I reliably conclude that it is likely that this co-worker works a lot more hours than me?"
            }, 
            "id": "cpvjgl7"
        }, 
        {
            "body": {
                "answer": "<p>If the shift distribution is completely random, then you can conclude that he works more or the same as you. And if he is there whenever you are then as the data sample grow you will find he works 24 hours a day.</p>", 
                "question": "I do shift work. If 100<percent> of the time that I am there a specific co-worker is there can I reliably conclude that it is likely that this co-worker works a lot more hours than me?"
            }, 
            "id": "cpvfsly"
        }, 
        {
            "body": {
                "answer": "<p>I wouldn<sq>t think that<sq>s a reliable conclusion, especially since you probably know more about the situation than you<sq>ve described. For instance, as /u/Oberst_Herzog and /u/Reductive have pointed out, you probably know something about both the expected and the maximum number of work hours that someone in your field can do per week. Another factor that hasn<sq>t been mentioned is whether you and your co-worker perform related functions. To see why this might matter, take an example from baseball. A given catcher might always play with a specific pitcher. He sees his colleague at every shift not because the colleague works so many more hours, but because the (somewhat) randomly assigned shifts are not random at the individual level, but at the tandem level.</p>", 
                "question": "I do shift work. If 100<percent> of the time that I am there a specific co-worker is there can I reliably conclude that it is likely that this co-worker works a lot more hours than me?"
            }, 
            "id": "cpvp48s"
        }, 
        {
            "body": {
                "answer": "<p>There<sq>s some subjectivity in what your definition of significant is.. But in the most simplistic case, this seems to me like a one sample proportion scenario. Check out how to calculate a test statistic or confidence interval and compare his win rate to 0.5. <br>[Check out the Normal approximation section](http<colon>//en.m.wikipedia.org/wiki/Binomial_proportion_confidence_interval#Normal_approximation_interval) for the easiest implementation.</p>", 
                "question": "Sorry for this extremely basic question - how many games of Gin Rummy would I have to play against my friend to determine if he is a significantly better player than me?"
            }, 
            "id": "cpu4gjl"
        }, 
        {
            "body": {
                "answer": "<p>How many observations do you generate ? <br><br>If you do all cross-interaction, you<sq>d get 26^2 + 26 variables for your multilinear regressions. If you have less observation than that, you will necessarly get a r^2 =1</p>", 
                "question": "A few questions about a multiple regression project I am creating using R."
            }, 
            "id": "cpsxbvm"
        }, 
        {
            "body": {
                "answer": "<p>This was just posted yesterday<colon><br>http<colon>//www.reddit.com/r/programmingtools/comments/2ve1ty/generatedatacom_an_awesome_site_to_generate_data/<br><br>Refers to<colon><br>http<colon>//www.generatedata.com/<br>and<colon> http<colon>//www.mockaroo.com/<br></p>", 
                "question": "A few questions about a multiple regression project I am creating using R."
            }, 
            "id": "cpt6gsg"
        }, 
        {
            "body": {
                "answer": "<p>Can you show all your code?<br><br>What are the values in A1, A2, etc.  How are you generating them?<br><br>I don<sq>t think you need the loop.<br><br><br>      Y=2.5*A1+3*A4*B3+2*A2*A3*B18+rnorm(1,0, N)<br><br>should work.<br><br><br></p>", 
                "question": "A few questions about a multiple regression project I am creating using R."
            }, 
            "id": "cptncg5"
        }, 
        {
            "body": {
                "answer": "<p>It<sq>s not entirely clear to me what your question is. What kind of variables do you want to correlate? Is your data *ranging* from 0 to 1 or is it dichotomous (0 *or* 1)? </p>", 
                "question": "Pearson and spearman correlation"
            }, 
            "id": "cprjvwk"
        }, 
        {
            "body": {
                "answer": "<p>Here is one wacky idea.  Make a 3-d graph with iso-probs (like iso quants in economics).  For example, [here is a plot of the logistic CDF where the equation is 1/(1+exp-(2x-1.5y-.2xy))](https<colon>//drive.google.com/file/d/0B3-F8BTZSbH9a2lMaGFDRHVqMW8/view?usp=sharing).  A little more useful is the [over head view, or top down projection](https<colon>//drive.google.com/file/d/0B3-F8BTZSbH9SEJxTWE4NXQ0N3M/view?usp=sharing).  These let you see some lines which each have equal probability of being y=1, increasing as you go from green to purple.  [Here is a third view with just the lines without the background colors.](https<colon>//drive.google.com/file/d/0B3-F8BTZSbH9WUpqcHpFRWNYMlk/view?usp=sharing)<br><br>Here I used Maple, but similar graphs can be made in Maxima (free).  Send me your equation and I send some code to make a nice version in Maxima with a legend for specified probabilities.<br><br><br></p>", 
                "question": "After logistic regression found an interaction can you draw a line of best fit through proportional data values on a graph?"
            }, 
            "id": "cpo581j"
        }, 
        {
            "body": {
                "answer": "<p>Just run a nonparametric correlation of the two variables. Spearman<sq>s rho should be good here.<br><br>Why? Correlation because you have two variables and you want to see if they<sq>re related. Nonparametric because <dq>level of trust<dq> isn<sq>t really numeric (it<sq>s an interval scale).</p>", 
                "question": "I want to compare peoples age and their levels of trust in Westminster to see if there is a relationship in SPSS. What type of analysis would be most appropriate?"
            }, 
            "id": "cpn1fm5"
        }, 
        {
            "body": {
                "answer": "<p>Any chance you can link the thread you mentioned?</p>", 
                "question": "Why PCA versus factor analysis? (With special reference to genetics)"
            }, 
            "id": "cpkuuvq"
        }, 
        {
            "body": {
                "answer": "<p>Barring the path-modeling approaches, factor analysis is, in general, just a rotated set of components produced by PCA. Rotations fit some  optimization criterion (e.g., varimax, oblimin). <br><br>There is no reason PCA can<sq>t be used in genetics\u2014or any other field for that matter.<br><br><br>PCA is simply the eigendecomposition of a correlation (or covariance) matrix (which is also identical to the SVD of a properly preprocessed rectangular table -- often centered and scaled).<br><br><br>PCA finds <dq>components<dq>. Components are new variables that are combinations of the original variables (i.e., coefficients in a linear equation) and works as follows<colon> <br><br>(1) Find the first component which explains maximum variance. <br><br>(2) Regress out/remove the first component from the data set<br><br>(3) Repeat steps until no variance exists.<br><br><br>This guarantees that each component discovered has two properties<colon> (1) maximal variance subsequent to prior components, and (2) all components are orthogonal.<br><br><br>Maximal variance and orthogonality are some of the simplest, and most important, properties in any statistical application. <br><br><br>A few final points<colon><br><br>(1) PCA used in GWAS are typically for identify, as you allude to, a population structure. If a component can be explained by, essentially, the direct opposition of two groups -- you<sq>ll want to remove that component (i.e., regress out the iformation explained by that component) from the data set and then do your analysis.<br><br>(2) PCA is used as a technique in many fields to either just visualize high dimensional data or find large sources of variance or to remove low variance information<br><br>(3) Could you link to that thread or provide more details about that thread?</p>", 
                "question": "Why PCA versus factor analysis? (With special reference to genetics)"
            }, 
            "id": "cpl2m03"
        }, 
        {
            "body": {
                "answer": "<p>You need some estimate of the effect size for the biomarker predicting the medical condition. Does a biomarker that<sq>s 12 points (1 sd) higher indicate you are 5<percent> more likely to have the condition? 50<percent>? An odds ratio would probably be ideal.</p>", 
                "question": "Sample size calculation of an observational study"
            }, 
            "id": "cperkfc"
        }, 
        {
            "body": {
                "answer": "<p>You are a novice, and worried about lots of wrong things for the wrong reasons.  I am not trying to be demeaning, just trying to help.  Please consult some or all of my econometrics series of videos at http<colon>//www.burkeyacademy.com/home/statistics-econometrics<br><br>Here are some brief comments<colon><br><br>1) Having a high R^2 is not very important, in general.  Somethings are simply hard to explain (at least with available data), and thus will have a very low R^2.  Don<sq>t obsess over it.<br><br>2) You say that your DATA violates normality.  DATA and NORMAL have nothing to do with assumptions for a regression.  What are <sq>ideally<sq> normal are the residuals-- not the <sq>data<sq>.   However, even if your residuals are not normally distributed, this only means two things<colon><br><br>a) there may be a more efficient estimator, e.g. maximum likelihood instead of OLS.  No biggie.<br><br>b) Since the t  distribution is based on the assumption of normal residuals, if they are not, the consequence is that your p values will be calculated imprecisely.  Again, not something to obsess over-- you just need to be a little skeptical of the p values, and allow that the true p value might be larger.<br><br>3) Heteroskedasticity<colon> Easy fix.  White<sq>s correction.  Not perfect, but pretty darn useful.<br><br>>I can get significant results<br><br>4) Dammit, this is not your goal. \u263a Your goal is SUPPOSED to be learning something about the world.  <dq>Getting significant results<dq> is a trivial exercise for a skilled p-hacker.  <dq>Trying to get as accurate results as possible<dq> is hard, and is the scientific pursuit that you should want to be engaged in.</p>", 
                "question": "Dealing with abnormal data"
            }, 
            "id": "cpe85mw"
        }, 
        {
            "body": {
                "answer": "<p>Try <br>gen single_male = 1 if hhsize==1 & r_sex==1</p>", 
                "question": "Using the Egen Command in STATA- multiple properties"
            }, 
            "id": "cpb5afe"
        }, 
        {
            "body": {
                "answer": "<p>try using <dq>==<dq><br><br>I<sq>m not familiar with egen, but i know gen. so under gen it would go<colon><br><br>gen single_male = 1 if hhsize==1 & r_sex==1, by(houseid)<br><br>gen single_female = 1 if hhsize==1 & r_sex==2, by(houseid)<br><br>edit<colon><br><br>you can also just do<colon><br><br>gen single_male =.<br><br>recode single_male .=1 if hhsize==1 & r_sex==1<br><br>by houseid (year**if year var is relevant), sort<colon> replace single_male = 1 if (then if conditions if needed)<br><br>*that last statement will <dq>fill in<dq> the missing variables if you have multiple observations with the same houseid and you want the single_male variable to be the same for all with same houseid - then (year) if you want to restrict it to only houseid variables within the same year.<br><br>and then just<colon><br><br>recode single_male .=0<br><br>^this will complete the variable as a binary 1 or 0 (1 for yes)</p>", 
                "question": "Using the Egen Command in STATA- multiple properties"
            }, 
            "id": "cparceq"
        }, 
        {
            "body": {
                "answer": "<p>You say that you have already clustered data based on euclidean distance. This can mean lots of things, but in the simplest case you are creating 10 clusters, each of which has a center.  When adding a new data point it seems to me that you would either<colon><br><br>1) recreate the clusters totally (probably not a great idea), or<br><br>2) See which of the centroids of the 10 groups the new data point is closest to.  Calculate 10 distances, find the minimum, and there you go.<br><br>Perhaps there is something more complicated about this that isn<sq>t clear to me. I am not sure why regressions or decisions trees would be needed...</p>", 
                "question": "What<sq>s the best approach for supervised distance-based classification?"
            }, 
            "id": "cpaiajs"
        }, 
        {
            "body": {
                "answer": "<p>Look up <dq>overfitting<dq>.</p>", 
                "question": "Good argument for simpler statistical tests"
            }, 
            "id": "cp4dxly"
        }, 
        {
            "body": {
                "answer": "<p>The 2x2x2 design is just one way to partition the SSQ among 7df. Mathematically, it is no better than any other 7 orthogonal contrasts. If you have an a priori alternative that tests specifically what you are interested in, then that is what you should use. </p>", 
                "question": "Good argument for simpler statistical tests"
            }, 
            "id": "cp4gbz2"
        }, 
        {
            "body": {
                "answer": "<p>Planned comparison allow you to do fewer posthoc tests, which means you have the correction for multiple testing will be less stringent.<br><br>I don<sq>t necessarily see the factorial anova as being simpler. Simpler in what sense?</p>", 
                "question": "Good argument for simpler statistical tests"
            }, 
            "id": "cp57h36"
        }, 
        {
            "body": {
                "answer": "<p>You have familiarity with a large variety of methods, but I would ask what specifically you<sq>d be responsible for doing. With google-fu and self-teaching abilities, you can teach yourself pretty much any code or statistical method. I have taken many courses in statistics and that<sq>s still how I learn new things. Statistical analysis using R or SAS or Stata may be fun for you, since that is the software-based side of analysis.   <br>  <br>So, in summary, I<sq>d say do what you want to do. If you love the statistics, go for it. It will be hard, but you will come out with more skills and knowledge. Let your interest/motivation carry you where it will. </p>", 
                "question": "Software Engineer thinking about moving to Statistics role"
            }, 
            "id": "cp42mue"
        }, 
        {
            "body": {
                "answer": "<p>Honestly, I am not sure if I understand your setting<colon><br><br>* You  have a blood and a urine sample per patient. <br>* If all blood samples are positive, why taking them at all?<br>* So, you are not just taking two samples but multiple pairs?</p>", 
                "question": "Help in figuring out what type of test to use"
            }, 
            "id": "cp2jvqp"
        }, 
        {
            "body": {
                "answer": "<p>Give that the transaction requires change be given and if every transaction has an equal probability of having any number of cents being charged, from .01 to .99, then there is an equal probability of receiving all values of change, from .99 to .01.<br><br>This is then a discrete uniform distribution for change from 0.01 to 0.99, with values at each cent, 0.01. The mean of this distribution is the endpoints divided by two<colon> (0.01 + 0.99)/2 = 0.5. <br><br>Intuitively, if all possible outcomes are equally likely, the average is simple mean of all possible outcomes. (0.01 + 0.02 + ... + 0.99) / 99 = 0.5<br><br>However, all of this assumes that the customers receive change in the first place. If we add the possibility that the charge can be even in terms of dollars, than we have to add .00 has a possible outcome. In such a case, the average value of change in any transaction would be (0.00 + 0.99)/2 = 0.495.<br><br>Of course in reality, not all charges are equally likely. Many items are priced at X.99 and there is sales tax to consider as well. This changes everything.</p>", 
                "question": "Cash transactions<colon> What is the average value of coins a customer receives in change?"
            }, 
            "id": "cp1eevk"
        }, 
        {
            "body": {
                "answer": "<p>That depends on the country.</p>", 
                "question": "Cash transactions<colon> What is the average value of coins a customer receives in change?"
            }, 
            "id": "cp1syrg"
        }, 
        {
            "body": {
                "answer": "<p>You should go more into the details of your trials. Both trials had the same patients? You want to test whether there is a different or whether both trials alre similar? What kind of data do you have<colon> Continuous, ordinal, nominal?</p>", 
                "question": "Which tests for my data I have a vague idea."
            }, 
            "id": "cp2k00k"
        }, 
        {
            "body": {
                "answer": "<p>You<sq>re conflating significance with effect size. The p-value tells you that your samples have a low probability of being drawn from the same distribution. The boxplots are telling you that, assuming there are two different distributions, they have very similar location and dispersion parameters, so a very small effect size.</p>", 
                "question": "Mann-Whitney U test suggests significant difference but box plots look very similar"
            }, 
            "id": "covtsrr"
        }, 
        {
            "body": {
                "answer": "<p>Are you really interested in testing the hypothesis<colon> <dq>Is the average rank of sample 1 different from the average rank of sample 2?<dq> Because that is what the Mann-Whitney test does, and your p-value is telling you that yes, the average rank of these two samples is different. So what? Do you care? What is your real question?<br><br>>Is there a more suitable [test] given the distribution of my data?<br><br>Maybe. Again what is your question? Do you care if some central tendency of these two groups is different? If so you<sq>ve got options<colon> hitch a ride on the assumption train to Asymtopia and embrace the t-test. Or if you really think that sample sizes of 1400 and 1000 aren<sq>t going to you the magical land where unicorns and Central Limit Theorem live, then pick your favorite statistic (sample median maybe, or get down with your sample mean) and rock a permutation test.</p>", 
                "question": "Mann-Whitney U test suggests significant difference but box plots look very similar"
            }, 
            "id": "cowgcql"
        }, 
        {
            "body": {
                "answer": "<p>Poisson distribution or negative binomial distribution are the best options.  Poisson distributions model event per time period while the exponential distribution models time between events.  </p>", 
                "question": "What distribution would the number of sales of a product in a given time period follow?"
            }, 
            "id": "colf8gy"
        }, 
        {
            "body": {
                "answer": "<p>> (time/events)<br><br>Not Poisson. Time is continuous.<br><br>*If* events follow a Poisson process you<sq>d expect the number of events per unit time to be Poisson but in practice you either have dependence or heterogeneity or both.<br><br>Frequently negative binomial is reasonable. Sometimes zero-inflated Poisson or a Poisson mixture will do quite well. Sometimes none of them are sufficient.<br></p>", 
                "question": "What distribution would the number of sales of a product in a given time period follow?"
            }, 
            "id": "com036i"
        }, 
        {
            "body": {
                "answer": "<p>[deleted]</p>", 
                "question": "Coin toss problem"
            }, 
            "id": "colh14m"
        }, 
        {
            "body": {
                "answer": "<p>This is a really great question.  You<sq>re essentially asking about getting a string of predictions correct or wrong in a row.  There was a [numberphile video](http<colon>//youtu.be/rwvIGNXY21Y) (or at least, the guy from numberphile) on how long it would take to toss 10 straight heads, so this is relevant to your question.  I<sq>m not quite sure about the answer, but my guess is that it<sq>s on the order of 1000-2000 tosses. On the video, each trial could be a number of tosses, so that needs to be taken into account.</p>", 
                "question": "Coin toss problem"
            }, 
            "id": "cola5at"
        }, 
        {
            "body": {
                "answer": "<p>Additionally, what if I only care about the variance between one groups and the others. I.E Test A vs Test B, Test A vs Test C but I don<sq>t care about Test B vs Test C. What would I do in that case?</p>", 
                "question": "Am I understanding statistical tests correctly?"
            }, 
            "id": "coe2bqw"
        }, 
        {
            "body": {
                "answer": "<p>A good choice is Dunnett<sq>s test [ref 1](http<colon>//en.m.wikipedia.org/wiki/Dunnett<percent>27s_test) [ref 2](http<colon>//davidmlane.com/hyperstat/B112114.html)</p>", 
                "question": "Am I understanding statistical tests correctly?"
            }, 
            "id": "coff8b6"
        }, 
        {
            "body": {
                "answer": "<p>It<sq>s not a hack but a widely used approach. (but you should say what was done and how many simulations were used)<br><br>Because it gives a sample proportion of values more extreme than observed it<sq>s a binomial proportion, so you can compute the standard error of the p-value (or an interval for it if you prefer). <br><br>However, if you do enough simulations (typically I do hundreds of thousands, computers are fast these days) that standard error figure will be so small that it<sq>s less than the accuracy I quote the esitmate to (i.e. if the p-value is 0.03281 and you did 500,000 simulations, the standard error is sqrt(.03281*(1-.03281)/500000), or about 0.000252. If you say <dq>p = 0.033<dq>, it will be sufficiently accurate to simply quote it to that many figures.<br><br></p>", 
                "question": "Reporting a Fisher<sq>s Exact Test P-Value that uses Monte Carlo estimation?"
            }, 
            "id": "coalbv8"
        }, 
        {
            "body": {
                "answer": "<p>When your BS variation is small and your WS variation is large some stat programs will report a negative ICC.  ICC is generally thought to be bounded to [0,1] but it can actually go below zero because are estimating an effect.</p>", 
                "question": "ICC coefficient of -29? How does this happen?"
            }, 
            "id": "co8yhn6"
        }, 
        {
            "body": {
                "answer": "<p><colon>/ did someone goof? ICC supposed to be 0-1... </p>", 
                "question": "ICC coefficient of -29? How does this happen?"
            }, 
            "id": "co8t9f1"
        }, 
        {
            "body": {
                "answer": "<p>Cronbach<sq>s alpha is a form of ICC, and that can go negative when you<sq>ve forgotten to reverse some items. Is it possible that something like that has happened?</p>", 
                "question": "ICC coefficient of -29? How does this happen?"
            }, 
            "id": "co9htjl"
        }, 
        {
            "body": {
                "answer": "<p>So in a study like this the results are going to depend highly on the demographics being asked. Do you have a link to the study?<br><br>In general, however, you would be surprised how small samples can be while still being reliable. Again, its depends highly on the nature of the population you are inquiring about as well as the model you are trying to fit. </p>", 
                "question": "Question about Interpreting Statistics"
            }, 
            "id": "co0atlf"
        }, 
        {
            "body": {
                "answer": "<p>2,500 sounds like a lot of people. For most studies where you want a nationally representative number at the end (e.g., X<percent> say Y), you can probably get by with about 1000 people. It all comes down to the sampling methodology and the weighting used. </p>", 
                "question": "Question about Interpreting Statistics"
            }, 
            "id": "co0b1h3"
        }, 
        {
            "body": {
                "answer": "<p>If the 2500 people represented a random sample of the population\\* then you can get a pretty good idea of the actual proportion in the population. <br><br>\\* more likely, it will be a *stratified sample*, but the principle still applies; you can get quite good estimates of the population proportion. Also it<sq>s likely not quite *everyone* has an equal chance to appear (if you<sq>re phoning people, you won<sq>t get people who don<sq>t have a phone, for example, while if you<sq>re doorknocking, you<sq>ll tend to miss the homeless and so on). Sometimes polling companies go to some length to deal with the possibility of missing/undersampling such difficult-to-sample subgroups, other times they don<sq>t (often because the people commissioning the study don<sq>t wish it). Nevertheless, *if* such groups are small, undersampling them doesn<sq>t change things terribly much.</p>", 
                "question": "Question about Interpreting Statistics"
            }, 
            "id": "co0f3t3"
        }, 
        {
            "body": {
                "answer": "<p>This seems pretty reliable-- IF the survey elects a fairly random sample of the population, then this answer is accurate to a least +/- 2<percent>.  Looking at historical figures, this number has hovered around 1 in 4 for decades now.<br><br>On the upside, the US seems to compare decently to other countries on these measures. <br><br>http<colon>//www.nsf.gov/statistics/seind12/c7/tt07-09.htm<br></p>", 
                "question": "Question about Interpreting Statistics"
            }, 
            "id": "co0h9fo"
        }, 
        {
            "body": {
                "answer": "<p>Sounds like you got yourself a regular ole randomized block design and also a balanced design. You want to treat field as random effect, and soil as a fixed effect. In practice this is easy to do in R, especially because you<sq>ve got the balance. It<sq>s as simple as<colon> aov(yield~soil+field, data=wtfjen<sq>s_data). (Make sure both soil and field are of the *factor* class.) Technically treats both factors as fixed, but gives you the equivalent answer to treating field as random.<br><br>In general though, you should be careful with just using aov(), it may not give you the equivalent answer to a true random effects model as using lme for example. Check out [this thread](https<colon>//stat.ethz.ch/pipermail/r-help/2007-December/147610.html) for more on that.</p>", 
                "question": "How can I factor out a categorical variable that isn<sq>t relevant to the question I<sq>m pursuing?"
            }, 
            "id": "cnz24va"
        }, 
        {
            "body": {
                "answer": "<p>Are you interested in differences *between* the three BMI groups, or do you want to look at them independently?</p>", 
                "question": "question with multiple regression with an ordinal/categorical dependent variable?"
            }, 
            "id": "cnxzjz8"
        }, 
        {
            "body": {
                "answer": "<p>You need to use multinomial logistic regression (or several binary logistic regressions).<br><br>What<sq>s your sample size?</p>", 
                "question": "question with multiple regression with an ordinal/categorical dependent variable?"
            }, 
            "id": "cny10xy"
        }, 
        {
            "body": {
                "answer": "<p>I<sq>m not sure how density and intensity are defined in this process, but I think I have some insight on the expected number issue. This is incomplete and may have formatting issues as I am on mobile.<br><br>Say you have a school and a school bus arrives every 5 minutes. When the bus parks, all of the students on board get out and stand near the bus<sq> door. The expected number of students near a given door would be /mu, and the area in which they are clustered is /pi * r^2.<br><br>The arrival of students at the school may be considered a Poisson process. Not all busses have the same number of students, but the distribution of students per bus may follow a Poisson distribution governed by /mu.</p>", 
                "question": "Poisson process question"
            }, 
            "id": "cnvj99l"
        }, 
        {
            "body": {
                "answer": "<p>I<sq>m not totally sure what the design is and what sort of data you<sq>re working with...your product<sq>s function is to determine whether a player is or is not holding a ball. However, you<sq>re testing this by throwing the ball at the person. Does this mean your product is supposed to ascertain whether they did or did not successfully catch the ball?<br><br>Once I<sq>m clear on that, maybe I can help. In general, though, it sounds like you<sq>re going to be operating in the space of signal detection analyses. That is, if I<sq>m understanding your design correctly, you<sq>re interested in how accurate your product is. Quantitatively, this will likely pan out in terms of True and False Positives, and True and False Negatives (that is, the correspondence between when your product determines that someone has the ball and when they actually do).<br><br>Here<sq>s a basic rundown of signal detection theory<colon> http<colon>//www-psych.stanford.edu/~lera/psych115s/notes/signal/<br><br>These sorts of data are often plotted on receiver operating characteristic (ROC) curves, which plot True Positive vs. False Positive rate. You<sq>ll see this a lot in literature about clinical diagnoses, but it works fine for any case where you care about how accurate a decision-making process is. See this classic paper for a more detailed description<colon> http<colon>//www.umich.edu/~ners580/ners-bioe_481/lectures/pdfs/1978-10-semNucMed_Metz-basicROC.pdf<br><br>One thing to consider - it<sq>s fine and good to demonstrate that your product is good at detecting true scenarios of someone having the ball (a high d<sq> (d-prime) value on an ROC curve, the higher your d<sq>, the more <dq>bowed upward<dq> the curve is, indicating better performance). However, it<sq>s more powerful when you have a control condition against which you can compare your targeted manipulation. This gives you a very simple x is better than y, p < 0.05 sort of statement. Another alternative is to test your observed classification accuracy against chance.</p>", 
                "question": "How to determine sample size for a test and which test to apply?"
            }, 
            "id": "cnvpp6l"
        }, 
        {
            "body": {
                "answer": "<p>Your friend is correct, as long as you<sq>re prepared to flip as many times as necessary.<br><br>Alternatively, if there<sq>s some finite number of tosses you<sq>d not be prepared to go beyond no matter how rarely, you can lump final <dq>reroll<dq> (re-toss) results into 2 and 5 (say) and deal with the fact that it<sq>s *incredibly close* to a fair d6 (perhaps a deal closer than typical manufacturing is capable of anyway).<br><br>So, retossing <dq>triples<dq> (HHH and TTT), you have<colon><br><br>HHT=1<br>HTH=2<br>HTT=3<br>THH=4<br>THT=5<br>TTH=6<br><br>If you were only prepared to toss 10 times at most, and the final HHH mapped to 2 and the final TTT mapped to 5, with fair coins the probability of tossing 1,3,4,6 are within one part in a million of what they should be, and 2,5 are within 2 parts in a million.<br><br>Since you<sq>ll never toss enough times to get close to being able to distinguish this from fair, it<sq>s fair for any practical purpose.</p>", 
                "question": "Probability question"
            }, 
            "id": "cnvz5in"
        }, 
        {
            "body": {
                "answer": "<p>Did you leave out part of what you meant to say? It seems like you start in the middle. What is this about?</p>", 
                "question": "Probability question"
            }, 
            "id": "cnudlka"
        }, 
        {
            "body": {
                "answer": "<p>I am trying to remember the name of the method where you randomly remove 1 sample\u2019s value and determine the average from that, then randomly remove 2 sample\u2019s values from the original dataset and determine the average from that, remove 3 samples etc. etc.</p>", 
                "question": "Question regarding estimating sampling requirements based on previous data sets"
            }, 
            "id": "cnntqi5"
        }, 
        {
            "body": {
                "answer": "<p>Assuming you are risk neutral, you should play if your individual bill is above the average of everyone else playing. If the bill is going to be split evenly and you pay your portion of the bill if you don<sq>t play, then it doesn<sq>t matter if you play or not.<br></p>", 
                "question": "Credit Card Roulette"
            }, 
            "id": "cnnssoe"
        }, 
        {
            "body": {
                "answer": "<p>Hello, I think I solved a general case for your problem. <br>http<colon>//imgur.com/Vm5dsGy</p>", 
                "question": "Credit Card Roulette"
            }, 
            "id": "cnof3kg"
        }, 
        {
            "body": {
                "answer": "<p>A z score of 0,76 lies between the corresponding P of 0,2 or 0,25 as you can see in the first and second column.<br><br>You could approximate linearly between these points and obtain approx 0,225 ( because 0,76 lies in between 0,67 and 0,84 in column 2).<br>Because it is technically not allowed to do this, due to the curve in the normal distribution, one must plug in the formula above. This is done with software and you will then obtain 0,224 as a correct corresponding signifiance level for a z score of 0,76.</p>", 
                "question": "How to find Z values from this normal distribution table?"
            }, 
            "id": "cnnjd8t"
        }, 
        {
            "body": {
                "answer": "<p>Good math background, but take the GRE again. Get that math score into the high 90s.</p>", 
                "question": "Statistics Graduate School<colon> How are my chances?"
            }, 
            "id": "cnlqi4z"
        }, 
        {
            "body": {
                "answer": "<p>Why are you only applying to one place? The thing is, if you<sq>re a great candidate, but there just aren<sq>t enough slots at that one program, you<sq>re out of luck.</p>", 
                "question": "Statistics Graduate School<colon> How are my chances?"
            }, 
            "id": "cnltk4t"
        }, 
        {
            "body": {
                "answer": "<p>You need to apply to more places.  Your portfolio looks solid as hell outside of the GRE.  Make sure to get a GRE review book so you know how they want you to answer the questions.  Sometimes having a lot of math knowledge may hurt you if you over think things or see things different than the test-maker.<br><br></p>", 
                "question": "Statistics Graduate School<colon> How are my chances?"
            }, 
            "id": "cnm3cri"
        }, 
        {
            "body": {
                "answer": "<p>It<sq>s a trick question.<br><br>Want to try again, before I tell you?<br><br>Scroll down.<br><br>Keep going.<br><br><br><br><br><br><br><br>The p-value that you<sq>ve calculated is correct IF you have a two-tailed hypothesis. It<sq>s a one tailed hypothesis, it says greater than 0. B3 is less than zero, so the p-value is (I guess) 1.00. (Or it<sq>s very high).<br><br>It<sq>s a stupid thing to do. You should never actually test a one-tailed hypothesis. </p>", 
                "question": "Need help on multiple regression. <colon>)"
            }, 
            "id": "cnkpmjf"
        }, 
        {
            "body": {
                "answer": "<p>I think what your friend is talking about is the union of events. <br><br>If A is the event you get accepted to school A, and B the even you get accepted to school B, etc...,  then, according to your probabilities<colon><br><br>P(A) = .055, P(B)=.08, P(C)=.145, P(D)=.19, P(E)=.235<br><br>If the events were mutually exclusive you could add them together, but they are not, since getting accepted to one school doesn<sq>t preclude you from getting accepted to a 2nd, 3rd, 4th or 5th school. So you cannot add them up, you need to take into consideration the possibility of being accepted to more than 1 school, otherwise you<sq>d be double counting. If this doesn<sq>t make sense look up Venn diagrams and the union of mutually exclusive events vs non mutually exclusive events.<br><br>Some background<colon> In set theory the probability of either A or B or both occurring is referred to as the union of A and B and is written P(A U B). If A and B are mutually exclusive events<colon> <br><br> P(A U B) = P(A)+P(B)<br><br>If A and B are NOT mutually exclusive, then<colon><br><br>  P(A U B) = P(A)+P(B)-P(A \u2229 B)<br><br>where P(A \u2229 B) is the probability of both A and B occurring, referred to as the intersection of A and B. If A and B are independent, which in this case I think they are, then<colon><br><br> P(A \u2229 B) = P(A)*P(B)<br><br>Extending this to the 5 events, the general pattern is<colon><br><br> * Add the probabilities of the individual events.<br> * Subtract the probabilities of the intersections of every pair of events.<br> * Add the probabilities of the intersection of every set of three events.<br> * Subtract the probabilities of the intersection of every set of four events.<br> * Continue this process until the last probability is the probability of the intersection of the total number of sets that we started with.<br><br>So for the grand finale, we get<colon><br><br>P(A U B U C U D U E) = P(A)+P(B)+P(C)+P(D)+P(E)-P(A \u2229 B)-P(A \u2229 C)-P(A \u2229 D)-P(A \u2229 E)-P(B \u2229 C)-P(B \u2229 D)-P(B \u2229 E)-P(C \u2229 D)-P(C \u2229 E)-P(D \u2229 E)+P(A \u2229 B \u2229 C)+P(A \u2229 B \u2229 D)+P(A \u2229 B \u2229 E)+P(A \u2229 C \u2229 D)+P(A \u2229 C \u2229 E)+P(A \u2229 D \u2229 E)+P(B \u2229 C \u2229 D)+P(B \u2229 C \u2229 E)+**P(B \u2229 D \u2229 E)**+(C \u2229 D \u2229 E)-P(A \u2229 B \u2229 C \u2229 D)-P(A \u2229 B \u2229 C \u2229 E)-P(A \u2229 B \u2229 C \u2229 D)-P(A \u2229 C \u2229 D \u2229 E)-P(B \u2229 C \u2229 D \u2229 E)+P(A \u2229 B \u2229 C \u2229 D \u2229 E)<br><br> And remember for the probability of the intersections, since the events here are independent, you can simply multiply them. For example, P(A \u2229 B) =P(A)*P(B)=(.055)*(.08)=.0044 and P(A \u2229 B \u2229 C)=P(A)*P(B)*P(C)=(.055)*(.08)*(.145)=.000638.<br><br>Disclaimer<colon> I know how to do this in theory but never actually had to calculate the union of 5 events so its possible I messed that equation up. Also it<sq>s so tedious that I<sq>ll leave it as an exercise to the reader to calculate <colon>P<br><br>edit<colon> formatting</p>", 
                "question": "Independent alternatives. Does this make sense?"
            }, 
            "id": "cnih2xp"
        }, 
        {
            "body": {
                "answer": "<p>/u/buttfoot<sq>s answer is exactly right. And to give you an example of why your friend<sq>s math is wrong, say that you weren<sq>t feeling particularly ambitious and applied only to schools Y and Z, which are bottom of the barrel law schools. You think your chance of getting into school Y is 80<percent>, and your chance of getting into school Z is 90<percent>.<br><br>Your friend<sq>s math would tell you that your chance of getting into law school is 0.8 + 0.9= 1.7! While it would be nice to have the confidence that you<sq>re 170<percent> getting in somewhere, that can<sq>t be right.<br><br>So we need to also account for the probability of getting into both Y and Z, which your friend is double counting since P(Y) includes P(Y and Z) and P(Z) also includes P(Z and Y). We only want to count it once, so we need to subtract P(Y and Z) once. Let<sq>s say that the chance you get into both Y and Z is 0.75. So, the true probability of getting into at least one school (P(Y or Z or both)) is<colon><br><br>P(Y) + P(Z) - P(Y and Z)= 0.8 + 0.9 - 0.75 = 0.95. <br><br>So you have a 95<percent> chance of getting into Y, Z, or both schools, which isn<sq>t quite as awesome as a 170<percent> chance but makes a lot more sense mathematically!</p>", 
                "question": "Independent alternatives. Does this make sense?"
            }, 
            "id": "cniji3f"
        }, 
        {
            "body": {
                "answer": "<p>Your friend is wrong that you can<sq>t add them.  If you want to find the probability of getting into at least one school and you assume independence, then you take 1 minus the probability of getting declined by each school.  As in, 1 - .945 * .92 * .855 * .81 * .765.  <br><br>However, the assumption of independence is incorrect.  If a student is a good candidate for one school, he/she is also a good candidate for another.  Everyone<sq>s chances should be lower than if you assume independence because the same people are getting into many schools.</p>", 
                "question": "Independent alternatives. Does this make sense?"
            }, 
            "id": "cniuotr"
        }, 
        {
            "body": {
                "answer": "<p>Imagine you applied to ten schools, where the second lot of 5 had the same percentages as the first 5. <br><br>That would add to 141<percent> .... Does that make sense?<br><br>Clearly not. <br><br>The problem is you<sq>re counting the cases where you get acceptances to more than one school more than once. <br><br>The usual way to do it is (if you can assume the chances are operating independently, which may not be realistic, but might in some situations get close enough) to work out the chances that none of them accept you.<br></p>", 
                "question": "Independent alternatives. Does this make sense?"
            }, 
            "id": "cnj1b5f"
        }, 
        {
            "body": {
                "answer": "<p>It sounds like what you want could be a standard two-sample t-test, with null hypothesis that means for both groups are equal (`t.test()` in R). This would interpret the outcome variable as continuous, and indeed there<sq>s some debate about whether that<sq>s justified (see e.g. [here](http<colon>//www.theanalysisfactor.com/can-likert-scale-data-ever-be-continuous/) or [here](http<colon>//stats.stackexchange.com/questions/10/under-what-conditions-should-likert-scales-be-used-as-ordinal-or-interval-data)). An alternative would be to use Mann-Whitney U test (`wilcox.test()` in R), which uses only information about ranks.</p>", 
                "question": "How to determine sample size (the measures are not really numbers but just ordinals)"
            }, 
            "id": "cneltr1"
        }, 
        {
            "body": {
                "answer": "<p>You could run a power analysis in R.  Here are the functions<colon><br><br>http<colon>//www.statmethods.net/stats/power.html</p>", 
                "question": "How to determine sample size (the measures are not really numbers but just ordinals)"
            }, 
            "id": "cnem9f5"
        }, 
        {
            "body": {
                "answer": "<p>Just a thought<colon> if you<sq>re not particularly interested in each test score, could you produce an overall test score for each individual? Should be easy if all test scores are out of 100.</p>", 
                "question": "Is it valid to plot values for correlation coefficients to compare different outcomes? Is there a better way to present this data? Details inside - input much appreciated!"
            }, 
            "id": "cne4ucx"
        }, 
        {
            "body": {
                "answer": "<p>Though the sample is small, PCA will do just fine to show you which items are correlated with one another, based on orthogonal slices of variance. <br><br>Another option is just to go with correlelograms. </p>", 
                "question": "Is it valid to plot values for correlation coefficients to compare different outcomes? Is there a better way to present this data? Details inside - input much appreciated!"
            }, 
            "id": "cneeiik"
        }, 
        {
            "body": {
                "answer": "<p>I would run 15 regressions with 2 explanatory variables, age and # of years of education.  Then see for which of the 15 tests you get large and possibly statistically significant coefficients for each. The downside is that with small n and a possible correlation between age and # years of education, your power will be low.  But still, you might be able to detect something interesting by doing this.<br></p>", 
                "question": "Is it valid to plot values for correlation coefficients to compare different outcomes? Is there a better way to present this data? Details inside - input much appreciated!"
            }, 
            "id": "cne6rfx"
        }, 
        {
            "body": {
                "answer": "<p>Bayes<sq> Theorem<colon><br>http<colon>//en.wikipedia.org/wiki/Bayes<percent>27_theorem</p>", 
                "question": "Help calculating risk"
            }, 
            "id": "cn6tdi8"
        }, 
        {
            "body": {
                "answer": "<p>Jesus, OP is just asking for the number, not wikipedia.<br>But yeah, bayesian updating is the key. A task almost all of the people do not do at all, due to the linear thinking we use our simplify our daily lifes.<br><br>Because of the (absurd) given parametrical specifications you the following.<br><br>We know<colon> <br><br>Chance a a correct test 0,9 <br><br>Chance of contracting hiv 0,0005 (=1/2000)<br><br>Hence a positive test occurs when<br>1) you have no HIV, but the test is incorrect (a false positive), or<br>1999/2000 * 0,1 = 0,09995<br>2) you have HIV and the test is correct<br>1/2000 * 0,9 = 0,00045<br><br>(Note that because of the 1/2000 ratio the result in 1) is a lot larger than 2.)<br><br>The chance of HIV, given a positive test is thus <br>1) / 1) + 2)  (this is bayes)<br>or <br>0,00045/ (0,00045 + 0,09995)= 0,448<percent><br><br>In less than 1<percent> of the cases will a positive test be associated with actual HIV.<br>Note that this is still small, the intuition being the idea that most positive tests are false positives.<br><br>Hope this helps!<br><br></p>", 
                "question": "Help calculating risk"
            }, 
            "id": "cn6w1ra"
        }, 
        {
            "body": {
                "answer": "<p>They are the same</p>", 
                "question": "Slightly confused about different notations for train vs. test error in An Introduction to Statistical Learning book"
            }, 
            "id": "cn4g6l1"
        }, 
        {
            "body": {
                "answer": "<p>I<sq>ll be cautious considering I haven<sq>t seen the results but here is my take on it. Significance is calculated using the effect size and sample size. Therefore you can have a large effect be significant with a small sample or a small effect be significant with a big sample. In your case it sounds like one of the categories has a large effect and this accounts for the significance overall. Or each category has a weak effect on the outcome but when added together the sample size becomes large enough for the effect to become significant.</p>", 
                "question": "p-values total vs. categories"
            }, 
            "id": "cmxod0j"
        }, 
        {
            "body": {
                "answer": "<p>It<sq>s not clear to me why you find this surprising. <br><br>It<sq>s even quite possible to have non-significant results on every category, yet significant results overall. </p>", 
                "question": "p-values total vs. categories"
            }, 
            "id": "cmy0y29"
        }, 
        {
            "body": {
                "answer": "<p>I can<sq>t tell what your research question is. This type of data might be good for social network analysis for problem #1 or for conditional probabilities for problem #2. Your girlfriend might respond better to F2F chat than in-depth statistical analysis ;)</p>", 
                "question": "Clustering methods (and other methods) to analyze chatroom flirting"
            }, 
            "id": "cmxf2fb"
        }, 
        {
            "body": {
                "answer": "<p>It sounds to me like you just want to take a simple idea and make it complicated. What<sq>s the question you<sq>re trying to answer?<br><br>If you<sq>re looking for ideas, some things you could do<colon><br><br>* Make a scatter plot of response times between two people (this will answer part of your question)<br>* Take a look at a single user, then look at all their response times by user they message. See how these evolve over time.<br>* Build a network diagram of messages between users to understand the different degrees of connection between users<br><br>Other than that, I<sq>m still not really sure what you<sq>re asking. If you get the data and then actually start exploring it a little, I<sq>m sure some ideas will come to you.</p>", 
                "question": "Clustering methods (and other methods) to analyze chatroom flirting"
            }, 
            "id": "cmxcjio"
        }, 
        {
            "body": {
                "answer": "<p>In what way are they non normally distributed? Can you link some histograms?</p>", 
                "question": "Dissertation/ Thesis Statistics help (Methane CO2 flux in a peatland)"
            }, 
            "id": "cmnx6pm"
        }, 
        {
            "body": {
                "answer": "<p>The first image with the right-tailed (positive) skew is definitely a candidate for log-transformation. The second, which I assume is temperature, looks bi-modal, but I think that<sq>s probably an artifact of sampling bias r.e. timing of collection biased towards warmer temps. It might also potentially be a slight left-tail (negative) skew but it<sq>s probably too little to bother correcting. It might be normal enough but it depends on what type of analysis you<sq>re aiming for. You can of course use non-parametric statistics options.</p>", 
                "question": "Dissertation/ Thesis Statistics help (Methane CO2 flux in a peatland)"
            }, 
            "id": "cmoisjh"
        }, 
        {
            "body": {
                "answer": "<p>Are you familiar with [Eureqa](http<colon>//www.nutonian.com/products/eureqa/)?  It has uses in teasing relationships out of complex data.</p>", 
                "question": "Dissertation/ Thesis Statistics help (Methane CO2 flux in a peatland)"
            }, 
            "id": "cmvfd87"
        }, 
        {
            "body": {
                "answer": "<p>Still need help?  We offer free statistical consulting.  Take a look at our service page and send us an email if you still have questions.  http<colon>//centerforopenscience.org/stats_consulting/</p>", 
                "question": "Dissertation/ Thesis Statistics help (Methane CO2 flux in a peatland)"
            }, 
            "id": "coiakiw"
        }, 
        {
            "body": {
                "answer": "<p>What is your response? Is it a measurement? A count? A proportion? something else?<br><br>How large is your sample size?<br><br>What will you be using the model *for*? </p>", 
                "question": "Need help improving linear regression model - normality and transformation concerns."
            }, 
            "id": "cmlfwqs"
        }, 
        {
            "body": {
                "answer": "<p>If most of your explanatory variables are binary, they aren<sq>t exactly continuous which would lead to odd looking residuals</p>", 
                "question": "Need help improving linear regression model - normality and transformation concerns."
            }, 
            "id": "cmle1fs"
        }, 
        {
            "body": {
                "answer": "<p>There is an empirical method for determining whether the dependent variable should be transformed, and what transformation to consider. The technique generates a confidence interval for the power that the dependent variable should be raised to. If <dq>1<dq> is within the confidence interval, then don<sq>t transform it. If <dq>1<dq> is not in it, the consider whether <dq>0<dq> (log) <dq>0.5<dq> (square root), <dq>2<dq> (square), <dq>3<dq> (cube) etc. are within the CI. I learned this technique from a class by cook that used an older edition of cook and weisberg<sq>s applied regression analysis. I expect it is in the current edition.<br><br>I suspect that there are similar approaches for examining transformation of the independent variable.<br><br>You might also consider whether the nature of your variable suggests a specific transformation (e.g., count data suggests log transformations).</p>", 
                "question": "Need help improving linear regression model - normality and transformation concerns."
            }, 
            "id": "cmlgvqv"
        }, 
        {
            "body": {
                "answer": "<p>Aren<sq>t most applications of statistics real life?  \u263a<br><br>In any case, I<sq>d be happy to to do some basic analysis if you send me the data.  The basic thing to do is to look at printing over time, and it would probably be best to control for <dq>seasonality<dq>-- which just means that there are probably high and low months for printing during the year.  After doing that, you just want to see if there is an upward trend over time after controlling for the possible seasonality.  Since this isn<sq>t a life-or-death super serious complicated analysis, I can do a basic job with some graphs and interpretation for you in an hour or so.  <br><br>In case you can<sq>t tell, I have a lot of my own work I am trying to procrastinate on!</p>", 
                "question": "Real-life application of statistics - Printer usage analysis"
            }, 
            "id": "cmke4jb"
        }, 
        {
            "body": {
                "answer": "<p>OLS refers to the estimation method. MLR refers to the use of multiple (2+) predictors. MLR is often done using OLS.</p>", 
                "question": "Difference between Multiple Linear Regression and Ordinary Least Squares"
            }, 
            "id": "cmje7fu"
        }, 
        {
            "body": {
                "answer": "<p>OLS is a method for estimation of linear regression models (whether simple or multiple)</p>", 
                "question": "Difference between Multiple Linear Regression and Ordinary Least Squares"
            }, 
            "id": "cmjej8c"
        }, 
        {
            "body": {
                "answer": "<p><br>First thing is you need to make sure your design doesn<sq>t introduce biases (like, say, you<sq>re more likely to watch a game you think they might win -- you might, since that would be more exciting). So you can<sq>t watch whichever games you want. You must randomize which ones you watch.<br><br>If there are other factors that impact their chance to win (e.g. home games vs on the road), a better design would deal with those factors and try to balance watching/not-watching across them, but you may not wish to deal with those complexities.<br><br>Once you have your design (which games you watch and don<sq>t watch), you collect your data. (I<sq>ll leave required sample size at a given power aside for the moment)<br><br>So under the null (<dq>it makes no difference if I watch<dq>), the proportion of losses is the same whether you watch or not.  Under the alternative, they lose more when you watch.<br><br>Under the alternative (which you<sq>ve expressed as a one-sided alternative), the proportion of losses increases when you watch.<br><br>If we let the letters A-D represent the counts in those states you list,<br>this suggests that <br><br>p1-p2 = C/(A+C) - D/(B+D) <br><br>(the sample proportions estimating p(lost|watched)-p(lost|didn<sq>t-watch)) <br><br>will be a random value near 0 if the null is true and will tend to be larger than zero when the alternative is true.<br><br>This is a one-sided (/one-tailed), two-sample proportions test.<br><br>The calculations are laid out (perhaps less clearly than they could be) here<colon><br><br>http<colon>//stattrek.com/hypothesis-test/difference-in-proportions.aspx<br><br></p>", 
                "question": "How would I hypo-test <dq>watching the Cleveland Browns causes them to lose<dq>?"
            }, 
            "id": "cmb65f2"
        }, 
        {
            "body": {
                "answer": "<p>First off, you don<sq>t need to assume a p(winning).  It will make your analysis a lot easier if you assume the p(winning) is the same for all opponents.  Clearly, this is not true, but if you watch enough games, your hypothesis test to determine if watching games has an effect on the result will still be valid.<br><br>Now to the experimental design.  You should randomly select games to watch and games not to watch.  If you just look at the games you<sq>ve watched in the past, that won<sq>t be valid since your choice of games to watch may have been a function of the opponent.  For example, if you always watch games against the Broncos because you like to watch Peyton Manning, the team will lose more often when you watch.  Of course this is not caused by your watching, but rather that they are playing against Peyton Manning more often when you are watching.  So determine which games to watch randomly, then watch and record the result.<br><br>All that is left is to do a hypothesis test for equality of proportions.  The null hypothesis is that p(win|watch)=p(win|didn<sq>t watch).  The alternative is p(win|watch)<p(win|didn<sq>t watch).  If you search for test for equality of proportions on Google, you will find the exact formulas for this test.</p>", 
                "question": "How would I hypo-test <dq>watching the Cleveland Browns causes them to lose<dq>?"
            }, 
            "id": "cmb49xe"
        }, 
        {
            "body": {
                "answer": "<p>Use a chi-square test for homogeneity.</p>", 
                "question": "How would I hypo-test <dq>watching the Cleveland Browns causes them to lose<dq>?"
            }, 
            "id": "cmglv3l"
        }, 
        {
            "body": {
                "answer": "<p>Practical Nonparametric Statistics by W. J. Conover its an obvious one...</p>", 
                "question": "Textbook recommendation for Nonparametrics"
            }, 
            "id": "cm893tp"
        }, 
        {
            "body": {
                "answer": "<p>For my class on nonparametrics statistics he takes it from <dq>Introduction to Modern Nonparametric Statistics<dq> by James Higgins, I think it is a pretty useful books, the only thing I wish was different is for more of an emphasis on R or SAS</p>", 
                "question": "Textbook recommendation for Nonparametrics"
            }, 
            "id": "cm8j7vw"
        }, 
        {
            "body": {
                "answer": "<p>Agresti, Categorical Data Analysis - someone made an R companion available on the net. Or check out some of his less advanced books - I believe there is plenty of R-material for them as well.</p>", 
                "question": "Textbook recommendation for Nonparametrics"
            }, 
            "id": "cm8o0rj"
        }, 
        {
            "body": {
                "answer": "<p>Very generally<colon><br><br>* Members of the same household/family<br>* Teeth in a mouth<br>* Houses in a suburb<br>* Eggs in a nest<br>* Fish in a stream<br>* Machines in a factory<br>* Longitudinal studies (special/different case of same issue)<br>* Students in a school in a district (classic case)<br></p>", 
                "question": "[OLS Assumptions] Could someone help me understand the concept of correlated/clustered residuals?"
            }, 
            "id": "cm4ywcu"
        }, 
        {
            "body": {
                "answer": "<p>CI for difference in two proportions.<br><br>http<colon>//www.kean.edu/~fosborne/bstat/06d2pop.html</p>", 
                "question": "How do I compute a 95<percent> confidence interval for population difference from a cross table?"
            }, 
            "id": "cm1rhq6"
        }, 
        {
            "body": {
                "answer": "<p>The normal distribution is defined on the whole real line. Values can occur at *any* distance from the mean ... they just become *extremely* unlikely past about 4 standard deviations or so.</p>", 
                "question": "Normal Distribution Extreme Values?"
            }, 
            "id": "cls6csr"
        }, 
        {
            "body": {
                "answer": "<p>If you<sq>re interested in extreme values, a beta distribution can be very convenient. There is a method for estimating its extremes. IMHO, often (but certainly not always) questions can be rethought of in terms of proportions, enabling use of a beta distribution.</p>", 
                "question": "Normal Distribution Extreme Values?"
            }, 
            "id": "cls8l1p"
        }, 
        {
            "body": {
                "answer": "<p>I assume you are talking about p-values?  It doesn<sq>t matter what the standard deviation or the mean is, they<sq>re the same for every normal distribution. p(3) = .0013. You have to look it up in a z table.<br>There<sq>s no theoretical limit, but anything beyond 3 standard deviations you probably don<sq>t have enough precision in your data for the values to mean anything.</p>", 
                "question": "Normal Distribution Extreme Values?"
            }, 
            "id": "cls4bdg"
        }, 
        {
            "body": {
                "answer": "<p>If you took measurements at 3 or more points in time, you need to use ANOVA. If you only have two groups to compare or only took measurements at two points in time, then you can use either the t-test or ANOVA and you will get the exact same results. <br><br>Think of a t-test as a simplified version of ANOVA that only works when you are comparing two groups. The t-test does (basically) the same thing as ANOVA, but it is much easier to calculate by hand. It *used* to save the old timers a lot of time doing analyses when they were only comparing two scores. However, nobody does stats by hand anymore so the t-test no longer has a meaningful advantage over ANOVA.<br><br>EDIT<colon> Clarity<br></p>", 
                "question": "In which cases would you use a Related Samples T-Test instead of a Repeated Measures ANOVA?"
            }, 
            "id": "clr45jk"
        }, 
        {
            "body": {
                "answer": "<p>Not a statistician but i am a process eng in electronics manufacturing. It sounds like you want to do a process capability analysis. There are 2 standard ways of dealing with non-normal process data, sub sampling to take advantage of central limit theorem or transformation(log normal, box-cox, Johnson). But also you need to make sure your data is independent and stable (look at time series of the results, control charts). <br><br>Cp/Cpk and Pp/Ppk are the standard indices that are generally reported in my industry for process capability. Now if you strictly just want the probability of getting less than 95<percent> maybe a statistician can weigh in on the best method. <br><br>I would recommend Intro to Statistical Quality Control by Montgomery as a good introduction on the topics. <br><br><br></p>", 
                "question": "Estimating failure rate"
            }, 
            "id": "clpxk1k"
        }, 
        {
            "body": {
                "answer": "<p>If all the products have a purity between 95-100<percent>, it is going hard to estimate it nonparameterically. Maybe try modeling you data with a beta distribution. It is really flexible and really good for modelling things between 0 and 1.</p>", 
                "question": "Estimating failure rate"
            }, 
            "id": "clqdo5t"
        }, 
        {
            "body": {
                "answer": "<p>[Imgur](http<colon>//i.imgur.com/Iu4jbvq.png)</p>", 
                "question": "Estimating failure rate"
            }, 
            "id": "clpa82k"
        }, 
        {
            "body": {
                "answer": "<p>To simulate defects and calculates them, usually in industry, they use [Weibull](http<colon>//en.wikipedia.org/wiki/Weibull_distribution), [Gumbel](http<colon>//en.wikipedia.org/wiki/Gumbel_distribution), or [Fr\u00e9chet](http<colon>//en.wikipedia.org/wiki/Fr<percent>C3<percent>A9chet_distribution) distributions. They are all three similar.</p>", 
                "question": "Estimating failure rate"
            }, 
            "id": "clrrlxg"
        }, 
        {
            "body": {
                "answer": "<p>Quick answer<colon><br><br>It shows the percentage of students who have received that score or less. For instance, the bar at a score of 14 is right at 75<percent>, this means that 75<percent> of the students scored 14 or less. The bar at a score of 20 is at 100<percent> because all of the students scored 20 or less. <br><br>Another way to think about it is that the height of a bar on the second graph for some particular score is the sum of the bars on the first graph that are less than or equal to that score. <br></p>", 
                "question": "Help with understanding graph"
            }, 
            "id": "clogyng"
        }, 
        {
            "body": {
                "answer": "<p><dq>degressively<dq>? What<sq>s that?</p>", 
                "question": "Help with understanding graph"
            }, 
            "id": "clomghq"
        }, 
        {
            "body": {
                "answer": "<p>Look up z score. it expresses an individual data point in terms of how many standard deviations away from the mean it is</p>", 
                "question": "(Noob question) - Specific individual vs various means?"
            }, 
            "id": "clnrguv"
        }, 
        {
            "body": {
                "answer": "<p>I<sq>ve typically just used Cohen<sq>s Kappa for this same thing, but I<sq>m being encouraged by my committee to use Krippendorf<sq>s Alpha instead<colon> http<colon>//en.wikipedia.org/wiki/Krippendorff<percent>27s_alpha. <br><br>Both calculate inter-rater reliability, but Krippendorf<sq>s takes more into account (I think) and is more robust. I<sq>m just starting to explore it. <br>I haven<sq>t found a great tool to help calculate it though.</p>", 
                "question": "Inter rater reliability for nominal data?"
            }, 
            "id": "cllacgd"
        }, 
        {
            "body": {
                "answer": "<p>A probability plot lets you check to see if your data are normally distributed, which is an important assumption/thing to know for various things like confidence intervals and hypothesis tests, especially if you have a small sample size. If your data are normally distributed then the central limit theorem applies and you can make reasonable conclusions about your parameter(s) of interest. So once you generated your probability plot, you would then know if your  data were normally distributed.<br><br>On a side note, i think the ehow link is sort of misleading when it says <dq>Probability paper is paper with vertical lines and evenly spaced horizontal lines that help you to plot a normal probability curve (sometimes called a bell curve).<dq> A standard normal curve is bell shaped, but a probability plot lets you check if your data are normally distributed by seeing if they fall generally in a straight line, like the figure in the 2nd link. In other words, data that are normally distributed will appear in a straight line on a probability plot, not a bell shaped curve. Anyway, I think if you had enough data points such that you plotted them resulting in a bell shaped curve, I don<sq>t think you<sq>d need to check for normality using a probability plot, cause your data would appear normally distributed.<br><br>All this being said - I<sq>m still learning this stuff myself someone else can probably expand on this/explain if I<sq>m wrong. </p>", 
                "question": "How does one use probability paper?"
            }, 
            "id": "cll8gw8"
        }, 
        {
            "body": {
                "answer": "<p>I followed the steps in the ehow tutorial (as well as the pdf doc).<br><br>The difference in their formula<sq>s are tremendous.  Ehow produces a bell shaped curve.<br><br>http<colon>//imgur.com/WGI4IgQ<br><br>the other one in the pdf produces a flat curve (45\u00ba angle)<br><br>So now... I guess I have some sort of stdev mapping? (highlighted data = graph, right most column is the results of the NORMSINV function on the quantile points (i.e. 1 to 15).</p>", 
                "question": "How does one use probability paper?"
            }, 
            "id": "cllbs6p"
        }, 
        {
            "body": {
                "answer": "<p>If I understood you correctly, you are looking for a link between categorical personality dimensions and categorical music preferences. That seems like a simple chi square test of association. <br><br>Unless, participants could choose multiple genres, rather than just one of several. Then, you can use a generalized logit regression</p>", 
                "question": "How to correlate categorical personality and music genre preference scores?"
            }, 
            "id": "clkg5l0"
        }, 
        {
            "body": {
                "answer": "<p>It largely depends on what exactly your personality and music data are like. Could you give a more in depth description of what your data is? That said the usual approach to this type of study is using structural equation modelling but that depends on your data like I said.</p>", 
                "question": "How to correlate categorical personality and music genre preference scores?"
            }, 
            "id": "clk4unw"
        }, 
        {
            "body": {
                "answer": "<p>Social and personality psychologist here.  The Big Five scores, if you did them correctly, should be continuous measures (you should have a score between 1 to 5 or so for each of the five dimensions for each person).<br><br>What are your musical preference variables?  Yes/no categorical questions about genres?  Something else?</p>", 
                "question": "How to correlate categorical personality and music genre preference scores?"
            }, 
            "id": "clkokjy"
        }, 
        {
            "body": {
                "answer": "<p>Not entirely sure, as I haven<sq>t looked at the stats, but it<sq>s worth considering a data collection issue. I suspect there is much better reporting and logging of these crimes in somewhere like the states than Bangladesh </p>", 
                "question": "How is it that a developing country like Bangladesh have far fewer homicides than most developing countries (including the USA) during a given year? Is this a statistical error?"
            }, 
            "id": "clja2fk"
        }, 
        {
            "body": {
                "answer": "<p>From the Wikipedia article<colon><br>> The legal definition of <dq>intentional homicide<dq> differs among countries. Intentional homicide may or may not include infanticide, assisted suicide or euthanasia.<br><br>and<br>> ... may also be underreported for political reasons.</p>", 
                "question": "How is it that a developing country like Bangladesh have far fewer homicides than most developing countries (including the USA) during a given year? Is this a statistical error?"
            }, 
            "id": "cljd4dv"
        }, 
        {
            "body": {
                "answer": "<p>This would be a great class project. As students we<sq>d have to research not only the statistics, but the collection methods and local culture.  </p>", 
                "question": "How is it that a developing country like Bangladesh have far fewer homicides than most developing countries (including the USA) during a given year? Is this a statistical error?"
            }, 
            "id": "cljdswf"
        }, 
        {
            "body": {
                "answer": "<p>The principle factor is probably unreported data. Local communities don<sq>t report their homicides to the state authorities and the state authorities process and report the data in full to the collectors. Developing nation states are notoriously inefficient at statistical accuracy. There is also cultural pressure and corruption that can keep negative statistics from seeing the light of day. However, its also possible your per capita homicide rate is lower than the United states, and the determination that your nations homicides arer getting worse is a false. The  feeling that things are way worse than they really are could be wrong. I don<sq>t know, the current data is all we have. </p>", 
                "question": "How is it that a developing country like Bangladesh have far fewer homicides than most developing countries (including the USA) during a given year? Is this a statistical error?"
            }, 
            "id": "clluki0"
        }, 
        {
            "body": {
                "answer": "<p>Try out some of the links here, especially under the commodities section<colon><br>http<colon>//quant.stackexchange.com/questions/141/what-data-sources-are-available-online<br><br>Quandl also looks pretty excellent as a search engine. Example<colon><br>https<colon>//www.quandl.com/search/steel<br><br>You<sq>ll probably want to download it in the json format. I know there<sq>s a few packages that seem good for R + json but I don<sq>t have experience with any of them.</p>", 
                "question": "R Stats Software - Can someone walk me through finding importing and charting a commodity?"
            }, 
            "id": "clj6uee"
        }, 
        {
            "body": {
                "answer": "<p>Probability calculations for an event specified after the event that triggers the desire for a calculation, but calculated *as if the event were specified before being observed*, yield nonsense.\\*<br><br>There<sq>s no obvious basis on which to make them make sense, except to say that, whatever way you can arrange things to make a more sensible calculation, the result will be much, much larger than the calculation that incorrectly ignores the fact that the event is only specified *post hoc*.<br><br>So ... even if you manage to coax someone into doing that calculation, I don<sq>t see how it can be made to make any sense. If we are to account for the fact that it<sq>s *post hoc*, what are the set of other events that might trigger such a question that should be included? Anything that might make a person post such a question here, perhaps? Then the that probability is high enough that it typically happens several times a week.<br><br>\\* Take a wheelbarrow full of distinguishable 20-sided dice (maybe they have tiny serial numbers, or RFID chips in them or something). Imagine there<sq>s ten thousand dice (numbered 0000 to 9999). Tip the wheelbarrow out from a good height onto a flat surface and record all the values. Now what are the chances you got that <dq>4<dq> on die #0001, <dq>17<dq> on  die #0002,  <dq>9<dq> on die #0003, ... <dq>12<dq> on die #9999 and <dq>2<dq> on die #0000? It<sq>s less than 10^-13000 --- an astoundingly small number, so vanishingly small that the words <sq>vanishingly small<sq> don<sq>t do it justice. Amazing! You just tipped it out, and rolled a much less than once-in-the-lifetime-of-the-universe sized miracle!<br><br>Now gather them together and repeat. An equally amazing event occurs (this time a 6 on die #0001, a 12 on die #0002, a 1 on die #0003, ...)! In fact, you can do it a hundred times a day. <br><br>Such vastly tiny probabilities can<sq>t happen by mere coincidence, yet you manage to do it over and over! You must be a wizard or something!<br> </p>", 
                "question": "Stats if possible for a case in/r/KarmaCourt"
            }, 
            "id": "cli1cmz"
        }, 
        {
            "body": {
                "answer": "<p>I would like to have the statistics requested by my learned colleague compared with<colon><br><br>What are the odds that a Redditor would make up a story and use an alternate account in order to pretend to be the other person in the story and confirm it?</p>", 
                "question": "Stats if possible for a case in/r/KarmaCourt"
            }, 
            "id": "cli0pgr"
        }, 
        {
            "body": {
                "answer": "<p>In principal, if you played in enough leagues in the same season you<sq>d have a chance to determine the skill element of FF.  <br>The one problem with doing this is that a single fantasy player<sq>s teams across leagues likely share players, so the results in these leagues are correlated.   <br><br>For example, perhaps you were high on AP (more than your opponents let<sq>s say) this year, and had him in most of your leagues.  Chances are your teams collectively aren<sq>t doing too well this year, even if you were a very skillful player.<br><br>A more technical explanation is that if your teams are independent the variance of the overall sum of results in all league are sum of the variance in each league.  This is no longer the case if your teams are correlated, and assuming independence likely underestimates the variance.  If you use multiple seasons, the correlation between results across seasons is likely to be less correlated, assuming you<sq>re in a redraft league.</p>", 
                "question": "Fantasy Football<colon> Variance & Sample Size"
            }, 
            "id": "clgxi9u"
        }, 
        {
            "body": {
                "answer": "<p>> I believe this is due to the fact that when we increase our sample size we in turn, increase the precision of our results.<br><br>The point here is that this depends on how independent those data points are. Let<sq>s say you choose exactly the same players in each of those leagues, then it might *seem* the <sq>good<sq> player is consistently good, but he really might have been consistently lucky. That is, if everyone would just choose random, and then use the same random team in each league, you would label one of the <sq>captains<sq> as better (because he won each time!), while actually he might as well be a monkey.<br><br>It is likely that although you may not choose the exact same teams across leagues, you will pick many of the same players. This creates the same effect as above, just to a lesser extent.<br><br>*You cannot increase your sample size with interdependent observations*<br><br>There is likely much less interdependence between seasons (and people can evaluate in betwee) so playing multiple seasons is therefore a much better strategy.</p>", 
                "question": "Fantasy Football<colon> Variance & Sample Size"
            }, 
            "id": "cll0b98"
        }, 
        {
            "body": {
                "answer": "<p>You should take a look at wOBA and wRC<colon>  <br>http<colon>//www.fangraphs.com/library/offense/woba/  <br>http<colon>//www.fangraphs.com/library/offense/wrc/</p>", 
                "question": "What is a homerun worth?"
            }, 
            "id": "clgpll7"
        }, 
        {
            "body": {
                "answer": "<p>Technically, **C01** can be any value.<br><br>It doesn<sq>t *HAVE* to be a balanced problem. Especially in this case where all values are made up.</p>", 
                "question": "Analyze this Bayesian Decision problem I<sq>ve got please"
            }, 
            "id": "cla22q0"
        }, 
        {
            "body": {
                "answer": "<p>C01 is 0.  The first digit indicates whether he asked her out or not and the second digit indicates whether she would say yes if he did.  These costs are not opportunity costs, they are costs (or negative payoffs).  They are only dependent on the result, not what is lost out on.  So if he doesn<sq>t ask the girl out, his cost will be 0 whether she would have said yes or not because either way, the result is the same - he won<sq>t go out with her, but he won<sq>t have to ask her out.  </p>", 
                "question": "Analyze this Bayesian Decision problem I<sq>ve got please"
            }, 
            "id": "clawx9y"
        }, 
        {
            "body": {
                "answer": "<p>Your first one. </p>", 
                "question": "What causes confidence intervals to be more accurate with higher sample sizes? (of a sample that is not necessarily normally distributed)"
            }, 
            "id": "cl79dns"
        }, 
        {
            "body": {
                "answer": "<p>Just like with the  p value, there is a negative relation between sample size and standard error, which in turn decreases the confidence interval. SE = SD / sqrt(n). The bigger the n, the bigger the detomenator, the smaller the SE and p. That<sq>s also a demonstration of why large sample sizes are better. Likewise, large samples sizes deflate p values to the point where they may become useless. I<sq>ve seen correlations of.07 be <dq>significant<dq> at p <. 001 with an n of ~7500. That low a correlation is useless. If you are working with large samples I<sq>d advise you to focus on effect sizes to interpret your data as they give you a more objective (albeit imperfect) index of your effect. Geoff Cummings has a great book on that called <dq>the new statistics<dq></p>", 
                "question": "What causes confidence intervals to be more accurate with higher sample sizes? (of a sample that is not necessarily normally distributed)"
            }, 
            "id": "cl7idjs"
        }, 
        {
            "body": {
                "answer": "<p>Not to be too picky but your reasons are two sides of the same coin.<br><br>The <dq>more accurate confidence interval<dq> relies on the standard error of the sample mean decreasing with increasing sample sizes.  This implies that the Central Limit Theorem applies...which says the sample distribution of the mean becomes normal with larger samples.  <br><br>But you could just as easily reverse the causality<colon> because the sampling distribution of the mean becomes normal, the central limit theorem applies and therefore the SE is smaller and the confidence intervals are smaller.<br><br>To stress the <dq>causes<dq>, generate simulated data from a distribution where the central limit theorem does NOT apply (can you say - Cauchy) and watch your confidence intervals grow and grow with increasing sample size.</p>", 
                "question": "What causes confidence intervals to be more accurate with higher sample sizes? (of a sample that is not necessarily normally distributed)"
            }, 
            "id": "clau0cm"
        }, 
        {
            "body": {
                "answer": "<p>I<sq>d be interested in this as well, especially with a breakdown by age, or front-facing vs rear-facing vs booster seats. </p>", 
                "question": "Child death/injury in car accidents before and after the implementation of car seats and seat belts."
            }, 
            "id": "cl44t9q"
        }, 
        {
            "body": {
                "answer": "<p>More of a question for /r/datasets. This subreddit is focused on statistical methods.</p>", 
                "question": "Child death/injury in car accidents before and after the implementation of car seats and seat belts."
            }, 
            "id": "cl46naf"
        }, 
        {
            "body": {
                "answer": "<p>I would start by googling for research articles on the topic and see what those people used.</p>", 
                "question": "Child death/injury in car accidents before and after the implementation of car seats and seat belts."
            }, 
            "id": "cl5a2tc"
        }, 
        {
            "body": {
                "answer": "<p>I simplified it a great deal <br><br>    T11 <- c(2,31,7)<br>    T12 <- c(10,21,16)<br>    T13 <- c(4.09,17.73,9.55)<br>    expect <- c(7,10,13)<br><br>    4*(3*((1*T11)+(2*T12)+(3*T13))-6*(T11+T12+T13))/(3*14-36)+<br>    ((T11+T12+T13)-(3*((T11+2*T12+3*T13)) -6*(T11+T12+T13))/(3*14-36)*6)/3<br><br># create two obvious variables<br>    wup <- (T11+2*T12+3*T13)/6<br>    wmean <- (T11+T12+T13)/3<br>    4*(3*(6*wup)-6*(3*wmean))/(3*14-36)+<br>    ((3*wmean)-(3*((6*wup)) -6*(3*wmean))/(3*14-36)*6)/3<br># simplify<br>     4*(18*wup-18*wmean)/(3*14-36)+<br>    ((3*wmean)-(3*((6*wup)) -6*(3*wmean))/(3*14-36)*6)/3<br># simplify<br>     4*(18*wup-18*wmean)/(3*14-36)+<br>     ((3*wmean)-(18*wup-18*wmean)/(3*14-36)*6)/3<br># make increase per year in ipy and simplify wup out<br>     ipy <- (T13-T11)/2<br># hence ipy <- 3*wup-3*wmean<br># hence wup <- wmean+ipy/3<br>     4*(18*(wmean+ipy/3)-18*wmean)/(3*14-36)+<br>    ((3*wmean)-(18*(wmean+ipy/3)-18*wmean)/(3*14-36)*6)/3<br># simplify<br>     4*(6*ipy)/(3*14-36)+<br>    ((3*wmean)-6*(6*ipy)/(3*14-36))/3<br>#simplify<br>    4*(6*ipy)/(3*14-36)+<br>    wmean  -(2*(6*ipy)/(3*14-36))<br>#simplify<br>    2*ipy + wmean  <br><br>Hence you take the average over the last three years, which should be approximately for 2 years ago, and add increase for 2 additional years<br><br></p>", 
                "question": "What is this calculation?"
            }, 
            "id": "ckt02ei"
        }, 
        {
            "body": {
                "answer": "<p>EDIT<colon> First off, copying in the formula and replacing the Excel cells with <dq>201X Total<dq> was ok, but really messed with me due to certain typos (<dq>T3042012Total<dq>?). Also the conversion of asterisks to <sq>x<sq> also threw me for a loop but easy to edit back out.  I used it to help me simplify the equation. I think the easiest way for you to have shared the formula would have been to put out the equation in A, B, C variables and say <dq>where A = 2011 total, B = 2012 total, and C = 2013 total). Now, to make it even easier, you could align those variable names to the corresponding column names. As I will do below<colon><br><br>___<br><br>Let B be the 2011 total per group, let C be the 2012 total per group, and let C be the 2013 total per group. We are making a forecast prediction for each group. [Here](http<colon>//www.wolframalpha.com/input/?i=4*<percent>283*<percent>28<percent>281*B<percent>29<percent>2B<percent>282*C<percent>29<percent>2B<percent>283*D<percent>29<percent>29-6*<percent>28B<percent>2BC<percent>2BD<percent>29<percent>29<percent>2F<percent>283*14-36<percent>29<percent>2B<percent>28<percent>28B<percent>2BC<percent>2BD<percent>29-<percent>283*<percent>28<percent>281*B<percent>29<percent>2B<percent>282*C<percent>29<percent>2B<percent>283*D<percent>29<percent>29-6*<percent>28B<percent>2BC<percent>2BD<percent>29<percent>29<percent>2F<percent>283*14-36<percent>29*6<percent>29<percent>2F3) is a link to the equation in Wolfram Alpha<sq>s symbolic format. <br><br>There<sq>s a couple patterns that we should be quick to notice.<br><br><dq>1\\*B + 2\\*C + 3\\*D<dq> indicates a moving average, based on lagged time series. Most recent data is three times more important than data which is 2 years old, but is only one and a half times more important than data which is merely 1 year old.<br><br>Furthermore, if you break down the structure of the formula, you will notice it becomes two terms, one with coefficient 1/3rd and one with coefficient 2/3rds. This is another weighted average (of two terms) where the one formula is given twice as much weight.<br><br>Now of the two terms, they are<colon><br><br>* 6(B + C + D) - 3(B + 2C + 3D) + B + C +D<br>* 3(B + 2C + 3D) - 6(B + C + D)<br><br>These simplify to<colon><br><br>* 4B + C - 2D<br>* 3D - 3B<br><br>Now we have to actually go look at the relationship between these variables and their meaning. Again, B => C => D is in order of increasing time (D is most recent, B is least recent). Both forecasts in both regular and simplified formats are just linear combinations. No other fancy math necessary.<br><br>I have no explanation for the prevalance of (3\\*14-36).<br><br>___<br><br>You have two choices. Either use the simplified format to determine what is being computed here, or go *the other direction* and determine <br>from the unsimplified format why he felt 6\\*(B+C+D) was important, or rather - why was 6 the chosen coefficient? We see two basic parts... <dq>B+C+D<dq> and <dq>B+2C+3D<dq>. How do these interact? Why do we manipulate them in the way we do? Etc.<br><br>For what it<sq>s worth, I think that a more typical stats answer for this would be to make forecast predictions using time series analysis, but if all you<sq>ve got are totals for the year then there<sq>s nothing abjectly wrong with linear regressions.</p>", 
                "question": "What is this calculation?"
            }, 
            "id": "cksu2v4"
        }, 
        {
            "body": {
                "answer": "<p>Thank you both for taking a look at this.  This is very helpful.  </p>", 
                "question": "What is this calculation?"
            }, 
            "id": "ckt337d"
        }, 
        {
            "body": {
                "answer": "<p>In any regression model applied to any field you<sq>re only estimating the effects of the variables for which you have data and which you decide to put in the model. Here<sq>s a silly example<colon><br><br>I want to explain variation in the number of seeds found in oranges. I include in my model information about how much water the plants received, where they are grown, and the time of year they were harvested. I find that the first two are significant. That doesn<sq>t mean that some other variable wouldn<sq>t have been a good one to include. Maybe more than anything, the biggest explanatory variable would have been the average seed size. The fact that it<sq>s not in my model is my fault. I didn<sq>t include it, and my model can<sq>t be taken to say that this variable which I did not include to begin with does not matter. Further, my model can<sq>t be taken to say that where the oranges grew is truly a major determining factor, because perhaps different breeds are grown in different regions, and so what I<sq>m interpreting as the impact of geography is really the impact of being from different cultivars.<br><br>So, if I<sq>m understanding your question correctly, yes, there are always limitations to interpreting the effect sizes. This is true of ANY regression.</p>", 
                "question": "Stats Gurus<colon> Noob question - Are there limits to interpreting effect sizes in multiple regression?"
            }, 
            "id": "cklu8pf"
        }, 
        {
            "body": {
                "answer": "<p>The main reason is because the variance isn<sq>t easy to interpret, since it measures the <dq>average squared deviation from the mean<dq>.  Taking square root give you a *common*, but not really average, distance your data are from the mean.  Most of your data will usually be within one standard deviation of average.<br><br>Another reason is that of all of the tests and calculations we do with this kind of measure, there are *very few* that use the variance, the majority of them use the standard deviation... once again because it is in the same units as the data we are interested in, instead of <dq>squared units<dq>.</p>", 
                "question": "If standard deviation is just the square root of the variance why is it necessary to calculate SD at all? Why not just use the variance?"
            }, 
            "id": "ckle426"
        }, 
        {
            "body": {
                "answer": "<p>One reason is that the interpretation of standard deviation is easier than variance because the standard deviation has the same unit of measurement as your variable; variance squares the unit of measurement. The variance of wage in dollars, for example, is in squared dollars while the standard deviation is in dollars. </p>", 
                "question": "If standard deviation is just the square root of the variance why is it necessary to calculate SD at all? Why not just use the variance?"
            }, 
            "id": "ckliw70"
        }, 
        {
            "body": {
                "answer": "<p>To put it simply<colon> If you were to multiply all your data by ten (e.g. 10cm = 100mm), the variance would be increased 100-fold (4cm^2 = 400mm^2 ,) but the standard deviation would be multiplied by ten (2cm = 20mm.) Thus standard deviation<sq>s relationship to the data is in line regardless of what unit of measurement is being used. (10cm <colon> 2cm <colon><colon> 100mm <colon> 20mm)</p>", 
                "question": "If standard deviation is just the square root of the variance why is it necessary to calculate SD at all? Why not just use the variance?"
            }, 
            "id": "ckljwdb"
        }, 
        {
            "body": {
                "answer": "<p>I TFed an intro undergrad course that used Alan Agresti<sq>s [Statistical Methods for the Social Sciences](http<colon>//www.amazon.com/Statistical-Methods-Social-Sciences-4th/dp/0130272957/ref=sr_1_3?s=books&ie=UTF8&qid=1410546278&sr=1-3).  I didn<sq>t read much of it, but the students seemed to like it.  He also has [another book](http<colon>//www.amazon.com/Statistics-Art-Science-Learning-Data/dp/0321755944/ref=sr_1_1?s=books&ie=UTF8&qid=1410546278&sr=1-1) that<sq>s probably also pretty good.  The intro course for non-stats students at my graduate school is [Applied Statistics for the Behavioral Sciences](http<colon>//www.amazon.com/Applied-Statistics-Behavioral-Sciences-Dennis/dp/0618124055/ref=sr_1_1?s=books&ie=UTF8&qid=1410546614&sr=1-1), which might also be worth a look.  If those are too technical or hands-on, then the <dq>for Dummies<dq> book might also be a good choice - it<sq>s in very plain language and tries to keep things relevant to real-life examples.<br><br>Many of the bigger-picture <dq>whys<dq> become more apparent when you have a solid grounding in probability theory and the theory behind statistical inference, though.  Some of them don<sq>t have very satisfying answers, either (Q<colon> Why p = 0.05? A<colon> Convention). In my opinion, the more you understand statistics, the more you realize it<sq>s less about finding exact answers than it is about quantifying imprecision.  That can be hard for a layperson to wrap their head around!</p>", 
                "question": "Working on explaining the <dq>why<dq> behind certain often-used methods"
            }, 
            "id": "ckgi11q"
        }, 
        {
            "body": {
                "answer": "<p>In my experience, the answer to the <dq>why<dq> of a lot of statistical questions is ultimately <dq>because that<sq>s how it<sq>s done<dq>, or <dq>that<sq>s what gets published<dq>. Or some similar, totally non-scientific reason. It kind of bums me out. A good book for you might be<colon> Philosophy of Statistics by Bandyopadhyah and Forester, but it costs a lot and is fairly dense. Another book I enjoyed early in my stats education was<colon> Statistics Without Math by Magnusson and Mourao. It attempts to explain statistics conceptually without much of the formulae, however it covers more intermediate-level concepts like multivariate stats from the perspective of experimental design. That might be just the thing for your administrator friend.</p>", 
                "question": "Working on explaining the <dq>why<dq> behind certain often-used methods"
            }, 
            "id": "ckilfsi"
        }, 
        {
            "body": {
                "answer": "<p>Gah, I know that Nick Bostrom is a heavyweight in Philosophy and has contributed significantly to probability theory and its philosophy. But this honestly reads like it was written by a novice. <br> <br>Consistently, he writes <dq>infinite<dq> probability where he means <dq>non-zero<dq> or <dq>strictly positive<dq> probability. Last time I checked probabilities always is between zero and unity -- never have I seem them to be <dq>infinite<dq>. <br> <br>Also, by writing  <br> <br>> We can see this formally as follows. [..] <br> <br>and then following with vague definition of variables E, B, T, and then putting up Bayes formula  <br> <br>> P(T|E&B) = P(E|T&B)P(T|B) / P(E|B). <br> <br>says nothing. It<sq>s a tautology. Anyway, he then writes <br> <br>> In order to determine whether E makes a difference to the probability of T (relative to the background assumption B), we need to compute the difference P(T|E&B) - P(T|B). By some simple algebra it is easy to see that <br>>  <br>> P(T|E&B) - P(T|B) ~ 0 if and only if P(E|T&B) ~ P(E|B). <br>>  <br>> This means that E will fail to give empirical support to T (modulo B) if E is about equally probable given T&B as it is given B.  <br> <br>Indeed, because it means that E is conditionally independent to T, and hence cannot provide information about T. Why write it such a backwards manner? Also why condition on B at all? All the formulas are valid without it. <br> <br>With the level of vagueness written anything can be implied. Hence, I have a tendency to dismiss the article as a kind of <dq>proof by intimidation<dq> though I realize that I may not be clever enough to weed out what is (thought to be) understood. (I doubt few on this subreddit are.) <br> <br>If you want some more constructive answers, I think the problem has to be cast in a small toy-example with the same philosophical consequences which can be examined rigoursly. <br><br>**Edit** Fixed some spelling and grammer mistakes.</p>", 
                "question": "Is the statistical reasoning in this paper bad or OK?"
            }, 
            "id": "ckejh0i"
        }, 
        {
            "body": {
                "answer": "<p>This sounds more like a blog post than a serious academic paper. Having an infinite amount of things, doesn<sq>t imply that you have every type of thing... that sentence sounds awkward, but think about it this way.<br><br>Imagine I have a hat with an infinite amount of numbers inside of it. What is the probability that the hat contains the number 3? By his reasoning it is 100<percent>, however it should be noted that there are an infinite amount of numbers between 0 and 1, so the hat could very well have an infinite amount of numbers but not contain 3, or even a single number greater than 1.</p>", 
                "question": "Is the statistical reasoning in this paper bad or OK?"
            }, 
            "id": "ckewn0h"
        }, 
        {
            "body": {
                "answer": "<p>If an event is possible and there an infinite number of possibilities for it to occur it will happen almost certainly (probably of 1 from the left). That said, certain things are literally impossible according to the rules of physics and won<sq>t occur.<br><br>So the statistics in this paper is technically correct but not very useful at all.</p>", 
                "question": "Is the statistical reasoning in this paper bad or OK?"
            }, 
            "id": "ckehnuq"
        }, 
        {
            "body": {
                "answer": "<p>It sounds like you use a chi-square test, but you give the expected frequencies, rather than calculating them. If you give chisq.test in R a vector of frequencies, it assumes equal probability and calculates the expected values itself. E.g. <br><br>    > chisq.test(x=c(120, 80))<br><br>\tChi-squared test for given probabilities<br><br>    data<colon>  c(120, 80)<br>    X-squared = 8, df = 1, p-value = 0.004678<br><br>But I can also give it expected frequencies. <br><br>    > chisq.test(x=c(120, 80), p=c(0.5, 0.5))<br><br>\tChi-squared test for given probabilities<br><br>    data<colon>  c(120, 80)<br>    X-squared = 8, df = 1, p-value = 0.004678<br><br>And if I give it 50<colon>50, then it<sq>s the same test, but I can have a null hypothesis that<sq>s different<colon><br><br>    > chisq.test(x=c(120, 80), p=c(0.75, 0.25))<br><br>\tChi-squared test for given probabilities<br><br>    data<colon>  c(120, 80)<br>   X-squared = 24, df = 1, p-value = 9.634e-07<br><br>If you only have two possible values, use a binomial test though<colon><br><br>    > binom.test(c(120, 80), 0.75 )<br><br>\tExact binomial test<br><br>    data<colon>  c(120, 80)<br>    number of successes = 120, number of trials = 200, p-value = 0.005685<br>    alternative hypothesis<colon> true probability of success is not equal to 0.5<br>    95 percent confidence interval<colon><br>    0.5285357 0.6684537<br>    sample estimates<colon><br>    probability of success <br>                   0.6 <br><br><br>(The p-value isn<sq>t quite the same, <sq>cos it<sq>s an exact test, you can get that from the chi-square test by putting simulate.p.value = TRUE).<br></p>", 
                "question": "Hypothesis test for Frequency?"
            }, 
            "id": "cka1epv"
        }, 
        {
            "body": {
                "answer": "<p>Just to be sure of what you<sq>re asking, it seems like you<sq>re looking for test of what I<sq>d call rates, more than what I<sq>d call frequencies. You want to basically test <dq>This coin is not a fair coin,<dq> not <dq>This signal has a 60Hz component,<dq> right?</p>", 
                "question": "Hypothesis test for Frequency?"
            }, 
            "id": "ck9z97p"
        }, 
        {
            "body": {
                "answer": "<p>Logistic Regression.  You could also use a CHAID Decision tree.....i<sq>ve also seen people use Assosciation Rules to <sq>mine<sq> this daa. Using basket/assosciating rules would be easy to compute in excel compared to the other ones but i dont think it<sq>s as valid. you<sq>re trying to assess credit risk here it seems...and using basket rules will take alot of time because you hjave to manually set the precedents and antecedents. Logistic regression would be ideal here but the hard part..with all machine learning algo<sq>s is properly training the data set & sampling. </p>", 
                "question": "How to build a model where I will try to predict if a customer will pay / will no pay?"
            }, 
            "id": "ck4a74w"
        }, 
        {
            "body": {
                "answer": "<p>I<sq>m feeling lazy, this should get you started. http<colon>//en.wikipedia.org/wiki/Binary_classification</p>", 
                "question": "How to build a model where I will try to predict if a customer will pay / will no pay?"
            }, 
            "id": "ck44lc8"
        }, 
        {
            "body": {
                "answer": "<p>I<sq>m also feeling lazy. <br>Just put everything into Stata and click on maximum likelihood estimation.<br>http<colon>//en.wikipedia.org/wiki/Maximum_likelihood<br><br>But don<sq>t expect any spectacular results. If your model is only slightly better than pure randomness that is already a success. Binary models are usually very hard to model.<br><br></p>", 
                "question": "How to build a model where I will try to predict if a customer will pay / will no pay?"
            }, 
            "id": "ck8xdfj"
        }, 
        {
            "body": {
                "answer": "<p>See pages 24-25 of my report here, where I think I explain it pretty clearly<colon><br><br>http<colon>//ideas.repec.org/p/pra/mprapa/36261.html<br><br><br></p>", 
                "question": "Presenting results of a categorical by categorical neg. binomial regression"
            }, 
            "id": "ck1gq98"
        }, 
        {
            "body": {
                "answer": "<p>Hi, <br><br>I<sq>ve been learning/teaching myself for the last few months with the Data Science specialisation.  I don<sq>t recommend that but there are two stats courses on Udacity which look good. <br><br>In terms of books I<sq>ve found that Online Statistics Education has a very intuitive guide.  It was developed by bods at Rice and Tufts.  <br><br>http<colon>//onlinestatbook.com/<br><br>Slightly more in depth is open intro series.  Still very intuitive with lots of examples.  They have an intro book (very good) and randomization and simulation book (haven<sq>t looked at this)<br><br>http<colon>//www.openintro.org/stat/<br><br>I<sq>ve been mostly reading those two. I tend to pick a topic and then read both books one after the other on the topic. <br><br>Now I<sq>ve doing more regression and have found the Introduction to Statistical Learning more helpful<colon><br><br>http<colon>//www-bcf.usc.edu/~gareth/ISL/<br><br>HTH. <br><br><br></p>", 
                "question": "Bad experiences with stats in past want to try again help?"
            }, 
            "id": "cjvep8q"
        }, 
        {
            "body": {
                "answer": "<p>I recommend the book <dq>Introduction to the Practice of Statistics<dq> by Moore and McCabe.  This book teaches in a logical way, with lots of great examples.  I would supplement this with YouTube videos- there are a lot of great ones out there (and some not so great). Khanacademy does a little statistics, and do I (www.Burkeyacademy.com), this guy<colon> https<colon>//www.youtube.com/user/BCFoltz, this gal<colon> https<colon>//www.youtube.com/user/CreativeHeuristics ...  Hopefully you can find a couple of people with a stlye that matches your learning type.</p>", 
                "question": "Bad experiences with stats in past want to try again help?"
            }, 
            "id": "cjvn0hv"
        }, 
        {
            "body": {
                "answer": "<p>If you have set cut scores for your screeners predicting dichotomous outcomes, you can compare their performance using sensitivity, specificity, and related statistics. These require you to create contingency tables for predicted and actual outcomes. Once you have the contingency tables, it is very easy to calculate the statistics of interest. I<sq>m currently on mobile and so don<sq>t have a link for you, but googling the above terms should give you everything you need. Good luck. </p>", 
                "question": "[SPSS?] I have two screening tools how do I know which is the best?"
            }, 
            "id": "cjuhi2s"
        }, 
        {
            "body": {
                "answer": "<p>It isn<sq>t a big deal. The further you get away from 50<percent>-50<percent> it will increase the standard error of the dummy variable estimate some(which will decrease t stats, increase p values), since the standard deviation of the independent variable goes in the denominator of the standard error.  The standard deviation of a binomial (dummy) is sqrt(p*(1-p)), and this is biggest when p=0.5. So, if this dummy variable is not statistically significant, it could be for many reasons (the variable really doesn<sq>t matter, collinearity with another variable, etc.), but one might be this issue. Watch my video https<colon>//www.youtube.com/watch?v=DJ3vSeuJ5wY for more about standard errors, and fast forward to around 6 minutes in for the one for slopes.<br><br>EDIT<colon> I just noticed you said you are doing a negative binomial regression.  The same logic applies, though my answer was geared more toward normal OLS. Most of the general principles involving precision of estimates carry over regardless of the type or estimation technique, which is very comforting! </p>", 
                "question": "I have a dummy indep. variable with 48 1s and 395 0s; is the big sample size difference a problem?"
            }, 
            "id": "cjo6aov"
        }, 
        {
            "body": {
                "answer": "<p>The issue here is one of power.  Given a limited total sample size, equal sample sizes in the two groups is optimal.  Increasing one group without changing the other does increase power up to about a 5<colon>1 ratio.  After that, you don<sq>t get much benefit by adding more and the smaller group is the limiting factor.<br><br>Homoscedasticity is irrelevant for NB regression, unless the data fails to come even close to the NB distribution.  The variance of Poisson and NB distributions increase with the mean, so some heteroscedasticity is probably expected, especially if the means are far apart.  <br><br>Imhave to ask -- why the NB distribution?</p>", 
                "question": "I have a dummy indep. variable with 48 1s and 395 0s; is the big sample size difference a problem?"
            }, 
            "id": "cjqqdvg"
        }, 
        {
            "body": {
                "answer": "<p>I<sq>m guessing it would be really hard to get a standard error for these. <br><br>They<sq>re point estimates - in a sense, we don<sq>t care if the difference is significant, we care which one is killing more people.  We shouldn<sq>t always get too excited about statistical significance, because we have to make a decision.<br><br>Here<sq>s a (slightly flippant) example<colon> You<sq>ve got a rare disease. Only 12 people have ever had the disease before. 6 of them took the red pill, and of them, 5 died. 6 of them took the blue pill, and of them, 1 died. Which  pill do you want to take. <br><br>We could do a significance test<colon><br><br>    > fisher.test(matrix(c(5, 1, 1, 5), nrow=2))<br>    <br>    \tFisher<sq>s Exact Test for Count Data<br>    <br>    data<colon>  matrix(c(5, 1, 1, 5), nrow = 2)<br>    p-value = 0.08009<br>    alternative hypothesis<colon> true odds ratio is not equal to 1<br>    95 percent confidence interval<colon><br>        0.800351 1389.408380<br>    sample estimates<colon><br>    odds ratio <br>      16.60315<br><br>Your doctor says <dq>Well, the difference isn<sq>t signficant, so it doesn<sq>t matter.<dq> I say <dq>I don<sq>t care if it<sq>s significant, give me the blue pill.<dq><br></p>", 
                "question": "Is this article on the mortality rates of various power sources statistically accurate (Deaths per Trillion kWhr)?"
            }, 
            "id": "cjmenfy"
        }, 
        {
            "body": {
                "answer": "<p>My first inclination was to say <dq>If these data are overall numbers from the total population of people and power sources, there is no standard error, and no <dq>statistical significance<dq>. Statistical significance only tests how confident we are that the sample data leads us to believe that there are differences in the true population numbers (in this case, death rates per kWhr). Since these seem to be the true population data, or close enough, there is no sampling error, and applicability for notions of statistical significance.<dq> HOWEVER<colon> The caveat here is that if we are studying extremely rare random events, we probably need to observe each energy source over many, many trillions of kWhr to get an accurate measure of the underlying process mean. Deaths from each source probably follow some sort of Poisson process-- if so then the mean is equal to the variance, and one could estimate confidence intervals for each energy type. But in this article is seems that they ARE observing lots of data over lots of time-- so these means are probably pretty accurate.<br><br>So, to echo <dq>jeremymiles<dq>, with huge sample sizes, here what we probably care about are <dq>important<dq> differences.  If one thing would end up killing 1,000,000 people and the other 1,000,001 then probably we should look to other factors other than danger to make our decision (pollution/waste spills, reliability, cost), since those other factors can end up killing people as well.</p>", 
                "question": "Is this article on the mortality rates of various power sources statistically accurate (Deaths per Trillion kWhr)?"
            }, 
            "id": "cjo6wcw"
        }, 
        {
            "body": {
                "answer": "<p>> How programming heavy is studying and pursuing a job in Statistics? <br><br>Depends on where you study and what job you do.<br><br>> So what<sq>s it like programming in Statistics? Is it wildly different than building applications and websites?<br><br>Yes, at least if my experience is anything to go by, but it depends on what job you do<br><br>>  Would you say its something you do rather uncommonly or is it something that is the bulk of your work?<br><br>A lot of what I do involves using statistical software, and that use involves some programming, but it<sq>s generally pretty short code. <br><br>For example, I wrote some code a few days ago to run a simulation -- but the whole thing was maybe ten lines long. R is pretty compact, so coding doesn<sq>t take long.<br></p>", 
                "question": "How programming heavy is studying and pursuing a job in Statistics?"
            }, 
            "id": "cji5sa6"
        }, 
        {
            "body": {
                "answer": "<p>It depends on the complexity of data structures you will be working with and the <sq>area<sq> of statistics you specialize in.  Very <sq>applied<sq> stat jobs (i.e., marketing research, quality control, polling\\survey) with simple data structures (i.e., independence of obs) and research designs can be accomplished using packages with drop-down menu driven capabilities.  But, as soon as you introduce complex data structures/research designs, non-standard scales of measurement, or specialized applications (e.g., test development/psychometrics, cross-classified/hierarchical data structures, repeated observations, ordinal/nominal scales of measurement), proficiency in database management/manipulation and programming is essential.  Programming languages in applied statistics range from very <sq>high-level<sq> (as seen in SAS, SPSS) allowing users to write macros, to very <sq>low-level<sq> (as seen in R, MATLAB) that allow users to write stand-alone functions and/or libraries.  <br>  </p>", 
                "question": "How programming heavy is studying and pursuing a job in Statistics?"
            }, 
            "id": "cjhx98t"
        }, 
        {
            "body": {
                "answer": "<p>I do statistics for a living. I have a statistician colleague that is able to code but prefers to work through the statistics from a conceptual standpoint and leave the coding to contracted collaborators. This broadens the set of topics he can tackle but, by his own admission, means his programming skills erode in time. He got into this position having demonstrated coding capability though. I have a hard time imagining how a new statistician can break into the field without that strong background, experience and skillset in coding. There are too many others who do have this background and lacking it seems like a self-administered hindrance. Then again, if programming isn<sq>t your thing, that sort of self-hindrance is likely to be a good thing. I<sq>m just not sure how far it can take you.</p>", 
                "question": "How programming heavy is studying and pursuing a job in Statistics?"
            }, 
            "id": "cjis0b1"
        }, 
        {
            "body": {
                "answer": "<p>I expect that, for the same reasons we still teach kids how to add and subtract before handing them a calculator, most educations in Statistics are still rather programming intensive. I use SPSS modeler in my job and it requires no programming whatsoever.</p>", 
                "question": "How programming heavy is studying and pursuing a job in Statistics?"
            }, 
            "id": "cjitw3f"
        }, 
        {
            "body": {
                "answer": "<p>Theory. <br><br>Rarely are you handed a dataset with 100 columns (which really isn<sq>t a lot) that are just labeled y, x1, x2, ..., xk. You know something about them. Y might be mortality and x1 through xk might be things like <dq>had surgery<dq>, <dq>age<dq>, <dq>bmi<dq>, <dq>number of comorbidities<dq>, <dq>favorite color<dq> and so on. Using theory you can start dropping things that don<sq>t make sense as predictors, such as <dq>favorite color<dq> and retaining things that do make sense such as surgery, age and so on. <br><br>Theory is also important if you want your model to be [identified](http<colon>//en.wikipedia.org/wiki/Identifiability) and to be able to make conclusions about the effects of x1 through xk on y. <br><br>If I were solely interested in predicting y *and* did not care about the relative importance of x1 through xk (such as is the task in many data mining applications), I would use a number of methods for feature selection. Of course, with 100 features for 1 million rows, I wouldn<sq>t worry about that much, if at all. The simplest method would to be use regression and fit k models, each of the form y ~ xi for i from 1 to k and select the model that minimizes AIC/maximizes R^2 or reduces cross validation error. Call the value of x that is selected xa. I would then fit y ~ xa + xi for i from 1 to k, excluding a. I would select the best model and then go again and again until 1) I get no change in model quality or 2) all parameters are in the model. This is known as forward stepwise regression. <br><br>You can also do what is known as backwards stepwise regression. In this case, you start with the model y ~ x1 + x2 + ... + xk and iteratively remove the <dq>worst<dq> predictor until the model quality starts to reduce. <br><br>There are also methods like LASSO regression but those are probably beyond what you are looking for. </p>", 
                "question": "How to determine the usefulness of data"
            }, 
            "id": "cjhuydw"
        }, 
        {
            "body": {
                "answer": "<p>Calculate the probability of a caution flag for a lap. Assuming p(flag) is not conditional on lap number, they should be independent. Then calculate the cumulative probability over a race of n laps. It should go to a high probability very quickly, but be asymptotic to 1. </p>", 
                "question": "Probability question."
            }, 
            "id": "cjg138f"
        }, 
        {
            "body": {
                "answer": "<p>Each variable has a unique mean, right? Looking through SPSS documentation, I<sq>m not seeing a way to do everything in one command in that case. Have you tried<colon><br><br>    RECODE VAR_5 (SYSMIS=mean.VAR_5).<br>    RECODE VAR_19 (SYSMIS=mean.VAR_19).<br>    RECODE VAR_23 (SYSMIS=mean.VAR_23).<br>    RECODE VAR_32 (SYSMIS=mean.VAR_32).<br>    RECODE VAR_50 (SYSMIS=mean.VAR_50).<br>    RECODE VAR_58 (SYSMIS=mean.VAR_58).<br><br>where mean.VAR_X is whatever that variable<sq>s computed mean is? Separating it out into a few different commands would probably be easier than trying to compact it all into one.<br><br>Am I understanding that correctly?</p>", 
                "question": "[SPSS Help] Is there a syntax for the value of a variable?"
            }, 
            "id": "cjaiu6j"
        }, 
        {
            "body": {
                "answer": "<p>I know this doesn<sq>t directly address your question, but this PDF file was a wonderful guide for me when learning SPSS syntax<colon> <br><br>ftp<colon>//public.dhe.ibm.com/software/analytics/spss/documentation/statistics/20.0/en/server/Manuals/IBM_SPSS_Statistics_Command_Syntax_Reference.pdf<br></p>", 
                "question": "[SPSS Help] Is there a syntax for the value of a variable?"
            }, 
            "id": "cjeinni"
        }, 
        {
            "body": {
                "answer": "<p>How much work are you planning to put into your study? One alternative if you can<sq>t get hold of a proper sampling frame is to use [respondent driven sampling](http<colon>//www.respondentdrivensampling.org/). In a nutshell, you recruit a first set of responders on e.g. Facebook, and then give an incentive for them to recruit their friend. If you construct the incentive in a particular way, you can then calculate unbiased estimates of the response distribution among your population of interest (teenagers in Bangladesh). <br><br>However, this requires a fair amount of set up, some sort of budget, and dealing with the data is not completely trivial. Another option is to use Google adwords, selecting preferences so as to target your demography, and have an ad with a link to your survey (again, some sort of incentive would probably be necessary). Since you<sq>ll only pay per click, this could be a fairly cost-effective method. However, it<sq>ll be more biased by <dq>answering tendency<dq> than the RDS method described above. <br><br>A final option that I just realised is that you could get in touch with [iccdr,b](http<colon>//www.icddrb.org/). They<sq>re really good, and have loads of experience with surveys and research in Bangladesh. If nothing else, you could have a look at their [Publications](http<colon>//www.icddrb.org/publications/cat_view/10043-icddrb-documents/10058-icddrb-reports-and-working-papers) and see if there<sq>s anything you could take after. <br></p>", 
                "question": "How do I reach teenagers here in Bangladesh for my survey?"
            }, 
            "id": "cj92dj9"
        }, 
        {
            "body": {
                "answer": "<p>The tutorial by Wehrens et al is very well readable</p>", 
                "question": "Any easy tutorial to better understand the concept of bootstrapping?"
            }, 
            "id": "cj5akc6"
        }, 
        {
            "body": {
                "answer": "<p>You have to do some reading as the answer is too long to be given here.<br>Google <dq>random effects poisson models<dq>.<br></p>", 
                "question": "Which test do I use for this data? (I read the sidebar I have more questions)"
            }, 
            "id": "cistvvx"
        }, 
        {
            "body": {
                "answer": "<p>Minimum entropy would be certainty. The problem is you don<sq>t know. <br><br>Maximum entropy maximizes the information <dq>in the noise<dq>, corresponding to you having the least information about it. In a particular sense, maximum entropy is <dq>given these conditions, *what<sq>s the worst*\\*\\*  *case*?<dq><sq><br><br>\\*\\*(i.e. least informed before the fact)<br><br>To quote the wikipedia article on the [principle of maximum entropy](https<colon>//en.wikipedia.org/wiki/Principle_of_maximum_entropy) (directly linked from the page you linked to)<colon><br><br>> In ordinary language, the principle of maximum entropy can be said to express a claim of epistemic modesty, or of maximum ignorance. The selected distribution is the one that makes the least claim to being informed beyond the stated prior data, that is to say the one that admits the most ignorance beyond the stated prior data.<br><br><br><br></p>", 
                "question": "Information Theory<colon> Principle of Maximum Entropy<colon> I<sq>m really confused"
            }, 
            "id": "cikjxqa"
        }, 
        {
            "body": {
                "answer": "<p>/u/efrique is spot on. But to give you a concrete example to think about (I will use loose and intuitive language here)<colon><br><br>Suppose all you know about a variable X is that it is random with mean 0.5 and it can take values in the range [0,1]. Which distribution should you use to model this variable, given this information? There are infinite choices. You could put a delta function at 0.5, which would be the minimum entropy distribution that satisfies what you know. The problem is that this is a very <dq>bold<dq> claim that is not supported by your prior knowledge. You are claiming X never takes any value other than 0.5; you have added <dq>information<dq> about the variable X that is not supported by what you know. A more modest claim would be to find the distribution that <dq>adds<dq> no more information<colon> the uniform distribution over [0,1]. This is the maximum entropy distribution.  <br><br>The papers by Shannon and Jayne are actually very readable and I suggest reading them if you are writing a research paper in the area. It will be well worth your effort. Hopefully the comments here give you the intuitive picture to get started though.</p>", 
                "question": "Information Theory<colon> Principle of Maximum Entropy<colon> I<sq>m really confused"
            }, 
            "id": "cikly6k"
        }, 
        {
            "body": {
                "answer": "<p>Logistic regression gives you an OR associated with a unit increase in the (continuous) independent variable.  If you instead want a RR for a unit increase in the independent variable, one option is a log-binomial regression, where the only difference in the setup compared to logistic regression is the link function<colon><br><br> * Log link<colon> log(p)<br> * Logit link<colon> log[p/(1-p)]<br><br>Another alternative that will give you RRs instead of ORs is Poisson regression with <dq>robust error variance.<dq>  Poisson regression assumes a poisson, rather than binomial, distribution, which is paired with a log link function.<br><br>Not an exhaustive list -- there are (always) other approaches -- but probably the two simplest options.<br><br>[This paper](http<colon>//biostats.bepress.com/uwbiostat/paper293/), though it<sq>s a bit old now, should give you an idea of how to implement your chosen procedure in your chosen language (if you<sq>re not interested in technical details, the code starts on page 18).  In R it should be fairly straightforward, but at the time the paper was written, no procedure existed for doing this in SPSS.  UCLA provides [very detailed instructions for implementing both approaches in SAS](http<colon>//www.ats.ucla.edu/stat/sas/faq/relative_risk.htm), but that doesn<sq>t sound like it will be helpful to you.<br><br>edit<colon> I forgot to mention that you may need to play around with multiple methods because, since these approaches find solutions via iterative methods, you always run the risk of any given algorithm/data combination not converging.</p>", 
                "question": "Likely easy question<colon> How can I calculate relative risk when the independent variable is ordinal/continuous?"
            }, 
            "id": "cieneic"
        }, 
        {
            "body": {
                "answer": "<p>Answer<colon> the central motherfuckin limit theorem. </p>", 
                "question": "Why is it generally accepted that a sample size can represent an entire population?"
            }, 
            "id": "cichqg9"
        }, 
        {
            "body": {
                "answer": "<p>It depends on what sort of sample you are talking about.  If you can take a truly random sample from the population, then you can actually calculate the degree to which the sample may likely be different from the population (the margin of error).<br><br>With convenience samples, this may not be the case, but it really depends on the focus of the study.  the researchers may be interested more in the internal validity (causal connection between the variables) & whether the effect can ever exist rather than the generalizability/external validity.</p>", 
                "question": "Why is it generally accepted that a sample size can represent an entire population?"
            }, 
            "id": "cich9j7"
        }, 
        {
            "body": {
                "answer": "<p>There is some difficulty and expertise needed to take a proper sample. But you can calculated how accurate your results will be based on your sample size and other factors. It<sq>s the basis of statistics. If you think about it, when you measure anything at all<colon> say, an experiment in a lab, you have to realize that you<sq>re only take a sample of all the possible (maybe infinite) measurements. </p>", 
                "question": "Why is it generally accepted that a sample size can represent an entire population?"
            }, 
            "id": "cici7hv"
        }, 
        {
            "body": {
                "answer": "<p>I think it is important to highlight that the way the media often talks about study findings is inaccurate. Too often we see news articles that over-generalize the results from a study. Most of the time it is not a single study, but many studies carried out by different researchers with different methods that allow us to begin to make generalizations.</p>", 
                "question": "Why is it generally accepted that a sample size can represent an entire population?"
            }, 
            "id": "cickkcf"
        }, 
        {
            "body": {
                "answer": "<p>Well, polynomials tend to blow up near the boundaries so that could be a reason for large errors in the beginning.</p>", 
                "question": "Error in Quintic Spline Interpolation"
            }, 
            "id": "ci5mxie"
        }, 
        {
            "body": {
                "answer": "<p>[deleted]</p>", 
                "question": "Can someone explain this Nature article?"
            }, 
            "id": "chx8m1t"
        }, 
        {
            "body": {
                "answer": "<p>The article referenced in the diagram is [here](http<colon>//www.stat.duke.edu/courses/Spring10/sta122/Labs/Lab6.pdf). It<sq>s a bit longer of a read than the Nature article, but by glancing at it<sq>s abstract and keywords it would appear that Bayesian reasoning is being used.</p>", 
                "question": "Can someone explain this Nature article?"
            }, 
            "id": "chxt8up"
        }, 
        {
            "body": {
                "answer": "<p>**Simple, Straight, and hopefully useful answer<colon>**<br><br>The standard deviation will be the same...which is good since that<sq>s all ya<sq> got.  So just subtract from 100 and use the same SD.<br><br><br><br><br>**Not very useful editorial comments<colon>**<br><br>* I hate meta-analyses...I think they are junk...This is not intended as a personal attack...It is just that I have this heretical tic...Kinda like statistical Asperger<sq>s syndrome.<br>* The real subject always matters and that is why it will confuse...but not **only** confuse.<br></p>", 
                "question": "Quick question - reversing some measurement scales"
            }, 
            "id": "chyq7u8"
        }, 
        {
            "body": {
                "answer": "<p>As statshelperguy posted, scale typically refers to nominal, ordinal, interval, and ratio data, collectively.  <br><br>If you mean you are saying it is ordinal and your professor says it is interval or ratio (continuous -- implied by your defense that they are discrete), I<sq>d say you are both incorrect.<br><br>Let<sq>s take the cases<colon><br>Ratio<colon>  Say they were ranked 1-5 in the order you listed them.  That does not mean that decapitation is 5 times more humane than the electric chair.  Therefore, not ratio scaled.<br><br>Interval<colon> It also does not mean that lethal injection is to electric chair as decapitation is to hanging.  Ruling out interval. <br><br>Nominal or Ordinal<colon><br>As to nominal, the categories *are* nominal...there is no natural order to them.  You are asking the subject to place their own <dq>order<dq> upon them.  It is subjects opinion on the order that has your interest...the order is the endpoint and not a real order.  So they are not <dq>objectively<dq> ordinal, in any meaningful way, because the <dq>ordinality<dq> is in the eye of the beholder<colon> subjectively ordinal.<br><br>So you will, in the end, get some type of median ranking for each execution method.  This is a typical summary for a nominal response and not an ordinal response.  I cannot see how it will usefully be treated as ordinal...perhaps as a very simple descriptive summary.  To gain much inference, you will need to do more than category medians.<br><br>What you will really have is 5 empirical distribution functions for each category consisting of a plot of percent responding (with that rank or less) plotted by the rank.  You will probably want to compare these EDFs.  I do not know enough about the study to suggest any more.  If you are interested in the EDF methodology, I would recommend the book <dq>Nonparametric analysis of longitudinal data in factorial experiments<dq> by Brunner, Domhof, and Langer.  It is a very fast read and very practical and more than you might need.  But it should point you in the right direction.  The simplest summary of this type will be the relative effect of each execution method.  This would be a better measure than the median of each.<br><br>A different alternative would be to analyze the data as a set of comparative judgements (which is what your subjects are implicitly doing  anyway).  But I<sq>d need to know more about what the point is to say more.<br><br>**Edit after your edit**<colon><br>It<sq>s a nifty example of a complicated endpoint value since it just begs to be called ordinal.  BTW, though my solution treats the execution manner as nominal, the rank values by each observer are properly treated as ordinal.  It is such a fun case that it tempers my disappointment that it may just be a hypothetical.  If it came from a real study, I<sq>d love to see what they did with it.  Do you or your professor have a reference?</p>", 
                "question": "Professor and I are debating whether or not this survey question is ordinal or scale. What do you think?"
            }, 
            "id": "chu950w"
        }, 
        {
            "body": {
                "answer": "<p>I<sq>d argue ranking does not give a scale.</p>", 
                "question": "Professor and I are debating whether or not this survey question is ordinal or scale. What do you think?"
            }, 
            "id": "chu8tuw"
        }, 
        {
            "body": {
                "answer": "<p>If the argument is whether it is ordinal or interval, it is almost certainly ordinal. </p>", 
                "question": "Professor and I are debating whether or not this survey question is ordinal or scale. What do you think?"
            }, 
            "id": "chuickb"
        }, 
        {
            "body": {
                "answer": "<p>I would say these data are ordinal. <br><br>Why? Well, this isn<sq>t nominal because you want some kind of meaningful order here (least to most humane). It<sq>s not interval because the differences in humanity between each type of punishment isn<sq>t the same. This is the same reason why it<sq>s not ratio. <br><br>Ordinal data have a specific order, but the difference between each ordered category isn<sq>t equal. For example, is the difference between hanging and firing squad the same as the difference between lethal injection and decapitation? To put it in another way<colon> ordinal, because well, it entails order. </p>", 
                "question": "Professor and I are debating whether or not this survey question is ordinal or scale. What do you think?"
            }, 
            "id": "ci1w0fo"
        }, 
        {
            "body": {
                "answer": "<p>You have to convert it to Fisher Z-values. There is an Excel Script around that let<sq>s you do the test, just Google it! </p>", 
                "question": "Question (I<sq>m ashamed to ask) <colon> I want to know if the difference between two correlation coefficients is statistically significant."
            }, 
            "id": "chlsl8b"
        }, 
        {
            "body": {
                "answer": "<p>Do a regression between x and y with sex in the model and the interaction between x and sex. The interaction tells you if they are different. </p>", 
                "question": "Question (I<sq>m ashamed to ask) <colon> I want to know if the difference between two correlation coefficients is statistically significant."
            }, 
            "id": "chluq8n"
        }, 
        {
            "body": {
                "answer": "<p><dq>Applied Regression Analysis and Other Multivariate Methods<dq> by Kleinbaum, Kupper, Nizam and Muller, and Applied Linear Regression Models by Kutner Nachsheim and Neter are the two I have and they put it into really simple terms<br></p>", 
                "question": "Any suggestions for simple but good quality introductory texts for Generalized Linear Models?"
            }, 
            "id": "chl9e6t"
        }, 
        {
            "body": {
                "answer": "<p>Is the .000 a test statistic or a p-value? </p>", 
                "question": "Turkey HSD question [VERY BASIC]"
            }, 
            "id": "chfv4re"
        }, 
        {
            "body": {
                "answer": "<p>Tukey* Also, a non-significant value depends on your predetermined alpha level. If you set your alpha level to .05, .01, or .001 (the common alpha levels), then your result is significant. I don<sq>t think newer versions of SPSS give astericks for significant results anymore. This is probably because significant results depend on your specified alpha level and not some generic .05 level</p>", 
                "question": "Turkey HSD question [VERY BASIC]"
            }, 
            "id": "chg79sm"
        }, 
        {
            "body": {
                "answer": "<p>The 3 repeated observations on a single sample do not make this a repeated measures design.  You put your finger on why<colon> <dq>but it<sq>s not like the conditions of the sample have changed between measurements<dq>.  Indeed they have not.  The information gained from the 3 observations is telling you about the measurement error in your system and not much about your <dq>treatments<dq>.  This may be important and you need to account for it.<br><br>But your statement of your design is ambiguous.  You said <dq>I have 4 replicates of each sample for each method.  Each of these samples was measured 3 times.<dq>  I cannot determine if this means you had 4 <dq>animals<dq> and you took 4 samples from each animal (one for each method), and then 3 samples to measure the results of each method.  Or does it mean you had 4 different animals for each method separately and measured these 3 times.</p>", 
                "question": "Repeated Measures not Separated by Time"
            }, 
            "id": "chvp2y4"
        }, 
        {
            "body": {
                "answer": "<p>You could convert the scores to z-scores, and check if the proportion of high-value z-scores corresponds with the assumed proportion of a normal distribution (e.g., only ~5<percent> of z-scores of your sample should be -3 or lower/3 or higher). I know for a fact that SPSS has an option to generate z-score lists to check for outliers.</p>", 
                "question": "Finding errors in demographic data sets"
            }, 
            "id": "chduidc"
        }, 
        {
            "body": {
                "answer": "<p>Oh my God where are your parents!</p>", 
                "question": "ELI5<colon> The Bonferroni Correction"
            }, 
            "id": "chaynft"
        }, 
        {
            "body": {
                "answer": "<p>The bonferroni correction is used to lower the p-value threshold in order to mitigate additional chance errors when you compute a series of individual, though possibly related analyses. For example, if you were to compute a series of t-tests and see which ones are significant. Bonferroni correction is especially important when you are just trying to fish for significant results by conducting a number of analyses.<br><br>Your analysis doesn<sq>t sound like it requires a bonferroni correction. Instead, a simple Multiple Linear Regression with all your variables as predictors and earnings on outcome should suffice. All done in one test.<br><br>Why are you interested in Bonferroni correction? </p>", 
                "question": "ELI5<colon> The Bonferroni Correction"
            }, 
            "id": "chazd6d"
        }, 
        {
            "body": {
                "answer": "<p>If you<sq>ve planned your study properly, you should never need p-value corrections. In any case, they provide a technically correct answer to a question that nobody every asked. In 2014, if your textbook discuses Bonferroni correction other than to tell you why you shouldn<sq>t do it, throw the textbook in the bin.</p>", 
                "question": "ELI5<colon> The Bonferroni Correction"
            }, 
            "id": "chbp9a1"
        }, 
        {
            "body": {
                "answer": "<p>Calculus up through partial differentiation, some series, and linear algebra are what<sq>s required in my program<colon> http<colon>//www.stat.colostate.edu/statprostudents/statdistance/statdistancemas/statprerequisitesmas.html </p>", 
                "question": "How much undergraduate math would you consider sufficient for applying to Masters programs in Applied Statistics?"
            }, 
            "id": "ch45u07"
        }, 
        {
            "body": {
                "answer": "<p>Most applied stat programs i<sq>ve seen only require through calc III and linear algebra.</p>", 
                "question": "How much undergraduate math would you consider sufficient for applying to Masters programs in Applied Statistics?"
            }, 
            "id": "chh9iiz"
        }, 
        {
            "body": {
                "answer": "<p>Out of curiosity, how do you have a pre-post setup where one measurement is normally distributed and the second isn<sq>t?<br><br>I<sq>m not completely certain about this particular scenario, but I think that if either of your two data arrays aren<sq>t normally distributed, it<sq>s probably best to go with Wilcoxon. Hopefully someone can correct me on that if I<sq>m wrong, though.</p>", 
                "question": "Quick Question- Doing a within subjects analysis and first condition is normally distributed but second is not do i use a t-test or wilcoxon?"
            }, 
            "id": "cgvhoox"
        }, 
        {
            "body": {
                "answer": "<p>><dq>No True Scotsman<dq> fallacy?<br><br>Not really a formal theory in statistics. <br><br>The data to collect (or exclude) should be driven by the research questions under study. Example<colon> if studying men<sq>s opinion, it is appropriate to include only men in your survey. The online pollster<sq>s biggest challenge to meeting inferential assumptions relate to *random sampling* and the lack thereof. </p>", 
                "question": "Question about population selection"
            }, 
            "id": "cgus615"
        }, 
        {
            "body": {
                "answer": "<p>A couple of things, firstly you<sq>re going to need to take into account the bias that may come about through response rate. For example even though you manually selected respondents, it is likely that only the more frequent and passionate men will respond. You should try and collect some information about them to try and quantify that bias.<br><br>Secondly strictly speaking a few of those questions would be less loaded if you rephrased them, for example when you ask people about whether men are treated unfairly it would be better to show a scale with females at one end and males at the other with the middle position being neutral and the question phrased to be which gender do you feel is treated unfairly (sorry I<sq>m on phone so can<sq>t recall the exact wording of the group). There are also a couple of issues further down where you ask about men<sq>s experience and it might be better not to prompt them with pre set answers and tick boxes - not only because you<sq>re prompting but also because there are many more possible categories than you have listed.<br><br>Happy to assist further, I<sq>m a survey statistician and enjoy developing surveys that represent the least amount of bias possible.</p>", 
                "question": "Question about population selection"
            }, 
            "id": "cgxa7jo"
        }, 
        {
            "body": {
                "answer": "<p>It is clearly email. If you want to make this formal you could use something like a modified Borda count which is investigated rigorously in a recent paper in the [Annals of Statistics](http<colon>//arxiv.org/abs/1204.1688).</p>", 
                "question": "Method for ranking multiple ordinal variables"
            }, 
            "id": "cgsr7tx"
        }, 
        {
            "body": {
                "answer": "<p>If you are interested in understanding the association between an observed variable (y) and an explanatory variable (x) where both are also associated with a third explanatory variable (z) i.e., when (x,z) and (y,z) are correlated, then PLS is one of the tools that can be used.<br><br>It helps explain the association between y conditioned on z (y | z) and x conditioned on z (x | z). While this may seem trivial for one z, it is quite challenging for hundreds of explanatory variables, such as expression of genes.<br><br>Often, there is an assumption of causality between z and each of x and y.<br>Made up example<colon><br>y = SAT score<br>x = Body Mass Index (BMI)<br>z = median family income<br><br>While y and x may be marginally associated, doing PLS with z as another explanatory variable may reveal not statistically meaningful relationship.</p>", 
                "question": "ELI5 Partial Least Squares Regression"
            }, 
            "id": "cgipbj9"
        }, 
        {
            "body": {
                "answer": "<p>You<sq>ll need to see how your times are distributed.  Collect all the times and then plot a histogram for each group.  If it<sq>s fairly normal, you can just use a simple t test or ANOVA.  I wouldn<sq>t be surprised if it<sq>s long-tailed though, in which case you can try transforming the times (maybe log) to normalize or otherwise use a non-parametric test.</p>", 
                "question": "Need help on what statistic test would be appropriate for my experiment?"
            }, 
            "id": "cgaz5sd"
        }, 
        {
            "body": {
                "answer": "<p>I<sq>m thinking you might need to duration/survival analysis since it<sq>s time based.</p>", 
                "question": "Need help on what statistic test would be appropriate for my experiment?"
            }, 
            "id": "cgbee55"
        }, 
        {
            "body": {
                "answer": "<p>quantile regression would be a solution if you have some weird distribution of times. qr is more robust to such violations </p>", 
                "question": "Need help on what statistic test would be appropriate for my experiment?"
            }, 
            "id": "cgbijbb"
        }, 
        {
            "body": {
                "answer": "<p>> ... weigh the data by sample size. bigger samples are more accurate and i want them to have a bigger effect.<br><br>That sounds weird. What do you mean by that? <br><br>The sample size is the number of observations in your sample. So, if you have two groups your sample sizes are the number of people in each of those groups.</p>", 
                "question": "Am I artificially inflating the data?"
            }, 
            "id": "cg5vla6"
        }, 
        {
            "body": {
                "answer": "<p>Yeah, SPSS interprets those weights as frequency weights, so you<sq>re effectively using whatever sites are made up of as your unit of analysis - this could be <dq>artificially inflating the data<dq> depending on your particular research questions.<br><br>The best solution is to use a package that supports other forms of weighting - Stata is the only package I know for a fact would handle this well.  Another option is to normalize the weights so they sum to your number of sites (mean weight is 1).</p>", 
                "question": "Am I artificially inflating the data?"
            }, 
            "id": "cg5xa9a"
        }, 
        {
            "body": {
                "answer": "<p>I think it<sq>s two one-way ANOVAs. With three groups you can do a more liberal Fisher<sq>s Protected LSD post-hoc test instead of a more conservative Tukey<sq>s.</p>", 
                "question": "What SPSS tests should I be running for my research?"
            }, 
            "id": "cg53z8p"
        }, 
        {
            "body": {
                "answer": "<p>You<sq>ll be using a two-way ANOVA actually, if is a 3x2 design. Or, if the DVs aren<sq>t conditional on each other, run two one-way ANOVAs. <br><br>You<sq>ll also be interested likely in post-hoc tests, to see exactly where mean differences lie (is 1>3, is 2<3, is 1<2?). All the basic ANOVA tells you in the significant F-test is that there are mean differences in your groups <dq>somewhere<dq>, but the post-hoc test will tell you where they are. I recommend Tukey<sq>s HSD post-hoc test.</p>", 
                "question": "What SPSS tests should I be running for my research?"
            }, 
            "id": "cg4zalr"
        }, 
        {
            "body": {
                "answer": "<p>First off, it depends on whether your DVs are measured categorically or continuously. <br><br>If your DVs are continuous (i.e., numbers), the simplest thing to do would be to run two one-way ANOVAs -- one for each DV. (You would **not** use a two-way ANOVA, which requires two IVs.) <br><br>If you find a significant *F*, you will then use post hoc tests to determine which group(s) is/are different from the others. (I would not Fisher<sq>s LSD, which increases the risk of a Type I error, and would go with Tukey instead.)<br><br>If your DVs are categorical, your best bet is probably two two-way chi-square tests -- again, one for each DV.</p>", 
                "question": "What SPSS tests should I be running for my research?"
            }, 
            "id": "cg9cz95"
        }, 
        {
            "body": {
                "answer": "<p>The probability of a yahtzee occurring on one turn is 0.04. The second turn is independent of the first turn and the probability is again 0.04. As the are independent the probability of a yahtzee on turns 1 and 2 both is 0.04 \\* 0.04. <br><br>For a given number of (independent) turns k, the probability of having a yahtzee on each of those turns (e.g., k yahtzees in k turns) is 0.04^k. </p>", 
                "question": "How to calculate provability of a Yahtzee on every turn?"
            }, 
            "id": "cfn5gi3"
        }, 
        {
            "body": {
                "answer": "<p>As per the actual rules of Yahtzee, it would be impossible*.  You are allowed to keep scoring Yahtzees without scoring other situations if they came up.  In other words, if you get 13 straight Yahtzees to start a game, you<sq>d mark those down (50 points for the first Yahtzee and 100 points for each subsequent Yahtzee) and keep playing, even if others have already filled out their score cards to completion.  <br><br>Technicalities aside, if the trials were independent, 0.04^n would be appropriate**.  <br><br>I<sq>d doubt the independence assumption pretty strongly though.  Most Yahtzees are not rolled like that initially (after all, the probability of an initial Yahtzee is much less than 4<percent>).  You get three rolls per turn in the game.  If strategically going for Yahtzees, you<sq>re more likely to get one.  <br><br>For example, if you get 6 6 6 6 5 on roll 1 of turn 1, you<sq>re most likely going to keep trying to get that Yahtzee by re-rolling the 5.  Suppose you get it on roll 2 or 3 of turn 1.  On turn 2, you<sq>ve already got a Yahtzee in the bag.  Now you<sq>re trying to check off the other big ticket items on the score sheet.  Let<sq>s say you roll 2 2 3 4 5 on roll 1 of turn 2.  Would you really re-roll the 3 4 5 in an attempt for a Yahtzee of 2s?  Most players would keep the 2 3 4 5 and re-roll the other 2 in pursuit of a large straight (with a small straight as a fallback).  <br><br>Basically, position in the game at the start of a roll makes a Yahtzee more or less likely.  As a final example, consider the end-game where the only blank on your scorecard is in the 6s space.  You<sq>ve played the top half well enough that you<sq>re going to get the 35 bonus points as long as you can get two sixes up top. You roll 1 1 1 1 6.  Do you roll for a Yahtzee of 1s, or do you keep the 6 in pursuit of top-half bonus points?  That will be a function of the current score, risk aversion, what<sq>s riding on the game (playing for fun? for money? etc.), and several other factors.<br><br>This is all just some food for thought.  The neat clean statistical answer is 0.04^n, but by making the independence assumption, you assume away the entire point of the game.  Yahtzee is fun to play in large part because the trials are not independent, so why go for the boring answer when you can perform a much more colorful analysis.<br><br>*Insofar as 0.04^999999 is zero in the limiting sense.<br><br>**I<sq>m just going to run with your 4<percent> assumption for now, assuming it was derived appropriately.  I<sq>m on my phone and haven<sq>t independently verified that number.</p>", 
                "question": "How to calculate provability of a Yahtzee on every turn?"
            }, 
            "id": "cfnd44e"
        }, 
        {
            "body": {
                "answer": "<p>There are two different notions of conditional distribution. It can help to be pedantic about notation when discussing this.  I will denote random variables by X and Y. An event E is a subset of points (x,y) in the domain of the random variable (X,Y).  <br><br>The first notion of conditional distribution is <dq>conditioning on an event,<dq> which is written P(Y|E) or P(Y|E).  These are events like <dq>what is the probability of getting three heads in a row in my ten coin flips given that I got three tails in a row at some point.<dq>  This is not the notion of conditional probability you are describing, but it is used quite often. <br><br><br>The other notion of conditional probability is <dq>conditioning on a random variable.<dq>  This is written P(Y|X=x).   If the set E<sq>={(x,y)|X=x} is well behaved (measurable with nonzero measure, etc) then you can simply treat this as a special case of the above.  However the definition can be extended to cases like picking a single point x when it has a continuous distribution on the real line.  This notion of conditional probability is a function from the domain of X to the set of probability distributions over the domain of Y.  When you look at the object P(Y|X=x) you<sq>re <dq>picking out<dq> a single point.  There<sq>s no concept of dependency anymore.  <br><br>The pedantic notation is helpful because you can explain it like this<colon> **<dq>X is a random variable, but x is not.  x is just a single point.<dq>**</p>", 
                "question": "Why is it that if you<sq>re modeling P(y|x) you don<sq>t have to account for dependencies in x (but account for them implicitly) while the converse is true if you model P(yx)?"
            }, 
            "id": "cf81l6r"
        }, 
        {
            "body": {
                "answer": "<p>Isn<sq>t anybody going to at least try to answer this?<br><br>OK, let<sq>s make some assumptions.  You were in traffic, so I guess you live in a city.  Let<sq>s assume its Chicago.  While trials are in progress for murderer suspects, they are held in [this building](http<colon>//en.wikipedia.org/wiki/Metropolitan_Correctional_Center,_Chicago), unless they pay bail.  Usually bail is [not an option](http<colon>//wiki.answers.com/Q/What_is_typical_bail_amount_for_first_degree_murder) in 1st degree murder trials.  Chicago had 415 murders in [2013](http<colon>//en.wikipedia.org/wiki/Crime_in_Chicago#Homicides_in_Chicago_by_year).  I can<sq>t find the number of annual homocide convictions in Chicago, but in DC its 70 annual [convictions](http<colon>//www.washingtonpost.com/investigations/as-dc-homicides-decline-murder-still-a-stubborn-crime-to-solve-and-prosecute/2012/10/13/e19132a2-fc23-11e1-a31e-804fccb658f9_story.html) for about 200 annual [murders](http<colon>//en.wikipedia.org/wiki/Crime_in_Washington,_D.C.).  Let<sq>s assume that Chicago also has a 35<percent> conviction to murder rate.  Then there are 175 convictions per year in Chicago.  A trial can apparently [last](http<colon>//answers.yahoo.com/question/index?qid=20090228085216AAdnoBg) anywhere from a week to a few months.  Suppose that half of the defendants are out on bail (generous, I think).  That leaves us with 87 ultimately convicted defendants being housed in downtown chicago.  If trials are spread out evenly across the year, at any given time there is someone who is going to be convicted of murder spending time in downtown Chicago.  Have you ever been to Millennium Park?  Then you have been within a km of someone ultimately convicted of murderer.    </p>", 
                "question": "What are the odds that I<sq>ve encountered a murderer in my life whether I<sq>ve known it or not?"
            }, 
            "id": "cehxyau"
        }, 
        {
            "body": {
                "answer": "<p>I know it<sq>s not an actual answer, but I sold one a camcorder and a car stereo once. Didn<sq>t find out until later that he was a killer, when I saw his picture on the front of the newspaper and was like, <dq>hey, I know that guy!<dq><br><br>I worked in a department store in the mid <sq>90s as my college job. This nice guy came in and bought the best camcorder we had. Paid with a credit card. A couple days later, he came back and bought a car stereo. A week or so later, when I got into work for my shift - I worked something like 5pm-11pm - one of my coworkers told me the head of corporate security called looking for me. I didn<sq>t think much of it. The next day, I see the paper and that<sq>s when I put it together. The guy killed an elderly man in Arizona and was heading north through Oregon using the man<sq>s credit cards.</p>", 
                "question": "What are the odds that I<sq>ve encountered a murderer in my life whether I<sq>ve known it or not?"
            }, 
            "id": "cehj5hv"
        }, 
        {
            "body": {
                "answer": "<p>> I wonder how many of them have murdered someone. <br><br>Depends on where you live and how long a period we<sq>re looking at, as well as how much you drive and how heavy the traffic is. Likely more than a few.<br><br>There are about 47 murders per year per million people across the USA (but a fair bit of variation by location). You probably drive past tens of thousands of different people a year. On the other hand a lot of the perpetrators will be in prison at any given time. In an average place in the US, you probably drive past several murderers every year. In some locations, probably dozens.<br><br><br>> what are the odds that I<sq>ve at the least, been within a 1km radius of a murderer within my life?<br><br>Depends on where you live/travel to. In a lot of places, I<sq>d say that<sq>s pretty much a certainty.<br><br>[Speaking for myself, I<sq>ve shaken hands with someone convicted of murder, so about as close as it comes.]<br></p>", 
                "question": "What are the odds that I<sq>ve encountered a murderer in my life whether I<sq>ve known it or not?"
            }, 
            "id": "cehhnmj"
        }, 
        {
            "body": {
                "answer": "<p>One quick thing<colon> your total is off. Instead of summing, use =COUNT(B2<colon>B51) and corresponding column name for other columns. You have total summing > 50, when you only have 50 entries...</p>", 
                "question": "I have to review and analyze a report for work but I<sq>m getting different values. Can you guys help?"
            }, 
            "id": "cebfs3e"
        }, 
        {
            "body": {
                "answer": "<p>Your countif logics are wrong. For an example, take a look at B55. You didn<sq>t subtract score of <dq>11<dq>. Since there aren<sq>t much, I<sq>d suggest logics along this line for clarity<sq>s sake<colon> =COUNTIF(B2<colon>B51, <dq>=9<dq>) + COUNTIF(B2<colon>B51,<dq>=10<dq>) and so on.</p>", 
                "question": "I have to review and analyze a report for work but I<sq>m getting different values. Can you guys help?"
            }, 
            "id": "cebfwje"
        }, 
        {
            "body": {
                "answer": "<p>Looks like numbers from final report didn<sq>t include <dq>Don<sq>t Know (11)<dq> into the denominator of your NPS. Your call on this one depending on business/survey context and discussion with your peers and manager. That should clear up the discrepancies. </p>", 
                "question": "I have to review and analyze a report for work but I<sq>m getting different values. Can you guys help?"
            }, 
            "id": "cebfz7u"
        }, 
        {
            "body": {
                "answer": "<p>Now that, that is a tricky question.<br><br>Unfortunately I<sq>m at the airport just now and can<sq>t dig into this question with the gusto that it requires. But here<sq>s my two cents anyway.<br><br>The thing about the Polya urn is that the outcomes of successive draws (and replacements) have a gradually lesser impact on the outcomes of successive draws that follow.<br><br>The rich get richer, so to speak.<br><br>Now if you mess around with the number(s) of similar units which you replace after a draw - let<sq>s call them coloured balls from here on out - things get a little confused. Let us suppose that the coloured balls form a rainbow, and that the red ball is most similar to the orange ball, then the yellow ball, and so on and so forth. If you replace the ball that is drawn with some number of the same coloured ball, and some strictly lesser number of the next colour, and some number of the next colour which is again strictly lesser, you should still see the same behaviour over infinite trials.<br><br>That is, the rich get richer.<br><br>In the most basic Polya urn, with two colours, the convergence is very fast. But with more different coloured balls, my gut tells me that you will have the same convergence, just a little slower.<br><br>This is about as far as I can go, I would suggest not doing the maths. That sounds horrible. Maybe have a go at a simulation based approach?<br><br>And assuming that this is all correct, perhaps this relationship will not hold if we replace those <dq>less than<dq> symbols with <dq>less than or equal to.<dq><br><br>Hope this helps.</p>", 
                "question": "How to figure out the asymptotic behavior of a version of Polya urn?"
            }, 
            "id": "cdqtzu3"
        }, 
        {
            "body": {
                "answer": "<p>The primary things you should be measuring are quality and efficiency.<br><br>For quality, you can measure error rate, percentage of defects found, percentage of rework needed. Quality is important so that you don<sq>t have to do the same work twice, and for customer satisfaction, and to reduce failure costs (refunds, warranty, lost contracts, etc)<br><br>For efficiency, divide your production by your man-hours. This metric shows that you aren<sq>t just using brute force to achieve production goals-- excessive overtime and the like.<br><br>Hope this helps.</p>", 
                "question": "Measuring effectiveness & productivity"
            }, 
            "id": "cd1yq3i"
        }, 
        {
            "body": {
                "answer": "<p>In doing a quick run of the efficiency measure, it comes out to<colon><br><br>Production (2,821) / Man-hours (21,077) = .134<br><br>How do I know whether .134 is good, bad, mediocre, etc?  I could look at prior month data, but I<sq>d still be left with the same question.  <br><br><br><br></p>", 
                "question": "Measuring effectiveness & productivity"
            }, 
            "id": "cd238di"
        }, 
        {
            "body": {
                "answer": "<p>In my own experience (also not a statistician), the easiest way to really understand Granger causality is a slightly roundabout way<colon><br>First get your head around the mutual information of two variables; for that you need to know how Shannon information/entropy is defined (which I hope you already do as a physics major) and what a Kulback-Leibler distance is. I cannot stress enough how helpful a true understanding of mutual information will be in you endeavour.   <br>Then read Thomas Schreiber<sq>s [original article on transfer entropy](http<colon>//arxiv.org/pdf/nlin.CD/0001042.pdf), which explains in detail the relationship to mutual information, and is on top of that beautifully written and very accessible.<br>Then explore the [relationship of transfer entropy with Granger causality](http<colon>//arxiv.org/pdf/0910.4514v2.pdf) or you just trust another group of physicists and accept that transfer entropy of normally distributed variables reduces to Granger causality. <br>The same authors have also looked at [mutivariate Granger causality](http<colon>//arxiv.org/pdf/1002.0299.pdf), which should be of use to you if you want to look at the relationship of all your 100 variables simultaneously. Like I said, I am not a statistician, but that should get you around the need for a Bonferroni correction and the like and be the better angle of attack anyway - you want to avoid inferrence of causality between variable pairs since the correlation may be only due to interaction with a <dq>hidden<dq> third variable. <br><br>Thomas Schreiber has also written a book on time series analysis in general, which I have found an invaluable resource. If you look around enough, you should be able to find a pdf online somewhere. ;-)<br><br>Hope that helps. <br></p>", 
                "question": "How can I best self learn granger causality and how to implement it via coding?"
            }, 
            "id": "ccytv76"
        }, 
        {
            "body": {
                "answer": "<p>You<sq>re definitely going to want to learn a little bit about time series analysis. In particular, you need to know how to represent time series, shift them, and compute correlation. You<sq>ll also want to learn a bit about significance testing and, given how many variables you<sq>re looking at, multiple comparison correction methods like Bonferroni.</p>", 
                "question": "How can I best self learn granger causality and how to implement it via coding?"
            }, 
            "id": "ccykju1"
        }, 
        {
            "body": {
                "answer": "<p>The article does admit that the sample is not representative of the general population (of the USA).  <br><br>The main issue however is that they are assuming that their method of questioning is more likely to give truthful answers.  This might or might not be true.  <br><br>Any time you ask someone <dq>are you gay?<dq>  they might lie, or even give an untruthful answer by accident.  <br><br>My personal view is that it<sq>s not a black and white question, and neither is the question <dq>are you anti-gay?<dq>  So, it is a very difficult issue to treat statistically.</p>", 
                "question": "Is this bad statistics?"
            }, 
            "id": "ccstp8h"
        }, 
        {
            "body": {
                "answer": "<p>What problem are you worried about? They are reporting on a proof of concept study that appears to show that most of the nationally representative studies on sexual orientation have pretty significant measurement bias. They then extrapolate from that, with proper caveats and hedging words, that these findings could have implications for the general public. </p>", 
                "question": "Is this bad statistics?"
            }, 
            "id": "ccswxts"
        }, 
        {
            "body": {
                "answer": "<p>Definitely doesn<sq>t seem to be anything amiss <dq>statistically.<dq> They randomized and cautioned about a possible lack of external validity. Methodologically it seems fairly sound, pretty clever actually (imho).</p>", 
                "question": "Is this bad statistics?"
            }, 
            "id": "cctbsbw"
        }, 
        {
            "body": {
                "answer": "<p>I think the answer to your question is based in method, not statistical analysis. If the original article wasn<sq>t behind a paywall I could comment on both. I am very interested in reading more about what the authors have to say about this \u201cveiled elicitation method\u201d. Anyone able to access the original?</p>", 
                "question": "Is this bad statistics?"
            }, 
            "id": "cctavns"
        }, 
        {
            "body": {
                "answer": "<p>Suppose you do 100 flips and get an unlikely result like 80 heads. Your suggesting that in order for the fraction of heads to approach 0.5 with many more flips there would need to be more tails than heads. <br><br>But suppose you have 900 more fair flips that are exactly balanced with the same number of heads and tails. The heads frequency is then 0.53. If you had 9900 more exactly balanced flips, the frequency would be 0.503.  So you don<sq>t need to have tails be more likely to get the frequency to approach 0.5. <br><br></p>", 
                "question": "Gambler<sq>s Fallacy and the law of large numbers."
            }, 
            "id": "cclknbg"
        }, 
        {
            "body": {
                "answer": "<p>The gambler<sq>s fallacy is making a statement about a single coin flip, as it is a result about the memoryless of the coin flips. Even though you may have flipped a (fair) coin 100 times and got a string of heads, the memoryless property says that you can forget about that streak; there is still a 50-50 chance of getting heads or tails on the next coin toss.<br><br>The law of large numbers is technically a statement about an infinite number of coin tosses. It does not make any predictions about how the heads or tails are distributed in a sequence of coin tosses, only that the long-run average of their frequency is 1/2. Even if you have observed 100 heads out of the first 100 coin tosses, the law of large numbers says nothing about the next coin flip, or the next one, or any individual coin flip; it only says that, if you let this experiment continue ad infinitum, the aggregate proportion of heads approaches 1/2.<br><br>If you<sq>re familiar with a bit of physics, you can make a (very rough) analogy to the behaviors of particles. It is very difficult to predict with any accuracy how an individual particle will behave, but we can start coming up with various laws (gas laws, thermodynamics, etc.) that predict the group behavior of a very large number of particles.</p>", 
                "question": "Gambler<sq>s Fallacy and the law of large numbers."
            }, 
            "id": "ccllubs"
        }, 
        {
            "body": {
                "answer": "<p>Thanks to everybody who replied I get it now. In my mind I was give ing the gambler to much credit. </p>", 
                "question": "Gambler<sq>s Fallacy and the law of large numbers."
            }, 
            "id": "cclq492"
        }, 
        {
            "body": {
                "answer": "<p>More than an third outcome, is missing data. There is an extensive treatment on missing data on literature, being one the most famous Little and Rubin<sq>s <dq>Statistical Analysis with Missing Data<dq>.<br>You could try to detect pattern of occurrence of missing data, predicting it based on other variables. The objective is test that missing data is MAR (Missing at random).  <br>The <dq>modern<dq> way to work with systematic missing data is using EM algorithm, that estimate the parameters using the complete distribution, using the information available. </p>", 
                "question": "How bad is it to treat bad data as an outcome?"
            }, 
            "id": "cbr9ezr"
        }, 
        {
            "body": {
                "answer": "<p>To compare the variability of your three populations you could try a nonparametric test like the squared-ranks test. Alternatively, a transformation to bring the populations closer to normality might work.</p>", 
                "question": "What distribution should I fit to these histograms to be able to compare their variability?"
            }, 
            "id": "cbfknbo"
        }, 
        {
            "body": {
                "answer": "<p>Why would you need to fit a distribution to compare variability?<br><br>What is it you actually want to achieve?<br><br>(In any case an ordinary 2-parameter gamma would be no good to you, because of the hard cut-off at 200.)<br></p>", 
                "question": "What distribution should I fit to these histograms to be able to compare their variability?"
            }, 
            "id": "cbfydau"
        }, 
        {
            "body": {
                "answer": "<p>Assuming below 200 nm gets registered as 200 nm they are not normal, because of the long right hand tail. With only a factor 5 between lowest and highest, i doubt that log normal would do the trick. <br><br>A lazy way out may just be resampling; take the current ratio<sq>s SD(control)/SD(1mM), SD(control)/SD(2mM), SD(1mM)/SD(2mM) .<br>Repeat 1000 times<colon> Build H0 distribution by heaping all data together and sample 1/3 to each category. calc SD ratios on sampled data. End repeat. Compare observed data with H0 distribution </p>", 
                "question": "What distribution should I fit to these histograms to be able to compare their variability?"
            }, 
            "id": "cbfp29m"
        }, 
        {
            "body": {
                "answer": "<p>I suggest also considering lognormal distributions.</p>", 
                "question": "What distribution should I fit to these histograms to be able to compare their variability?"
            }, 
            "id": "cbftssi"
        }, 
        {
            "body": {
                "answer": "<p>Hi, and welcome to r/AskStatistics!<br><br>Ah time series data, for some reason this field only attracts the maths-ey nerds to it. <br><br>What you have there is data with variance dependent on time. Or, more importantly, you want to discern the effect that time has on the variance. In periods of low variance you will find the periods of stability.<br><br>I would look into auto-regressive conditional hetetoscedasticity models, or ARCH models for short. These models (along with generalised ARCH models) explicit model the variance, and can predict upcoming periods of instability based on what the data is doing.<br><br>Hope this helps.</p>", 
                "question": "How do I identify periods of stability in a timeseries dataset?"
            }, 
            "id": "cbfg2e7"
        }, 
        {
            "body": {
                "answer": "<p>I think I would just do differencing<colon> Subtract the value at time t from the value at t-1, then grab all the data within a certain threshold. <br></p>", 
                "question": "How do I identify periods of stability in a timeseries dataset?"
            }, 
            "id": "cbeu47e"
        }, 
        {
            "body": {
                "answer": "<p>You could find the standard deviation of each set. There<sq>s a function in excel to calculate it if your data is in excel. This is a pretty general way to measure spread for comparisons.</p>", 
                "question": "Consistency in data"
            }, 
            "id": "cauxtpr"
        }, 
        {
            "body": {
                "answer": "<p>Do you want the variability of the values themselves, or do you want their variation away from some line?</p>", 
                "question": "Consistency in data"
            }, 
            "id": "cauxumd"
        }, 
        {
            "body": {
                "answer": "<p>I<sq>d create a bunch of new variables, called <dq>digit1<dq>, <dq>digit2<dq>, etc.<br><br>Use substring<colon><br>compute digit1 = numeric(substring(zip, 1, 1)).<br>compute digit2 = nuimeric(substring(zip, 2, 1)).<br><br>Then it should be missing if that digit isn<sq>t a number. If all 5 aren<sq>t missing, you<sq>ve got a zip code!<br><br><br><br><br></p>", 
                "question": "Help filtering zip codes in SPSS?"
            }, 
            "id": "cbhtnk4"
        }, 
        {
            "body": {
                "answer": "<p>The only thing I can think of right away is Levene<sq>s test or the Brown\u2013Forsythe test. They are non-parametric alternatives to Bartlett<sq>s and the choice between the two normally depends on the underlying distribution of the data. However, I can<sq>t say how the paired dynamic of the data plays into this. Hopefully that gives you a starting point if nothing else.</p>", 
                "question": "Testing and reporting statistics between two samples with same median but different IQRs"
            }, 
            "id": "ca9gtmk"
        }, 
        {
            "body": {
                "answer": "<p>wilcoxon is a rank order test. you don<sq>t have to make any distributional assumptions. </p>", 
                "question": "Testing and reporting statistics between two samples with same median but different IQRs"
            }, 
            "id": "ca9ho60"
        }, 
        {
            "body": {
                "answer": "<p>>  Does Wilcoxon only check for differences between the location of the distribution? <br><br>Correct, but the signed rank test also assumes the differences are symmetrically distributed; if the original distributions are skewed with different spreads, the differences won<sq>t be symmetric; however, you don<sq>t need a test - if the medians are the same no reasonable test is going to say the two differ in location.<br></p>", 
                "question": "Testing and reporting statistics between two samples with same median but different IQRs"
            }, 
            "id": "ca9vdqc"
        }, 
        {
            "body": {
                "answer": "<p>This sounds like [binomial probability](http<colon>//stattrek.com/online-calculator/binomial.aspx)  problem. <br><br>Let<sq>s say the expected (non-bug) error rate is .001 (1/1000). You sample 1000 records and find 2 errors. There is an 26<percent> chance of observing 2+ errors in 1000 samples if true error rate is .001. Plug in 20 errors in 10,000, and odds drop to 0.4<percent> that you<sq>d see 20+ errors if the true rate were only 1/1000. </p>", 
                "question": "Statistics question<colon> Given X data records to read through how many do I need to verify as correct to assert that all are correct at a 95<percent> confidence level?"
            }, 
            "id": "ca7q6nw"
        }, 
        {
            "body": {
                "answer": "<p>Not sure if this fits, but the <dq>rule of three<dq> for Poisson count data says that the upper bound of a 95<percent> confidence interval where you have zero events is simply n/3.   If you<sq>ve had zero failures in 100 trials, the 95<percent> confidence interval for the event rate is between zero and 3.33<percent>. Sixty gives you an upper bound of 5<percent> error, or 95<percent> successs.</p>", 
                "question": "Statistics question<colon> Given X data records to read through how many do I need to verify as correct to assert that all are correct at a 95<percent> confidence level?"
            }, 
            "id": "cazi6iq"
        }, 
        {
            "body": {
                "answer": "<p>You<sq>ll have to make a number of somewhat unlikely assumptions if you want to treat it as binomial...</p>", 
                "question": "Statistics question<colon> Given X data records to read through how many do I need to verify as correct to assert that all are correct at a 95<percent> confidence level?"
            }, 
            "id": "ca7w7e8"
        }, 
        {
            "body": {
                "answer": "<p>This link gives the same formula that is in my Statistics book<colon> http<colon>//www.isixsigma.com/tools-templates/sampling-data/how-determine-sample-size-determining-sample-size/<br><br>My book goes on to say <dq>When the population standard deviation is unknown, it is common practice to either conduct a preliminary study to determine the sample standard deviation and use it as an estimate of the population standard deviation or use the results from previous studies to obtain it. When using this approach, the size of the sample should be at least 30<dq><br><br>Hopefully that was helpful...</p>", 
                "question": "Statistics question<colon> Given X data records to read through how many do I need to verify as correct to assert that all are correct at a 95<percent> confidence level?"
            }, 
            "id": "ca86acm"
        }, 
        {
            "body": {
                "answer": "<p>http<colon>//en.wikipedia.org/wiki/Student<sq>s_t-test#Unequal_.28or_equal.29_sample_sizes.2C_unequal_variances<br><br>Probably the best choice but I never liked treating 1-10 scores as normal.</p>", 
                "question": "2 different samples with a mean and standard deviation. Need to work out probability of one being higher than the other."
            }, 
            "id": "c9x8v08"
        }, 
        {
            "body": {
                "answer": "<p>The average is meaningless because you<sq>ve got rank-ordered data. You need a non-parametric test for the median.<br><br>The Mann-Whitney U test may be a better choice if the distributions are similar. <br><br>https<colon>//en.wikipedia.org/wiki/Mann<percent>E2<percent>80<percent>93Whitney_U<br><br>In classical significance testing<colon> you don<sq>t know whether the difference in quality score is <dq>significant<dq> until you do the test. You need to set your alpha level first, before you go poking away at the numbers. Otherwise you don<sq>t know how to interpret the result. It seems like splitting hairs, but I can<sq>t emphasize enough how important it is to set your highest acceptable probability of a Type I error before you do the test. Since you<sq>ve already calculated the means, you now MUST use a two-tailed test.<br><br><dq>How significant<dq> doesn<sq>t mean anything. I think what you mean is that you want to know the exact p-value; that is, the probability of getting scores this dissimilar through sampling error if the null hypothesis is correct and the two distributions are actually the same. </p>", 
                "question": "2 different samples with a mean and standard deviation. Need to work out probability of one being higher than the other."
            }, 
            "id": "c9xn0cz"
        }, 
        {
            "body": {
                "answer": "<p>Personally, I think it<sq>s inappropriate to be doing a post-hoc power analysis. But to answer your question, you<sq>ll have to give a description of your study. The calculation will be based on the statistical test(s) you have.</p>", 
                "question": "How To Calculate Power Of My Study & Identify Sample Size Needed To Gain Future Significant Results?"
            }, 
            "id": "c9smp62"
        }, 
        {
            "body": {
                "answer": "<p>If you put a people before and after the accident as two cases, you break the assumption of sample independence. The principal problem will be the time frame, because the time inside the company is different for every people, and you should control it.  <br>I prefer a simple approach, using the rate of absences per year and weighting the cases using the years on service.</p>", 
                "question": "Poisson regression for change in count following an event"
            }, 
            "id": "c9lu6w1"
        }, 
        {
            "body": {
                "answer": "<p>If you search on <dq>Poisson regression GEE R<dq> you<sq>ll find some examples.  You need a way to account for your paired data.  GEE models do that.  With only two data points per subject, any correlation structure should give the same answer.  Use the offset to account for the person tome.</p>", 
                "question": "Poisson regression for change in count following an event"
            }, 
            "id": "ca234zg"
        }, 
        {
            "body": {
                "answer": "<p>I don<sq>t see how adding the average per month would give you that much. In a regression, that column would have a singular value, basically a constant, not a variable that can explain anything. That being said, with the (potentially) plausible assumption that governments act similarly through the developed world, maybe you could use US gov spending data to model a monthly pattern. You could always go the other direction and compress the monthly data you do have into yearly data, but I don<sq>t really see that being ideal as you<sq>d hate to throw away more precise data for more general. Other than that, I don<sq>t see a way to incorporate it. There may be better answers out there though. I<sq>ve worked a lot with regression, a lot with time series models, but haven<sq>t spent a ton of time on combining the two, per se.<br><br>Edit<colon> for some reason I was thinking the data was from one year. The yearly spending split into monthly averages would give you an estimate of the changes caused by that spending. Still don<sq>t know if this is the best thing to do though.</p>", 
                "question": "Question concerning data in a regression analysis"
            }, 
            "id": "c9ip1h6"
        }, 
        {
            "body": {
                "answer": "<p>I<sq>m not sure that I fully understand your data. One thing to keep in mind, regression does not require, to my knowledge, for the unit of measurement to be the same across predictors. <br><br>But, interpretation might be easier if the units of measurement are the same. If you<sq>re interested in using the same unit, it will be easier and more elegant to transform the monthly data into yearly data than to attempt to estimate monthly averages, which will introduce all kinds of bias into your analysis. </p>", 
                "question": "Question concerning data in a regression analysis"
            }, 
            "id": "c9iuflb"
        }, 
        {
            "body": {
                "answer": "<p>Though I am not sure of the specifics of your assignment, I would leave out the government spending as a covariate. The reason is that government spending has too many addition covariates that will complicate matters. By that I mean that government spending varies by seasons and is dependent on too many other factors to hold steady throughout the course of a year.</p>", 
                "question": "Question concerning data in a regression analysis"
            }, 
            "id": "c9tsmq9"
        }, 
        {
            "body": {
                "answer": "<p>Heterogeneity can certainly contribute to variation, but it<sq>s not always what<sq>s going on, in the sense that even after you account for conceivable heterogeneity you can still get variation.</p>", 
                "question": "Can variance be thought of as a measure of the heterogeneity of your population?"
            }, 
            "id": "c9a0h9p"
        }, 
        {
            "body": {
                "answer": "<p>Partly, although measurement error and differences in the <dq>environment of measurement<dq> also play a part, depending on what you are measuring.<br><br>In my research I look for genes that are high-variance in disease (cancer) compared to health -- frequently these are genes that are differentially expressed in a subset of patients with different prognosis, etc<br>EDIT<colon> damn auto-correct....</p>", 
                "question": "Can variance be thought of as a measure of the heterogeneity of your population?"
            }, 
            "id": "c9a1uk8"
        }, 
        {
            "body": {
                "answer": "<p>>  To account for weighting, one would duplicate grades to weight them heavier. <br><br>There<sq>s already a perfectly good notion of a weighted median; you don<sq>t need to <sq>duplicate<sq> things, you can weight them arbitrarily by real numbers on [0,1], just like you would a mean.<br><br>If your (sorted) values are *x1, ..., xn* with weights *w1,...,wn* then the weighted median is the value *x_i* where the summed weights pass from below 0.5 to above 0.5. If the sum of the weights exactly equals 0.5 after adding in observation *i*, the median is any value between the *i&^th and (*i*+1)^th , and would typically be taken to be half-way between them.<br><br>http<colon>//en.wikipedia.org/wiki/Weighted_median<br><br>>  but didn<sq>t know from a theoretical point of view when one should be using a median vs when one should be using the mean<br><br>What theory is that? It depends on what you want to achieve. <br><br>There are also many other measures that behave more like a mean with the typical scores and more like a median with the extreme scores - such as a trimmed mean, for example (though there are many other ways to achieve the same result). <br><br>These carry the advantage over the median that there<sq>s a point to making marginal effort. If my grades (equally weighted in this case for simplicity, but the argument carries over) so far are 40<percent>, 42<percent>, 50<percent> and 54<percent>, and <br><br>(i) I<sq>ve just done some work that would get me 55<percent>, *there<sq>s no point it trying to get that to 65<percent>* because it doesn<sq>t help me. Once I pass the lowest score above the median, there<sq>s simply no gain in a little extra effort.<br><br>(ii) I<sq>ve just done some would that would get me 30<percent>. I can see how to improve it and guess that I could get 45<percent> if I went to the effort and did it. Should I spend the two hours it would take, or should I play video games? Well, it turns out that *I gain nothing whatever from making the effort*, because unless I can get it past 50<percent>, it doesn<sq>t do anything. Might as well play video games.<br><br>As such, most of the time you want to get *some return for effort*. So - while you might want something more robust than a mean - you may not want something as insensitive as a median.<br><br></p>", 
                "question": "Medians vs. means in student grading"
            }, 
            "id": "c90pzg2"
        }, 
        {
            "body": {
                "answer": "<p>In general, using the median will give you a more *robust* measurement of typical performance. Like you said, extreme grades may just be  a fluke.<br><br>The main difficulty with using a median is weighting. A reasonable alternative might be something like Tukey<sq>s robust biweight mean, which acts like a normal mean that discounts outliers.</p>", 
                "question": "Medians vs. means in student grading"
            }, 
            "id": "c90pfkq"
        }, 
        {
            "body": {
                "answer": "<p>Medians dampen the effect of outliers.  It saves you from one horrible grade, but it also stops you from hitting a homerun to save a mediocre history.</p>", 
                "question": "Medians vs. means in student grading"
            }, 
            "id": "cazij91"
        }, 
        {
            "body": {
                "answer": "<p>Try here<colon> http<colon>//databank.worldbank.org/ddp/home.do</p>", 
                "question": "Where can I find household debt-to-income ratio data?"
            }, 
            "id": "c8xy7wl"
        }, 
        {
            "body": {
                "answer": "<p>I think you<sq>re dealing with a non-statistician posting the job.  Given correlation and ANOVA, the next obvious piece would be categorical data analysis, which would be used to analyze frequency data.  This would include analysis of frequencies, rates and proportions.</p>", 
                "question": "Frequency Analysis<colon> What are the methods that fall under this category?"
            }, 
            "id": "c8ilgfo"
        }, 
        {
            "body": {
                "answer": "<p>Not a tutorial, but a GUI like Rcmdr will make it really easy to run regressions and other tests, and also spits out the code so you can see how to code on your own in the future.  </p>", 
                "question": "Multiple linear regressions from same dataset in R."
            }, 
            "id": "c8dw7j4"
        }, 
        {
            "body": {
                "answer": "<p>By separate regressions do you mean <dq>I want different intercepts and slopes for each group<dq> or do you mean <dq>I don<sq>t even want the same estimate of standard error!<dq>. The answers for how to do those two things are different!<br><br>You might find a book like *An R Companion to Applied Regression* of some help.<br><br>There are a number of other potentially useful books.<br><br>You might find Dalgaard<sq>s book on introductory stats with R better to start, perhaps.<br><br>Alternatively, there<sq>s a bunch of documents here - <br><br>http<colon>//cran.r-project.org/other-docs.html<br><br>some of which cover regression at various levels, and many of which cover the very basics.<br><br>The introductory manual that *comes with R* is indispensible. Do you have that?<br><br>At the back it has a sample R session you can work through.<br><br>> anyone knows any really awesome tutorials (preferably videos) along the lines of <dq>R for total idiots,<br><br>There<sq>s an <sq>R for Dummies<sq> book ... but it doesn<sq>t really do much stats.<br><br>There are *tons* of intro R videos. You might try Damico<sq>s *two minute tutorials* for example.<br><br>try searches like this<colon><br><br>https<colon>//www.google.com/search?q=resources+for+R+beginners<br><br>Click on the links one by one and follow *their* links. You<sq>ll be flooded with information.<br><br>For example, two clicks from that search can take you [here](http<colon>//rtutorialseries.blogspot.com.au/2009/11/r-tutorial-series-simple-linear.html)<br><br>Quick R is a very handy resource<br><br>The R mailing list is useful (which has several locations with searchable archives), as is <br><br>http<colon>//stackoverflow.com/questions/tagged/r<br><br>http<colon>//stats.stackexchange.com/questions/tagged/r<br><br>Use the search function (you need to use the [r] tag along with your search term) at either of those<br><br></p>", 
                "question": "Multiple linear regressions from same dataset in R."
            }, 
            "id": "c8e47b3"
        }, 
        {
            "body": {
                "answer": "<p>there<sq>s ton sof tutorials out there.<br><br>lm(dv~X,subset(data,group==<dq>R<dq>))<br>lm(dv~X,subset(data,group==<dq>M<dq>))<br><br></p>", 
                "question": "Multiple linear regressions from same dataset in R."
            }, 
            "id": "c8dtvrx"
        }, 
        {
            "body": {
                "answer": "<p>Look at this http<colon>//www.personality-project.org/r/html/paired.r.html<br>The idea is to use the approximation of correlations to z, using Fisher<sq>s formula, and perform an z or t test, depending of sample size. You must have correlations not close to -1 or +1.<br>Bootstrap could help you, if you have large sample size and correlations near -1 / +1</p>", 
                "question": "Tests for evaluating  the effect of a categorical variable on the the relationship between 2 continuous variables?"
            }, 
            "id": "c84zhkm"
        }, 
        {
            "body": {
                "answer": "<p>When you say you had <sq>one replicate<sq> I<sq>d normally take that to mean that the experiment was replicated (i.e. *repeated*) once (giving *two* observations under each combination of factors). However, I gather that you mean you didn<sq>t replicate the conditions at all, only doing each combination once. (Is that right?)<br><br>This would prevent you estimating the size of the error when fitting a model with all interactions present (a few more observations scattered around would make it possible, if the noise is as low as you suggest and the assumptions hold, even though you won<sq>t be able to check them). That is, you can<sq>t do inference on a full-factorial model (though you could estimate the effects). <br><br>[You say there<sq>s <sq>previous knowledge of the process<sq> - do you have other data?]<br><br>Given only the data you mention with no additional runs or earlier experiments, you have to lose at least the three way interaction to do any inference (though 1 d.f. for error is not going to leave you with much power; you<sq>d be relying on that low-noise as well as the assumptions holding well).<br><br>If you really don<sq>t expect interaction (which sounds like a risky assumption to me), you could just fit a main-effects model; you then have a healthier-sounding number of degrees of freedom at the cost of being able to identify no interactions. If you  want to test for/estimate a *particular* two-way interaction between factors under the assumption the others aren<sq>t present, that could be done.<br></p>", 
                "question": "Can I use ANOVA for this? Is there a better test?"
            }, 
            "id": "c7yte42"
        }, 
        {
            "body": {
                "answer": "<p>Without replication you need to leave out the three-way interaction ABC, since it is now your error term. It<sq>s really that simple.<br><br>* A df = 2-1 = 1<br><br>* B df = 2-1 = 1<br><br>* C df = 3-1 = 2<br><br>* AB df = (2-1)(2-1) = 1<br><br>* AC df = (2-1)(3-1) = 2<br><br>* BC df = (2-1)(3-1) = 2<br><br>* Error df = (2-1)(2-1)(3-1) = 2<br><br>* Total  df = 12 - 1 = 11</p>", 
                "question": "Can I use ANOVA for this? Is there a better test?"
            }, 
            "id": "c7yudlx"
        }, 
        {
            "body": {
                "answer": "<p>I<sq>d like to help you but I<sq>m afraid I don<sq>t quite know what you are trying to analyze. Can you explain what you are trying to investigate in plain english? It is difficult to know what kind of test to suggest if I don<sq>t know what being measured or manipulated.  You said you had <dq>12 runs<dq> -- was that all between-subjects? <br><br>What do you mean when you say you had an ordinal factor? As in, people did one thing first, another thing second, and another third? I ask because <dq>ordinal<dq> typically refers to a type of measurement, and not a type of factor. Did you have people rank order something?</p>", 
                "question": "Can I use ANOVA for this? Is there a better test?"
            }, 
            "id": "c7ysaj2"
        }, 
        {
            "body": {
                "answer": "<p>Not sure if this is where you<sq>re going but here<sq>s a try.<br><br>If you<sq>ve got continuous, normal data, they<sq>re essentially the same.  There is a greater reliance on getting the correlation structure correct for mixed models.  Thus if you have the correct structure, mixed models are more powerful.  GEE models are less sensitive.  For non-gaussian models, I tend to use GEE models because they<sq>re more stable and don<sq>t require initial estimates.<br><br>Another way to think about it has to do with whether you want population averaged (GEE) or conditional (mixed) interpretations.<br><br>I also know there are some sensitivities to cluster sizes with correlated observations, but I<sq>m blanking on that now.  Perhaps someone else can chime in.<br><br>Hope this helps.</p>", 
                "question": "Anyone know when to use generalized estimating equations vs. mixed effect models?"
            }, 
            "id": "c8iml0f"
        }, 
        {
            "body": {
                "answer": "<p>If you<sq>re drawing numbers with replacement, then each draw is independent of the next. So if I wanted the probability of drawing <dq>1<dq> twice in a row it would just be (1/25)*(1/25) = 1/625. Since they<sq>re independent events the probability of both of them happening is equal to the product of the individual probabilities of the two events. <br><br>I don<sq>t know what you mean by <dq>probability of a 4 in 25 chance of something happening<dq> though. Are you asking what<sq>s the probability of drawing a <dq>1<dq> four times if you take 25 draws with replacement?</p>", 
                "question": "Probability question"
            }, 
            "id": "c7f0nm6"
        }, 
        {
            "body": {
                "answer": "<p>BTW<colon> these kinds of questions are also welcome at /r/probabilitytheory <colon>) </p>", 
                "question": "Probability question"
            }, 
            "id": "c7f3h0c"
        }, 
        {
            "body": {
                "answer": "<p>2008<colon> http<colon>//projects.iq.harvard.edu/eda/data<br>http<colon>//uselectionatlas.org/FORUM/index.php?topic=97595.0</p>", 
                "question": "Looking for precinct data from the 2008 and 2012 elections.  "
            }, 
            "id": "c70ybe7"
        }, 
        {
            "body": {
                "answer": "<p>I don<sq>t think this can be done statistically; highly colinear variables have to be parsed out by experimental design beforehand, not after.<br><br>However, it sounds like these two variables are operational definitions of the same thing (so no duh they<sq>re colinear). If they both describe the same slice of the external world, why bother trying to parse them? What additional insight do you gain?</p>", 
                "question": "Separating out individual contributions of highly collinear variables"
            }, 
            "id": "c6qif93"
        }, 
        {
            "body": {
                "answer": "<p>have you tried ICA? (Independent Components Analysis)</p>", 
                "question": "Separating out individual contributions of highly collinear variables"
            }, 
            "id": "c6qs9nw"
        }, 
        {
            "body": {
                "answer": "<p>I<sq>m not sure I completely follow.<br><br>If you sample a large enough area, then yes, you are more and more likely to have observed some kind of large or rare event in that sample.<br><br>One thing about rainfall over a large area is that the intensity of a 5 year (a better phrase is 20<percent> annual chance exceedance) storm for a very small area, where the rainfall is concentrated, would be much higher than that for the average of a large area. This just has to do with how much water can actually be carried in the atmosphere and then dumped over an area.<br><br>I am not sure what you are trying to calculate though - if we are trying to determine the intensity of the 5-year 1-hour event for a large area, it is likely we will do frequency analysis for a number of rain gauges in the area then interpolate. All this result would mean is the *point* estimate for the 5-year 1-hour event for any point in the interpolated region.<br><br>If you can lay out what you need a little more I can offer you more advice!</p>", 
                "question": "Rainfall Area Adjustment Factor (details in post)"
            }, 
            "id": "c6o11mb"
        }, 
        {
            "body": {
                "answer": "<p>It all depends on whether theres <dq>replacement.<dq> Here<sq>s some quick info about this. http<colon>//en.wikipedia.org/wiki/Simple_random_sample<br>and http<colon>//www.ma.utexas.edu/users/parker/sampling/repl.htm<br><br>AFAIK, you are correct that sampling without replacement means that the sample values aren<sq>t independent. What you choose first impacts the sample pool for subsequent cases. <br><br>In large enough samples, the probability of choosing two items that are the same is slim and the impact of each item being removed without replacement on the sample pool is minimized. In these cases, I think that we accept that it<sq>s a non-zero but non-significant impact.</p>", 
                "question": "Simple Random Sampling<colon> Using the top hat method how can you argue that each unit has an equal chance of getting chosen if that is constantly changing?"
            }, 
            "id": "c6d7vta"
        }, 
        {
            "body": {
                "answer": "<p>A priori, the probability that a particular name is among the ten chosen *is* equal for all of the 100 names. That is, we *can* say it then.<br><br>After you have chosen one name, the remaining 99 *all still have equal chances to each other* - but *at that point* they don<sq>t have the same probability to come up next as the one that was already chosen (since it can<sq>t come up again).<br><br>I am not sure why conditioning on being in the middle of the process changes anything; the point is that each name is equally likely to be chosen by the process <sq>draw ten names from the hat<sq>. </p>", 
                "question": "Simple Random Sampling<colon> Using the top hat method how can you argue that each unit has an equal chance of getting chosen if that is constantly changing?"
            }, 
            "id": "c6d83v1"
        }, 
        {
            "body": {
                "answer": "<p>The 1/99 probability is conditional on those names having *not* been picked on the first draw. There<sq>s a 99/100 chance of that happening, and so a 1/100 chance for any name to be drawn on the second draw.<br><br>Here are some situations for you to think about<colon><br><br>* Draw a random name from the hat.<br>* Draw five random names from the hat in a row, without looking at the results between draws. Without looking at the draws, return four random names to the hat.<br>* Draw five random names from the hat in a row, without looking at the results between draws. Without looking at the draws, return the first four draws to the hat.<br>* Draw five random names from the hat, looking at the results between draws. Return four random names to the hat.<br>* Draw five random names from the hat, looking at the results between draws. Return the first four draws to the hat.<br><br>Each of these leaves you with a single name. You should be able to show (rigorously using direct computation) that each name has probability 1/100 of being the chosen name.</p>", 
                "question": "Simple Random Sampling<colon> Using the top hat method how can you argue that each unit has an equal chance of getting chosen if that is constantly changing?"
            }, 
            "id": "c6dgmcx"
        }, 
        {
            "body": {
                "answer": "<p>The resampling you describe, where the probability changes, is commonly used for permutation tests (scrambling assignment of data).<br><br>The way to keep equal probability is to put things back into the hat each time you take out of the hat. This is commonly referred to as bootstrapping. </p>", 
                "question": "Simple Random Sampling<colon> Using the top hat method how can you argue that each unit has an equal chance of getting chosen if that is constantly changing?"
            }, 
            "id": "c6dds11"
        }, 
        {
            "body": {
                "answer": "<p>I am not sure what you are thinking of when you want tests put on a graph automatically, but you might try *Mystat* which is available [here](http<colon>//www.systat.com/MystatProducts.aspx).  It<sq>s free and it will do most of the elementary stuff you might want.  It will do regressions and t-tests and elementary anovas as well as scatter-plots, bar charts and function plots.  <br><br>Full disclosure<colon>  In another life, I worked for the makers of SYSTAT for several years.  I still think it is the best software program available on the PC.  <br><br></p>", 
                "question": "What is a good software package for making bar graphs and scatter plots that automatically does statistical tests and includes them in the graph?"
            }, 
            "id": "c61ncfc"
        }, 
        {
            "body": {
                "answer": "<p>Prism will do it pretty easily -- you just hit the little I in the menu bar and add them or copy and paste them from the results table and the update dynamically. </p>", 
                "question": "What is a good software package for making bar graphs and scatter plots that automatically does statistical tests and includes them in the graph?"
            }, 
            "id": "c61trte"
        }, 
        {
            "body": {
                "answer": "<p>ggplot2 includes confidence-bands with smoothing lines<colon> http<colon>//had.co.nz/ggplot2/stat_smooth.html</p>", 
                "question": "What is a good software package for making bar graphs and scatter plots that automatically does statistical tests and includes them in the graph?"
            }, 
            "id": "c61vtwv"
        }, 
        {
            "body": {
                "answer": "<p>Have you considered using MATLAB?</p>", 
                "question": "What is a good software package for making bar graphs and scatter plots that automatically does statistical tests and includes them in the graph?"
            }, 
            "id": "c64n41h"
        }, 
        {
            "body": {
                "answer": "<p>Taking the mean of the points <dq>in the plateau<dq> sounds a bit sketchy by itself. If there is a biological reason for the plateau there should be a model that describes the data so that you can curve fit to get the value of the plateau with an error.<br><br>If you are going to use the mean of the plateau. Since you already have error measurements for each of the points the correct way to calculate std error would be by taking the square root of the sum of the squared error values for all of the points. <br><br>Ie if the points  in the plateau you<sq>re taking the mean of are X1 \u00b1 SE1, X2 \u00b1SE2, X3 \u00b1 SE3.... XN \u00b1 SEN and the mean is (X1 + X2 + X3 +... XN)/N your error would be sqrt(SE1^2 + SE2^2 + SE3^2 +.... SEN^2 )<br>http<colon>//en.wikipedia.org/wiki/Propagation_of_error<br><br>http<colon>//chemwiki.ucdavis.edu/Analytical_Chemistry/Quantifying_Nature/Propagation_of_Error</p>", 
                "question": "Question about deciding how to calculate standard error when I have three potential levels of replication to <dq>choose<dq> from."
            }, 
            "id": "c5lyzr0"
        }, 
        {
            "body": {
                "answer": "<p>I<sq>m getting lost among the variances of variances of variances, but here<sq>s what I think you<sq>re doing<colon><br><br>You have a sequence of position measurements, p(t). You also have a fixed point q. You compute d(t) = ||p(t) - q||, the distance between the fixed point and the measured position at time t. This, I take it, is what you actually want to study.<br><br>You say that (d(t) - d(t+1))^2 is your <dq>variance among the distance measurements<dq>. But this is not the variance of d(t). The variance of d(t) is the mean of (d(t) - mu)^2 where mu is the mean distance. The quantity (d(t) - d(t+1))^2 is the square of a forward difference. Similarly, the quantities (d(t) - d(t + k))^2 are the squares of the difference k steps ahead in time. They<sq>re kind of like an autocorrelation, but they<sq>re not exactly the same.  I<sq>m going to refer to them as k-step time differences, and I<sq>ll write a sequence of k step time differences as Delta_k d(t).<br><br>Each Delta_k d(t) has a mean and a variance (in the usual statistical sense). You say that these are messy and that you pool them. I can think of three ways of doing that. In your diagram, you pool the measurements first, then compute the time differences. It seems to me that this is biologically invalid, because you<sq>re not taking differences that are separated by a fixed time step; you<sq>re taking differences from two random times in two random cells, and who knows what that gives? In your post, it sounds like you compute all the means and variances for each Delta_k d(t) for each cell you measured, then take the means of those. This is mathematically invalid, because the mean of a collection of means doesn<sq>t necessarily weight the data properly (and similarly for variance).  Imagine, for example, that you had 50 measurements of one cell and 100 measurements of another cell. If you take the mean of the first cell measurements, the mean of the second cell measurements, and then the mean of those two means, then the measurements from the first cell get twice the weight of the measurements of the second cell; this is no good. The last way--which I think it both biologically and statistically valid--is to compute every Delta_k d(t) for every cell, pool the values of Delta_k d(t) for different cells to create one big array of data, and then compute the mean and variance of the pooled array. Notice that in this method, you never compare two values of the pooled array against each other.<br><br>Finally, you look for a plateau in the variance of the time differences of the distance measurements. You say that there<sq>s a biological reason for this; I can<sq>t think what that could be. If it were that the measured position p(t) can only change at a limited rate, then you would only want to study the first difference Delta_1 d(t), and the variance of the time differences would grow as time increased, not plateau. If there were a limit to how far away the measured position p(t) can be from the fixed position q, then that would force the variance to plateau, but that plateau doesn<sq>t tell you anything obvious (I think). It<sq>s some weird combination of how fast the measured position is changing and how much space it has to move around in.<br><br>It<sq>s hard to know what you ought to calculate without knowing what you<sq>re trying to quantify, but I<sq>ll try anyway.<br><br>First, you really ought to do what procrastinating_PhD recommends and try to fit a good biological model of the behavior you<sq>re studying to your data. That<sq>s more likely to give you valid results than anything else you can do. If your advisor doesn<sq>t believe in curve fitting, then your advisor is wrong. I suggest finding a sympathetic statistician or mathematician at your university and using them as leverage. (You may be able to get a statistics grad student to help you.) If nothing else, try to convince your advisor that the curve fit is worth including in your paper.<br><br>Second, you<sq>re right to worry that the distance measurements are not independent. However, people often pretend that their measurements are independent and do all of their analysis under that assumption. In many situations, dependence is hard to account for and usually has only small effects. The only way to be sure, however, is to have a good biological reason for the measurements to be independent. If you don<sq>t have that, then maybe you should think ways in which the measurements can be correlated and test for them. If your tests show correlation, then you have evidence that something more sophisticated is needed ... like a curve fit.<br><br>Third, your model for the plateau probably predicts what kind of errors you expect to be seeing. It might not be stated as part of the model, but it should be there somehow. You need some assumption on what kind of noise the predictor variables have. Sometimes Gaussian noise is good, other times it should follow a log-normal distribution or a Poisson distribution or something else. Again, a statistician should be able to help with this (but your statistician friend will need your biological insight to know what<sq>s reasonable). Once you have an idea what kind of noise there might be, you can follow that noise through the model and see what kind of noise comes out at the end where it predicts the plateau. (If it<sq>s hard to find an equation for, then you can try plugging noisy numbers into the predictor variables for the model and seeing what comes out.)<br><br>I expect that the amount of noise goes down as you take larger time differences. Just how it goes down depends on exactly how the model works--but once you know that, you<sq>ll know how to correct the variance of the plateau measurements. Once the variances are all on the same footing, you can compute a standard error and (assuming independence) get useful information out.<br><br>Good luck!</p>", 
                "question": "Question about deciding how to calculate standard error when I have three potential levels of replication to <dq>choose<dq> from."
            }, 
            "id": "c5mbt0y"
        }, 
        {
            "body": {
                "answer": "<p>In the social sciences, transformation of variables is not done as much nowadays because we can use [generalized linear models](http<colon>//en.wikipedia.org/wiki/Generalized_linear_model) that more directly model the non-normal distribution.<br><br>As for unequal variances, ANOVA is only robust to the extent that group sizes are approximately equal. Now it is possible to model the group variances separately so I<sq>m not sure why a non-parametric test would be necessary.</p>", 
                "question": "Non-parametric tests of parametric data"
            }, 
            "id": "c59e92b"
        }, 
        {
            "body": {
                "answer": "<p>What field were these papers in?  In my field (psychology/marketing) I would easily believe that the authors did this to satisfy a reviewer who nitpicked them over the unequal variances issue until they switched to an obscure test to satisfy them.</p>", 
                "question": "Non-parametric tests of parametric data"
            }, 
            "id": "c59dx2x"
        }, 
        {
            "body": {
                "answer": "<p>> I was under the impression (and I believe, taught), that data which is not normally distributed should be transformed,<br><br><dq>should be<dq> is *far* too strong; that<sq>s a possibility, and may not be the best one. <br><br>A failure of an assumption to which a procedure is not particularly robust requires some action, which may be moving to a procedure that is not making that assumption, or moving to a procedure more robust to that assumption, or manipulation of the data - most likely by transformation - to reduce the extent to which the assumptions are violated.<br><br>[However, nonparametric procedures aren<sq>t necessarily all that robust to violation of the assumption of equivalence of distribution under the null (i.e. to differing spreads) in any case. Some may be less sensitive than others]<br><br>The biggest problem with the third alternative for hypothesis tests is that you<sq>re *no longer testing for equality of population means on the original scale*. This may be an issue.<br><br>A second problem is that sometimes *no* monotonic transformation can give you what you want.<br><br>> and that ANOVA is sufficiently robust that it can cope with non equal variances<br><br>The problem is that it affects the distribution of the significance level under the null, sometimes fairly dramatically. In particular, you<sq>re more likely to conclude a difference in sample means is significant than you should.<br><br>This can be approximately dealt with by a Welch-like procedure<br><br>e.g. http<colon>//en.wikipedia.org/wiki/Welch<percent>E2<percent>80<percent>93Satterthwaite_equation<br><br>But there are other approaches.<br><br>If the nonequal variances are caused by a relationship between mean and variance (and often, by non-normality at the same time, such as with count data) then GLMs is a common choice; equivalents of almost all the usual tests and analyses are available that way.<br><br><br>> So at what point should you decide to stop with parametric tests and switch to non-parametric tests?<br><br>It<sq>s a much more complex question than the way you cast it there.<br><br>The questions you need to deal with include<colon><br><br>1) To what extent are my inferences impacted by the various violations of assumptions?<br><br>2) What do I need to get out of my analysis?<br><br>The first tells you about what things need to be worried about, and if they need to be worried about at all. The second guides the choice of what to do about it.<br><br>This unfortunately, requires an investigation of the properties of the tools that you would normally use and might propose to use in their place. The questions you need to answer can generally be tackled well enough by simple simulation studies, though some such questions have already been answered in the literature (sometimes by simulation, sometimes by more sophisticated analyses).<br></p>", 
                "question": "Non-parametric tests of parametric data"
            }, 
            "id": "c59ynkt"
        }, 
        {
            "body": {
                "answer": "<p>You have a situation where you<sq>re violating the assumptions of ANOVA.  You either fix the data or do a test that has fewer assumptions.  Nonparametric tests lose a bit of power (not as much as most think) and will give you a useable significance test, but not much info if you<sq>re needing to make predictions from the model.  Transformations can be hard to get right and while you get quantifiable answers, they can be hard to interpret as the transformation distorts the data.   Doing ANOVA and then either of these tests and comparing them is acceptable.  If you get a different answer (broadly speaking) then you<sq>ve got a problem.  If not, go with the ANOVA and explain how you checked it withthe other approach.</p>", 
                "question": "Non-parametric tests of parametric data"
            }, 
            "id": "c68dfzy"
        }, 
        {
            "body": {
                "answer": "<p>You could do something simple like count the number of times a person gives the same number both times. If a person gives the same number n<percent> of the time, the test is n<percent> reliable.<br><br>If you give more information about the assessment I can be of more help, but something like this is simple and, if n is close to 100, very compelling to a layman.</p>", 
                "question": "Correspondence between two versions of an assessment?"
            }, 
            "id": "c58n8i2"
        }, 
        {
            "body": {
                "answer": "<p>You could do a chi-square test for association.  Scores on the old assessment would be the expected values.  In SPSS, you can ask for Cramer<sq>s V when you run the chi-square test....I<sq>m not sure how to do this in other stat packages.  Cramer<sq>s V describes the strength of the relationship, ranging from 0 to 1.  If you take this route, you should look at the cross-tab first to make sure that things look the way they should (people who scored 0 on the old assessment are also scoring 0 on the new assessment etc.).  The association you find with chi-square might be showing you a consistent discrepancy between the two measures.</p>", 
                "question": "Correspondence between two versions of an assessment?"
            }, 
            "id": "c59fwnr"
        }, 
        {
            "body": {
                "answer": "<p>I think you want a version of Cohen<sq>s kappa that can be used for multiple response data.  This gives the level of agreement, above that which would be expected by chance.  </p>", 
                "question": "Correspondence between two versions of an assessment?"
            }, 
            "id": "c6togox"
        }, 
        {
            "body": {
                "answer": "<p>[deleted]</p>", 
                "question": "What<sq>s \u201cthe best way\u201d to calculate the final grade in a class?"
            }, 
            "id": "c4mzikg"
        }, 
        {
            "body": {
                "answer": "<p>I<sq>m glad you brought this up. I think there<sq>s an even bigger problem though besides those you brought up, which is fundamental to modern grading, and that is almost every grade is the conflation of several orthogonal dimensions of judgement, and is used and judged differently by different teachers, students, administrators, and post-secondary institutions. An incomplete list of these dimensions is<colon> <br><br>1. aptitude in synthesis and presentation of relevant ideas. Sometimes this would or should include <dq>creativity<dq>, sometimes that should perhaps be considered a separate dimension.<br>2. mastery of the facts, techniques and ideas of the subject area.<br>3. quantity of effort perceived by the teacher.<br>4. work completion.<br>4. other laudable traits as judged by the teacher<br>5. unlaudable traits as judged by the teacher, e.g. disciplinary matters, uncooperativeness, disagreement, unwillingness to submit, antisocial behaviors, etc.<br><br>These different dimensions should be treated differently. I think it<sq>s appropriate to normalize a measure of aptitude in synthesis as there<sq>s no very good way to judge that except in relation to other students or works. But, that means anyone reviewing that particular grade should know that it<sq>s not absolute, that in some schools kids even in the 10th percentile, for instance, have the abilities needed for a particular job or particular college class, etc.<br><br>Mastery, on the other hand, I see no good reason to normalize. The goal with mastery should always be 100<percent> mastery of all facts, techniques, and ideas of the subject area, for all students. That<sq>s simply not a realistic goal for the other dimensions, but it can be for mastery.<br><br>By conflating all these orthogonal dimensions grades become almost meaningless, every teacher has a different idea of what they<sq>re meant to mean and everyone who reads them has a different idea of what they mean. Schools make policies to try to standardize the meaning, but they almost always focus mainly on mastery and work completion, and use the inappropriate paradigm (rarely a system) of normalized scores, expecting that for some reason those dimensions should fit a bell curve rather than a power law distribution, leaving teachers to try to incorporate the other dimensions in ad hoc unstandardized ways without communicating the means to understand them to anyone but their students, and maybe parents, if that.</p>", 
                "question": "What<sq>s \u201cthe best way\u201d to calculate the final grade in a class?"
            }, 
            "id": "c4mzzlk"
        }, 
        {
            "body": {
                "answer": "<p>I like the Angoff method.<br><br>Gather other teachers in your field. Show them your homework and test items. Ask them what is the probability a minimal D student would get an item correctly. What is the probability a minimal C student will get an item correct. etc. <br><br>Average the ratings across all items. Create a confidence interval. Within that confidence interval use your judgement of what the cutoff should be. Put these cutoffs in your syllabus. </p>", 
                "question": "What<sq>s \u201cthe best way\u201d to calculate the final grade in a class?"
            }, 
            "id": "c4n5uo7"
        }, 
        {
            "body": {
                "answer": "<p>The whole concept of grades is based in social structures (social promotion, the concept of <dq>60<percent> is passing<dq>, teacher<sq>s knowledge of typical minimum acceptable grades for the students to achieve their goals) and not just math. To answer your question<colon><br><br>> If you suppose a class where the weighting is supposed to be 60<percent> homework assignments, 40<percent> exams, and but homework assignments have a standard deviation of 5<percent> and exams have a standard deviation of 15<percent><colon> if we just do 0.6 * assignment scores + 0.4 exam scores, it will produce totals that are most heavily influenced by exam performance, not assignment scores as claimed in the syllabus. How can that be fair\u2026?<br><br>One answer to that question is that the breakdown you suggest means that a student who does ALL the homework and does terribly on the exams can probably pass, albeit with a very low grade. The teacher has to weigh several factors in determining course component weights. Homework is easier to cheat on, but it rewards students for hard work even if they have trouble with the material. Exams reward students who work well under time pressure and those who are good at memorization. There<sq>s nothing magic about the weights, but the teacher usually sets them so that the grades come out in the preferred distribution. <br><br>The preferred distribution is going to depend on the type of class. In a typical humanities graduate seminar, for example, no one gets less than a B+. In a first-year law class, perhaps the preferred distribution is that the bottom third of the class fails to <dq>weed out the weak<dq>. What would be unfair would be for one humanities teacher to decide to grade the class like it was a first-year law or engineering class. A failing grade in an Engineering class could easily mean <dq>Worked hard but just couldn<sq>t get it<dq>. A failing grade in English almost always means <dq>didn<sq>t do dick all year<dq> (OK, not always, but you get the point).<br><br>In my opinion, the difference between grading on a curve and not grading on a curve is mostly whether the teacher thinks it<sq>s more fair to compare each student with their current classmates (the sample) or with all past-and-future classmates (the population).<br><br>I personally don<sq>t believe in grading on a curve. I<sq>ve taught classes where no one got an A, because no one student that year performed to an A standard. Students HATE that (as do their parents), because they look at the sample, not at the population. Lots of students got an A in that class<colon> just not that year. Many teachers in that situation would tweak the situation by adding a few points to everyone<sq>s grade or just bumping up the letter grade of the highest-performing student or two.<br><br>*tl;dr you<sq>re focused on the weighting calculations, but error in the calculations is tiny compared in error in how the input numbers and weights are generated in the first place.*</p>", 
                "question": "What<sq>s \u201cthe best way\u201d to calculate the final grade in a class?"
            }, 
            "id": "c4ncgh4"
        }, 
        {
            "body": {
                "answer": "<p>Sure, there are lots of ways to do this. Intuitively, the statistical test would work like this, suppose that you simulated your poker hands millions of times. The average (say winning <percent> or whatever variable of interest you have in mind) of these simulations would converge to the expected value of your 1000 hands, and the variance of the simulated variable of interest would tell you the reasonable range of <sq>statistically likely<sq> outcomes. <br><br>Suppose that hand one has probability of winning a1, hand two has probability of winning a2, etc. Your expected number of wins will be a1 + a2 + a3 + ... + an = sum(ai). <br><br>The variance will be a1(1-a1) + a2(1-a2) + ... +an(1-an). If the variance is not too small, you can use the normal approximation and say that it is approximately distributed normal(mean, variance). With 1000 hands this will probably be a safe assumption. You can then use standard statistical tests to see if your values were statistically likely. </p>", 
                "question": "Is it possible to determine the odds of a succession of events occured - each with different probabilities?"
            }, 
            "id": "c4lum8j"
        }, 
        {
            "body": {
                "answer": "<p>Since no card is repeated it doesn<sq>t matter what subset of cards you<sq>re given, only that they are in order. There are 10! permutations of any subset and only one is in order so 1/(10!) = 1/3,628,800. <br></p>", 
                "question": "Has anyone ever played the game Rack-o? What are the chances of being dealt a winning hand?"
            }, 
            "id": "c4kip2s"
        }, 
        {
            "body": {
                "answer": "<p>Good question. I<sq>d imagine backwards induction would provide the easiest answer, but it a bit beyond my ability at the moment.</p>", 
                "question": "Has anyone ever played the game Rack-o? What are the chances of being dealt a winning hand?"
            }, 
            "id": "c4keuev"
        }, 
        {
            "body": {
                "answer": "<p>Well, the number of possible card permutations is 60P10, so that<sq>s <br><br>60!/(60-10)!=60!/50!=60x59x58x57x56x55x54x53x52x51 ~2.73 x 10^17.<br><br>If your first card is 1-9, that<sq>s an automatic no, so there<sq>s really only 51 choices for the first card. The second card can never be 1-8, so that factor is reduced to 51. By extending this reasoning, we lower the number of permutations to<colon> <br><br>51x51x51x51x51x51x51x51x51x51~1.2 x 10^17.<br><br>From here, I<sq>m less sure of where to go. You could brute force it for first card picks of 10, 11, or maybe a bit higher, but I can<sq>t say I know what to do from here mathematically. I<sq>ll see if I find anything helpful.</p>", 
                "question": "Has anyone ever played the game Rack-o? What are the chances of being dealt a winning hand?"
            }, 
            "id": "c4klhqd"
        }, 
        {
            "body": {
                "answer": "<p>It<sq>s an estimate.  Depending on how often your methodology has been previously used, yo may be wildly under- or over-estimating your effect size, which is critical for estimating power.  So your sample size may not be sufficient.  And then, if you do a significance test and find your sample size is not adequate, you may be tempted to collect more data to see if you can get a significant result (whereas you may really be falling prey to a multiple comparison type one error).</p>", 
                "question": "Are there any negatives to estimating power before collecting data?"
            }, 
            "id": "c45efy4"
        }, 
        {
            "body": {
                "answer": "<p>Power estimation is usually going to be a function of sample size, effect size, and noise intensity. Only the first one is easy to understand and predict. Sometimes the second one is workable if you know exactly how big of an effect size would be interesting to you, the third one is often unattainable before you<sq>ve actually done the experiment.</p>", 
                "question": "Are there any negatives to estimating power before collecting data?"
            }, 
            "id": "c45icd4"
        }, 
        {
            "body": {
                "answer": "<p>I don<sq>t think there are any negatives, and I think it<sq>s important to estimate power prior to conducting a robust study. The main determinants of power are the effect size and the size of the study population. You should use clinical relevance to decide the effect size you think is beneficial and decide the power you think is needed (usually 80<percent> for superiority and 90-95<percent> for noninferiority). This tells you the sample size you need. </p>", 
                "question": "Are there any negatives to estimating power before collecting data?"
            }, 
            "id": "c4d545x"
        }, 
        {
            "body": {
                "answer": "<p>I wouldn<sq>t mistake assuming that alpha tells you anything about construct validity. It is traditionally a measure of reliability. This is an important characteristic of your survey. How consistently examinees performance on your survey can be generalized to the content domain of items that might have been asked. To estimate this alpha describes how consistently the examinees performed across items in your survey. Alpha does not tell you about whether there is underlying structure to your data causing certain items to covary more or less which might support a theory that some latent trait connects them. That is where factor analysis comes in. <br><br>For exploratory factor analysis I would write many more items than your currently that you think might have a relation with your latent traits, without the assumption any of them are going to work. Then run the analysis, remove bad items based on the item-total correlation like you said. Then remove items that do not load on a factor or that load on too many factors. Repeating until you are happy with the pattern of the loadings. Then you attempt to assign meaning to the factors and hypothesize what the underlying latent traits might be. <br><br>However I am not convinced that your current scale would be appropriate for an exploratory factor analysis which is used when you do not have an a priori theory about factor structure. Here you seem to have an idea of which items relate to which latent traits. Confirmatory factor analysis might be more appropriate in which you indicate before hand which items relate to which latent traits and how the latent traits relate to each other. However with this you can<sq>t go around modifying your results if they do not match your theory. </p>", 
                "question": "Validating survey instruments (EFA/Cronbach<sq>s Alpha etc.)"
            }, 
            "id": "c3mxawm"
        }, 
        {
            "body": {
                "answer": "<p>Validity has many facets. You should read classic Cronbach and Meehl (1955) [paper](http<colon>//psychclassics.yorku.ca/Cronbach/construct.htm).<br>There are no shortcuts to establish  the validity of an instrument.<br><br>- Q1<colon> Short answer<colon> (a) No and (b) maybe. Long answer<colon> Cronbach<sq>s Alpha is a lower bound estimate of reliability of a composite. Doesn<sq>t tell you anything about unidimensionality of constructs, only about the expected correlation with another random parallel form. Use EFA for inspection of items, but remember than estimates of eigenvalues are very erratics with small samples. .<br>- Q2<colon> No problem, but you should calculate al psychometric properties again.<br>- Q3<colon> In the sense of use a separate scale? I agree </p>", 
                "question": "Validating survey instruments (EFA/Cronbach<sq>s Alpha etc.)"
            }, 
            "id": "c3mxenv"
        }, 
        {
            "body": {
                "answer": "<p>http<colon>//www.icpsr.umich.edu/icpsrweb/ICPSR/access/subject.jsp</p>", 
                "question": "What are some good sites for interesting data?"
            }, 
            "id": "c2yvkhd"
        }, 
        {
            "body": {
                "answer": "<p>http<colon>//data.worldbank.org/</p>", 
                "question": "What are some good sites for interesting data?"
            }, 
            "id": "c2z2ibm"
        }, 
        {
            "body": {
                "answer": "<p>https<colon>//www.cia.gov/library/publications/the-world-factbook/<br><br><dq>The World Factbook provides information on the history, people, government, economy, geography, communications, transportation, military, and transnational issues for 267 world entities. Our Reference tab includes<colon> maps of the major world regions, as well as Flags of the World, a Physical Map of the World, a Political Map of the World, and a Standard Time Zones of the World map.<dq></p>", 
                "question": "What are some good sites for interesting data?"
            }, 
            "id": "c3ytxcg"
        }, 
        {
            "body": {
                "answer": "<p>[/r/datasets](/r/datasets) </p>", 
                "question": "What are some good sites for interesting data?"
            }, 
            "id": "c45ltc5"
        }, 
        {
            "body": {
                "answer": "<p>compute your own R^2 ? sqrt(1 - SSR/SST)? It should still work..ish I think (at least that<sq>s what I<sq>ve been doing for a rule of thumb)</p>", 
                "question": "Nonlinear curve fitting<colon> how do I measure the quality of my fit? "
            }, 
            "id": "c2olqb5"
        }, 
        {
            "body": {
                "answer": "<p>You might try cross validation. It<sq>s easy enough to do, though you<sq>ll need lots of data (or is harder, maybe, if you do something like GCV). Alternatively you could try posterior model checks a la Andrew Gelman<sq>s methods. <br><br>There<sq>s no such thing as a general nonlinear R value as that comes directly from the linear model.</p>", 
                "question": "Nonlinear curve fitting<colon> how do I measure the quality of my fit? "
            }, 
            "id": "c2r89w2"
        }, 
        {
            "body": {
                "answer": "<p>Typically, I would compute the R2 for each of the model and compare. No matter how non-linear, you are still trying to find the best fit most that explains the most variance.<br></p>", 
                "question": "Nonlinear curve fitting<colon> how do I measure the quality of my fit? "
            }, 
            "id": "c3dg0l9"
        }, 
        {
            "body": {
                "answer": "<p>Interesting, I am working on a similar problem with SciPy - I have fitted spline curves to individual sets but have not determined a good way to compare those fittings.</p>", 
                "question": "Nonlinear curve fitting<colon> how do I measure the quality of my fit? "
            }, 
            "id": "c2okevr"
        }, 
        {
            "body": {
                "answer": "<p>As cuginhamer says, if you know what random number generator (RNG) is used, it is always possible to devise some kind of recognition test. <br><br>However, not all tests are relevant for statistical applications. A good RNG generator for statistical use should pass the [diehard tests](http<colon>//en.wikipedia.org/wiki/Diehard_tests) devised by Marsaglia. Many of the usual RNG do.<br><br>Note that cryptographic use is more demanding. For that purpose, harware generators have been devised. Read this<colon><br> <br> * [wikipedia on Hardware RNG](http<colon>//en.wikipedia.org/wiki/Hardware_random_number_generator)<br><br> * [A particular Hardware RNG from Intel](http<colon>//spectrum.ieee.org/semiconductors/processors/behind-intels-new-randomnumber-generator/?utm_source=techalert&utm_medium=email&utm_campaign=090111)<br><br>Other relevant reading, for the mathematical aspects of the question<colon> http<colon>//en.wikipedia.org/wiki/Algorithmically_random_sequence<br></p>", 
                "question": "Possible to differentiate set of random numbers from set of psuedorandom numbers through statistics?"
            }, 
            "id": "c2lg5cd"
        }, 
        {
            "body": {
                "answer": "<p>This is a guess, but since nobody has replied yet I<sq>ll state the obvious---if a a specific deterministic algorithm was used to generate the pseudorandom number stream, them it would be theoretically possible to search a set of such practical computer algorithms, find the culprit in question, and successfully predict the next number in the stream. Meanwhile a truly random number stream could never be <dq>solved<dq>. But just by looking for local patterns in the 0s and 1s, I don<sq>t think one could tell a set of random numbers derived from every 50th decimal place of pi rounded up to 1 or down to 0 from a decay-event generator. </p>", 
                "question": "Possible to differentiate set of random numbers from set of psuedorandom numbers through statistics?"
            }, 
            "id": "c2ldta8"
        }, 
        {
            "body": {
                "answer": "<p>There could be. I love [this page](http<colon>//www.boallen.com/random-numbers.html) for demonstrating that PRNGs are not always even sort of random.<br><br>I don<sq>t know a whole lot about PRNG sequences, but I<sq>d side with cuginhamer to say that since there are definitely (on finite computers) a finite number of PRNG sequences, there will certainly, as your testing observations approach the recurrence times in the sequences, exist ways of distinguishing pseudorandoms.<br><br>It may be that any computer able to perform such an algorithm would be able to perform a *better* PRNG algorithm, though, but that<sq>s speculation.</p>", 
                "question": "Possible to differentiate set of random numbers from set of psuedorandom numbers through statistics?"
            }, 
            "id": "c2ldw41"
        }, 
        {
            "body": {
                "answer": "<p>[Related](http<colon>//www.reddit.com/r/AskComputerScience/comments/k1bog/given_a_large_enough_sample_size_is_possible_to/), and [my response](http<colon>//www.reddit.com/r/AskComputerScience/comments/k1bog/given_a_large_enough_sample_size_is_possible_to/c2gsmkl).</p>", 
                "question": "Possible to differentiate set of random numbers from set of psuedorandom numbers through statistics?"
            }, 
            "id": "c2lkufx"
        }, 
        {
            "body": {
                "answer": "<p>Literally, just because it<sq>s defined that way. You could easily compute expectations of other functions. They just wouldn<sq>t be the variance.<br><br>apeescape brings up the valid reason why variance is such a useful tool, though. Most of statistics up until modern times has been motivated by analytic tractability instead of questions about whether square or absolute error is more pertinent to the current application.<br><br>Furthermore, variance has deep connections to ideas like spectral power density in random sequences and processes. </p>", 
                "question": "Can you explain the why a little better regarding variance and standard deviation for normal distributions?"
            }, 
            "id": "c2hcscl"
        }, 
        {
            "body": {
                "answer": "<p>If you mean <dq>which of these two results can I be more certain is correct<dq> then the second one, apparently, given that the p-value is 17x higher. The Odds Ratios tells a slightly different story in that the alternative model fits better for the first one, but Odds Ratio depends a lot on what the alternative model is.<br><br>HonestAbe is right to say you<sq>re probably more interested in effect sizes, though. Here you should be doing another test for r1 > r2.<br><br>--- <br><br>As for fungicides, there<sq>s not enough information to say whether they could be significant\u2014it depends on the power of the test which depends on the sample variance. Obviously, at the power you<sq>ve garnered so far there isn<sq>t enough resolution to find an effect, but if you<sq>ve got good reason to believe that there is a small effect (much smaller than the effects of pesticides/herbicides if you used the same sample size and the sample variances are comparable) then you definitely shouldn<sq>t interpret the <dq>negative<dq> result as there being no effect.<br><br>If it<sq>s implausible for the effect to be so small\u2014if it<sq>s either there must be an effect or there must not\u2014then you can probably be fairly confident that the negative result is actually negative.</p>", 
                "question": "How do you compare the significance of different p-values that have reached significance?"
            }, 
            "id": "c2dy833"
        }, 
        {
            "body": {
                "answer": "<p>Try looking at effect sizes, such as Cohen<sq>s *d*. They<sq>re designed to show differences rather than just statistical significance or confidence intervals. You can even calculate them in excel. </p>", 
                "question": "How do you compare the significance of different p-values that have reached significance?"
            }, 
            "id": "c2dwia8"
        }, 
        {
            "body": {
                "answer": "<p>I think you need first an introduction to epidemiology... </p>", 
                "question": "What do I need to learn in order to be able to follow this analysis?"
            }, 
            "id": "c2c18c0"
        }, 
        {
            "body": {
                "answer": "<p>I don<sq>t really have time right now to read over the post and papers to understand the methods\u2014it looks like a rather complicated model that I<sq>m not familiar with. If you have more specific questions or can call out specific parts which are confusing I might be able to direct you to the maths they use to build those arguments.</p>", 
                "question": "What do I need to learn in order to be able to follow this analysis?"
            }, 
            "id": "c2c2qv3"
        }, 
        {
            "body": {
                "answer": "<p>What is your question?</p>", 
                "question": "Question about statistical significance of this test?"
            }, 
            "id": "c2drav6"
        }, 
        {
            "body": {
                "answer": "<p>Yeah, the findings actually do seem to be significant. Main effect for <br>stimulus type<dq> (version of porn they watched)  F( 2,124) = 23.67, p< .001.  <br><br>The study found that homophobic men showed significantly more penile circumference increase than self-reported arousal to the homosexual video. What this means is not that all homophobic men are secretly homosexual; instead, this is saying men tend to underreport their level of arousal to homosexual videos, as compared with a more object measure (i.e. penile erection). </p>", 
                "question": "Question about statistical significance of this test?"
            }, 
            "id": "c317rs0"
        }, 
        {
            "body": {
                "answer": "<p>It<sq>s the same principle as testing whether a difference between means is significantly different from zero. You can either<colon><br><br>* subtract 50 from the mean and test whether the difference is significantly different from zero using the standard error for the sample (there<sq>s no variability in your reference point of 50 so it doesn<sq>t add anything to the variability of the difference), or<br><br>* construct a 100<percent>-alpha confidence interval using the appropriate critical value of t (ie for 95<percent> or 99<percent> confidence or whatever alpha you choose) and see whether it contains 50.<br><br>The two are functionally equivalent in that if p is exactly 0.05 (say) then one end of the 95<percent> confidence interval will be zero. But confidence intervals are a more useful approach because they emphasise the size of the difference and the uncertainty in the estimate rather than burying all the interesting information in a binary yes/no hypothesis test.<br><br>Why are you comparing the two groups separately? It<sq>s not necessarily a problem but you sometimes see it in p-hacking. eg *There<sq>s no difference overall but if i split the group by x I get a significant result for one of the groups!* If this is just an assignment, do what they asked you to. If it<sq>s real data then you should be handling it properly. This is a useful series of short articles on the topic, in case it<sq>s relevant<colon> <br><br>[Statistics Notes<colon> Interaction 1<colon> heterogeneity of effects](http<colon>//www.bmj.com/content/313/7055/486)<br><br>[Statistics Notes<colon> Interaction 2<colon> compare effect sizes not P values](http<colon>//www.bmj.com/content/313/7060/808)<br><br>[Statistics notes<colon> Interaction 3<colon> How to examine heterogeneity](http<colon>//www.bmj.com/content/313/7061/862)<br><br>You should also bear this in mind when interpreting hypothesis tests<colon> [The p value and the base rate fallacy](https<colon>//www.statisticsdonewrong.com/p-value.html)</p>", 
                "question": "How do you carry out a t -test against a particular mean?"
            }, 
            "id": "dg81zkk"
        }, 
        {
            "body": {
                "answer": "<p>I don<sq>t know anything about hockey, so I can<sq>t comment on the statistic.  <br><br>If you<sq>re building a predictive model, and just throwing in variables, I wouldn<sq>t worry much about correlative variables, or their inter-dependency.  I would just test whichever model had the best performance, via cross-validation, and go with that.  Some <dq>blackbox<dq> models can even provide relative variable importance - for example, random forests.<br><br>If your model does prove to have too much variance, due to a lack of independence, you can include more bias (such as using a ridge or lasso regression).  These methods also provide relative importance.<br><br>Although these methods aren<sq>t nearly as rigorous as the more classical approaches (such as your standard regression), if you<sq>re strictly looking for inference.</p>", 
                "question": "I<sq>m looking to evaluate the relative importance of several hockey statistics in a grand predictive model. The challenge is many observations are dependent on one-another. Any advice? More details within."
            }, 
            "id": "dg6yu5i"
        }, 
        {
            "body": {
                "answer": "<p>You could use a latent growth curve model<colon>  <br><br>http<colon>//support.sas.com/resources/papers/proceedings10/273-2010.pdf</p>", 
                "question": "Repeated measures? Please help."
            }, 
            "id": "dg37qiv"
        }, 
        {
            "body": {
                "answer": "<p>This is interesting and adds a whole new spin to poker.  You have to take chips from others to cover your interest.  </p>", 
                "question": "Playing Poker With Interest"
            }, 
            "id": "dg091qu"
        }, 
        {
            "body": {
                "answer": "<p>I feel that computing is an important component of applied statistics. I see that the minor requires some core quantitative courses, maybe you go for the more computational 141A-C series to round it out?<br><br>Knowing how to actually apply the statistical knowledge to a real data file on your laptop is a practical, valuable skill distinct from understanding the analysis process. A minor is to get a taste of a field and a taste of applied statistics without a little exposure to how to do basic analysis in R or another high-level language would be pretty bland imho.</p>", 
                "question": "Thinking of doing a statistics minor - which of these courses seem most useful to a social sciences major?"
            }, 
            "id": "dfyss2x"
        }, 
        {
            "body": {
                "answer": "<p>I<sq>d recommend nonparametrics. It<sq>s a subfield based around theory not being adequate enough to use typical statistical methods (for example, your sample size being too small), so it is used quite often in the social sciences, especially psych </p>", 
                "question": "Thinking of doing a statistics minor - which of these courses seem most useful to a social sciences major?"
            }, 
            "id": "dfytrb7"
        }, 
        {
            "body": {
                "answer": "<p>I have a psych undergrad and a masters in statistics. I<sq>d recommend 135 or 144 from that list. Multivariate data analysis will be useful in any field you go into, and if you<sq>re interested in doing any type of psych research, sampling is quite important.</p>", 
                "question": "Thinking of doing a statistics minor - which of these courses seem most useful to a social sciences major?"
            }, 
            "id": "dfz2aey"
        }, 
        {
            "body": {
                "answer": "<p>Is there some reason you have to choose now? Might be better to take the first couple required classes, plus some intro classes for your social science interests, and use that to get a better feel for the field and what you might want to focus on. E.g. there are some areas where time series analysis is critical, and others where it<sq>s rare/nonexistent.</p>", 
                "question": "Thinking of doing a statistics minor - which of these courses seem most useful to a social sciences major?"
            }, 
            "id": "dfz9rr9"
        }, 
        {
            "body": {
                "answer": "<p>[This](https<colon>//www.amazon.com/Statistical-Inference-George-Casella/dp/0534243126) is a classic. I took a grad level course with this textbook and every problem is nasty. But yes, it is really a classic.<br><br>Also, I just begun Data Analysis Using Regression and Multilevel/Hierarchical Models by Andrew Gelman and Jennifer Hill. Love his interpretation of linear regression. Linear regression might sound like basics, but it lays the foundation work for everything else and from time to time I feel compelled to review it. This book gave me a new way to look at a familiar topic.<br><br>If you are familiar with any statistical programming language/packages, I would highly suggest you implement the learnings from any books you have.</p>", 
                "question": "Book recommendations for someone who has a reasonable background in probability / basic statistics and is trying to take it to the next level?"
            }, 
            "id": "dfz6q4o"
        }, 
        {
            "body": {
                "answer": "<p>You can<sq>t </p>", 
                "question": "Question about grade distribution mainly standard deviation."
            }, 
            "id": "dfxxdej"
        }, 
        {
            "body": {
                "answer": "<p>There<sq>s no way. You need more information. </p>", 
                "question": "Question about grade distribution mainly standard deviation."
            }, 
            "id": "dfxrkdz"
        }, 
        {
            "body": {
                "answer": "<p>This was already answered on /r/bioinformatics, so to save others from answering<colon><br>Here is a good explanation of logarithms and how to operate with them<colon> http<colon>//mathworld.wolfram.com/Logarithm.html<br>log(x/y) = log(x) - log(y)</p>", 
                "question": "calculating the log2 fold change for already loged2 values"
            }, 
            "id": "dfwk2yb"
        }, 
        {
            "body": {
                "answer": "<p>[Student<sq>s t-test.](https<colon>//en.wikipedia.org/wiki/Student<percent>27s_t-test)</p>", 
                "question": "How do I test statistical significance comparing two averages?"
            }, 
            "id": "dfvr5vj"
        }, 
        {
            "body": {
                "answer": "<p>You should be using a 2 sample z-test and not a t-test because you know the population standard deviation </p>", 
                "question": "How do I test statistical significance comparing two averages?"
            }, 
            "id": "dfyuhaa"
        }, 
        {
            "body": {
                "answer": "<p>Try this<colon> https<colon>//www.spss-tutorials.com/draw-a-stratified-random-sample/</p>", 
                "question": "How would I do a stratified sampling using SPSS or SAS enterprise guide?"
            }, 
            "id": "dfvqe91"
        }, 
        {
            "body": {
                "answer": "<p>If you run two ANOVAs, one for each species, you won<sq>t be able to compare the species - and both of your research questions are about comparing the species. You should run a single ANOVA.</p>", 
                "question": "Difference between one ANOVA on two species vs one ANOVA for each species"
            }, 
            "id": "dfudby6"
        }, 
        {
            "body": {
                "answer": "<p>Interactions can be very important findings. I think you will have better luck interpreting your data by plotting the two-way interactions separately for each level of the third variable than by examining pairs of means or individual means and standard errors. You can find example plots [here](http<colon>//onlinestatbook.com/2/analysis_of_variance/multiway.html). Figure 6 is for a 3-way. </p>", 
                "question": "Help interpreting results on two-way (three-way) ANOVA"
            }, 
            "id": "dfsk7w1"
        }, 
        {
            "body": {
                "answer": "<p>A chi-square test is fine. [Here](https<colon>//www.medcalc.org/calc/comparison_of_proportions.php) is an online calculator that performs the calculations.</p>", 
                "question": "correct statistical test to use to compare two sets of binary?"
            }, 
            "id": "dfpbrks"
        }, 
        {
            "body": {
                "answer": "<p>Welcome to the difficulty of random sampling! In order that the whole population has the same chance of being selected (or even just *a chance*) you need a list of the whole population from which to sample. This is why many studies do not use a truly random sample, and why the census is so important.<br>  <br>You might consider a multi stage probability sample, which is a lot easier. As an example, if you have a list of all the Apple stores, first select some stores randomly (weighting for the relative size of the store) and then try to get a list of all staff from those stores selected. <br> <br>Edit; if you can<sq>t get a list for those stores, take the staff member with the next birthday, or randomly generate a date to call/visit and sample the nth person you speak to, where n is a random number.</p>", 
                "question": "Random sampling"
            }, 
            "id": "dfoq9bb"
        }, 
        {
            "body": {
                "answer": "<p>See the edit in my post for a couple of ways to sample staff once you know which stores have been included.  <br>  <br>Make sure to weight the stores for their size, otherwise staff working in a tiny store have a much higher chance of selection. </p>", 
                "question": "Random sampling"
            }, 
            "id": "dfq0m7b"
        }, 
        {
            "body": {
                "answer": "<p>Why not two repeated measure anovas, one for the first pre/post outcome one for the second pre/post outcome?</p>", 
                "question": "Difficulty with Repeated Measures ANOVA"
            }, 
            "id": "dflivs3"
        }, 
        {
            "body": {
                "answer": "<p>So you<sq>ve got two pretest variables and two posttest variables across three separate conditions? Trying to make sure I<sq>ve got the setup straight. If there<sq>s more than one DV I think you<sq>ll end up needing a MANOVA. </p>", 
                "question": "Difficulty with Repeated Measures ANOVA"
            }, 
            "id": "dfma2y4"
        }, 
        {
            "body": {
                "answer": "<p>Just a few thoughts at the top of my head<colon><br><br>* Sounds like some type of regression could be applied.<br>* I<sq>d probably try fitting a distributed lag model. These models basically assume that variables can have longer effects, not just today or tomorrow.</p>", 
                "question": "I want to discover what food I<sq>m intolerant to."
            }, 
            "id": "dfknupi"
        }, 
        {
            "body": {
                "answer": "<p>I recently discovered [Nomie](https<colon>//nomie.io/) and I<sq>m downloading it right now. As I understood it tracks everything you want, but doesn<sq>t really interfere all the variables to find the best/worst match. </p>", 
                "question": "I want to discover what food I<sq>m intolerant to."
            }, 
            "id": "dfk4q3k"
        }, 
        {
            "body": {
                "answer": "<p>I found an [unanswered question](https<colon>//stats.stackexchange.com/questions/167958/test-hypothesis-of-rare-event-with-real-life-data-using-a-bayesian-model?newreg=bc501ca07aed40acaa0ca95e4db160c0) on StackExchange<colon> it talks about Bayesian statistics, I don<sq>t know much about that other than a quick read on its [wikipedia page](https<colon>//en.wikipedia.org/wiki/Bayesian_statistics). It seems I<sq>m getting nearer to what I<sq>m looking for, but I still don<sq>t know how to make use of it. </p>", 
                "question": "I want to discover what food I<sq>m intolerant to."
            }, 
            "id": "dfkhm73"
        }, 
        {
            "body": {
                "answer": "<p>Your link is to a Google search, and we might be looking at different things- what shows for me are mostly formulas that test<colon><br><br>Ho<colon> **All** slopes in model are = 0<br><br>Ha<colon> At least one !=0. <br><br>This is a different test than seeing if ONLY the few dummies are zero, comparing a model with k predictors to k-j. [This link gives the formula about halfway down the page.](https<colon>//tien89.wordpress.com/2010/03/18/testing-multiple-linear-restrictions-the-f-test/)</p>", 
                "question": "F-test and joint significance"
            }, 
            "id": "dg7vt65"
        }
    ]
}