Input file is ../data/Posts.xml.ranker.csv
Solr cluster is sc5352d79e_c165_44a6_97ca_8384501d30dd
Solr collection is AI
Ranker name is AI
Rows per query 10
Generating training data...
curl -k -s  -u 55729e6b-2d18-46f8-b6a0-3e87cdd80797:ANlCCDTtOHr4 -d "q=<p>I've seen emotional intelligence defined as the capacity to be aware of, control, and express one's emotions, and to handle interpersonal relationships judiciously and empathetically.  What are some strategies for artificial intelligence to begin to tackle this problem and develop emotional intelligence for computers?  Are there examples where this is already happening to a degree today?  For example, wouldn't a computer that passes a Turing test necessarily express emotional intelligence or it would be seen as an obvious computer?  Perhaps that is why early programs that pass the test represented young people, who presumably have lower emotional intelligence.</p><br>&gt=189,10,177,4,1737,1,1612,0,1754,0&generateHeader=true&rows=10&returnRSInput=true&wt=json" "https://gateway.watsonplatform.net/retrieve-and-rank/api/v1/solr_clusters/sc5352d79e_c165_44a6_97ca_8384501d30dd/solr/AI/fcselect"
Output:
{"responseHeader":{"status":0,"QTime":31},"response":{"numFound":749,"start":0,"maxScore":130.07776,"docs":[{"body":["{u'answer': u'<p>Architectures for recognising and generating emotion are typically somewhat complex and don\\'t generally have short descriptions, so it\\'s probably better to reference the literature rather than give a misleading soundbite:</p>\\n\\n<p>Some of the early work in `Affective Computing\\' was done by Rosamund Picard.\\nThere is a <a href=\"http://affect.media.mit.edu/\">research group at MIT</a> specializing in this area.</p>\\n\\n<p>Some of the more developed architectural ideas are due to Marvin Minsky.\\nA pre-publication draft of his book, `The Emotion Machine\\' is available via <a href=\"https://en.wikipedia.org/wiki/The_Emotion_Machine\">Wikipedia</a>.</p>\\n\\n<p>Emotional intelligence would certainly seem to be a necessary component of passing the Turing test - indeed, in the original Turing test essay in <a href=\"http://www.csee.umbc.edu/courses/471/papers/turing.pdf\">Computing Machinery and Intelligence</a> implied some degree of \"Theory of Mind\" about Mr Pickwick\\'s preferences:</p>\\n\\n<p><em>\"Yet Christmas is a Winter\\u2019s day, and I do not think Mr Pickwick would mind the comparison.\"</em></p>\\n', u'question': u\"<p>I've seen emotional intelligence defined as the capacity to be aware of, control, and express one's emotions, and to handle interpersonal relationships judiciously and empathetically.  What are some strategies for artificial intelligence to begin to tackle this problem and develop emotional intelligence for computers?  Are there examples where this is already happening to a degree today?  For example, wouldn't a computer that passes a Turing test necessarily express emotional intelligence or it would be seen as an obvious computer?  Perhaps that is why early programs that pass the test represented young people, who presumably have lower emotional intelligence.</p><br>\"}"],"id":"189","_version_":1559279277152665600,"score":130.07776,"featureVector":"65.03888 56.563995 37.809135 57.59139 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 65.03888 56.563995 37.809135 57.59139 1.0 0 0.6931471805599453 130.07776"},{"body":["{u'answer': u'<p><a href=\"https://en.wikipedia.org/wiki/Emotion\" rel=\"nofollow\">Emotions</a> aren\\'t something that you can implement - they\\'re very complex. However, you can attempt to mimic them. Human emotions are closely related to conscious experience characterized by intense mental activity, which is based on interpretation of events.</p>\\n\\n<p>Recent brain studies (including research in cognitive psychology and neurophysiology) suggests that human emotional assessment of every action or event plays an important role in human mental processes.</p>\\n\\n<p>The recent <a href=\"http://bica2016.bicasociety.org/\" rel=\"nofollow\">2016 Annual Meeting of the BICA Society</a> brought together scientists from around the world to approach principles and mechanisms of human thought to create biologically inspired AI.</p>\\n\\n<p>For example, in Samsonovich\\'s (a professor in the Cybernetics Department at the <a href=\"https://en.wikipedia.org/wiki/National_Research_Nuclear_University_MEPhI\" rel=\"nofollow\">MEPhI</a>) proposal, the idea is to test AI in computer games which involves actions with emotional content, where AI may engage with players in different types of social relationships (such as trust, subordination or leadership).</p>\\n\\n<p>Jonathan Gratch of the <a href=\"https://en.wikipedia.org/wiki/Institute_for_Creative_Technologies\" rel=\"nofollow\">ICT</a>, invented virtual characters capable of identifying and expressing emotions by communicating with humans in their natural language based on the situations where for example AI can deceive a human to achieve the desired result. The effect is obviously not achieved by re-creating human consciousness, but by achieving statistically adjusting parameters.</p>\\n\\n<p>Researchers from the Institute of Cyber Intelligence Systems in MEPhI are hoping to be able to create in the near future future virtual beings which are capable of planning, setting goals and establishing social relationships with humans, also by possessing both emotional and narrative intelligence which can interpret context of events.</p>\\n\\n<p><sup>Source: <a href=\"http://phys.org/news/2016-07-social-emotions-artificial-intelligence.html\" rel=\"nofollow\">Researcher proposes social emotions test for artificial intelligence</a></sup></p>\\n', u'question': u\"<p>I've seen emotional intelligence defined as the capacity to be aware of, control, and express one's emotions, and to handle interpersonal relationships judiciously and empathetically.  What are some strategies for artificial intelligence to begin to tackle this problem and develop emotional intelligence for computers?  Are there examples where this is already happening to a degree today?  For example, wouldn't a computer that passes a Turing test necessarily express emotional intelligence or it would be seen as an obvious computer?  Perhaps that is why early programs that pass the test represented young people, who presumably have lower emotional intelligence.</p><br>\"}"],"id":"1612","_version_":1559279285002305536,"score":128.37535,"featureVector":"64.187675 51.054077 34.35193 51.96937 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 64.187675 51.054077 34.35193 51.96937 1.0 1 0.4054651081081644 128.37535"},{"body":["{u'answer': u'<p>The way i do emotion in a AGI system are by a bunch of little\\nparts, agents, voting in system statis state registers. If the union\\nof agents are working together correctly. This is the subconscious part.</p>\\n\\n<p>The conscious part that plan out movement in the environment\\ninclude these system statis state registers in all planned movements.</p>\\n\\n<p>All emotions can be derived from these registers:</p>\\n\\n<p><a href=\"https://groups.google.com/forum/#!topic/artificial-general-intelligence/pxWmHClAAdA\" rel=\"nofollow\">https://groups.google.com/forum/#!topic/artificial-general-intelligence/pxWmHClAAdA</a>  </p>\\n\\n<p><a href=\"https://groups.google.com/forum/#!topic/artificial-general-intelligence/jWdzPaxYHmU\" rel=\"nofollow\">https://groups.google.com/forum/#!topic/artificial-general-intelligence/jWdzPaxYHmU</a>  </p>\\n\\n<p><a href=\"https://groups.google.com/forum/#!forum/artificial-general-intelligence\" rel=\"nofollow\">https://groups.google.com/forum/#!forum/artificial-general-intelligence</a>  </p>\\n', u'question': u\"<p>I've seen emotional intelligence defined as the capacity to be aware of, control, and express one's emotions, and to handle interpersonal relationships judiciously and empathetically.  What are some strategies for artificial intelligence to begin to tackle this problem and develop emotional intelligence for computers?  Are there examples where this is already happening to a degree today?  For example, wouldn't a computer that passes a Turing test necessarily express emotional intelligence or it would be seen as an obvious computer?  Perhaps that is why early programs that pass the test represented young people, who presumably have lower emotional intelligence.</p><br>\"}"],"id":"1754","_version_":1559279287516790785,"score":127.61251,"featureVector":"63.806255 58.018517 39.96519 59.208523 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 63.806255 58.018517 39.96519 59.208523 1.0 2 0.2876820724517809 127.61251"},{"body":["{u'answer': u'<p>As for your comment about a computer program showing lower emotional intelligence, you may find Eliza (which you can try <a href=\"http://manifestation.com/neurotoys/eliza.php3/\" rel=\"nofollow\">here</a>) interesting. It is a classical in the history of AI, and pretends to mimic an analyst (psychology).</p>\\n\\n<p>However, I think your question fits nowadays more in the field of <a href=\"https://en.wikipedia.org/wiki/Human%E2%80%93robot_interaction\" rel=\"nofollow\">Human-Robot Interaction</a>, which relies largely on <a href=\"http://nordicapis.com/20-emotion-recognition-apis-that-will-leave-you-impressed-and-concerned/\" rel=\"nofollow\">vision</a> for recognition of gestures and follow movements, as well as <em>soft, natural</em> movements as a response. Note that the movements of the face and hands belong to the most complex tasks, involving many muscles at a time. </p>\\n\\n<p>I strongly recommend the film <a href=\"http://www.plugandpray-film.de/en/trailer.html\" rel=\"nofollow\">Plug&amp;Pray</a> to have an idea of what people are researching in this area.</p>\\n\\n<p>On the <em>purely human</em> end of the scale, I sometimes wonder about our (my) emotional intelligence myself. Would I want to implement such an intelligence in an artificial agent at all?</p>\\n\\n<hr>\\n\\n<p>I remember why I thought of Eliza: not because of its emotional intelligence, but because it was <a href=\"http://www.alicebot.org/articles/wallace/eliza.html\" rel=\"nofollow\">apparently taken seriously</a> by a couple of humans. Could this be taken as a sort of (approved) Turing test? What does it say about the humans it met?</p>\\n', u'question': u\"<p>I've seen emotional intelligence defined as the capacity to be aware of, control, and express one's emotions, and to handle interpersonal relationships judiciously and empathetically.  What are some strategies for artificial intelligence to begin to tackle this problem and develop emotional intelligence for computers?  Are there examples where this is already happening to a degree today?  For example, wouldn't a computer that passes a Turing test necessarily express emotional intelligence or it would be seen as an obvious computer?  Perhaps that is why early programs that pass the test represented young people, who presumably have lower emotional intelligence.</p><br>\"}"],"id":"177","_version_":1559279276804538369,"score":122.633545,"featureVector":"61.316772 52.73632 34.30809 53.671696 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 61.316772 52.73632 34.30809 53.671696 1.0 3 0.22314355131420976 122.633545"},{"body":["{u'answer': u'<p>So you may be familiar with Word2Vec, (W2V) which as <a href=\"http://en.wikipedia.org/wiki/Word2vec\" rel=\"nofollow\">Wikipedia describes</a><sup>1</sup> \"captures the linguistic contexts of words\" using vector arithmetic. For example, subtract \\'Paris\\' from \\'France\\' and add \\'Italy\\' and you get \\'Rome\\'.</p>\\n\\n<p>What you need is something like a Sentiment2Vec (S2V) that captures the similarities between emotional transitions. Something like: subtract \\'fear\\' from \\'sadness\\', add \\'joy\\' and you get \\'hope\\'. Or: subtract \\'sting\\' from \\'papercut\\', add \\'smashed\\' and you get \\'throbbing\\'.</p>\\n\\n<p>The catch is that you don\\'t have an easily accessible corpus of emotional contexts to train with, like you have with words. If you had a million hours of fMRI - mapping the transitions between emotions in hundreds of subjects - then you could use that data to build an S2V. You probably don\\'t have that data though.</p>\\n\\n<p>In the mean time, you could just build a W2V that specializes in sentiment. You could even try to use a current sentiment analysis engine to bootstrap it. Perhaps if you read enough text that says \"I got a papercut and it stings\" and \"I smashed my finger and it\\'s throbbing\" then you could eventually produce an S2V. Children\\'s books often use explicit language regarding emotional context (\"this made the boy feel sad\").</p>\\n\\n<p>But words are still a far cry from the experiential context that a connectome map would provide. To test whether you have something useful or not, you might want to implement your S2V in a mouse foraging simulation - see whether it produces typical behavior and if any cooperative or competitive dynamics can organically grow out of your S2V.</p>\\n\\n<p>Some further info on the subject: </p>\\n\\n<p>In 2014, <a href=\"http://www.bbc.com/news/uk-scotland-glasgow-west-26019586\" rel=\"nofollow\">Glasgow University claimed</a><sup>2</sup> that there are four primary emotions: happiness, sadness, fear and anger. </p>\\n\\n<p><a href=\"http://changingminds.org/explanations/emotions/basic%20emotions.htm\" rel=\"nofollow\">This website</a><sup>3</sup> provides nice (if somewhat short) hierarchical breakdown of secondary and tertiary emotions under primary emotions.</p>\\n\\n<hr>\\n\\n<p><strong>References</strong></p>\\n\\n<p><sub><sup>1</sup>: <a href=\"http://en.wikipedia.org/wiki/Word2vec\" rel=\"nofollow\">en.wikipedia.org/wiki/Word2vec</a></sub></p>\\n\\n<p><sub><sup>2</sup>: <a href=\"http://www.bbc.com/news/uk-scotland-glasgow-west-26019586\" rel=\"nofollow\">www.bbc.com/news/uk-scotland-glasgow-west-26019586</a></sub></p>\\n\\n<p><sub><sup>3</sup>: <a href=\"http://changingminds.org/explanations/emotions/basic%20emotions.htm\" rel=\"nofollow\">changingminds.org/explanations/emotions/basic%20emotions.htm</a></sub></p>\\n', u'question': u\"<p>I've seen emotional intelligence defined as the capacity to be aware of, control, and express one's emotions, and to handle interpersonal relationships judiciously and empathetically.  What are some strategies for artificial intelligence to begin to tackle this problem and develop emotional intelligence for computers?  Are there examples where this is already happening to a degree today?  For example, wouldn't a computer that passes a Turing test necessarily express emotional intelligence or it would be seen as an obvious computer?  Perhaps that is why early programs that pass the test represented young people, who presumably have lower emotional intelligence.</p><br>\"}"],"id":"1737","_version_":1559279287093166080,"score":112.50212,"featureVector":"56.25106 44.198174 29.490288 45.04521 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 56.25106 44.198174 29.490288 45.04521 1.0 4 0.18232155679395462 112.50212"},{"body":["{u'answer': u'<h3>Strong AIs</h3>\\n\\n<p>For a strong AI, the short answer is to call for help, when they might not even know what the supposed help could be.</p>\\n\\n<p>It depends on what the AI would do. If it is supposed to solve a single easy task perfectly and professionally, sure emotions would not be very useful. But if it is supposed to learn random new things, there would be a point that it encounters something it cannot handle.</p>\\n\\n<p>In Lee Sedol vs AlphaGo match 4, some pro who has said computer doesn\\'t have emotions previously, commented that maybe AlphaGo has emotions too, and stronger than human. In this case, we know that AlphaGo\\'s crazy behavior isn\\'t caused by some deliberately added things called \"emotions\", but a flaw in the algorithm. But it behaves exactly like it has panicked.</p>\\n\\n<p>If this happens a lot for an AI. There might be advantages if it could know this itself and think twice if it happens. If AlphaGo could detect the problem and change its strategy, it might play better, or worse. It\\'s not unlikely to play worse if it didn\\'t do any computations for other approaches at all. In case it would play worse, we might say it suffers from having \"emotions\", and this might be the reason some people think having emotions could be a flaw of human beings. But that wouldn\\'t be the true cause of the problem. The true cause is it just doesn\\'t know any approaches to guarantee winning, and the change in strategy is only a try to fix the problem. Commentators thinks there are better ways (which also don\\'t guarantee winning but had more chance), but its algorithm isn\\'t capable to find out in this situation. Even for human, the solution to anything related to emotion is unlikely to remove emotions, but some training to make sure you understand the situation enough to act calmly.</p>\\n\\n<p>Then someone has to argue about whether this is a kind of emotion or not. We usually don\\'t say small insects have human-like emotions, because we don\\'t understand them and are unwilling to help them. But it\\'s easy to know some of them could panic in desperate situations, just like AlphaGo did. I\\'d say these reactions are based on the same logic, and they are at least the reason why human-like emotions could be potentially useful. They are just not expressed in human-understandable ways, as they didn\\'t intend to call a human for help.</p>\\n\\n<p>If they tries to understand their own behavior, or call someone else for help, it might be good to be exactly human-like. Some pets can sense human emotions and express human-understandable emotion to some degree. The purpose is to interact with humans. They evolved to have this ability because they needed it at some point. It\\'s likely a full strong AI would need it too. Also note that, the opposite of having full emotions might be becoming crazy.</p>\\n\\n<p>It is probably a quick way to lose any trust if someone just implement emotions imitating humans with little understanding right away in the first generations, though.</p>\\n\\n<h3>Weak AIs</h3>\\n\\n<p>But is there any purposes for them to have emotions before someone wanted a strong AI? I\\'d say no, there isn\\'t any inherent reasons that they must have emotions. But inevitably someone will want to implement imitated emotions anyway. Whether \"we\" need them to have emotions is just nonsense.</p>\\n\\n<p>The fact is even some programs without any intelligence contained some \"emotional\" elements in their user interfaces. They may look unprofessional, but not every task needs professionality so they could be perfectly acceptable. They are just like the emotions in musics and arts. Someone will design their weak AI in this way too. But they are not really the AIs\\' emotions, but their creators\\'. If you feel better or worse because of their emotions, you won\\'t treat individul AIs so differently, but this model or brand as a whole.</p>\\n\\n<p>Alternatively someone could plant some personallities like in a role-playing game there. Again, there isn\\'t a reason they must have that, but inevitably someone will do it, because they obviously had some market when a role-playing game does.</p>\\n\\n<p>In either cases, the emotions don\\'t really originate from the AI itself. And it would be easy to implement, because a human won\\'t expect them to be exactly like a human, but tries to understand what they intended to mean. It could be much easier to accept these emotions realizing this.</p>\\n\\n<h3>Aspects of emotions</h3>\\n\\n<p>Sorry about posting some original research here. I made a list of emotions in 2012 and from which I see 4 aspects of emotions. If they are all implemented, I\\'d say they are exactly the same emotions as of humans. They don\\'t seem real if only some of them are implemented, but that doesn\\'t mean they are completely wrong.</p>\\n\\n<ul>\\n<li>The reason, or the original logical problem that the AI cannot solve. AlphaGo already had the reason, but nothing else. If I have to make an accurate definition, I\\'d say it\\'s the state that multiple equally important heuristics disagreeing with each other.\\n\\n<ul>\\n<li>The context, or which part of the current approach is considered not working well and should probably be replaced. This distinguishes sadness-related, worry-related and passionate-related.</li>\\n<li>The current state, or whether it feels leading, or whether its belief or the fact is supposed to turn bad first (or was bad all along) if things go wrong. This distinguishes sadness-related, love-related and proud-related.</li>\\n</ul></li>\\n<li>The plan or request. I suppose some domesticated pets already had this. And I suppose these had some fixed patterns which is not too difficult to have. Even arts can contain them easily. Unlike the reasons, these are not likely inherent in any algorithms, and multiple of them can appear together.\\n\\n<ul>\\n<li>Who supposedly had the responsibility if nothing is changed by the emotion. This distinguishes curiosity, rage and sadness.</li>\\n<li>What is the supposed plan if nothing is changed by the emotion. This distinguishes disappointment, sadness and surprise.</li>\\n</ul></li>\\n<li>The source. Without context, even a human cannot reliably tell someone is crying for being moved or thankful, or smiling for some kind of embarrassment. In most other cases there aren\\'t even words describing them. It doesn\\'t make that much difference if an AI doesn\\'t distinguish or show this specially. It\\'s likely they would learn these automatically (and inaccurately as a human) at the point they could learn to understand human languages.</li>\\n<li>The measurements, such as how urgent or important the problem is, or even how likely the emotions are true. I\\'d say it cannot be implemented in the AI. Humans don\\'t need to respect them even if they are exactly like humans. But humans will learn how to understand an AI if that really matters, even if they are not like humans at all. In fact, I feel that some of the extremely weak emotions (such as thinking something is too stupid and boring that you don\\'t know how to comment) exist almost exclusively in emoticons, where someone intend to show you exactly this emotion, and hardly noticeable in real life or any complex scenerios. I supposed this could also be the case in the beginning for AIs. In the worst case, they are firstly conventionally known as \"emotions\" since emoticons works in these cases, so it\\'s easier to group them together, but very few people seriously think they are, just like the example I gave.</li>\\n</ul>\\n\\n<p>So when strong AIs become possible, none of these would be unreachable, though there might be a lot of work to make the connections. So I\\'d say if there would be the need for strong AIs, they absolutely would have emotions.</p>\\n', u'question': u'<p>In a <a href=\"http://www.wsj.com/articles/whats-next-for-artificial-intelligence-1465827619\">recent Wall Street Journal article</a>, Yann LeCunn makes the following statement:</p><br><br><blockquote><br>  <p>The next step in achieving human-level ai is creating intelligent\\u2014but not autonomous\\u2014machines. The AI system in your car will get you safely home, but won\\u2019t choose another destination once you\\u2019ve gone inside. From there, we\\u2019ll add basic drives, along with emotions and moral values. If we create machines that learn as well as our brains do, it\\u2019s easy to imagine them inheriting human-like qualities\\u2014and flaws. </p><br></blockquote><br><br><p>Personally, I have generally taken the position that talking about emotions for artificial intelligences is silly, because there would be no <em>reason</em> to create AI\\'s that experience emotions.  Obviously Yann disagrees.  So the question is:  what end would be served by doing this?  Does an AI <em>need</em> emotions to serve as a useful tool?  </p><br>'}"],"id":"1918","_version_":1559279291486699520,"score":40.76508,"featureVector":"20.38254 0.12193821 0.4864601 0.20993814 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 20.38254 0.12193821 0.4864601 0.20993814 0.5 5 0.1541506798272583 40.76508"},{"body":["{u'answer': u'<p>Not going into details of Stanford\\u2013Binet test, but just looking at <a href=\"https://en.wikipedia.org/wiki/Stanford%E2%80%93Binet_Intelligence_Scales\" rel=\"nofollow noreferrer\">wikipedia page</a> it shows many subtests like knowledge, reasoning, verbal tests etc. Most of the efforts in the artificial intelligence today is directed into research of specific areas like computer vision, natural language processing, machine learning, but also combination of fields like implementation of self driving cars.</p>\\n\\n<p>Within every field there are still other subfields and problems that are not solved yet. For example, development of human-like natural language processing (NLP) is necessary for intelligent agent to pass any verbal tests, or even non-verbal tests that requires processing of sentences of human language. Famous test that tests intelligence by asking questions in natural language and expects answers in the same form is Turing test. NLP still struggles with many (basic) human skills like listening, speaking, parsing and forming sentences. No one knows when we\\'ll have system that can do these things as good as human. Since this system is crucial, but also far from human-like it\\'s likely cause of delay in developing AI that passes intelligence test. Are these problems AI-hard? Do we need to develop strong AI to solve them?</p>\\n\\n<p>You can look at speech and listening as interfaces used for expressing and affecting inner processes of human brain. Same goes for other senses like eyesight which is being approximated by computer vision. One could say that we only need to develop convincing mimics of human senses and incorporate them in one big system that will become first human-like AI. That is the minimum requirement. <strong>I doubt this will be achieved in this century.</strong></p>\\n\\n<p>(Other thoughts)<br/>\\nWhat truly defines intelligence is brain activity. Since it\\'s really complex and one artificial neuron is not equal to one neuron in brain, increase in computation power will not necessarily help achieving human-like AI. Also recognizing such system by mere intelligence test is questionable. For now it\\'s only philosophical discussion but by the time we are able to design such machine I think we\\'ll also have better understanding of human brain. Someone in 2100 might not read this answer on quantum computer with integrated AI OS powered from fusion reactor in his self-flying car, but will probably have many systems that help him in everyday tasks far more than we imagine today.</p>\\n', u'question': u'<p>What are the current best estimates as to what year artificial intelligence will be able to score 100 points on the <a href=\"https://en.wikipedia.org/wiki/Stanford%E2%80%93Binet_Intelligence_Scales\" rel=\"nofollow noreferrer\">Stanford Binet IQ test</a>?</p><br>'}"],"id":"2346","_version_":1559279301080121344,"score":27.883644,"featureVector":"13.941822 0.5813472 0.4940742 0.5813472 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 13.941822 0.5813472 0.4940742 0.5813472 0.35999998 6 0.13353139262452263 27.883644"},{"body":["{u'answer': u'<p>Emotion in an AI is useful, but not necessary depending on your objective (in most cases, it\\'s not).</p>\\n\\n<p>In particular, <strong>emotion recognition/analysis</strong> is very well advanced, and it\\'s used in a wide range of applications very successfully, from robot teacher for autistic children (see developmental robotics) to gambling (poker) to personal agents and politics sentiment/lies analysis.</p>\\n\\n<p><strong>Emotional cognition</strong>, the experience of emotions for a robot, is much less developed, but there are very interesting researchs (see <a href=\"https://en.wikipedia.org/wiki/Affect_heuristic\" rel=\"nofollow\">Affect Heuristic</a>, <a href=\"http://cdn.intechopen.com/pdfs/33737/InTech-A_multidisciplinary_artificial_intelligence_model_of_an_affective_robot.pdf\" rel=\"nofollow\">Lovotics\\'s Probabilistic Love Assembly</a>, and others...). Indeed, I can\\'t see why we couldn\\'t model emotions such as <a href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3898540/\" rel=\"nofollow\">love as they are just signals that can already be cut in humans brains (see Brian D. Earp paper)</a>. It\\'s difficult, but not impossible, and actually there are several robots reproducing partial emotional cognition.</p>\\n\\n<p>I am of the opinion that the claim <a href=\"https://en.wikipedia.org/wiki/Synthetic_intelligence\" rel=\"nofollow\">\"robots can just simulate but not feel\" is just a matter of semantics</a>, not of objective capacity: for example, does a submarine swim like fish swim? However, planes fly, but not at all like birds do. In the end, does the technical mean really matters when in the end we get the same behavior? Can we really say that a robot like <a href=\"https://en.wikipedia.org/wiki/Chappie_(film)\" rel=\"nofollow\">Chappie</a>, if it ever gets made, does not feel anything just like a simple thermostat?</p>\\n\\n<p>However, what would be the use of emotional cognition for an AI? This question is still in great debates, but I will dare offer my own insights:</p>\\n\\n<ol>\\n<li><p>Emotions in humans (and animals!) are known to affect memories. They are now well known in neuroscience as additional modalities, or meta-data if you prefer, of long term memories: they allow to modulate how the memory is stored, how it is associated/related with other memories, and how it will be retrieved.</p></li>\\n<li><p>As such, we can hypothesize that the main role of emotions is to add additional meta-information to memories to help in heuristic inference/retrieval. Indeed, our memories are huge, there are a lot of information we store over our lifetime, so emotions can maybe be used as \"labels\" to help retrieve faster the relevant memories.</p></li>\\n<li><p>Similar \"labels\" can be more easily associated together (memories of scary events together, memories of happy events together, etc.). As such, they can help survival by quickly reacting and applying known strategies (fleeing!) from scary strategies, or to take the most out of benefitting situations (happy events, eat the most you can, will help survive later on!). And actually, neuroscience studies discovered that there are specific pathways for fear-inducing sensory stimuli, so that they reach actuators faster (make you flee) than by passing through the usual whole somato-sensory circuit as every other stimuli. This kind of associative reasoning could also lead to solutions and conclusions that could not be reached otherwise.</p></li>\\n<li><p>By feeling empathy, this could ease robots/humans interaction (eg, drones helping victims of catastrophic events).</p></li>\\n<li><p>A virtual model of an AI with emotions could be useful for neuroscience and medical research in emotional disorders as computational models to understand and/or infer the underlying parameters (this is often done for example with Alzheimer and other neurodegenerative diseases, but I\\'m not sure if it was ever done for emotional disorders as they are quite new in the DSM).</p></li>\\n</ol>\\n\\n<p>So yes, \"cold\" AI is already useful, but emotional AI could surely be applied to new areas that could not be explored by using cold AI alone. It will also surely help in understanding our own brain, as emotions are an integral part.</p>\\n', u'question': u'<p>In a <a href=\"http://www.wsj.com/articles/whats-next-for-artificial-intelligence-1465827619\">recent Wall Street Journal article</a>, Yann LeCunn makes the following statement:</p><br><br><blockquote><br>  <p>The next step in achieving human-level ai is creating intelligent\\u2014but not autonomous\\u2014machines. The AI system in your car will get you safely home, but won\\u2019t choose another destination once you\\u2019ve gone inside. From there, we\\u2019ll add basic drives, along with emotions and moral values. If we create machines that learn as well as our brains do, it\\u2019s easy to imagine them inheriting human-like qualities\\u2014and flaws. </p><br></blockquote><br><br><p>Personally, I have generally taken the position that talking about emotions for artificial intelligences is silly, because there would be no <em>reason</em> to create AI\\'s that experience emotions.  Obviously Yann disagrees.  So the question is:  what end would be served by doing this?  Does an AI <em>need</em> emotions to serve as a useful tool?  </p><br>'}"],"id":"1845","_version_":1559279289350750208,"score":27.775127,"featureVector":"13.887564 0.0 0.2508335 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 13.887564 0.0 0.2508335 0.0 0.34 7 0.11778303565638346 27.775127"},{"body":["{u'answer': u'<p>No one has attempted to make a system that could pass a serious Turing test. All the systems that are claimed to have \"passed\" Turing tests have done so with low success rates simulating \"special\" people.  Even relatively sophisticated systems like Siri and learning systems like <a href=\"http://www.cleverbot.com\" rel=\"nofollow noreferrer\">Cleverbot</a> are trivially stumped.</p>\\n\\n<p>To pass a real Turing test, you would both have to create a human-level AGI and equip it with the specialized ability to deceive people about itself convincingly (of course, that might come automatically with the human-level AGI).  We don\\'t really know how to create a human-level AGI and available hardware appears to be orders of magnitude short of what is required.  Even if we were to develop the AGI, it wouldn\\'t necessarily be useful to enable/equip/motivate? it to have the deception abilities required for the Turing test.</p>\\n', u'question': u\"<p>The Turing Test has been the classic test of artificial intelligence for a while now. The concept is deceptively simple - to trick a human into thinking it is another human on the other end of a conversation line, not a computer - but from what I've read, it has turned out to be very difficult in practice.</p><br><br><p>How close have we gotten to tricking a human in the Turing Test? With things like chat bots, Siri, and incredibly powerful computers, I'm thinking we're getting pretty close. If we're pretty far, why are we so far? What is the main problem?</p><br>\"}"],"id":"2440","_version_":1559279303614529536,"score":25.020227,"featureVector":"12.510114 4.0267096 0.8783016 4.190153 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 12.510114 4.0267096 0.8783016 4.190153 0.35999998 8 0.1053605156578263 25.020227"},{"body":["{u'answer': u'<p>I think the fundamental question is: Why even attempt to build an AI? If that objective is clear, it will provide clarity to whether or not having emotional quotient in AI make sense. Some attempts like \"Paro\" that were developed for therapeutic reasons requires they exhibit some human like emotions. Again, note that \"displaying\" emotions and \"feeling\" emotions are two completely different things. </p>\\n\\n<p>You can program a thing like paro to modulate the voice tones or facial twitches to express sympathy, affection, companionship, or whatever - but while doing so, a paro does NOT empathize with its owner - it is simply faking it by performing the physical manifestations of an emotion. It never \"feels\" anything remotely closer to what that emotion evokes in human brain. </p>\\n\\n<p>So this distinction is really important. For you to feel something, there needs to be an independent autonomous subject that has the capacity to feel. Feeling cannot be imposed by an external human agent. </p>\\n\\n<p>So going back to the question of what purpose it solves - answer really is - It depends. And the most I think we will achieve ever with silicone based AIs will remain the domain of just physical representations of emotions. </p>\\n', u'question': u'<p>In a <a href=\"http://www.wsj.com/articles/whats-next-for-artificial-intelligence-1465827619\">recent Wall Street Journal article</a>, Yann LeCunn makes the following statement:</p><br><br><blockquote><br>  <p>The next step in achieving human-level ai is creating intelligent\\u2014but not autonomous\\u2014machines. The AI system in your car will get you safely home, but won\\u2019t choose another destination once you\\u2019ve gone inside. From there, we\\u2019ll add basic drives, along with emotions and moral values. If we create machines that learn as well as our brains do, it\\u2019s easy to imagine them inheriting human-like qualities\\u2014and flaws. </p><br></blockquote><br><br><p>Personally, I have generally taken the position that talking about emotions for artificial intelligences is silly, because there would be no <em>reason</em> to create AI\\'s that experience emotions.  Obviously Yann disagrees.  So the question is:  what end would be served by doing this?  Does an AI <em>need</em> emotions to serve as a useful tool?  </p><br>'}"],"id":"1714","_version_":1559279286294151168,"score":25.008791,"featureVector":"12.5043955 0.0013227507 0.09286651 0.0013227507 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 12.5043955 0.0013227507 0.09286651 0.0013227507 0.28 9 0.09531017980432487 25.008791"}]},"RSInput":"question_id,f0,f1,f2,f3,f4,f5,f6,f7,f8,f9,f10,f11,f12,f13,f14,f15,f16,f17,f18,f19,f20,r1,r2,s,ground_truth\ne7a3d4dd-8d1a-4d8c-8992-668bcb2cfb0d,65.03888,56.563995,37.809135,57.59139,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,65.03888,56.563995,37.809135,57.59139,1.0,0,0.6931471805599453,130.07776,10\ne7a3d4dd-8d1a-4d8c-8992-668bcb2cfb0d,64.187675,51.054077,34.35193,51.96937,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,64.187675,51.054077,34.35193,51.96937,1.0,1,0.4054651081081644,128.37535,0\ne7a3d4dd-8d1a-4d8c-8992-668bcb2cfb0d,63.806255,58.018517,39.96519,59.208523,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,63.806255,58.018517,39.96519,59.208523,1.0,2,0.2876820724517809,127.61251,0\ne7a3d4dd-8d1a-4d8c-8992-668bcb2cfb0d,61.316772,52.73632,34.30809,53.671696,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,61.316772,52.73632,34.30809,53.671696,1.0,3,0.22314355131420976,122.633545,4\ne7a3d4dd-8d1a-4d8c-8992-668bcb2cfb0d,56.25106,44.198174,29.490288,45.04521,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,56.25106,44.198174,29.490288,45.04521,1.0,4,0.18232155679395462,112.50212,1\ne7a3d4dd-8d1a-4d8c-8992-668bcb2cfb0d,20.38254,0.12193821,0.4864601,0.20993814,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,20.38254,0.12193821,0.4864601,0.20993814,0.5,5,0.1541506798272583,40.76508,0\ne7a3d4dd-8d1a-4d8c-8992-668bcb2cfb0d,13.941822,0.5813472,0.4940742,0.5813472,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,13.941822,0.5813472,0.4940742,0.5813472,0.35999998,6,0.13353139262452263,27.883644,0\ne7a3d4dd-8d1a-4d8c-8992-668bcb2cfb0d,13.887564,0.0,0.2508335,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,13.887564,0.0,0.2508335,0.0,0.34,7,0.11778303565638346,27.775127,0\ne7a3d4dd-8d1a-4d8c-8992-668bcb2cfb0d,12.510114,4.0267096,0.8783016,4.190153,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,12.510114,4.0267096,0.8783016,4.190153,0.35999998,8,0.1053605156578263,25.020227,0\ne7a3d4dd-8d1a-4d8c-8992-668bcb2cfb0d,12.5043955,0.0013227507,0.09286651,0.0013227507,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,12.5043955,0.0013227507,0.09286651,0.0013227507,0.28,9,0.09531017980432487,25.008791,0\n"}

curl -k -s  -u 55729e6b-2d18-46f8-b6a0-3e87cdd80797:ANlCCDTtOHr4 -d "q=<p>I'm trying to make a conversational chatbot, so the user inputs are quite wide ranging - beyond just "turn lights on". I want to detect the category of the user intents from their inputs and prepare responses.</p><br><br><p>I've looked at MS' Luis and api.ai and the intents require a lot of training. Can people suggest other techniques for untrained intent detection?</p><br><br><p>For example if the user says "Pasta is my favorite dish to cook" then detect "intent preference entity pasta" - then I can gradually build up responses to different categories of inputs.</p><br><br><p>Perhaps the crowd-sourced intents that wit.ai (facebook) has access to could do this but I'm not sure if all end-users have access to those models.</p><br>&gt=&generateHeader=false&rows=10&returnRSInput=true&wt=json" "https://gateway.watsonplatform.net/retrieve-and-rank/api/v1/solr_clusters/sc5352d79e_c165_44a6_97ca_8384501d30dd/solr/AI/fcselect"
Output:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader"><int name="status">0</int><int name="QTime">10</int></lst><result name="response" numFound="749" start="0" maxScore="13.562774"><doc><arr name="body"><str>{u'answer': u'&lt;p&gt;I like your choice of "induce" instead of "produce," because the delusions came from the users. This means the answer has to do mostly with human psychology; people come equipped with lots of mental machinery specialized for dealing with other humans and not very much mental machinery specialized for dealing with software. So ELIZA behaved in ways that some people classified it as a person and behaved accordingly, and others didn\'t.&lt;/p&gt;\n\n&lt;p&gt;What features will trip up a person\'s internal person classification system seem like they vary heavily from person to person, and also with experience and familiarity. Going into more detail is, as mentioned in the comments, more appropriate for sites specializing on the human side of the keyboard.&lt;/p&gt;\n', u'question': u'&lt;p&gt;&lt;a href="http://www.alicebot.org/articles/wallace/eliza.html" rel="nofollow"&gt;From Eliza to A.L.I.C.E.&lt;/a&gt;:&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote&gt;&lt;br&gt;  &lt;p&gt;Weizenbaum tells us that he was shocked by the experience of releasing ELIZA (also known as "Doctor") to the nontechnical staff at the MIT AI Lab. Secretaries and nontechnical administrative staff thought the machine was a "real" therapist, and spent hours revealing their personal problems to the program. When Weizenbaum informed his secretary that he, of course, had access to the logs of all the conversations, she reacted with outrage at this invasion of her privacy. Weizenbaum was shocked by this and similar incidents to find that such a simple program could so easily deceive a naive user into revealing personal information.&lt;/p&gt;&lt;br&gt;&lt;/blockquote&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;Wikipedia\'s article on the &lt;a href="https://en.wikipedia.org/wiki/ELIZA_effect" rel="nofollow"&gt;"ELIZA Effect"&lt;/a&gt;:&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote&gt;&lt;br&gt;  &lt;p&gt;Though designed strictly as a mechanism to support "natural language conversation" with a computer, ELIZA\'s DOCTOR script was found to be surprisingly successful in eliciting emotional responses from users who, in the course of interacting with the program, began to ascribe understanding and motivation to the program\'s output. As Weizenbaum later wrote, &lt;strong&gt;"I had not realized ... that extremely short exposures to a relatively simple computer program could induce powerful delusional thinking in quite normal people."&lt;/strong&gt; Indeed, ELIZA\'s code had not been designed to evoke this reaction in the first place. Upon observation, researchers discovered users unconsciously assuming ELIZA\'s questions implied interest and emotional involvement in the topics discussed, &lt;em&gt;even when they consciously knew that ELIZA did not simulate emotion.&lt;/em&gt;&lt;/p&gt;&lt;br&gt;&lt;/blockquote&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;ELIZA, despite its simplicity, was incredibly successful at its task of tricking other human beings. Even those who knew ELIZA was a bot would still talk to it. Obviously, ELIZA served as an inspiration for various other, more intelligent chatbots, such as &lt;a href="http://www.nytimes.com/2015/08/04/science/for-sympathetic-ear-more-chinese-turn-to-smartphone-program.html?_r=0" rel="nofollow"&gt;Xiaoice&lt;/a&gt;. But I would like to know what &lt;em&gt;exactly&lt;/em&gt; led to such a simple program like ELIZA to be so successful in the first place.&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;This is very useful knowledge for a programmer since a simple program is one that would be easily maintainable.&lt;/p&gt;&lt;br&gt;'}</str></arr><str name="id">1717</str><long name="_version_">1559279286668492800</long><float name="score">13.562774</float><str name="featureVector">6.781387 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 6.781387 0.0 0.0 0.0 0.4666667 0 0.6931471805599453 13.562774</str></doc><doc><arr name="body"><str>{u'answer': u'&lt;p&gt;Shane Legg and Marcus Hutter &lt;a href="http://www.vetta.org/documents/42.pdf" rel="nofollow"&gt;proposed one&lt;/a&gt; in 2006. The main descriptive quotes (see the paper for the actual formula):&lt;/p&gt;\n\n&lt;blockquote&gt;\n  &lt;p&gt;Intelligence measures an agent\u2019s general ability to achieve goals in a wide range of environments&lt;/p&gt;\n  \n  &lt;p&gt;...&lt;/p&gt;\n  \n  &lt;p&gt;It is clear by construction that universal intelligence measures the general ability of an agent to perform well in a very wide range of environments, as required by our informal definition of intelligence given earlier. The definition places no restrictions on the internal workings of the agent; it only requires that the agent is capable of generating output and receiving input which includes a reward signal.&lt;/p&gt;\n&lt;/blockquote&gt;\n', u'question': u"&lt;p&gt;We can measure the power of the machine with the number of operation per second or the frequency of the processor. But does units similar of IQ for humans exist for a AI?&lt;br/&gt;&lt;br&gt;I'm asking for a unit which can give countable result so something different from a Turing Test which only give a binary result.&lt;/p&gt;&lt;br&gt;"}</str></arr><str name="id">1413</str><long name="_version_">1559279281097408513</long><float name="score">9.453717</float><str name="featureVector">4.7268586 2.726194 0.0 2.726194 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 4.7268586 2.726194 0.0 2.726194 0.40000004 1 0.4054651081081644 9.453717</str></doc><doc><arr name="body"><str>{u'answer': u'&lt;p&gt;The terms &lt;em&gt;strong&lt;/em&gt; and &lt;em&gt;weak&lt;/em&gt; don\'t actually refer to processing, or optimization power, or any interpretation leading to "strong AI" being &lt;em&gt;stronger&lt;/em&gt; than "weak AI". It holds conveniently in practice, but the terms come from elsewhere. In 1980, &lt;a href="https://en.wikipedia.org/wiki/John_Searle"&gt;John Searle&lt;/a&gt; coined the following statements:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;AI hypothesis, strong form: an AI system can &lt;em&gt;think&lt;/em&gt; and have a &lt;em&gt;mind&lt;/em&gt; (in the philosophical definition of the term);&lt;/li&gt;\n&lt;li&gt;AI hypothesis, weak form: an AI system can only &lt;em&gt;act&lt;/em&gt; like it thinks and has a mind.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;So &lt;em&gt;strong AI&lt;/em&gt; is a shortcut for an AI systems that verifies the &lt;em&gt;strong AI hypothesis&lt;/em&gt;. Similarly, for the weak form. The terms have then evolved: strong AI refers to AI that performs as well as humans (who have minds), weak AI refers to AI that doesn\'t.&lt;/p&gt;\n\n&lt;p&gt;The problem with these definitions is that they\'re fuzzy. For example, &lt;a href="https://en.wikipedia.org/wiki/AlphaGo"&gt;AlphaGo&lt;/a&gt; is an example of weak AI, but is "strong" by Go-playing standards. A hypothetical AI replicating a human baby would be a strong AI, while being "weak" at most tasks.&lt;/p&gt;\n\n&lt;p&gt;Other terms exist: &lt;a href="https://en.wikipedia.org/wiki/Artificial_general_intelligence"&gt;Artificial General Intelligence&lt;/a&gt; (AGI), which has cross-domain capability (like humans), can learn from a wide range of experiences (like humans), among other features. Artificial Narrow Intelligence refers to systems bound to a certain range of tasks (where they may nevertheless have superhuman ability), lacking capacity to significantly improve themselves.&lt;/p&gt;\n\n&lt;p&gt;Beyond AGI, we find Artificial Superintelligence (ASI), based on the idea that a system with the capabilities of an AGI, without the physical limitations of humans would learn and improve far beyond human level.&lt;/p&gt;\n', u'question': u"&lt;p&gt;I've heard the terms strong-AI and weak-AI used.  Are these well defined terms or subjective ones?  How are they generally defined?&lt;/p&gt;&lt;br&gt;"}</str></arr><str name="id">141</str><long name="_version_">1559279276103041024</long><float name="score">9.090328</float><str name="featureVector">4.545164 1.5801773 0.0 1.5801773 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 4.545164 1.5801773 0.0 1.5801773 0.33333334 2 0.2876820724517809 9.090328</str></doc><doc><arr name="body"><str>{u'answer': u'&lt;p&gt;The &lt;a href="https://en.wikipedia.org/wiki/Technological_singularity" rel="nofollow"&gt;technological singularity&lt;/a&gt; is a theoretical point in time at which a self-improving &lt;a href="https://en.wikipedia.org/wiki/Artificial_general_intelligence" rel="nofollow"&gt;artificial general intelligence&lt;/a&gt; becomes able to understand and manipulate concepts outside of the human brain\'s range, that is, the moment when it can understand things humans, by biological design, can\'t.&lt;/p&gt;\n\n&lt;p&gt;The fuzziness about the singularity comes from the fact that, from the singularity onwards, history is effectively unpredictable. Humankind would be unable to predict any future events, or explain any present events, as science itself becomes incapable of describing machine-triggered events. Essentially, machines would think of us the same way we think of ants. Thus, we can make no predictions past the singularity. Furthermore, as a logical consequence, we\'d be unable to define the point at which the singularity may occur at all, or even recognize it when it happens.&lt;/p&gt;\n\n&lt;p&gt;However, in order for the singularity to take place, AGI needs to be developed, and &lt;a href="https://en.wikipedia.org/wiki/Artificial_general_intelligence#Feasibility" rel="nofollow"&gt;whether that is possible is quite a hot debate&lt;/a&gt; right now. Moreover, an algorithm that creates superhuman intelligence out of bits and bytes would have to be designed. By definition, a human programmer wouldn\'t be able to do such a thing, as his/her brain would need to be able to comprehend concepts beyond its range. There is also the argument that an intelligence explosion (the mechanism by which a technological singularity would theoretically be formed) would be impossible due to the difficulty of the design challenge of making itself more intelligent, getting larger proportionally to its intelligence, and that the difficulty of the design itself may overtake the intelligence required to solve said challenge (last point credit to &lt;a href="http://ai.stackexchange.com/users/47/god-of-llamas"&gt;god of llamas&lt;/a&gt; in the comments).&lt;/p&gt;\n\n&lt;p&gt;Also, there are related theories involving machines taking over humankind and all of that sci-fi narrative. However, that\'s unlikely to happen, if &lt;a href="https://en.wikipedia.org/wiki/Three_Laws_of_Robotics" rel="nofollow"&gt;Asimov\'s laws&lt;/a&gt; are followed appropriately. Even if Asimov\'s laws were not enough, a series of constraints would still be necessary in order to avoid the misuse of AGI by misintentioned individuals, and Asimov\'s laws are the nearest we have to that.&lt;/p&gt;\n', u'question': u"&lt;p&gt;I've heard the idea of the technological singularity, what is it and how does it relate to Artificial Intelligence?  Is this the theoretical point where Artificial Intelligence machines have progressed to the point where they grow and learn on their own beyond what humans can do and their growth takes off?  How would we know when we reach this point?&lt;/p&gt;&lt;br&gt;"}</str></arr><str name="id">45</str><long name="_version_">1559279274281664512</long><float name="score">8.848466</float><str name="featureVector">4.424233 0.67088574 0.0 0.67088574 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 4.424233 0.67088574 0.0 0.67088574 0.40000004 3 0.22314355131420976 8.848466</str></doc><doc><arr name="body"><str>{u'answer': u'&lt;p&gt;Three possibilities come to mind:&lt;/p&gt;\n\n&lt;p&gt;The easiest is zero padding. Basically you take a rather big input size and just add zeroes if your concrete input is too small. Of course this is pretty limited and certainly not useful if your input ranges from a few words to full texts.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Recurrent_neural_network" rel="nofollow"&gt;RNNs&lt;/a&gt; are a very natural NN to chose if you have texts of varying size as input. You input words as word vectors just one after another and the internal state of the RNN is supposed to encode the meaning of the full string of words. &lt;a href="http://www.iro.umontreal.ca/~lisa/pointeurs/RNNSpokenLanguage2013.pdf" rel="nofollow"&gt;This is one&lt;/a&gt; of the earlier papers.&lt;/p&gt;\n\n&lt;p&gt;Another possibility is using &lt;a href="https://en.wikipedia.org/wiki/Recursive_neural_network" rel="nofollow"&gt;recursive NNs&lt;/a&gt;. This is basically a form of preprocessing in which a text is recursively reduced to a smaller number of word vectors until only one is left - your input, which is supposed to encode the whole text. This makes a lot of sense from a linguistic point of view if your input consists of sentences (which can vary a lot in size), because sentences are structured recursively (For example the word vector for "the man", should be similar to the word vector for "the man who mistook his wife for a hat", because noun phrases act like nouns etc.). Often you can use linguistic information to guide your recursion on the sentence. If you want to go way beyond the wiki article, &lt;a href="http://nlp.stanford.edu/~socherr/thesis.pdf" rel="nofollow"&gt;this is probably a good start&lt;/a&gt;.&lt;/p&gt;\n', u'question': u"&lt;p&gt;As far as I can tell, neural networks have a &lt;strong&gt;fixed number of neurons&lt;/strong&gt; in the input layer.&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;If neural networks are used in a context like for example NLP, sentences or blocks of text of varying sizes are fed to a network. How is the &lt;strong&gt;varying input size&lt;/strong&gt; reconciled with the &lt;strong&gt;fixed size&lt;/strong&gt; of the input layer of the network? In other words: how is such a network made flexible enough to deal with an input that might be anywhere from one word to multiple pages of text?&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;If my assumption of a fixed number of input neurons is wrong and new input neurons are added to/removed from the network to match the input size I don't see how these can ever be trained.&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;I give the example of NLP, but lots of problems have an inherently unpredictable input size, I'm interested in the general approach for dealing with this.&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;edit: For images, it's clear you can up/downsample to a fixed size, but for text this seems to be an impossible approach since adding/removing text changes the meaning of the original input.&lt;/p&gt;&lt;br&gt;"}</str></arr><str name="id">2009</str><long name="_version_">1559279293891084288</long><float name="score">8.645144</float><str name="featureVector">4.322572 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 4.322572 0.0 0.0 0.0 0.4666667 4 0.18232155679395462 8.645144</str></doc><doc><arr name="body"><str>{u'answer': u'&lt;p&gt;&lt;a href="http://www.arcadelearningenvironment.org/wp-content/uploads/2012/07/bellemare13arcade.pdf" rel="nofollow"&gt;Here&lt;/a&gt; is a description of the input to an ALE agent:\nPercept state: A single game screen (frame): a 2D array of 7-bit pixels, 160 pixels wide by 210 pixels high. \nActions: 18 discrete actions defined by the joystick controller&lt;/p&gt;\n\n&lt;p&gt;Regarding VGDL, as far as I can see, the main site associated with it is gvgai.net, which is currently down. The associated API is described &lt;a href="http://julian.togelius.com/Perez20152014.pdf" rel="nofollow"&gt;in this paper&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;Percept state for GVGAI is more structured than for ALE, but the closest correspondence to ALE appears to be an \'Observation grid\', consisting of a 2D array of sprite identifiers.&lt;/p&gt;\n\n&lt;p&gt;Actions: ACTION_NIL, ACTION_UP, ACTION_LEFT, ACTION_DOWN, ACTION_RIGHT and ACTION_USE (stated as \'typical\' values). &lt;/p&gt;\n\n&lt;p&gt;Of the two, it would seem that ALE is more suitable for AGI, because of the more \'free form\' nature of the input.&lt;/p&gt;\n\n&lt;p&gt;However, one of the issues with &lt;em&gt;either&lt;/em&gt; of these approaches is that the set of possible actions is strongly constrained. These domains are therefore \'operationalised\' - the hard task of working out what actions are possible has already been solved for the AI by the API, effectively acting as a bottleneck on the complexity of mapping from input to output.&lt;/p&gt;\n\n&lt;p&gt;A range of alternative game-playing frameworks are listed &lt;a href="http://cig16.image.ece.ntua.gr/competitions/" rel="nofollow"&gt;here&lt;/a&gt; and one alternative (which I personally believe is more useful for AGI purposes) is the &lt;a href="http://atkrye.github.io/IEEE-CIG-Text-Adventurer-Competition/" rel="nofollow"&gt;Artificial Text Adventurer&lt;/a&gt;, in which (at each turn) agent is presented with natural language input describing the scene and must then output a command in natural language. Disclaimer: I am associated with this competition.&lt;/p&gt;\n', u'question': u'&lt;p&gt;For benefits of testing AGI, is using a high-level video game description language (VGDL) gives more reliable and accurate results of general intelligence than using Arcade Learning Environment (ALE)?&lt;/p&gt;&lt;br&gt;'}</str></arr><str name="id">1499</str><long name="_version_">1559279283020496897</long><float name="score">8.163114</float><str name="featureVector">4.081557 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 4.081557 0.0 0.0 0.0 0.33333334 5 0.1541506798272583 8.163114</str></doc><doc><arr name="body"><str>{u'answer': u'&lt;p&gt;The rhetorical point of the Turing Test is that it places the \'test\' for \'humanity\' in &lt;em&gt;observable outcomes&lt;/em&gt;, instead of in &lt;em&gt;internal components&lt;/em&gt;. If you would behave the same in interacting with an AI as you would with a person, how could &lt;em&gt;you&lt;/em&gt; know the difference between them?&lt;/p&gt;\n\n&lt;p&gt;But that doesn\'t mean it\'s reliable, because intelligence has many different components and there are many sorts of intellectual tasks. The Turing Test, in some respects, is about the reaction of people to behavior, which is not at all reliable--remember that many people thought &lt;a href="https://en.wikipedia.org/wiki/ELIZA"&gt;ELIZA&lt;/a&gt;, a very simple chatbot, was an excellent listener and got deeply emotionally involved very quickly. It calls to mind the &lt;a href="https://www.youtube.com/watch?v=dBqhIVyfsRg"&gt;Ikea commercial about throwing out a lamp&lt;/a&gt;, where the emotional attachment comes &lt;em&gt;from the human viewer&lt;/em&gt; (and the music), rather than from the lamp.&lt;/p&gt;\n\n&lt;p&gt;Turing tests for specific economic activities are much more practically interesting--if one can write an AI that replaces an Uber driver, for example, what that will imply is much clearer than if someone can create a conversational chatbot.&lt;/p&gt;\n', u'question': u'&lt;p&gt;The &lt;a href="https://en.wikipedia.org/wiki/Turing_test"&gt;Turing Test&lt;/a&gt; was the first test of artificial intelligence and is now a bit outdated. The &lt;a href="https://en.wikipedia.org/wiki/Turing_test#Total_Turing_test"&gt;Total Turing Test&lt;/a&gt; aims to be a more modern test which requires a much more sophisticated system. What techniques can we use to identify an artificial intelligence (weak AI) and an &lt;a href="https://en.wikipedia.org/wiki/Artificial_general_intelligence"&gt;artificial general intelligence&lt;/a&gt; (strong AI)?&lt;/p&gt;&lt;br&gt;'}</str></arr><str name="id">39</str><long name="_version_">1559279273879011329</long><float name="score">7.9842634</float><str name="featureVector">3.9921317 2.2828054 0.0 2.2828054 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 3.9921317 2.2828054 0.0 2.2828054 0.20000002 6 0.13353139262452263 7.9842634</str></doc><doc><arr name="body"><str>{u'answer': u'&lt;h1&gt;No.&lt;/h1&gt;\n\n&lt;p&gt;In that the question includes "knowingly" which would require that any AI &lt;em&gt;knows&lt;/em&gt; anything. If this is anything like the way humans know things (though interestingly it doesn\'t require &lt;em&gt;actually&lt;/em&gt; knowing things), it would require some sense of individuality, probably self-awareness, possibly some kind of consciousness, the ability to render an opinion and probably some way to test its knowledge. Most of these features only exist, at best, arguably.&lt;/p&gt;\n\n&lt;p&gt;Further, the term "lie" implies a sense of self-interest, an independent understanding of resource flow in a game-theoretic sense, and not trivially, an understanding of whether the other entity in the conversation is lying, in order to make a decision with any degree of accuracy. So, no AI can lie to anyone other than in the trivial scenarios suggested in the other answers, rendering false information based on certain contexts, which is just simple input/output.&lt;/p&gt;\n\n&lt;p&gt;As an experienced software developer, I can attest to the fact that if the objective is to render the correct output based on any input, it\'s actually at least as easy if not much easier to render false information. &lt;/p&gt;\n', u'question': u"&lt;p&gt;AI systems today are very capable machines, and recently the area of Natural Language Processing and Response has been exploding with innovation, as well as the fundamental algorithmic structure of AI machines.&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;I am asking if, given these recent breakthroughs, have any AI systems been developed that are able to (preferably with some measure of success) knowingly lie to humans about facts that it knows?&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;Note, what I'm asking goes beyond the canonical discussions of the Turing Test. I'm asking of machines that can 'understand' facts and then formulate a lie against this fact, perhaps using other facts to produce a believable 'cover-up' as part of the lie.&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;E.G.: CIA supercomputer is stolen by spies and they try to use the computer to do things, but the computer keeps saying it's missing dependencies though it really isn't or gives correct-looking but wrong answers knowingly. Or gives incorrect location of a person, knowing that the person frequents some place but isn't there at the moment. Doesn't have to be this sophisticated, of course.&lt;/p&gt;&lt;br&gt;"}</str></arr><str name="id">1821</str><long name="_version_">1559279288897765376</long><float name="score">7.9271274</float><str name="featureVector">3.9635637 0.038471717 0.0 0.33694336 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 3.9635637 0.038471717 0.0 0.33694336 0.6 7 0.11778303565638346 7.9271274</str></doc><doc><arr name="body"><str>{u'answer': u"&lt;p&gt;I believe AI is rarely used in mainstream apps, but it could be, and I think slowly will be.&lt;/p&gt;\n\n&lt;p&gt;If the information an app's AI must learn arises within the app, from user interaction or error, it'd be smart if the program could log that kind of information and then look for patterns in the logs.  It could profile users to see ehat tasks are done most often, how many steps are needed.  Then when it recognizes that task recurring, it could ask the user if they wanted it to execute a macro that did the following [then it presents then with a list of the steps, allowing them to edit as needed].  Then it executes the 'macro' that it learned from observing the user.&lt;/p&gt;\n\n&lt;p&gt;Another use of AI is error detection, not only in the software, but in user error when the software was used inefficiently, redundantly, or improperly.  If the software were designed such that it was given a set of models of user tasks (like AI plans), it could observe users in the way they achieve known tasks, and offer suggestions or ask for confirmation that imminent unusual outcomes are intended.&lt;/p&gt;\n\n&lt;p&gt;And of course, AI could be used extensively in user interface design, on devices, web sites, or apps.  Some of this, like voice recognition, is entering the mainstream of daily use just now.  As conversations with apps that can add their own data and models of tasks/concepts/domains develop further, the need for AI &lt;em&gt;inside&lt;/em&gt; the app will only grow.&lt;/p&gt;\n\n&lt;p&gt;There are a &lt;em&gt;ton&lt;/em&gt; of ways that AI could be used in apps.  A few of these have started to arise in mobile devices and their apps, usually in fusion of user mobility with external web-based databases (e.g. GPS and maps), but IMO it's been slow.&lt;/p&gt;\n", u'question': u'&lt;p&gt;I\'m curious about Artificial Intelligence. In my everyday job I develop standard applications, like websites with basic functionalities like user subscription, file upload, forms saved in a database... &lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;I mainly know of AI being used in games or robotics fields. But can it be useful in "standard" application development?&lt;/p&gt;&lt;br&gt;'}</str></arr><str name="id">2160</str><long name="_version_">1559279296659324929</long><float name="score">7.5868664</float><str name="featureVector">3.7934332 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 3.7934332 0.0 0.0 0.0 0.26666668 8 0.1053605156578263 7.5868664</str></doc><doc><arr name="body"><str>{u'answer': u'&lt;p&gt;Let\'s use a simple test based on common sense: how often do you see a human being solve problems requiring the use of reason when they\'re unconscious? Yes, you can find instances of geniuses like Ramanujan solving complex problem during or after a dream state, but those involve partial consciousness. You don\'t see guys like Einstein coming up with the theory of relativity while in a coma; the Founding Fathers didn\'t write the Declaration of Independence while sleep-walking; in fact, you can\'t even find instances of housewives putting together their shopping list for the week during deep delta-wave sleep. This is predicated on a hard definition of intelligence, requiring the use of reason; no one says, "That fly is intelligent" or "that squirrel is intelligent" precisely because neither is capable of using reason. This is a very high bar for A.I., but it is the common sense definition used by ordinary people as a matter of practicality, in everyday speech.  Likewise, in practice, everyone assumes consciousness is necessary to the exercise of that kind of intelligence.&lt;/p&gt;\n\n&lt;p&gt;Conversely, we can come up with another common-sense based criterion for judging objections to this argument, particularly the solipsist one, based on 3 elements: 1) practicality; 2) the effect the objections have on those who hold them sincerely; and 3) the effect that actions based on those beliefs have on others. &lt;strong&gt;It\'s going to take me several paragraphs to make this case, but the length is necessary if I want to make the case in a complete, thorough fashion&lt;/strong&gt;. It is true that we cannot prove that another human being possesses consciousness, if our standard is absolute proof. We cannot, in fact, provide absolute proof for anything; there\'s always room for some objection, no matter how ridiculous or trifling. As some philosophers have pointed out, perhaps all of reality as we know it is just a dream, or the product of some long, involved conspiracy like the plot of the Jim Carrey movie The Truman Show. The key to meeting such objections is that they require an infinite regress of increasingly untenable objections, whose likelihood plunges with each additional step required to justify such unreasonable doubts; I\'ve always wondered if we could come up with a "Ridiculousness Metric" for Machine Learning based on the cardinality of such objections (or the pickiness of fuzzy sets). If we were to allow critics to stick their foot in the door with all manner of unreasonable objections, it would be impossible to close any debate. The human race would be paralyzed in inaction because nothing would be decidable; but as the rock band Rush once pointed out, "If you choose not to decide, you still have made a choice." At some point we must apply a test to decide such things, even in the absence of absolute proof; refusal to apply a test also constitutes a choice. Settling an argument of this kind is like a game  of the Chinese game Go - once the other player\'s surrounded and has no more moves left to make, the game is over; if a person\'s evidence has debunked and they have no further justifications left, then we can conclude that they\'re acting unreasonably. There are people running around claiming the Holocaust never happened, or the Flat Earth Society, etc., but their existence shouldn\'t and doesn\'t stop us from taking action contrary to their ideas. We can debunk the objections of cranks like the Flat Earth Society beyond a reasonable doubt because in the end, they simply can\'t answer all of our rebuttals. I\u2019m glad that qualia and Philosophical Zombies were brought up because they make for interesting conversation and food for thought, but solipsism is acted upon as rarely as the ideas of the Flat Earth Society precisely because the incomplete evidence we &lt;strong&gt;do&lt;/strong&gt; have runs against it.&lt;/p&gt;\n\n&lt;p&gt;As G.K. Chesterton (a.k.a. "The Apostle of Common Sense") points out in his classic &lt;a href="http://www.gutenberg.org/ebooks/130" rel="nofollow"&gt;Orthodoxy&lt;/a&gt;, radical doubt of the kind many classical philosophers preached is not a path to wisdom but to madness; once we go beyond a reasonable doubt, we end up acting unreasonably. He says that in the absence of absolute proof we can fall back on another secondary form of evidence: whether a person\'s philosophy leads a man to Hanwell, the infamous British mental institution. Chesterton makes a good case that when people actually act on ideas like solipsism (rather than merely debating them in a pedantic manner in an ivy-covered classroom) they go mad The Philosophical Zombie argument is close to solipsism, which is actually one of the diagnostic criteria for certain forms of schizophrenia. The dehumanization that occurs when radical doubt is applied to qualia is also intimately tied in with sociopathic behavior, Although GKC does not cite his scary example directly, Rene Descartes was himself living proof. He was a brilliant mathematician who is still cheered for doubting all except his own existence, with the famous maxim "I think, therefore I am." But Descartes also used to carry a mannequin of his dead sister with him to European cafes, where he could be seen chatting with it. The gist of all this is that we can judge the worth of an idea by how it affects the well-being of the believer, or by how they in turn affect others through ethical choices based on those beliefs. When people actually act on radical doubt of the kind expressed in solipsism and denial of common qualia, it often has a bad effect on them and others they come in contact with.&lt;/p&gt;\n\n&lt;p&gt;In a roundabout way, the A.I. community also faces a quite serious risk - perhaps a permanent temptation - towards making the opposite mistake, of ascribing common qualia, consciousness and the like to its Machine Learning products without adequate proof. I recently heard a case made on shockingly bad logical grounds by well-respected academics to the effect that plants possess "intelligence," based on really weak definitions and clear confusion with self-organization. We cannot provide absolute proof that a rock doesn\'t have intelligence, which amounts to the old problem of disproving a negative. Thankfully, few men actually act on such beliefs at present, because when they do, they end up losing their minds. If we take such arguments seriously, we might see laws passed to protect the kind of Pet Rocks that were popular in the \'70s (I\'m still upset that mine was stolen LOL). It would be a lot easier, however, to make the same mistake of ascribing consciousness, intelligence and other such qualities to a state-of-the-art machine, because of wishful thinking, hubris, the lofty credentials of the inventors, the influence of science fiction and the modern love affair with technology. In the future, I have little doubt that we\'ll have Cargo Cult of A.I. - perhaps legally protected like some kind of endangered species, with civil rights, but having no more consciousness, soul or actual intelligence than a rock. Don\'t quote me on this, but I believe Rod Serling once wrote a story to this effect.&lt;/p&gt;\n\n&lt;p&gt;The best way to avoid this fate is to stick to the common sense interpretations and definitions of these things, which we keep backing away from in large part because they set a very high bar for A.I. that we may never be able to surpass in our lifetimes, if ever. Perhaps A.I. isn\'t even logically possible, at any level of technology; I recall a few proofs that can be interpreted to that effect. Those high but reasonable standards may be increasingly difficult to stick to if Chesterton and colleagues like Hilaire Belloc and Arnold Lunn were correct in their assessment that the use of reason has actually been breaking down in Western civilization, at least as far back as the Enlightenment; Lunn\'s 1931 book The Flight from Reason is a classic in this regard and has yet to be rebutted.  This historical trend is a broad topic in and of itself - but suffice it to say that the denial of reason and obsession with technology are both directly relevant in obvious ways to the field of A.I. If the Flight from Reason is still under way, then we will be increasingly tempted to resort to feckless, facile objections in order to demote the use of reason and indispensable qualities like consciousness in our definitions of A.I., but come up with increasingly weak criteria for proving it; simultaneously, our technology will continue to improve, thereby boosting the "Artificial" side of Artificial Intelligence.&lt;/p&gt;\n\n&lt;p&gt;Don\'t get me wrong: if I didn\'t think we can do some really exciting things with A.I., I wouldn\'t be here. But most of them can be achieved without ever replicating actual human intelligence, by solving whole classes of tangential problems that are difficult for humans to think about, but which do not require consciousness or the use of reason that marks human intelligence. The image recognition capabilities of convolutional neural nets are one example, for instance; if we want human intelligence, we can always manufacture it through the easiest, most economical and time-tested way, by having babies. Perhaps these tangential forms of A.I. should be enough for us for now. We cannot inject the use of reason into our machines if we do not possess enough of it ourselves to decide whether reason is necessary for A.I., or even to discern what it consists of. We can\'t engineer or deprecate consciousness for A.I. till we\'re conscious of its significance. I\'d wager, however, that everyone reading this thread and weighing intelligent responses is doing so in a conscious state. That in and of itself ought to answer our question satisfactorily for now.&lt;/p&gt;\n', u'question': u'&lt;p&gt;Consciousness &lt;a href="http://www.iep.utm.edu/consciou/"&gt;is challenging to define&lt;/a&gt;, but for this question let\'s define it as "actually experiencing sensory input as opposed to just putting a bunch of data through an inanimate machine." Humans, of course, have minds; for normal computers, all the things they "see" are just more data. One could alternatively say that humans are &lt;a href="http://philosophy.stackexchange.com/a/4687"&gt;sentient&lt;/a&gt;, while traditional computers are not.&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;Setting aside the question of whether it\'s possible to build a sentient machine, does it actually make a difference if an AI is sentient or not? In other words, are there are tasks that are made impossible - not just more difficult - by a lack of sentience?&lt;/p&gt;&lt;br&gt;'}</str></arr><str name="id">1919</str><long name="_version_">1559279291488796672</long><float name="score">7.5235114</float><str name="featureVector">3.7617557 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 3.7617557 0.0 0.0 0.0 0.6 9 0.09531017980432487 7.5235114</str></doc></result>
</response>

Command:
curl -k -s  -u 55729e6b-2d18-46f8-b6a0-3e87cdd80797:ANlCCDTtOHr4 -d "q=<p>I'm trying to make a conversational chatbot, so the user inputs are quite wide ranging - beyond just "turn lights on". I want to detect the category of the user intents from their inputs and prepare responses.</p><br><br><p>I've looked at MS' Luis and api.ai and the intents require a lot of training. Can people suggest other techniques for untrained intent detection?</p><br><br><p>For example if the user says "Pasta is my favorite dish to cook" then detect "intent preference entity pasta" - then I can gradually build up responses to different categories of inputs.</p><br><br><p>Perhaps the crowd-sourced intents that wit.ai (facebook) has access to could do this but I'm not sure if all end-users have access to those models.</p><br>&gt=&generateHeader=false&rows=10&returnRSInput=true&wt=json" "https://gateway.watsonplatform.net/retrieve-and-rank/api/v1/solr_clusters/sc5352d79e_c165_44a6_97ca_8384501d30dd/solr/AI/fcselect"
Response:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader"><int name="status">0</int><int name="QTime">10</int></lst><result name="response" numFound="749" start="0" maxScore="13.562774"><doc><arr name="body"><str>{u'answer': u'&lt;p&gt;I like your choice of "induce" instead of "produce," because the delusions came from the users. This means the answer has to do mostly with human psychology; people come equipped with lots of mental machinery specialized for dealing with other humans and not very much mental machinery specialized for dealing with software. So ELIZA behaved in ways that some people classified it as a person and behaved accordingly, and others didn\'t.&lt;/p&gt;\n\n&lt;p&gt;What features will trip up a person\'s internal person classification system seem like they vary heavily from person to person, and also with experience and familiarity. Going into more detail is, as mentioned in the comments, more appropriate for sites specializing on the human side of the keyboard.&lt;/p&gt;\n', u'question': u'&lt;p&gt;&lt;a href="http://www.alicebot.org/articles/wallace/eliza.html" rel="nofollow"&gt;From Eliza to A.L.I.C.E.&lt;/a&gt;:&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote&gt;&lt;br&gt;  &lt;p&gt;Weizenbaum tells us that he was shocked by the experience of releasing ELIZA (also known as "Doctor") to the nontechnical staff at the MIT AI Lab. Secretaries and nontechnical administrative staff thought the machine was a "real" therapist, and spent hours revealing their personal problems to the program. When Weizenbaum informed his secretary that he, of course, had access to the logs of all the conversations, she reacted with outrage at this invasion of her privacy. Weizenbaum was shocked by this and similar incidents to find that such a simple program could so easily deceive a naive user into revealing personal information.&lt;/p&gt;&lt;br&gt;&lt;/blockquote&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;Wikipedia\'s article on the &lt;a href="https://en.wikipedia.org/wiki/ELIZA_effect" rel="nofollow"&gt;"ELIZA Effect"&lt;/a&gt;:&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote&gt;&lt;br&gt;  &lt;p&gt;Though designed strictly as a mechanism to support "natural language conversation" with a computer, ELIZA\'s DOCTOR script was found to be surprisingly successful in eliciting emotional responses from users who, in the course of interacting with the program, began to ascribe understanding and motivation to the program\'s output. As Weizenbaum later wrote, &lt;strong&gt;"I had not realized ... that extremely short exposures to a relatively simple computer program could induce powerful delusional thinking in quite normal people."&lt;/strong&gt; Indeed, ELIZA\'s code had not been designed to evoke this reaction in the first place. Upon observation, researchers discovered users unconsciously assuming ELIZA\'s questions implied interest and emotional involvement in the topics discussed, &lt;em&gt;even when they consciously knew that ELIZA did not simulate emotion.&lt;/em&gt;&lt;/p&gt;&lt;br&gt;&lt;/blockquote&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;ELIZA, despite its simplicity, was incredibly successful at its task of tricking other human beings. Even those who knew ELIZA was a bot would still talk to it. Obviously, ELIZA served as an inspiration for various other, more intelligent chatbots, such as &lt;a href="http://www.nytimes.com/2015/08/04/science/for-sympathetic-ear-more-chinese-turn-to-smartphone-program.html?_r=0" rel="nofollow"&gt;Xiaoice&lt;/a&gt;. But I would like to know what &lt;em&gt;exactly&lt;/em&gt; led to such a simple program like ELIZA to be so successful in the first place.&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;This is very useful knowledge for a programmer since a simple program is one that would be easily maintainable.&lt;/p&gt;&lt;br&gt;'}</str></arr><str name="id">1717</str><long name="_version_">1559279286668492800</long><float name="score">13.562774</float><str name="featureVector">6.781387 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 6.781387 0.0 0.0 0.0 0.4666667 0 0.6931471805599453 13.562774</str></doc><doc><arr name="body"><str>{u'answer': u'&lt;p&gt;Shane Legg and Marcus Hutter &lt;a href="http://www.vetta.org/documents/42.pdf" rel="nofollow"&gt;proposed one&lt;/a&gt; in 2006. The main descriptive quotes (see the paper for the actual formula):&lt;/p&gt;\n\n&lt;blockquote&gt;\n  &lt;p&gt;Intelligence measures an agent\u2019s general ability to achieve goals in a wide range of environments&lt;/p&gt;\n  \n  &lt;p&gt;...&lt;/p&gt;\n  \n  &lt;p&gt;It is clear by construction that universal intelligence measures the general ability of an agent to perform well in a very wide range of environments, as required by our informal definition of intelligence given earlier. The definition places no restrictions on the internal workings of the agent; it only requires that the agent is capable of generating output and receiving input which includes a reward signal.&lt;/p&gt;\n&lt;/blockquote&gt;\n', u'question': u"&lt;p&gt;We can measure the power of the machine with the number of operation per second or the frequency of the processor. But does units similar of IQ for humans exist for a AI?&lt;br/&gt;&lt;br&gt;I'm asking for a unit which can give countable result so something different from a Turing Test which only give a binary result.&lt;/p&gt;&lt;br&gt;"}</str></arr><str name="id">1413</str><long name="_version_">1559279281097408513</long><float name="score">9.453717</float><str name="featureVector">4.7268586 2.726194 0.0 2.726194 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 4.7268586 2.726194 0.0 2.726194 0.40000004 1 0.4054651081081644 9.453717</str></doc><doc><arr name="body"><str>{u'answer': u'&lt;p&gt;The terms &lt;em&gt;strong&lt;/em&gt; and &lt;em&gt;weak&lt;/em&gt; don\'t actually refer to processing, or optimization power, or any interpretation leading to "strong AI" being &lt;em&gt;stronger&lt;/em&gt; than "weak AI". It holds conveniently in practice, but the terms come from elsewhere. In 1980, &lt;a href="https://en.wikipedia.org/wiki/John_Searle"&gt;John Searle&lt;/a&gt; coined the following statements:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;AI hypothesis, strong form: an AI system can &lt;em&gt;think&lt;/em&gt; and have a &lt;em&gt;mind&lt;/em&gt; (in the philosophical definition of the term);&lt;/li&gt;\n&lt;li&gt;AI hypothesis, weak form: an AI system can only &lt;em&gt;act&lt;/em&gt; like it thinks and has a mind.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;So &lt;em&gt;strong AI&lt;/em&gt; is a shortcut for an AI systems that verifies the &lt;em&gt;strong AI hypothesis&lt;/em&gt;. Similarly, for the weak form. The terms have then evolved: strong AI refers to AI that performs as well as humans (who have minds), weak AI refers to AI that doesn\'t.&lt;/p&gt;\n\n&lt;p&gt;The problem with these definitions is that they\'re fuzzy. For example, &lt;a href="https://en.wikipedia.org/wiki/AlphaGo"&gt;AlphaGo&lt;/a&gt; is an example of weak AI, but is "strong" by Go-playing standards. A hypothetical AI replicating a human baby would be a strong AI, while being "weak" at most tasks.&lt;/p&gt;\n\n&lt;p&gt;Other terms exist: &lt;a href="https://en.wikipedia.org/wiki/Artificial_general_intelligence"&gt;Artificial General Intelligence&lt;/a&gt; (AGI), which has cross-domain capability (like humans), can learn from a wide range of experiences (like humans), among other features. Artificial Narrow Intelligence refers to systems bound to a certain range of tasks (where they may nevertheless have superhuman ability), lacking capacity to significantly improve themselves.&lt;/p&gt;\n\n&lt;p&gt;Beyond AGI, we find Artificial Superintelligence (ASI), based on the idea that a system with the capabilities of an AGI, without the physical limitations of humans would learn and improve far beyond human level.&lt;/p&gt;\n', u'question': u"&lt;p&gt;I've heard the terms strong-AI and weak-AI used.  Are these well defined terms or subjective ones?  How are they generally defined?&lt;/p&gt;&lt;br&gt;"}</str></arr><str name="id">141</str><long name="_version_">1559279276103041024</long><float name="score">9.090328</float><str name="featureVector">4.545164 1.5801773 0.0 1.5801773 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 4.545164 1.5801773 0.0 1.5801773 0.33333334 2 0.2876820724517809 9.090328</str></doc><doc><arr name="body"><str>{u'answer': u'&lt;p&gt;The &lt;a href="https://en.wikipedia.org/wiki/Technological_singularity" rel="nofollow"&gt;technological singularity&lt;/a&gt; is a theoretical point in time at which a self-improving &lt;a href="https://en.wikipedia.org/wiki/Artificial_general_intelligence" rel="nofollow"&gt;artificial general intelligence&lt;/a&gt; becomes able to understand and manipulate concepts outside of the human brain\'s range, that is, the moment when it can understand things humans, by biological design, can\'t.&lt;/p&gt;\n\n&lt;p&gt;The fuzziness about the singularity comes from the fact that, from the singularity onwards, history is effectively unpredictable. Humankind would be unable to predict any future events, or explain any present events, as science itself becomes incapable of describing machine-triggered events. Essentially, machines would think of us the same way we think of ants. Thus, we can make no predictions past the singularity. Furthermore, as a logical consequence, we\'d be unable to define the point at which the singularity may occur at all, or even recognize it when it happens.&lt;/p&gt;\n\n&lt;p&gt;However, in order for the singularity to take place, AGI needs to be developed, and &lt;a href="https://en.wikipedia.org/wiki/Artificial_general_intelligence#Feasibility" rel="nofollow"&gt;whether that is possible is quite a hot debate&lt;/a&gt; right now. Moreover, an algorithm that creates superhuman intelligence out of bits and bytes would have to be designed. By definition, a human programmer wouldn\'t be able to do such a thing, as his/her brain would need to be able to comprehend concepts beyond its range. There is also the argument that an intelligence explosion (the mechanism by which a technological singularity would theoretically be formed) would be impossible due to the difficulty of the design challenge of making itself more intelligent, getting larger proportionally to its intelligence, and that the difficulty of the design itself may overtake the intelligence required to solve said challenge (last point credit to &lt;a href="http://ai.stackexchange.com/users/47/god-of-llamas"&gt;god of llamas&lt;/a&gt; in the comments).&lt;/p&gt;\n\n&lt;p&gt;Also, there are related theories involving machines taking over humankind and all of that sci-fi narrative. However, that\'s unlikely to happen, if &lt;a href="https://en.wikipedia.org/wiki/Three_Laws_of_Robotics" rel="nofollow"&gt;Asimov\'s laws&lt;/a&gt; are followed appropriately. Even if Asimov\'s laws were not enough, a series of constraints would still be necessary in order to avoid the misuse of AGI by misintentioned individuals, and Asimov\'s laws are the nearest we have to that.&lt;/p&gt;\n', u'question': u"&lt;p&gt;I've heard the idea of the technological singularity, what is it and how does it relate to Artificial Intelligence?  Is this the theoretical point where Artificial Intelligence machines have progressed to the point where they grow and learn on their own beyond what humans can do and their growth takes off?  How would we know when we reach this point?&lt;/p&gt;&lt;br&gt;"}</str></arr><str name="id">45</str><long name="_version_">1559279274281664512</long><float name="score">8.848466</float><str name="featureVector">4.424233 0.67088574 0.0 0.67088574 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 4.424233 0.67088574 0.0 0.67088574 0.40000004 3 0.22314355131420976 8.848466</str></doc><doc><arr name="body"><str>{u'answer': u'&lt;p&gt;Three possibilities come to mind:&lt;/p&gt;\n\n&lt;p&gt;The easiest is zero padding. Basically you take a rather big input size and just add zeroes if your concrete input is too small. Of course this is pretty limited and certainly not useful if your input ranges from a few words to full texts.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Recurrent_neural_network" rel="nofollow"&gt;RNNs&lt;/a&gt; are a very natural NN to chose if you have texts of varying size as input. You input words as word vectors just one after another and the internal state of the RNN is supposed to encode the meaning of the full string of words. &lt;a href="http://www.iro.umontreal.ca/~lisa/pointeurs/RNNSpokenLanguage2013.pdf" rel="nofollow"&gt;This is one&lt;/a&gt; of the earlier papers.&lt;/p&gt;\n\n&lt;p&gt;Another possibility is using &lt;a href="https://en.wikipedia.org/wiki/Recursive_neural_network" rel="nofollow"&gt;recursive NNs&lt;/a&gt;. This is basically a form of preprocessing in which a text is recursively reduced to a smaller number of word vectors until only one is left - your input, which is supposed to encode the whole text. This makes a lot of sense from a linguistic point of view if your input consists of sentences (which can vary a lot in size), because sentences are structured recursively (For example the word vector for "the man", should be similar to the word vector for "the man who mistook his wife for a hat", because noun phrases act like nouns etc.). Often you can use linguistic information to guide your recursion on the sentence. If you want to go way beyond the wiki article, &lt;a href="http://nlp.stanford.edu/~socherr/thesis.pdf" rel="nofollow"&gt;this is probably a good start&lt;/a&gt;.&lt;/p&gt;\n', u'question': u"&lt;p&gt;As far as I can tell, neural networks have a &lt;strong&gt;fixed number of neurons&lt;/strong&gt; in the input layer.&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;If neural networks are used in a context like for example NLP, sentences or blocks of text of varying sizes are fed to a network. How is the &lt;strong&gt;varying input size&lt;/strong&gt; reconciled with the &lt;strong&gt;fixed size&lt;/strong&gt; of the input layer of the network? In other words: how is such a network made flexible enough to deal with an input that might be anywhere from one word to multiple pages of text?&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;If my assumption of a fixed number of input neurons is wrong and new input neurons are added to/removed from the network to match the input size I don't see how these can ever be trained.&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;I give the example of NLP, but lots of problems have an inherently unpredictable input size, I'm interested in the general approach for dealing with this.&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;edit: For images, it's clear you can up/downsample to a fixed size, but for text this seems to be an impossible approach since adding/removing text changes the meaning of the original input.&lt;/p&gt;&lt;br&gt;"}</str></arr><str name="id">2009</str><long name="_version_">1559279293891084288</long><float name="score">8.645144</float><str name="featureVector">4.322572 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 4.322572 0.0 0.0 0.0 0.4666667 4 0.18232155679395462 8.645144</str></doc><doc><arr name="body"><str>{u'answer': u'&lt;p&gt;&lt;a href="http://www.arcadelearningenvironment.org/wp-content/uploads/2012/07/bellemare13arcade.pdf" rel="nofollow"&gt;Here&lt;/a&gt; is a description of the input to an ALE agent:\nPercept state: A single game screen (frame): a 2D array of 7-bit pixels, 160 pixels wide by 210 pixels high. \nActions: 18 discrete actions defined by the joystick controller&lt;/p&gt;\n\n&lt;p&gt;Regarding VGDL, as far as I can see, the main site associated with it is gvgai.net, which is currently down. The associated API is described &lt;a href="http://julian.togelius.com/Perez20152014.pdf" rel="nofollow"&gt;in this paper&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;Percept state for GVGAI is more structured than for ALE, but the closest correspondence to ALE appears to be an \'Observation grid\', consisting of a 2D array of sprite identifiers.&lt;/p&gt;\n\n&lt;p&gt;Actions: ACTION_NIL, ACTION_UP, ACTION_LEFT, ACTION_DOWN, ACTION_RIGHT and ACTION_USE (stated as \'typical\' values). &lt;/p&gt;\n\n&lt;p&gt;Of the two, it would seem that ALE is more suitable for AGI, because of the more \'free form\' nature of the input.&lt;/p&gt;\n\n&lt;p&gt;However, one of the issues with &lt;em&gt;either&lt;/em&gt; of these approaches is that the set of possible actions is strongly constrained. These domains are therefore \'operationalised\' - the hard task of working out what actions are possible has already been solved for the AI by the API, effectively acting as a bottleneck on the complexity of mapping from input to output.&lt;/p&gt;\n\n&lt;p&gt;A range of alternative game-playing frameworks are listed &lt;a href="http://cig16.image.ece.ntua.gr/competitions/" rel="nofollow"&gt;here&lt;/a&gt; and one alternative (which I personally believe is more useful for AGI purposes) is the &lt;a href="http://atkrye.github.io/IEEE-CIG-Text-Adventurer-Competition/" rel="nofollow"&gt;Artificial Text Adventurer&lt;/a&gt;, in which (at each turn) agent is presented with natural language input describing the scene and must then output a command in natural language. Disclaimer: I am associated with this competition.&lt;/p&gt;\n', u'question': u'&lt;p&gt;For benefits of testing AGI, is using a high-level video game description language (VGDL) gives more reliable and accurate results of general intelligence than using Arcade Learning Environment (ALE)?&lt;/p&gt;&lt;br&gt;'}</str></arr><str name="id">1499</str><long name="_version_">1559279283020496897</long><float name="score">8.163114</float><str name="featureVector">4.081557 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 4.081557 0.0 0.0 0.0 0.33333334 5 0.1541506798272583 8.163114</str></doc><doc><arr name="body"><str>{u'answer': u'&lt;p&gt;The rhetorical point of the Turing Test is that it places the \'test\' for \'humanity\' in &lt;em&gt;observable outcomes&lt;/em&gt;, instead of in &lt;em&gt;internal components&lt;/em&gt;. If you would behave the same in interacting with an AI as you would with a person, how could &lt;em&gt;you&lt;/em&gt; know the difference between them?&lt;/p&gt;\n\n&lt;p&gt;But that doesn\'t mean it\'s reliable, because intelligence has many different components and there are many sorts of intellectual tasks. The Turing Test, in some respects, is about the reaction of people to behavior, which is not at all reliable--remember that many people thought &lt;a href="https://en.wikipedia.org/wiki/ELIZA"&gt;ELIZA&lt;/a&gt;, a very simple chatbot, was an excellent listener and got deeply emotionally involved very quickly. It calls to mind the &lt;a href="https://www.youtube.com/watch?v=dBqhIVyfsRg"&gt;Ikea commercial about throwing out a lamp&lt;/a&gt;, where the emotional attachment comes &lt;em&gt;from the human viewer&lt;/em&gt; (and the music), rather than from the lamp.&lt;/p&gt;\n\n&lt;p&gt;Turing tests for specific economic activities are much more practically interesting--if one can write an AI that replaces an Uber driver, for example, what that will imply is much clearer than if someone can create a conversational chatbot.&lt;/p&gt;\n', u'question': u'&lt;p&gt;The &lt;a href="https://en.wikipedia.org/wiki/Turing_test"&gt;Turing Test&lt;/a&gt; was the first test of artificial intelligence and is now a bit outdated. The &lt;a href="https://en.wikipedia.org/wiki/Turing_test#Total_Turing_test"&gt;Total Turing Test&lt;/a&gt; aims to be a more modern test which requires a much more sophisticated system. What techniques can we use to identify an artificial intelligence (weak AI) and an &lt;a href="https://en.wikipedia.org/wiki/Artificial_general_intelligence"&gt;artificial general intelligence&lt;/a&gt; (strong AI)?&lt;/p&gt;&lt;br&gt;'}</str></arr><str name="id">39</str><long name="_version_">1559279273879011329</long><float name="score">7.9842634</float><str name="featureVector">3.9921317 2.2828054 0.0 2.2828054 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 3.9921317 2.2828054 0.0 2.2828054 0.20000002 6 0.13353139262452263 7.9842634</str></doc><doc><arr name="body"><str>{u'answer': u'&lt;h1&gt;No.&lt;/h1&gt;\n\n&lt;p&gt;In that the question includes "knowingly" which would require that any AI &lt;em&gt;knows&lt;/em&gt; anything. If this is anything like the way humans know things (though interestingly it doesn\'t require &lt;em&gt;actually&lt;/em&gt; knowing things), it would require some sense of individuality, probably self-awareness, possibly some kind of consciousness, the ability to render an opinion and probably some way to test its knowledge. Most of these features only exist, at best, arguably.&lt;/p&gt;\n\n&lt;p&gt;Further, the term "lie" implies a sense of self-interest, an independent understanding of resource flow in a game-theoretic sense, and not trivially, an understanding of whether the other entity in the conversation is lying, in order to make a decision with any degree of accuracy. So, no AI can lie to anyone other than in the trivial scenarios suggested in the other answers, rendering false information based on certain contexts, which is just simple input/output.&lt;/p&gt;\n\n&lt;p&gt;As an experienced software developer, I can attest to the fact that if the objective is to render the correct output based on any input, it\'s actually at least as easy if not much easier to render false information. &lt;/p&gt;\n', u'question': u"&lt;p&gt;AI systems today are very capable machines, and recently the area of Natural Language Processing and Response has been exploding with innovation, as well as the fundamental algorithmic structure of AI machines.&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;I am asking if, given these recent breakthroughs, have any AI systems been developed that are able to (preferably with some measure of success) knowingly lie to humans about facts that it knows?&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;Note, what I'm asking goes beyond the canonical discussions of the Turing Test. I'm asking of machines that can 'understand' facts and then formulate a lie against this fact, perhaps using other facts to produce a believable 'cover-up' as part of the lie.&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;E.G.: CIA supercomputer is stolen by spies and they try to use the computer to do things, but the computer keeps saying it's missing dependencies though it really isn't or gives correct-looking but wrong answers knowingly. Or gives incorrect location of a person, knowing that the person frequents some place but isn't there at the moment. Doesn't have to be this sophisticated, of course.&lt;/p&gt;&lt;br&gt;"}</str></arr><str name="id">1821</str><long name="_version_">1559279288897765376</long><float name="score">7.9271274</float><str name="featureVector">3.9635637 0.038471717 0.0 0.33694336 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 3.9635637 0.038471717 0.0 0.33694336 0.6 7 0.11778303565638346 7.9271274</str></doc><doc><arr name="body"><str>{u'answer': u"&lt;p&gt;I believe AI is rarely used in mainstream apps, but it could be, and I think slowly will be.&lt;/p&gt;\n\n&lt;p&gt;If the information an app's AI must learn arises within the app, from user interaction or error, it'd be smart if the program could log that kind of information and then look for patterns in the logs.  It could profile users to see ehat tasks are done most often, how many steps are needed.  Then when it recognizes that task recurring, it could ask the user if they wanted it to execute a macro that did the following [then it presents then with a list of the steps, allowing them to edit as needed].  Then it executes the 'macro' that it learned from observing the user.&lt;/p&gt;\n\n&lt;p&gt;Another use of AI is error detection, not only in the software, but in user error when the software was used inefficiently, redundantly, or improperly.  If the software were designed such that it was given a set of models of user tasks (like AI plans), it could observe users in the way they achieve known tasks, and offer suggestions or ask for confirmation that imminent unusual outcomes are intended.&lt;/p&gt;\n\n&lt;p&gt;And of course, AI could be used extensively in user interface design, on devices, web sites, or apps.  Some of this, like voice recognition, is entering the mainstream of daily use just now.  As conversations with apps that can add their own data and models of tasks/concepts/domains develop further, the need for AI &lt;em&gt;inside&lt;/em&gt; the app will only grow.&lt;/p&gt;\n\n&lt;p&gt;There are a &lt;em&gt;ton&lt;/em&gt; of ways that AI could be used in apps.  A few of these have started to arise in mobile devices and their apps, usually in fusion of user mobility with external web-based databases (e.g. GPS and maps), but IMO it's been slow.&lt;/p&gt;\n", u'question': u'&lt;p&gt;I\'m curious about Artificial Intelligence. In my everyday job I develop standard applications, like websites with basic functionalities like user subscription, file upload, forms saved in a database... &lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;I mainly know of AI being used in games or robotics fields. But can it be useful in "standard" application development?&lt;/p&gt;&lt;br&gt;'}</str></arr><str name="id">2160</str><long name="_version_">1559279296659324929</long><float name="score">7.5868664</float><str name="featureVector">3.7934332 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 3.7934332 0.0 0.0 0.0 0.26666668 8 0.1053605156578263 7.5868664</str></doc><doc><arr name="body"><str>{u'answer': u'&lt;p&gt;Let\'s use a simple test based on common sense: how often do you see a human being solve problems requiring the use of reason when they\'re unconscious? Yes, you can find instances of geniuses like Ramanujan solving complex problem during or after a dream state, but those involve partial consciousness. You don\'t see guys like Einstein coming up with the theory of relativity while in a coma; the Founding Fathers didn\'t write the Declaration of Independence while sleep-walking; in fact, you can\'t even find instances of housewives putting together their shopping list for the week during deep delta-wave sleep. This is predicated on a hard definition of intelligence, requiring the use of reason; no one says, "That fly is intelligent" or "that squirrel is intelligent" precisely because neither is capable of using reason. This is a very high bar for A.I., but it is the common sense definition used by ordinary people as a matter of practicality, in everyday speech.  Likewise, in practice, everyone assumes consciousness is necessary to the exercise of that kind of intelligence.&lt;/p&gt;\n\n&lt;p&gt;Conversely, we can come up with another common-sense based criterion for judging objections to this argument, particularly the solipsist one, based on 3 elements: 1) practicality; 2) the effect the objections have on those who hold them sincerely; and 3) the effect that actions based on those beliefs have on others. &lt;strong&gt;It\'s going to take me several paragraphs to make this case, but the length is necessary if I want to make the case in a complete, thorough fashion&lt;/strong&gt;. It is true that we cannot prove that another human being possesses consciousness, if our standard is absolute proof. We cannot, in fact, provide absolute proof for anything; there\'s always room for some objection, no matter how ridiculous or trifling. As some philosophers have pointed out, perhaps all of reality as we know it is just a dream, or the product of some long, involved conspiracy like the plot of the Jim Carrey movie The Truman Show. The key to meeting such objections is that they require an infinite regress of increasingly untenable objections, whose likelihood plunges with each additional step required to justify such unreasonable doubts; I\'ve always wondered if we could come up with a "Ridiculousness Metric" for Machine Learning based on the cardinality of such objections (or the pickiness of fuzzy sets). If we were to allow critics to stick their foot in the door with all manner of unreasonable objections, it would be impossible to close any debate. The human race would be paralyzed in inaction because nothing would be decidable; but as the rock band Rush once pointed out, "If you choose not to decide, you still have made a choice." At some point we must apply a test to decide such things, even in the absence of absolute proof; refusal to apply a test also constitutes a choice. Settling an argument of this kind is like a game  of the Chinese game Go - once the other player\'s surrounded and has no more moves left to make, the game is over; if a person\'s evidence has debunked and they have no further justifications left, then we can conclude that they\'re acting unreasonably. There are people running around claiming the Holocaust never happened, or the Flat Earth Society, etc., but their existence shouldn\'t and doesn\'t stop us from taking action contrary to their ideas. We can debunk the objections of cranks like the Flat Earth Society beyond a reasonable doubt because in the end, they simply can\'t answer all of our rebuttals. I\u2019m glad that qualia and Philosophical Zombies were brought up because they make for interesting conversation and food for thought, but solipsism is acted upon as rarely as the ideas of the Flat Earth Society precisely because the incomplete evidence we &lt;strong&gt;do&lt;/strong&gt; have runs against it.&lt;/p&gt;\n\n&lt;p&gt;As G.K. Chesterton (a.k.a. "The Apostle of Common Sense") points out in his classic &lt;a href="http://www.gutenberg.org/ebooks/130" rel="nofollow"&gt;Orthodoxy&lt;/a&gt;, radical doubt of the kind many classical philosophers preached is not a path to wisdom but to madness; once we go beyond a reasonable doubt, we end up acting unreasonably. He says that in the absence of absolute proof we can fall back on another secondary form of evidence: whether a person\'s philosophy leads a man to Hanwell, the infamous British mental institution. Chesterton makes a good case that when people actually act on ideas like solipsism (rather than merely debating them in a pedantic manner in an ivy-covered classroom) they go mad The Philosophical Zombie argument is close to solipsism, which is actually one of the diagnostic criteria for certain forms of schizophrenia. The dehumanization that occurs when radical doubt is applied to qualia is also intimately tied in with sociopathic behavior, Although GKC does not cite his scary example directly, Rene Descartes was himself living proof. He was a brilliant mathematician who is still cheered for doubting all except his own existence, with the famous maxim "I think, therefore I am." But Descartes also used to carry a mannequin of his dead sister with him to European cafes, where he could be seen chatting with it. The gist of all this is that we can judge the worth of an idea by how it affects the well-being of the believer, or by how they in turn affect others through ethical choices based on those beliefs. When people actually act on radical doubt of the kind expressed in solipsism and denial of common qualia, it often has a bad effect on them and others they come in contact with.&lt;/p&gt;\n\n&lt;p&gt;In a roundabout way, the A.I. community also faces a quite serious risk - perhaps a permanent temptation - towards making the opposite mistake, of ascribing common qualia, consciousness and the like to its Machine Learning products without adequate proof. I recently heard a case made on shockingly bad logical grounds by well-respected academics to the effect that plants possess "intelligence," based on really weak definitions and clear confusion with self-organization. We cannot provide absolute proof that a rock doesn\'t have intelligence, which amounts to the old problem of disproving a negative. Thankfully, few men actually act on such beliefs at present, because when they do, they end up losing their minds. If we take such arguments seriously, we might see laws passed to protect the kind of Pet Rocks that were popular in the \'70s (I\'m still upset that mine was stolen LOL). It would be a lot easier, however, to make the same mistake of ascribing consciousness, intelligence and other such qualities to a state-of-the-art machine, because of wishful thinking, hubris, the lofty credentials of the inventors, the influence of science fiction and the modern love affair with technology. In the future, I have little doubt that we\'ll have Cargo Cult of A.I. - perhaps legally protected like some kind of endangered species, with civil rights, but having no more consciousness, soul or actual intelligence than a rock. Don\'t quote me on this, but I believe Rod Serling once wrote a story to this effect.&lt;/p&gt;\n\n&lt;p&gt;The best way to avoid this fate is to stick to the common sense interpretations and definitions of these things, which we keep backing away from in large part because they set a very high bar for A.I. that we may never be able to surpass in our lifetimes, if ever. Perhaps A.I. isn\'t even logically possible, at any level of technology; I recall a few proofs that can be interpreted to that effect. Those high but reasonable standards may be increasingly difficult to stick to if Chesterton and colleagues like Hilaire Belloc and Arnold Lunn were correct in their assessment that the use of reason has actually been breaking down in Western civilization, at least as far back as the Enlightenment; Lunn\'s 1931 book The Flight from Reason is a classic in this regard and has yet to be rebutted.  This historical trend is a broad topic in and of itself - but suffice it to say that the denial of reason and obsession with technology are both directly relevant in obvious ways to the field of A.I. If the Flight from Reason is still under way, then we will be increasingly tempted to resort to feckless, facile objections in order to demote the use of reason and indispensable qualities like consciousness in our definitions of A.I., but come up with increasingly weak criteria for proving it; simultaneously, our technology will continue to improve, thereby boosting the "Artificial" side of Artificial Intelligence.&lt;/p&gt;\n\n&lt;p&gt;Don\'t get me wrong: if I didn\'t think we can do some really exciting things with A.I., I wouldn\'t be here. But most of them can be achieved without ever replicating actual human intelligence, by solving whole classes of tangential problems that are difficult for humans to think about, but which do not require consciousness or the use of reason that marks human intelligence. The image recognition capabilities of convolutional neural nets are one example, for instance; if we want human intelligence, we can always manufacture it through the easiest, most economical and time-tested way, by having babies. Perhaps these tangential forms of A.I. should be enough for us for now. We cannot inject the use of reason into our machines if we do not possess enough of it ourselves to decide whether reason is necessary for A.I., or even to discern what it consists of. We can\'t engineer or deprecate consciousness for A.I. till we\'re conscious of its significance. I\'d wager, however, that everyone reading this thread and weighing intelligent responses is doing so in a conscious state. That in and of itself ought to answer our question satisfactorily for now.&lt;/p&gt;\n', u'question': u'&lt;p&gt;Consciousness &lt;a href="http://www.iep.utm.edu/consciou/"&gt;is challenging to define&lt;/a&gt;, but for this question let\'s define it as "actually experiencing sensory input as opposed to just putting a bunch of data through an inanimate machine." Humans, of course, have minds; for normal computers, all the things they "see" are just more data. One could alternatively say that humans are &lt;a href="http://philosophy.stackexchange.com/a/4687"&gt;sentient&lt;/a&gt;, while traditional computers are not.&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;Setting aside the question of whether it\'s possible to build a sentient machine, does it actually make a difference if an AI is sentient or not? In other words, are there are tasks that are made impossible - not just more difficult - by a lack of sentience?&lt;/p&gt;&lt;br&gt;'}</str></arr><str name="id">1919</str><long name="_version_">1559279291488796672</long><float name="score">7.5235114</float><str name="featureVector">3.7617557 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 3.7617557 0.0 0.0 0.0 0.6 9 0.09531017980432487 7.5235114</str></doc></result>
</response>

